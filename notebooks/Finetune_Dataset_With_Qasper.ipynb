{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Finetune Dataset with Qasper\n",
    "\n",
    "In this notebook we will create a finetuning dataset with Qasper References."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "164182c3c22a1036"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Up Dataset\n",
    "\n",
    "We pick up 50 random papers from test dataset and 200 random papers from train dataset.\n",
    "\n",
    "The result will be stored in pandas dataframe"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7dc3d3febf8d470"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-17T01:21:56.118178Z",
     "start_time": "2023-11-17T01:21:49.557857Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from qasper_data.qasper_dataset import QasperDataset, PaperIndex\n",
    "from qasper_data.qasper_evaluator import EvidenceEvaluator\n",
    "\n",
    "\n",
    "train = QasperDataset(\"train\", 64)\n",
    "test = QasperDataset(\"test\", 64)\n",
    "\n",
    "train_papers = train.random_sample(200)\n",
    "test_papers = test.random_sample(80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Train Dataframe"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecf4816f5717bfe0"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame()\n",
    "\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context =ServiceContext.from_defaults(llm=None, embed_model=\"local\")\n",
    "\n",
    "for paper in train_papers:\n",
    "    paper_index = PaperIndex(paper, service_context)\n",
    "    query_engine = paper_index.as_index().as_query_engine(mode=\"no_text\", similarity_top_k=8)\n",
    "    evaluator = EvidenceEvaluator.from_defaults(service_context)\n",
    "    df_paper = evaluator.get_evaluation_dataframe(paper, query_engine)\n",
    "    if df_paper.empty:\n",
    "        continue\n",
    "    if df_train.empty:\n",
    "        df_train = pd.DataFrame(df_paper)\n",
    "    else:\n",
    "        df_train = pd.concat([df_train, df_paper], ignore_index=True)\n",
    "    del paper_index\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T01:22:47.418180Z",
     "start_time": "2023-11-17T01:21:56.122310Z"
    }
   },
   "id": "cd9212ce06ee6697"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             question  \\\n0             What language are the conversations in?   \n1             What language are the conversations in?   \n2   How is morphology knowledge implemented in the...   \n3   How is morphology knowledge implemented in the...   \n4   How is morphology knowledge implemented in the...   \n5   How is morphology knowledge implemented in the...   \n6         How does the word segmentation method work?   \n7         How does the word segmentation method work?   \n8         How does the word segmentation method work?   \n9         How does the word segmentation method work?   \n10  How do they gather data for the query explanat...   \n11  How do they gather data for the query explanat...   \n12  How do they gather data for the query explanat...   \n13  How do they gather data for the query explanat...   \n14  How do they gather data for the query explanat...   \n15  How do they gather data for the query explanat...   \n16  How do they gather data for the query explanat...   \n17  How do they gather data for the query explanat...   \n18  Which query explanation method was preffered b...   \n19  Which query explanation method was preffered b...   \n20  Which query explanation method was preffered b...   \n21  Which query explanation method was preffered b...   \n22  Which query explanation method was preffered b...   \n23  Which query explanation method was preffered b...   \n24  Which query explanation method was preffered b...   \n25  Which query explanation method was preffered b...   \n26  What modalities are being used in different da...   \n27  What modalities are being used in different da...   \n28  What modalities are being used in different da...   \n29  What modalities are being used in different da...   \n\n                                              context  score  \n0   The patients are from 31 provincial-level admi...    1.0  \n1   Telemedicine refers to the practice of deliver...    0.0  \n2   Huck BIBREF18 explored target-side segmentatio...    0.0  \n3   We denote this method as SSS. The segmented wo...    1.0  \n4   We will elaborate the number settings for our ...    0.0  \n5   Neural machine translation (NMT) has achieved ...    0.0  \n6   We denote this method as SSS. The segmented wo...    1.0  \n7   We will elaborate the number settings for our ...    0.0  \n8   Huck BIBREF18 explored target-side segmentatio...    0.0  \n9   Neural machine translation (NMT) has achieved ...    1.0  \n10  The definition of the relevant provenance rule...    0.0  \n11  Natural language interfaces have been gaining ...    0.0  \n12  The cells Fiji and Tonga are part of INLINEFOR...    0.0  \n13  To train a semantic parser that is unconstrain...    0.0  \n14  The explanations are presented to non-technica...    1.0  \n15  Rather than relying on a small set of NL test ...    0.0  \n16  Lambda DCS was originally designed for buildin...    0.0  \n17  We design a model for multilevel cell-based pr...    0.0  \n18  The definition of the relevant provenance rule...    0.0  \n19  Natural language interfaces have been gaining ...    0.0  \n20  Rather than relying on a small set of NL test ...    1.0  \n21  To train a semantic parser that is unconstrain...    0.0  \n22  The explanations are presented to non-technica...    0.0  \n23  The cells Fiji and Tonga are part of INLINEFOR...    0.0  \n24  This task of generalizing to unseen domains is...    0.0  \n25  Lambda DCS was originally designed for buildin...    0.0  \n26  The code, hyperparameters and instruction on d...    1.0  \n27  It is the current state of the art for CMU-MOS...    0.0  \n28  We additionally compare to a stronger recurren...    0.0  \n29  $M$2 is the dimensionality of the memory for m...    0.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>context</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What language are the conversations in?</td>\n      <td>The patients are from 31 provincial-level admi...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What language are the conversations in?</td>\n      <td>Telemedicine refers to the practice of deliver...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>How is morphology knowledge implemented in the...</td>\n      <td>Huck BIBREF18 explored target-side segmentatio...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How is morphology knowledge implemented in the...</td>\n      <td>We denote this method as SSS. The segmented wo...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>How is morphology knowledge implemented in the...</td>\n      <td>We will elaborate the number settings for our ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>How is morphology knowledge implemented in the...</td>\n      <td>Neural machine translation (NMT) has achieved ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>How does the word segmentation method work?</td>\n      <td>We denote this method as SSS. The segmented wo...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>How does the word segmentation method work?</td>\n      <td>We will elaborate the number settings for our ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>How does the word segmentation method work?</td>\n      <td>Huck BIBREF18 explored target-side segmentatio...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>How does the word segmentation method work?</td>\n      <td>Neural machine translation (NMT) has achieved ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>How do they gather data for the query explanat...</td>\n      <td>The definition of the relevant provenance rule...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>How do they gather data for the query explanat...</td>\n      <td>Natural language interfaces have been gaining ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>How do they gather data for the query explanat...</td>\n      <td>The cells Fiji and Tonga are part of INLINEFOR...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>How do they gather data for the query explanat...</td>\n      <td>To train a semantic parser that is unconstrain...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>How do they gather data for the query explanat...</td>\n      <td>The explanations are presented to non-technica...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>How do they gather data for the query explanat...</td>\n      <td>Rather than relying on a small set of NL test ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>How do they gather data for the query explanat...</td>\n      <td>Lambda DCS was originally designed for buildin...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>How do they gather data for the query explanat...</td>\n      <td>We design a model for multilevel cell-based pr...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Which query explanation method was preffered b...</td>\n      <td>The definition of the relevant provenance rule...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Which query explanation method was preffered b...</td>\n      <td>Natural language interfaces have been gaining ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Which query explanation method was preffered b...</td>\n      <td>Rather than relying on a small set of NL test ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Which query explanation method was preffered b...</td>\n      <td>To train a semantic parser that is unconstrain...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Which query explanation method was preffered b...</td>\n      <td>The explanations are presented to non-technica...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Which query explanation method was preffered b...</td>\n      <td>The cells Fiji and Tonga are part of INLINEFOR...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Which query explanation method was preffered b...</td>\n      <td>This task of generalizing to unseen domains is...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Which query explanation method was preffered b...</td>\n      <td>Lambda DCS was originally designed for buildin...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>What modalities are being used in different da...</td>\n      <td>The code, hyperparameters and instruction on d...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>What modalities are being used in different da...</td>\n      <td>It is the current state of the art for CMU-MOS...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>What modalities are being used in different da...</td>\n      <td>We additionally compare to a stronger recurren...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>What modalities are being used in different da...</td>\n      <td>$M$2 is the dimensionality of the memory for m...</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T01:22:47.460305Z",
     "start_time": "2023-11-17T01:22:47.417395Z"
    }
   },
   "id": "ec54c47cf2c58e86"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data_folder = \"data\"\n",
    "\n",
    "df_train.to_csv(f\"{data_folder}/fine_tuning_2.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T01:22:47.639650Z",
     "start_time": "2023-11-17T01:22:47.427983Z"
    }
   },
   "id": "fd4a43bc1fda3414"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "validation = QasperDataset(\"validation\", 64)\n",
    "\n",
    "df_validation = pd.DataFrame()\n",
    "\n",
    "for paper in validation.random_sample(20):\n",
    "    paper_index = PaperIndex(paper, service_context)\n",
    "    query_engine = paper_index.as_index().as_query_engine(mode=\"no_text\", similarity_top_k=8)\n",
    "    evaluator = EvidenceEvaluator.from_defaults(service_context)\n",
    "    df_paper = evaluator.get_evaluation_dataframe(paper, query_engine)\n",
    "    if df_paper.empty:\n",
    "        continue\n",
    "    if df_validation.empty:\n",
    "        df_validation = pd.DataFrame(df_paper)\n",
    "    else:\n",
    "        df_validation = pd.concat([df_validation, df_paper], ignore_index=True)\n",
    "    del paper_index\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T01:22:52.579812Z",
     "start_time": "2023-11-17T01:22:47.631609Z"
    }
   },
   "id": "b113ce3ed8d48b34"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             question  \\\n0                by how much did nus outperform abus?   \n1                by how much did nus outperform abus?   \n2                by how much did nus outperform abus?   \n3                by how much did nus outperform abus?   \n4                by how much did nus outperform abus?   \n5                by how much did nus outperform abus?   \n6                by how much did nus outperform abus?   \n7              what corpus is used to learn behavior?   \n8              what corpus is used to learn behavior?   \n9              what corpus is used to learn behavior?   \n10             what corpus is used to learn behavior?   \n11             what corpus is used to learn behavior?   \n12             what corpus is used to learn behavior?   \n13             what corpus is used to learn behavior?   \n14             what corpus is used to learn behavior?   \n15             what corpus is used to learn behavior?   \n16             what corpus is used to learn behavior?   \n17             what corpus is used to learn behavior?   \n18             what corpus is used to learn behavior?   \n19             what corpus is used to learn behavior?   \n20             what corpus is used to learn behavior?   \n21            What novel PMI variants are introduced?   \n22            What novel PMI variants are introduced?   \n23            What novel PMI variants are introduced?   \n24            What novel PMI variants are introduced?   \n25            What novel PMI variants are introduced?   \n26            What novel PMI variants are introduced?   \n27            What novel PMI variants are introduced?   \n28            What novel PMI variants are introduced?   \n29  What semantic and syntactic tasks are used as ...   \n\n                                              context  score  \n0   This could indicate that the policy “overfits”...    0.0  \n1   The maximum dialogue length was 25 turns. The ...    1.0  \n2   Spoken Dialogue Systems (SDS) allow human-comp...    0.0  \n3   In that case the value will be kept. The behav...    0.0  \n4   Thus, an SDS that is trained with a natural la...    0.0  \n5   If the current dialogue turn is turn INLINEFOR...    0.0  \n6   However, even if the size of the corpus is lar...    0.0  \n7   However, even if the size of the corpus is lar...    0.0  \n8   However, even if the size of the corpus is lar...    0.0  \n9   This could indicate that the policy “overfits”...    0.0  \n10  This could indicate that the policy “overfits”...    0.0  \n11  The maximum dialogue length was 25 turns. The ...    0.0  \n12  The maximum dialogue length was 25 turns. The ...    0.0  \n13  Spoken Dialogue Systems (SDS) allow human-comp...    1.0  \n14  Spoken Dialogue Systems (SDS) allow human-comp...    0.0  \n15  Thus, an SDS that is trained with a natural la...    0.0  \n16  Thus, an SDS that is trained with a natural la...    0.0  \n17  In that case the value will be kept. The behav...    0.0  \n18  In that case the value will be kept. The behav...    0.0  \n19  If the current dialogue turn is turn INLINEFOR...    0.0  \n20  If the current dialogue turn is turn INLINEFOR...    1.0  \n21  However, it is not clear what is lost by clipp...    1.0  \n22  However, it is not clear what is lost by clipp...    1.0  \n23  Dense word vectors (or embeddings) are a key c...    0.0  \n24  Dense word vectors (or embeddings) are a key c...    0.0  \n25  We thus conclude that accounting for scale in ...    0.0  \n26  We thus conclude that accounting for scale in ...    0.0  \n27  For example, $-\\mathit {CPMI}_{\\texttt {-}2}$ ...    0.0  \n28  For example, $-\\mathit {CPMI}_{\\texttt {-}2}$ ...    0.0  \n29  For example, $-\\mathit {CPMI}_{\\texttt {-}2}$ ...    1.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>context</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>by how much did nus outperform abus?</td>\n      <td>This could indicate that the policy “overfits”...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>by how much did nus outperform abus?</td>\n      <td>The maximum dialogue length was 25 turns. The ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>by how much did nus outperform abus?</td>\n      <td>Spoken Dialogue Systems (SDS) allow human-comp...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>by how much did nus outperform abus?</td>\n      <td>In that case the value will be kept. The behav...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>by how much did nus outperform abus?</td>\n      <td>Thus, an SDS that is trained with a natural la...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>by how much did nus outperform abus?</td>\n      <td>If the current dialogue turn is turn INLINEFOR...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>by how much did nus outperform abus?</td>\n      <td>However, even if the size of the corpus is lar...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>what corpus is used to learn behavior?</td>\n      <td>However, even if the size of the corpus is lar...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>what corpus is used to learn behavior?</td>\n      <td>However, even if the size of the corpus is lar...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>what corpus is used to learn behavior?</td>\n      <td>This could indicate that the policy “overfits”...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>what corpus is used to learn behavior?</td>\n      <td>This could indicate that the policy “overfits”...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>what corpus is used to learn behavior?</td>\n      <td>The maximum dialogue length was 25 turns. The ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>what corpus is used to learn behavior?</td>\n      <td>The maximum dialogue length was 25 turns. The ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>what corpus is used to learn behavior?</td>\n      <td>Spoken Dialogue Systems (SDS) allow human-comp...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>what corpus is used to learn behavior?</td>\n      <td>Spoken Dialogue Systems (SDS) allow human-comp...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>what corpus is used to learn behavior?</td>\n      <td>Thus, an SDS that is trained with a natural la...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>what corpus is used to learn behavior?</td>\n      <td>Thus, an SDS that is trained with a natural la...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>what corpus is used to learn behavior?</td>\n      <td>In that case the value will be kept. The behav...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>what corpus is used to learn behavior?</td>\n      <td>In that case the value will be kept. The behav...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>what corpus is used to learn behavior?</td>\n      <td>If the current dialogue turn is turn INLINEFOR...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>what corpus is used to learn behavior?</td>\n      <td>If the current dialogue turn is turn INLINEFOR...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>What novel PMI variants are introduced?</td>\n      <td>However, it is not clear what is lost by clipp...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>What novel PMI variants are introduced?</td>\n      <td>However, it is not clear what is lost by clipp...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>What novel PMI variants are introduced?</td>\n      <td>Dense word vectors (or embeddings) are a key c...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>What novel PMI variants are introduced?</td>\n      <td>Dense word vectors (or embeddings) are a key c...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>What novel PMI variants are introduced?</td>\n      <td>We thus conclude that accounting for scale in ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>What novel PMI variants are introduced?</td>\n      <td>We thus conclude that accounting for scale in ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>What novel PMI variants are introduced?</td>\n      <td>For example, $-\\mathit {CPMI}_{\\texttt {-}2}$ ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>What novel PMI variants are introduced?</td>\n      <td>For example, $-\\mathit {CPMI}_{\\texttt {-}2}$ ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>What semantic and syntactic tasks are used as ...</td>\n      <td>For example, $-\\mathit {CPMI}_{\\texttt {-}2}$ ...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.head(30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T01:22:52.589004Z",
     "start_time": "2023-11-17T01:22:52.584306Z"
    }
   },
   "id": "d73d45f446f4a786"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df_validation.to_csv(f\"{data_folder}/fine_tuning_2_validation.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T01:22:52.654102Z",
     "start_time": "2023-11-17T01:22:52.588249Z"
    }
   },
   "id": "69b8caa21a0efb8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Finetuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aebe3721b8a7a3e6"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b7e2d3b4a5c4fe2966bc63211275a24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/150 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6088ae1bf7794c76928055281bdf305f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/150 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d75add4b42104085a58af03ac588835e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import List\n",
    "from llama_index.finetuning.cross_encoders.cross_encoder import (\n",
    "    CrossEncoderFinetuneEngine,\n",
    "    CrossEncoderFinetuningDatasetSample\n",
    ")\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_folder = \"data\"\n",
    "\n",
    "version = 2\n",
    "\n",
    "df_finetuning = pd.read_csv(os.path.join(data_folder, f\"fine_tuning_{version}.csv\"), index_col=0)\n",
    "\n",
    "finetuning_dataset: List[CrossEncoderFinetuningDatasetSample] = []\n",
    "\n",
    "for _, row in df_finetuning.iterrows():\n",
    "    finetuning_dataset.append(\n",
    "        CrossEncoderFinetuningDatasetSample(\n",
    "            query=row[\"question\"],\n",
    "            context=row[\"context\"],\n",
    "            score=row[\"score\"]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "validation_dataset: List[CrossEncoderFinetuningDatasetSample] = []\n",
    "\n",
    "df_validation = pd.read_csv(os.path.join(data_folder, f\"fine_tuning_{version}_validation.csv\"), index_col=0)\n",
    "\n",
    "for _, row in df_validation.iterrows():\n",
    "    validation_dataset.append(\n",
    "        CrossEncoderFinetuningDatasetSample(\n",
    "            query=row[\"question\"],\n",
    "            context=row[\"context\"],\n",
    "            score=row[\"score\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "finetuning_engine = CrossEncoderFinetuneEngine(\n",
    "    dataset=finetuning_dataset, epochs=2, batch_size=16, model_output_path=\"../models/fine_tuned_cross_encoder\", val_dataset=validation_dataset\n",
    ")\n",
    "\n",
    "# Finetune the cross-encoder model\n",
    "finetuning_engine.finetune()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T05:19:48.988701Z",
     "start_time": "2023-11-17T01:22:52.624339Z"
    }
   },
   "id": "4ac572eb272fca47"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate Test Dataframe\n",
    "\n",
    "Test Hit rate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7726688266bdb8"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b1/kcj0_5zx05b_z6jvrr00b12w0000gn/T/ipykernel_18032/3432872676.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_test_base = pd.concat([df_test_base, df_paper_base], ignore_index=True)\n",
      "/var/folders/b1/kcj0_5zx05b_z6jvrr00b12w0000gn/T/ipykernel_18032/3432872676.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_test_rerank = pd.concat([df_test_rerank, df_paper_rerank], ignore_index=True)\n",
      "/var/folders/b1/kcj0_5zx05b_z6jvrr00b12w0000gn/T/ipykernel_18032/3432872676.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_test_rerank_fine_tuned = pd.concat([df_test_rerank_fine_tuned, df_paper_rerank_fine_tuned], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "service_context =ServiceContext.from_defaults(llm=None, embed_model=\"local\")\n",
    "\n",
    "rerank_base = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\", top_n=3\n",
    ")\n",
    "\n",
    "rerank_fine_tuned = SentenceTransformerRerank(\n",
    "    model=\"../models/fine_tuned_cross_encoder\", top_n=3\n",
    ")\n",
    "\n",
    "df_test_base = pd.DataFrame()\n",
    "df_test_rerank = pd.DataFrame()\n",
    "df_test_rerank_fine_tuned = pd.DataFrame()\n",
    "\n",
    "for paper in test_papers:\n",
    "    paper_index = PaperIndex(paper, service_context)\n",
    "    query_base = paper_index.as_index().as_query_engine(mode=\"no_text\", similarity_top_k=3)\n",
    "    query_rerank = paper_index.as_index().as_query_engine(mode=\"no_text\", similarity_top_k=8,         node_postprocessors=[rerank_base])\n",
    "    query_rerank_fine_tuned = paper_index.as_index().as_query_engine(mode=\"no_text\", similarity_top_k=8, node_postprocessors=[rerank_fine_tuned])\n",
    "    evaluator = EvidenceEvaluator.from_defaults(service_context)\n",
    "    df_paper_base = evaluator.get_evaluation_dataframe(paper, query_base)\n",
    "    df_paper_rerank = evaluator.get_evaluation_dataframe(paper, query_rerank)\n",
    "    df_paper_rerank_fine_tuned = evaluator.get_evaluation_dataframe(paper, query_rerank_fine_tuned)\n",
    "    if df_test_base.empty:\n",
    "        df_test_base = pd.DataFrame(df_paper_base)\n",
    "    else:\n",
    "        df_test_base = pd.concat([df_test_base, df_paper_base], ignore_index=True)\n",
    "    if df_test_rerank.empty:\n",
    "        df_test_rerank = pd.DataFrame(df_paper_rerank)\n",
    "    else:\n",
    "        df_test_rerank = pd.concat([df_test_rerank, df_paper_rerank], ignore_index=True)\n",
    "    if df_test_rerank_fine_tuned.empty:\n",
    "        df_test_rerank_fine_tuned = pd.DataFrame(df_paper_rerank_fine_tuned)\n",
    "    else:\n",
    "        df_test_rerank_fine_tuned = pd.concat([df_test_rerank_fine_tuned, df_paper_rerank_fine_tuned], ignore_index=True)\n",
    "    del paper_index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T05:35:55.004178Z",
     "start_time": "2023-11-17T05:19:49.034941Z"
    }
   },
   "id": "4f3f3a76e2632327"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Hit: 430.0\n"
     ]
    }
   ],
   "source": [
    "base_hit = df_test_base[\"score\"].sum()\n",
    "print(f\"Base Hit: {base_hit}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T05:35:55.010002Z",
     "start_time": "2023-11-17T05:35:54.992504Z"
    }
   },
   "id": "50f6b7d0c8a6cb9e"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerank Hit: 567.0\n"
     ]
    }
   ],
   "source": [
    "rerank_hit = df_test_rerank[\"score\"].sum()\n",
    "print(f\"Rerank Hit: {rerank_hit}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T05:35:55.010635Z",
     "start_time": "2023-11-17T05:35:55.001222Z"
    }
   },
   "id": "5d58c3ffcff4affc"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerank Fine Tuned Hit: 567.0\n"
     ]
    }
   ],
   "source": [
    "rerank_fine_tuned_hit = df_test_rerank_fine_tuned[\"score\"].sum()\n",
    "print(f\"Rerank Fine Tuned Hit: {rerank_fine_tuned_hit}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T05:35:55.013789Z",
     "start_time": "2023-11-17T05:35:55.007610Z"
    }
   },
   "id": "4442db3042a62d76"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "data_folder = \"data\"\n",
    "\n",
    "df_test_base.to_csv(f\"{data_folder}/test_base.csv\")\n",
    "df_test_rerank.to_csv(f\"{data_folder}/test_rerank.csv\")\n",
    "df_test_rerank_fine_tuned.to_csv(f\"{data_folder}/test_rerank_fine_tuned.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T05:35:55.586179Z",
     "start_time": "2023-11-17T05:35:55.007907Z"
    }
   },
   "id": "564eb951538fa1aa"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b1/kcj0_5zx05b_z6jvrr00b12w0000gn/T/ipykernel_18032/1739261400.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_f1s = pd.concat([df_f1s, pd.DataFrame({\"baseline\": f1s_base, \"rerank\": f1s_rerank, \"rerank_fine_tuned\": f1s_rerank_fine_tuned})], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df_f1s = pd.DataFrame(columns=[\"baseline\", \"rerank\", \"rerank_fine_tuned\"])\n",
    "\n",
    "for paper in test_papers:\n",
    "    paper_index = PaperIndex(paper, service_context)\n",
    "    query_base = paper_index.as_index().as_query_engine(mode=\"no_text\", similarity_top_k=3)\n",
    "    query_rerank = paper_index.as_index().as_query_engine(mode=\"no_text\", similarity_top_k=8,         node_postprocessors=[rerank_base])\n",
    "    query_rerank_fine_tuned = paper_index.as_index().as_query_engine(mode=\"no_text\", similarity_top_k=8, node_postprocessors=[rerank_fine_tuned]) \n",
    "    evaluator = EvidenceEvaluator.from_defaults(service_context)\n",
    "    \n",
    "    f1s_base = pd.Series(evaluator.evaluate(paper, query_base))\n",
    "    f1s_rerank = pd.Series(evaluator.evaluate(paper, query_rerank))\n",
    "    f1s_rerank_fine_tuned = pd.Series(evaluator.evaluate(paper, query_rerank_fine_tuned))\n",
    "    \n",
    "    df_f1s = pd.concat([df_f1s, pd.DataFrame({\"baseline\": f1s_base, \"rerank\": f1s_rerank, \"rerank_fine_tuned\": f1s_rerank_fine_tuned})], ignore_index=True)\n",
    "\n",
    "    del paper_index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T05:36:28.016569Z",
     "start_time": "2023-11-17T05:35:55.516645Z"
    }
   },
   "id": "a7de4b8ab3f0cbb0"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "   baseline    rerank  rerank_fine_tuned\n0  0.014868  0.065095           0.065095\n1  0.013738  0.044581           0.044581\n2  0.020323  0.065574           0.065574\n3  0.017029  0.073781           0.073781\n4  0.014601  0.044581           0.044581",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>baseline</th>\n      <th>rerank</th>\n      <th>rerank_fine_tuned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.014868</td>\n      <td>0.065095</td>\n      <td>0.065095</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.013738</td>\n      <td>0.044581</td>\n      <td>0.044581</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.020323</td>\n      <td>0.065574</td>\n      <td>0.065574</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.017029</td>\n      <td>0.073781</td>\n      <td>0.073781</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.014601</td>\n      <td>0.044581</td>\n      <td>0.044581</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f1s.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T05:36:28.092213Z",
     "start_time": "2023-11-17T05:36:28.017021Z"
    }
   },
   "id": "43618cacd9edea6a"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T05:36:28.092933Z",
     "start_time": "2023-11-17T05:36:28.043263Z"
    }
   },
   "id": "da18efe5003ac449"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
