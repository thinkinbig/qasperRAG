{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b996823beb834d16",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Fine-tuning Cross-Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-16T17:58:54.808363Z",
     "start_time": "2023-11-16T17:20:29.798872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5064a5a54ba54901a14d0e3ea151e3ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/300 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9dfcd979ce7d4a1d95c3502b3417afa1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/300 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e32dbd0b90474436a93a317c887d3f8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import List\n",
    "from llama_index.finetuning.cross_encoders.cross_encoder import (\n",
    "    CrossEncoderFinetuneEngine,\n",
    "    CrossEncoderFinetuningDatasetSample\n",
    ")\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_folder = \"data\"\n",
    "\n",
    "version = 1\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(data_folder, f\"train_{version}.csv\"), index_col=0)\n",
    "df_test = pd.read_csv(os.path.join(data_folder, f\"test_{version}.csv\"), index_col=0)\n",
    "df_finetuning = pd.read_csv(os.path.join(data_folder, f\"fine_tuning_{version}.csv\"), index_col=0)\n",
    "\n",
    "finetuning_dataset: List[CrossEncoderFinetuningDatasetSample] = []\n",
    "\n",
    "for _, row in df_finetuning.iterrows():\n",
    "    finetuning_dataset.append(\n",
    "        CrossEncoderFinetuningDatasetSample(\n",
    "            query=row[\"question\"],\n",
    "            context=row[\"context\"],\n",
    "            score=row[\"score\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "finetuning_engine = CrossEncoderFinetuneEngine(\n",
    "    dataset=finetuning_dataset, epochs=2, batch_size=8, model_output_path=\"../models/fine_tuned_cross_encoder\"\n",
    ")\n",
    "\n",
    "# Finetune the cross-encoder model\n",
    "finetuning_engine.finetune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdda2c5945a284a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluate Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c4f7963702078",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T21:46:51.256586Z",
     "start_time": "2023-11-06T21:46:51.247130Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95d53a788f139d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T21:47:14.507689Z",
     "start_time": "2023-11-06T21:47:14.503464Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Reranking Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "\n",
    "data_folder = \"data\"\n",
    "version = 1\n",
    "\n",
    "df_reranking = pd.read_csv(os.path.join(data_folder, f\"reranking_test_{version}.csv\"), index_col=0)\n",
    "df_reranking[\"questions\"] = df_reranking[\"questions\"].apply(ast.literal_eval)\n",
    "df_reranking[\"context\"] = df_reranking[\"context\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388985584c07214d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T21:47:15.585272Z",
     "start_time": "2023-11-06T21:47:15.574169Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_reranking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87fa4221e906bc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T21:14:17.599476Z",
     "start_time": "2023-11-06T21:14:17.586536Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-Ei89D8xbv2ve98FT6e629a55Ca1341518f1238BaB53dAf20\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://ai-yyds.com/v1\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69167369c3c19bfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T21:47:29.940927Z",
     "start_time": "2023-11-06T21:47:20.600391Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We evaluate by calculating hits for each (question, context) pair,\n",
    "# we retrieve top-k documents with the question, and\n",
    "# itâ€™s a hit if the results contain the context\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index import Document\n",
    "\n",
    "service_context_reranker_eval = ServiceContext.from_defaults(chunk_size=256, llm=\"local\", embed_model=\"local\")\n",
    "rerank_base = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\", top_n=3\n",
    ")\n",
    "\n",
    "rerank_finetuned = SentenceTransformerRerank(\n",
    "    model=\"../models/cross_encoder\", top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68190961803c0904",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T21:54:38.878836Z",
     "start_time": "2023-11-06T21:53:15.311318Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "without_reranker_hits = 0\n",
    "base_reranker_hits = 0\n",
    "finetuned_reranker_hits = 0\n",
    "total_number_of_context = 0\n",
    "for index, row in df_reranking.iterrows():\n",
    "    documents = [Document(text=row[\"paper\"])]\n",
    "    query_list = row[\"questions\"]\n",
    "    context_list = row[\"context\"]\n",
    "\n",
    "    assert len(query_list) == len(context_list)\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        documents, service_context=service_context_reranker_eval\n",
    "    )\n",
    "\n",
    "    retriever_without_reranker = vector_index.as_query_engine(\n",
    "        similarity_top_k=3, response_mode=\"no_text\"\n",
    "    )\n",
    "    retriever_with_base_reranker = vector_index.as_query_engine(\n",
    "        similarity_top_k=8,\n",
    "        response_mode=\"no_text\",\n",
    "        node_postprocessors=[rerank_base],\n",
    "    )\n",
    "    retriever_with_finetuned_reranker = vector_index.as_query_engine(\n",
    "        similarity_top_k=8,\n",
    "        response_mode=\"no_text\",\n",
    "        node_postprocessors=[rerank_finetuned],\n",
    "    )\n",
    "\n",
    "    for index in range(0, len(query_list)):\n",
    "        query = query_list[index]\n",
    "        context = context_list[index]\n",
    "        total_number_of_context += 1\n",
    "\n",
    "        response_without_reranker = retriever_without_reranker.query(query)\n",
    "        without_reranker_nodes = response_without_reranker.source_nodes\n",
    "\n",
    "        for node in without_reranker_nodes:\n",
    "            if context in node.node.text or node.node.text in context:\n",
    "                without_reranker_hits += 1\n",
    "\n",
    "        response_with_base_reranker = retriever_with_base_reranker.query(query)\n",
    "        with_base_reranker_nodes = response_with_base_reranker.source_nodes\n",
    "\n",
    "        for node in with_base_reranker_nodes:\n",
    "            if context in node.node.text or node.node.text in context:\n",
    "                base_reranker_hits += 1\n",
    "\n",
    "        response_with_finetuned_reranker = (\n",
    "            retriever_with_finetuned_reranker.query(query)\n",
    "        )\n",
    "        with_finetuned_reranker_nodes = (\n",
    "            response_with_finetuned_reranker.source_nodes\n",
    "        )\n",
    "\n",
    "        for node in with_finetuned_reranker_nodes:\n",
    "            if context in node.node.text or node.node.text in context:\n",
    "                finetuned_reranker_hits += 1\n",
    "        \n",
    "        assert (\n",
    "            len(with_finetuned_reranker_nodes)\n",
    "            == len(with_base_reranker_nodes)\n",
    "            == len(without_reranker_nodes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f59e34c0a78831",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Result\n",
    "As we can see below we get more hits with finetuned_cross_encoder compared to other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426b22f3cb805c58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T21:55:25.043578Z",
     "start_time": "2023-11-06T21:55:25.039380Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "without_reranker_scores = [without_reranker_hits]\n",
    "base_reranker_scores = [base_reranker_hits]\n",
    "finetuned_reranker_scores = [finetuned_reranker_hits]\n",
    "reranker_eval_dict = {\n",
    "    \"Metric\": \"Hits\",\n",
    "    \"BGE_Embeddings\": without_reranker_scores,\n",
    "    \"Base_cross_encoder\": base_reranker_scores,\n",
    "    \"Finetuned_cross_encoder\": finetuned_reranker_hits,\n",
    "    \"Total Relevant Context\": total_number_of_context,\n",
    "}\n",
    "df_reranker_eval_results = pd.DataFrame(reranker_eval_dict)\n",
    "display(df_reranker_eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feec2b82e902ecf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af3514a294d98a4b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T09:42:31.750008Z",
     "start_time": "2023-11-14T09:42:31.334358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in the test sample:- 80\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "\n",
    "data_folder = \"data\"\n",
    "\n",
    "version = 1\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(data_folder, f\"test_{version}.csv\"), index_col=0)\n",
    "df_test[\"questions\"] = df_test[\"questions\"].apply(ast.literal_eval)\n",
    "df_test[\"answers\"] = df_test[\"answers\"].apply(ast.literal_eval)\n",
    "print(f\"Number of papers in the test sample:- {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8111fdef2fa11e49",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T09:42:31.750761Z",
     "start_time": "2023-11-14T09:42:31.742556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                               paper  \\\n0  Identifying Condition-Action Statements in Med...   \n1  Multilingual is not enough: BERT for Finnish\\t...   \n2  Efficient Dynamic WFST Decoding for Personaliz...   \n3  A system for the 2019 Sentiment, Emotion and C...   \n4  Towards Automatic Bot Detection in Twitter for...   \n\n                                           questions  \\\n0  [What supervised machine learning models do th...   \n1  [By how much did the new model outperform mult...   \n2  [What does the cache consist of?, What languag...   \n3  [Did they pre-train on existing sentiment corp...   \n4  [Do the authors report results on only English...   \n\n                                             answers  \n0  [Unacceptable, Unacceptable, 1470 sentences, U...  \n1  [For POS,  improvements for cased BERT are 1.2...  \n2  [Unacceptable, Unacceptable, A model that cont...  \n3      [Unacceptable, Unacceptable, 2, Unacceptable]  \n4  [Unacceptable, Unacceptable, An existing bot d...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paper</th>\n      <th>questions</th>\n      <th>answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Identifying Condition-Action Statements in Med...</td>\n      <td>[What supervised machine learning models do th...</td>\n      <td>[Unacceptable, Unacceptable, 1470 sentences, U...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Multilingual is not enough: BERT for Finnish\\t...</td>\n      <td>[By how much did the new model outperform mult...</td>\n      <td>[For POS,  improvements for cased BERT are 1.2...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Efficient Dynamic WFST Decoding for Personaliz...</td>\n      <td>[What does the cache consist of?, What languag...</td>\n      <td>[Unacceptable, Unacceptable, A model that cont...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A system for the 2019 Sentiment, Emotion and C...</td>\n      <td>[Did they pre-train on existing sentiment corp...</td>\n      <td>[Unacceptable, Unacceptable, 2, Unacceptable]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Towards Automatic Bot Detection in Twitter for...</td>\n      <td>[Do the authors report results on only English...</td>\n      <td>[Unacceptable, Unacceptable, An existing bot d...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905aab142c89fbd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Baseline Evaluation\n",
    "Use BAAI/bge-small-en as the base model for RAG without reranker\n",
    "\n",
    "#### Eval Method:-\n",
    "1. Iterate over each row of the test dataset:-\n",
    "    1. For the current row being iterated, create a vector index using the paper document provided in the paper column of the dataset\n",
    "    2. Query the vector index with a top_k value of top 3 nodes without any reranker\n",
    "    3. Compare the generated answers with the reference answers of the respective sample using Pairwise Comparison Evaluator and add the scores to a list\n",
    "5. Repeat 1 untill all the rows have been iterated\n",
    "6. Calculate avg scores over all samples/ rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a70858b32d292e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T09:57:00.731438Z",
     "start_time": "2023-11-14T09:42:31.753621Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset qasper (/Users/zeyuli/.cache/huggingface/datasets/allenai___qasper/qasper/0.3.0/2bfcd239e581ab83f9ab7b76a82e42c6bcf574a13246ae6cc5a6c357c35f96f9)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "402e196629e64a6681cd6e8a42e61082"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /Users/zeyuli/Library/Caches/llama_index/models/llama-2-13b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:             blk.15.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:             blk.15.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:            blk.2.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.2.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.2.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:              blk.2.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:         blk.2.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.2.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:              blk.2.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:            blk.3.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.3.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:              blk.3.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:              blk.3.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:         blk.3.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.3.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:              blk.3.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:            blk.4.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.4.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:              blk.4.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:              blk.4.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:         blk.4.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:              blk.4.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:              blk.4.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:            blk.5.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:            blk.5.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:              blk.5.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:              blk.5.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:         blk.5.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:              blk.5.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:              blk.5.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:            blk.6.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:            blk.6.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:              blk.6.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:              blk.6.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:         blk.6.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:              blk.6.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:              blk.6.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:            blk.7.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.7.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.7.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.7.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:         blk.7.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:              blk.7.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:              blk.7.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:            blk.8.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:            blk.8.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:              blk.8.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:              blk.8.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:         blk.8.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:              blk.8.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:              blk.8.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:            blk.9.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:            blk.9.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:              blk.9.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:              blk.9.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:         blk.9.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:              blk.9.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:              blk.9.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:           blk.15.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:        blk.15.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.16.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.16.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.16.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.16.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.17.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:           blk.17.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:             blk.17.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:        blk.17.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:             blk.17.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.18.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:           blk.18.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:        blk.18.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:             blk.18.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.19.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:           blk.19.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:        blk.19.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:             blk.19.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.20.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:           blk.20.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:        blk.20.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:             blk.20.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.21.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:           blk.21.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:        blk.21.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:             blk.21.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.22.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:           blk.22.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:        blk.22.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:             blk.22.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.23.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:           blk.23.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:        blk.23.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:             blk.23.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.24.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:        blk.24.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:             blk.24.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.25.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:        blk.25.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:             blk.25.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.26.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:             blk.26.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:        blk.26.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:             blk.26.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.27.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:        blk.27.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:             blk.27.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.28.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:        blk.28.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:             blk.28.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.29.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:        blk.29.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:             blk.29.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:           blk.30.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:           blk.32.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:           blk.32.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:             blk.32.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:        blk.32.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:             blk.32.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:           blk.33.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:           blk.33.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:             blk.33.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:        blk.33.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:             blk.33.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:           blk.34.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:           blk.34.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:             blk.34.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:        blk.34.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:             blk.34.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:           blk.35.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:           blk.35.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:             blk.35.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:        blk.35.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:             blk.35.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:           blk.36.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:           blk.36.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:             blk.36.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:        blk.36.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:             blk.36.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:           blk.37.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:           blk.37.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:             blk.37.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:        blk.37.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:             blk.37.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:           blk.38.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:           blk.38.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:             blk.38.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:        blk.38.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:             blk.38.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:           blk.39.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:           blk.39.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:             blk.39.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:        blk.39.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:             blk.39.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  18:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.13 MB\n",
      "llm_load_tensors: mem required  = 7024.03 MB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  = 3046.88 MB\n",
      "llama_build_graph: non-view tensors processed: 924/924\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7 (1007)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 10922.67 MB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 348.93 MB\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.61 MB, ( 7025.11 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  3046.89 MB, (10072.00 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   342.31 MB, (10414.31 / 10922.67)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ZeroR, NaÃ¯ve Bayes, J48, and random forest classifiers\\t', 'ZeroR, NaÃ¯ve Bayes, J48, and random forest \\t'], [], [], [\"Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation\\t\", 'If patients have asthma, then beta-blockers, including eye drops, are contraindicated\\t']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      11.37 ms /    74 runs   (    0.15 ms per token,  6508.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   23019.76 ms /   916 tokens (   25.13 ms per token,    39.79 tokens per second)\n",
      "llama_print_timings:        eval time =    5242.38 ms /    73 runs   (   71.81 ms per token,    13.92 tokens per second)\n",
      "llama_print_timings:       total time =   28463.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      30.24 ms /   129 runs   (    0.23 ms per token,  4265.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5058.27 ms /    19 tokens (  266.22 ms per token,     3.76 tokens per second)\n",
      "llama_print_timings:        eval time =    8790.31 ms /   128 runs   (   68.67 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =   14383.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      14.60 ms /    59 runs   (    0.25 ms per token,  4041.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9417.98 ms /   798 tokens (   11.80 ms per token,    84.73 tokens per second)\n",
      "llama_print_timings:        eval time =    4003.80 ms /    58 runs   (   69.03 ms per token,    14.49 tokens per second)\n",
      "llama_print_timings:       total time =   13713.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      11.65 ms /    84 runs   (    0.14 ms per token,  7212.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9548.77 ms /   825 tokens (   11.57 ms per token,    86.40 tokens per second)\n",
      "llama_print_timings:        eval time =    5828.31 ms /    83 runs   (   70.22 ms per token,    14.24 tokens per second)\n",
      "llama_print_timings:       total time =   15598.71 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['absolute improvements for FinBERT cased ranging between 0.4 and 1.7% points\\tLAS results are 2.3â€“3.6% points above the previous state of the art\\tabsolute advantage for FinBERT models ranging from 3% points for 1K examples to just over 1% point for 100K examples\\t'], ['ELMo \\tULMFit \\tBERT\\t', 'che2018towards\\tlim2018sex\\tFiNER-tagger BIBREF32\\tgungor2018\\tHIT-SCIR BIBREF22\\tBIBREF33\\t'], ['Yle corpus\\tSTT corpus\\tSuomi24 corpus (version 2017H2)\\tluotolahti2015towards\\tCommon Crawl\\tFinnish Wikipedia\\t', 'news, online discussion, and an internet crawl\\t']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      11.93 ms /   106 runs   (    0.11 ms per token,  8885.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12292.37 ms /   653 tokens (   18.82 ms per token,    53.12 tokens per second)\n",
      "llama_print_timings:        eval time =    6967.75 ms /   105 runs   (   66.36 ms per token,    15.07 tokens per second)\n",
      "llama_print_timings:       total time =   19452.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      28.13 ms /   140 runs   (    0.20 ms per token,  4976.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9363.19 ms /   650 tokens (   14.40 ms per token,    69.42 tokens per second)\n",
      "llama_print_timings:        eval time =    9633.65 ms /   139 runs   (   69.31 ms per token,    14.43 tokens per second)\n",
      "llama_print_timings:       total time =   19482.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =       5.21 ms /    41 runs   (    0.13 ms per token,  7866.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7590.07 ms /   533 tokens (   14.24 ms per token,    70.22 tokens per second)\n",
      "llama_print_timings:        eval time =    2721.62 ms /    40 runs   (   68.04 ms per token,    14.70 tokens per second)\n",
      "llama_print_timings:       total time =   10392.71 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['static public cache stores the most frequent states\\tlifetime of a private cache actually can last for the entire dialog section for a specific user\\tsubsequent utterances faster as more states are composed and stored\\t'], [], [' contains the expected user-specific entities\\t']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      19.40 ms /   125 runs   (    0.16 ms per token,  6443.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13058.30 ms /   734 tokens (   17.79 ms per token,    56.21 tokens per second)\n",
      "llama_print_timings:        eval time =    8644.12 ms /   124 runs   (   69.71 ms per token,    14.35 tokens per second)\n",
      "llama_print_timings:       total time =   22044.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =       2.67 ms /    21 runs   (    0.13 ms per token,  7865.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9442.02 ms /   809 tokens (   11.67 ms per token,    85.68 tokens per second)\n",
      "llama_print_timings:        eval time =    1372.72 ms /    20 runs   (   68.64 ms per token,    14.57 tokens per second)\n",
      "llama_print_timings:       total time =   10857.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      11.90 ms /    93 runs   (    0.13 ms per token,  7816.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7284.86 ms /   552 tokens (   13.20 ms per token,    75.77 tokens per second)\n",
      "llama_print_timings:        eval time =    6448.96 ms /    92 runs   (   70.10 ms per token,    14.27 tokens per second)\n",
      "llama_print_timings:       total time =   13953.37 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], ['unigrams and bigrams\\tword2vec\\tmanually constructed lexica\\tsentiment embeddings\\t'], ['2\\t'], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =       7.98 ms /    55 runs   (    0.15 ms per token,  6893.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15326.45 ms /   778 tokens (   19.70 ms per token,    50.76 tokens per second)\n",
      "llama_print_timings:        eval time =    3762.49 ms /    54 runs   (   69.68 ms per token,    14.35 tokens per second)\n",
      "llama_print_timings:       total time =   19259.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      19.91 ms /   114 runs   (    0.17 ms per token,  5724.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7176.48 ms /   279 tokens (   25.72 ms per token,    38.88 tokens per second)\n",
      "llama_print_timings:        eval time =   12903.61 ms /   113 runs   (  114.19 ms per token,     8.76 tokens per second)\n",
      "llama_print_timings:       total time =   20510.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =       5.65 ms /    31 runs   (    0.18 ms per token,  5491.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12059.08 ms /   749 tokens (   16.10 ms per token,    62.11 tokens per second)\n",
      "llama_print_timings:        eval time =    2150.33 ms /    30 runs   (   71.68 ms per token,    13.95 tokens per second)\n",
      "llama_print_timings:       total time =   14334.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      19.07 ms /    99 runs   (    0.19 ms per token,  5191.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12208.50 ms /   825 tokens (   14.80 ms per token,    67.58 tokens per second)\n",
      "llama_print_timings:        eval time =    7339.12 ms /    98 runs   (   74.89 ms per token,    13.35 tokens per second)\n",
      "llama_print_timings:       total time =   19949.15 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:24<00:00, 24.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], ['413 $(4\\\\%)$ users were annotated as \"bot,\" 7849 $(75.35\\\\%)$ as \"non-bot,\" and $20.69$ $(19.9\\\\%)$ as \"unavailable\"\\t', 'Tweet Diversity\\tURL score\\tMean Daily Posts\\tTopics\\tMean Post Length\\tProfile Picture\\t', 'a sample of $10,417$ users from a database containing more than 400 million publicly available tweets posted by more than $100,000$ users who have announced their pregnancy on Twitter\\tTwo professional annotators manually categorized the $10,417$ users as \"bot,\" \"non-bot,\" or \"unavailable,\" based on their publicly available Twitter sites\\tUsers were annotated broadly as \"bot\" if, in contrast to users annotated as \"non-bot,\" they do not appear to be posting personal information\\t Users were annotated as \"unavailable\" if their Twitter sites could not be viewed at the time of annotation\\t'], ['Using a machine learning algorithm on top of an existing bot detection system, and a set of simple derived features, we were able to significantly improve bot detection performance in health-related data. \\t', 'simple derived features, we were able to significantly improve bot detection performance in health-related data\\t'], ['Social networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth.\\t Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject recruitment for cancer trials BIBREF2, and characterizing drug abuse BIBREF3, to name a few. \\t', ' drug reaction detection\\tsyndromic surveillance\\tsubject recruitment for cancer trials\\tcharacterizing drug abuse\\t', 'almost exclusively on population-level studies\\tvery recent research tasks have focused on performing longitudinal data analysis at the user level, such as mining health-related information from cohorts of pregnant women\\t']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      16.55 ms /    98 runs   (    0.17 ms per token,  5919.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15005.23 ms /   848 tokens (   17.69 ms per token,    56.51 tokens per second)\n",
      "llama_print_timings:        eval time =    6667.09 ms /    97 runs   (   68.73 ms per token,    14.55 tokens per second)\n",
      "llama_print_timings:       total time =   22045.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      32.88 ms /   256 runs   (    0.13 ms per token,  7786.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18028.04 ms /   820 tokens (   21.99 ms per token,    45.48 tokens per second)\n",
      "llama_print_timings:        eval time =   18065.98 ms /   255 runs   (   70.85 ms per token,    14.11 tokens per second)\n",
      "llama_print_timings:       total time =   36757.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      25.84 ms /   217 runs   (    0.12 ms per token,  8397.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10994.72 ms /   815 tokens (   13.49 ms per token,    74.13 tokens per second)\n",
      "llama_print_timings:        eval time =   15375.80 ms /   216 runs   (   71.18 ms per token,    14.05 tokens per second)\n",
      "llama_print_timings:       total time =   26811.38 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      27.03 ms /   107 runs   (    0.25 ms per token,  3959.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9255.53 ms /   784 tokens (   11.81 ms per token,    84.71 tokens per second)\n",
      "llama_print_timings:        eval time =    7416.06 ms /   106 runs   (   69.96 ms per token,    14.29 tokens per second)\n",
      "llama_print_timings:       total time =   17313.53 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:16<00:00, 16.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['N18-1126\\tUDPipe\\tD15-1272\\tMorfette\\t', 'N18-1126\\tUDPipe system of K17-3009\\tD15-1272\\tMorfette\\t'], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      12.71 ms /   104 runs   (    0.12 ms per token,  8185.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13205.66 ms /   794 tokens (   16.63 ms per token,    60.13 tokens per second)\n",
      "llama_print_timings:        eval time =    6858.19 ms /   103 runs   (   66.58 ms per token,    15.02 tokens per second)\n",
      "llama_print_timings:       total time =   20311.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =       2.81 ms /    23 runs   (    0.12 ms per token,  8193.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13296.77 ms /   740 tokens (   17.97 ms per token,    55.65 tokens per second)\n",
      "llama_print_timings:        eval time =    1433.85 ms /    22 runs   (   65.17 ms per token,    15.34 tokens per second)\n",
      "llama_print_timings:       total time =   14798.38 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], ['stacked bilstms\\t'], ['English Penn Treebank\\tspmrl datasets\\t', ' English Penn Treebank\\tspmrl datasets\\t']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =       8.90 ms /    85 runs   (    0.10 ms per token,  9554.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11808.01 ms /   703 tokens (   16.80 ms per token,    59.54 tokens per second)\n",
      "llama_print_timings:        eval time =    5524.84 ms /    84 runs   (   65.77 ms per token,    15.20 tokens per second)\n",
      "llama_print_timings:       total time =   17555.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      13.29 ms /    94 runs   (    0.14 ms per token,  7071.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10355.43 ms /   744 tokens (   13.92 ms per token,    71.85 tokens per second)\n",
      "llama_print_timings:        eval time =    6366.83 ms /    93 runs   (   68.46 ms per token,    14.61 tokens per second)\n",
      "llama_print_timings:       total time =   16932.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =       6.87 ms /    60 runs   (    0.11 ms per token,  8728.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9732.38 ms /   676 tokens (   14.40 ms per token,    69.46 tokens per second)\n",
      "llama_print_timings:        eval time =    4028.47 ms /    59 runs   (   68.28 ms per token,    14.65 tokens per second)\n",
      "llama_print_timings:       total time =   13900.96 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:17<00:00, 17.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], ['Seq2seq\\tGPT2-FT\\tSpeaker\\tECM\\tSkeleton-to-Response (SR)\\tRetrieval + Style Transfer (RST)\\tRetrieval + Reranking (RRe)\\t', 'Generative Approaches ::: Seq2seq\\tGenerative Approaches ::: GPT2-FT:\\tGenerative Approaches ::: Speaker:\\tGenerative Approaches ::: ECM:\\tRetrieval-Based Approaches ::: Skeleton-to-Response (SR)\\tRetrieval-Based Approaches ::: Retrieval + Style Transfer (RST)\\tRetrieval-Based Approaches ::: Retrieval + Style Transfer (RST)\\tRetrieval-Based Approaches ::: Retrieval + Reranking (RRe)\\t'], ['Chinese\\tEnglish\\t'], ['gender-specific (Chinese) dataset\\temotion-specific (Chinese) dataset\\tsentiment-specific (English) dataset\\t', 'Gender-Specific Dialogue Dataset\\tEmotion-Specific Dialogue Dataset\\tSentiment-Specific Dialogue Dataset\\t']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      30.22 ms /   176 runs   (    0.17 ms per token,  5823.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14960.75 ms /   831 tokens (   18.00 ms per token,    55.55 tokens per second)\n",
      "llama_print_timings:        eval time =   12476.74 ms /   175 runs   (   71.30 ms per token,    14.03 tokens per second)\n",
      "llama_print_timings:       total time =   28126.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      42.71 ms /   256 runs   (    0.17 ms per token,  5994.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11423.41 ms /   793 tokens (   14.41 ms per token,    69.42 tokens per second)\n",
      "llama_print_timings:        eval time =   18488.57 ms /   255 runs   (   72.50 ms per token,    13.79 tokens per second)\n",
      "llama_print_timings:       total time =   30739.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   17686.12 ms\n",
      "llama_print_timings:      sample time =      21.34 ms /   126 runs   (    0.17 ms per token,  5903.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11038.78 ms /   775 tokens (   14.24 ms per token,    70.21 tokens per second)\n",
      "llama_print_timings:        eval time =    9291.16 ms /   125 runs   (   74.33 ms per token,    13.45 tokens per second)\n",
      "llama_print_timings:       total time =   20698.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 61\u001B[0m\n\u001B[1;32m     59\u001B[0m response_list \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m query \u001B[38;5;129;01min\u001B[39;00m query_list:\n\u001B[0;32m---> 61\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mquery_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m     response_list\u001B[38;5;241m.\u001B[39mappend(response\u001B[38;5;241m.\u001B[39mresponse)\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# Compare the generated answers with the reference answers of the respective sample using Ragas Metrics\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# and add the scores to a list\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_index/indices/query/base.py:31\u001B[0m, in \u001B[0;36mBaseQueryEngine.query\u001B[0;34m(self, str_or_query_bundle)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(str_or_query_bundle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m     30\u001B[0m     str_or_query_bundle \u001B[38;5;241m=\u001B[39m QueryBundle(str_or_query_bundle)\n\u001B[0;32m---> 31\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstr_or_query_bundle\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_index/query_engine/retriever_query_engine.py:182\u001B[0m, in \u001B[0;36mRetrieverQueryEngine._query\u001B[0;34m(self, query_bundle)\u001B[0m\n\u001B[1;32m    176\u001B[0m         nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretrieve(query_bundle)\n\u001B[1;32m    178\u001B[0m         retrieve_event\u001B[38;5;241m.\u001B[39mon_end(\n\u001B[1;32m    179\u001B[0m             payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mNODES: nodes},\n\u001B[1;32m    180\u001B[0m         )\n\u001B[0;32m--> 182\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_response_synthesizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msynthesize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_bundle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    187\u001B[0m     query_event\u001B[38;5;241m.\u001B[39mon_end(payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mRESPONSE: response})\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_index/response_synthesizers/base.py:147\u001B[0m, in \u001B[0;36mBaseSynthesizer.synthesize\u001B[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m     query \u001B[38;5;241m=\u001B[39m QueryBundle(query_str\u001B[38;5;241m=\u001B[39mquery)\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback_manager\u001B[38;5;241m.\u001B[39mevent(\n\u001B[1;32m    145\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mSYNTHESIZE, payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mQUERY_STR: query\u001B[38;5;241m.\u001B[39mquery_str}\n\u001B[1;32m    146\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m event:\n\u001B[0;32m--> 147\u001B[0m     response_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery_str\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtext_chunks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_content\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmetadata_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMetadataMode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLLM\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnodes\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[43m        \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    152\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    153\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    155\u001B[0m     additional_source_nodes \u001B[38;5;241m=\u001B[39m additional_source_nodes \u001B[38;5;129;01mor\u001B[39;00m []\n\u001B[1;32m    156\u001B[0m     source_nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(nodes) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlist\u001B[39m(additional_source_nodes)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_index/response_synthesizers/compact_and_refine.py:38\u001B[0m, in \u001B[0;36mCompactAndRefine.get_response\u001B[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# use prompt helper to fix compact text_chunks under the prompt limitation\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# TODO: This is a temporary fix - reason it's temporary is that\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# the refine template does not account for size of previous answer.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m new_texts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001B[0;32m---> 38\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_str\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext_chunks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_texts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprev_response\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprev_response\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_index/response_synthesizers/refine.py:127\u001B[0m, in \u001B[0;36mRefine.get_response\u001B[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001B[0m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m text_chunk \u001B[38;5;129;01min\u001B[39;00m text_chunks:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m prev_response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    125\u001B[0m         \u001B[38;5;66;03m# if this is the first chunk, and text chunk already\u001B[39;00m\n\u001B[1;32m    126\u001B[0m         \u001B[38;5;66;03m# is an answer, then return it\u001B[39;00m\n\u001B[0;32m--> 127\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_give_response_single\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    128\u001B[0m \u001B[43m            \u001B[49m\u001B[43mquery_str\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kwargs\u001B[49m\n\u001B[1;32m    129\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    130\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    131\u001B[0m         \u001B[38;5;66;03m# refine response if possible\u001B[39;00m\n\u001B[1;32m    132\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_refine_response_single(\n\u001B[1;32m    133\u001B[0m             prev_response, query_str, text_chunk, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mresponse_kwargs\n\u001B[1;32m    134\u001B[0m         )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_index/response_synthesizers/refine.py:182\u001B[0m, in \u001B[0;36mRefine._give_response_single\u001B[0;34m(self, query_str, text_chunk, **response_kwargs)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_streaming:\n\u001B[1;32m    179\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    180\u001B[0m         structured_response \u001B[38;5;241m=\u001B[39m cast(\n\u001B[1;32m    181\u001B[0m             StructuredRefineResponse,\n\u001B[0;32m--> 182\u001B[0m             \u001B[43mprogram\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m                \u001B[49m\u001B[43mcontext_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcur_text_chunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m                \u001B[49m\u001B[43moutput_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_output_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    187\u001B[0m         )\n\u001B[1;32m    188\u001B[0m         query_satisfied \u001B[38;5;241m=\u001B[39m structured_response\u001B[38;5;241m.\u001B[39mquery_satisfied\n\u001B[1;32m    189\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m query_satisfied:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_index/response_synthesizers/refine.py:53\u001B[0m, in \u001B[0;36mDefaultRefineProgram.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m StructuredRefineResponse:\n\u001B[0;32m---> 53\u001B[0m     answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_llm_predictor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m StructuredRefineResponse(answer\u001B[38;5;241m=\u001B[39manswer, query_satisfied\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_index/llm_predictor/base.py:196\u001B[0m, in \u001B[0;36mLLMPredictor.predict\u001B[0;34m(self, prompt, output_cls, **prompt_args)\u001B[0m\n\u001B[1;32m    194\u001B[0m     formatted_prompt \u001B[38;5;241m=\u001B[39m prompt\u001B[38;5;241m.\u001B[39mformat(llm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_llm, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprompt_args)\n\u001B[1;32m    195\u001B[0m     formatted_prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extend_prompt(formatted_prompt)\n\u001B[0;32m--> 196\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_llm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcomplete\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatted_prompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    197\u001B[0m     output \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mtext\n\u001B[1;32m    199\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(output)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_index/llms/base.py:312\u001B[0m, in \u001B[0;36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001B[0;34m(_self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m wrapper_logic(_self) \u001B[38;5;28;01mas\u001B[39;00m callback_manager:\n\u001B[1;32m    303\u001B[0m     event_id \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_event_start(\n\u001B[1;32m    304\u001B[0m         CBEventType\u001B[38;5;241m.\u001B[39mLLM,\n\u001B[1;32m    305\u001B[0m         payload\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    309\u001B[0m         },\n\u001B[1;32m    310\u001B[0m     )\n\u001B[0;32m--> 312\u001B[0m     f_return_val \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_self\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    313\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(f_return_val, Generator):\n\u001B[1;32m    314\u001B[0m         \u001B[38;5;66;03m# intercept the generator and add a callback to the end\u001B[39;00m\n\u001B[1;32m    315\u001B[0m         \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_gen\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m CompletionResponseGen:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_index/llms/llama_cpp.py:223\u001B[0m, in \u001B[0;36mLlamaCPP.complete\u001B[0;34m(self, prompt, **kwargs)\u001B[0m\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_formatted:\n\u001B[1;32m    221\u001B[0m     prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompletion_to_prompt(prompt)\n\u001B[0;32m--> 223\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m CompletionResponse(text\u001B[38;5;241m=\u001B[39mresponse[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchoices\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m], raw\u001B[38;5;241m=\u001B[39mresponse)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_cpp/llama.py:1500\u001B[0m, in \u001B[0;36mLlama.__call__\u001B[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001B[0m\n\u001B[1;32m   1454\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m   1455\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1456\u001B[0m     prompt: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1476\u001B[0m     grammar: Optional[LlamaGrammar] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1477\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[Completion, Iterator[CompletionChunk]]:\n\u001B[1;32m   1478\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Generate text from a prompt.\u001B[39;00m\n\u001B[1;32m   1479\u001B[0m \n\u001B[1;32m   1480\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;124;03m        Response object containing the generated text.\u001B[39;00m\n\u001B[1;32m   1499\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1500\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_completion\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[43m        \u001B[49m\u001B[43msuffix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msuffix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1505\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1506\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1507\u001B[0m \u001B[43m        \u001B[49m\u001B[43mecho\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mecho\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1510\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepeat_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepeat_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1512\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1513\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1514\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtfs_z\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtfs_z\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1515\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmirostat_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmirostat_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1516\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmirostat_tau\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmirostat_tau\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1517\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmirostat_eta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmirostat_eta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1518\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1519\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1520\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1521\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrammar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrammar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_cpp/llama.py:1451\u001B[0m, in \u001B[0;36mLlama.create_completion\u001B[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001B[0m\n\u001B[1;32m   1449\u001B[0m     chunks: Iterator[CompletionChunk] \u001B[38;5;241m=\u001B[39m completion_or_chunks\n\u001B[1;32m   1450\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m chunks\n\u001B[0;32m-> 1451\u001B[0m completion: Completion \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(completion_or_chunks)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m   1452\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m completion\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_cpp/llama.py:1014\u001B[0m, in \u001B[0;36mLlama._create_completion\u001B[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001B[0m\n\u001B[1;32m   1012\u001B[0m finish_reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlength\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1013\u001B[0m multibyte_fix \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1014\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1015\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprompt_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1016\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1017\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1018\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtemp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1019\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtfs_z\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtfs_z\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1020\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmirostat_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmirostat_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1021\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmirostat_tau\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmirostat_tau\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1022\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmirostat_eta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmirostat_eta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1023\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1025\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrepeat_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepeat_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1026\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1027\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1028\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrammar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrammar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1029\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   1030\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_token_eos\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   1031\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcompletion_tokens\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_cpp/llama.py:828\u001B[0m, in \u001B[0;36mLlama.generate\u001B[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001B[0m\n\u001B[1;32m    825\u001B[0m     grammar\u001B[38;5;241m.\u001B[39mreset()\n\u001B[1;32m    827\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 828\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    829\u001B[0m     token \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msample(\n\u001B[1;32m    830\u001B[0m         top_k\u001B[38;5;241m=\u001B[39mtop_k,\n\u001B[1;32m    831\u001B[0m         top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    841\u001B[0m         grammar\u001B[38;5;241m=\u001B[39mgrammar,\n\u001B[1;32m    842\u001B[0m     )\n\u001B[1;32m    843\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stopping_criteria \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m stopping_criteria(\n\u001B[1;32m    844\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_ids, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_scores[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]\n\u001B[1;32m    845\u001B[0m     ):\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_cpp/llama.py:562\u001B[0m, in \u001B[0;36mLlama.eval\u001B[0;34m(self, tokens)\u001B[0m\n\u001B[1;32m    560\u001B[0m n_past \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(n_ctx \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_ids))\n\u001B[1;32m    561\u001B[0m n_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch)\n\u001B[0;32m--> 562\u001B[0m return_code \u001B[38;5;241m=\u001B[39m \u001B[43mllama_cpp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllama_eval\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m    \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mllama_cpp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllama_token\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    566\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_past\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_past\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    568\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllama_eval returned \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreturn_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/myRAG/lib/python3.11/site-packages/llama_cpp/llama_cpp.py:1040\u001B[0m, in \u001B[0;36mllama_eval\u001B[0;34m(ctx, tokens, n_tokens, n_past)\u001B[0m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mllama_eval\u001B[39m(\n\u001B[1;32m   1035\u001B[0m     ctx: llama_context_p,\n\u001B[1;32m   1036\u001B[0m     tokens,  \u001B[38;5;66;03m# type: Array[llama_token]\u001B[39;00m\n\u001B[1;32m   1037\u001B[0m     n_tokens: Union[c_int, \u001B[38;5;28mint\u001B[39m],\n\u001B[1;32m   1038\u001B[0m     n_past: Union[c_int, \u001B[38;5;28mint\u001B[39m],\n\u001B[1;32m   1039\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m-> 1040\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_lib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllama_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_past\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from qasper_data.data import AnswerType\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    Document,\n",
    ")\n",
    "\n",
    "\n",
    "import openai\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-Ei89D8xbv2ve98FT6e629a55Ca1341518f1238BaB53dAf20\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://ai-yyds.com/v1\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "\n",
    "from qasper_data.qasper_dataset import QasperDataset\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size=256, llm=\"local\", embed_model=\"local\")\n",
    "\n",
    "test_dataset = QasperDataset(\"test\")\n",
    "\n",
    "df_all_papers = pd.DataFrame()\n",
    "\n",
    "df_test = df_test.head(10)\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    documents = [Document(text=row[\"paper\"])]\n",
    "    query_list = row[\"questions\"]\n",
    "    reference_answers_list = list(map(lambda x: x.split(\",\"), row[\"answers\"]))\n",
    "    vector_index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    contexts = []\n",
    "    title = row[\"paper\"].split(\"\\t\")[0]\n",
    "    paper = test_dataset.match_paper_by_title(title) \n",
    "    for qa in paper.qas:\n",
    "        context = []\n",
    "        for answer in qa.answers:\n",
    "            if answer.answer_type == AnswerType.EXTRACTIVE:\n",
    "                spans = \"\"\n",
    "                for span in answer.extractive_spans:\n",
    "                    spans += span + \"\\t\"\n",
    "                context.append(spans)\n",
    "        contexts.append(context)        \n",
    "    print(contexts)\n",
    "    # Query the vector index with a top_k value of top 3 documents without any reranker\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=3)\n",
    "    response_list = []\n",
    "    for query in query_list:\n",
    "        response = query_engine.query(query)\n",
    "        response_list.append(response.response)\n",
    "    # Compare the generated answers with the reference answers of the respective sample using Ragas Metrics\n",
    "    # and add the scores to a list\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"question\": query_list,\n",
    "            \"answer\": response_list,\n",
    "            \"contexts\": contexts,\n",
    "            \"ground_truths\": reference_answers_list\n",
    "        }\n",
    "    )\n",
    "    ragas_result = evaluate(\n",
    "        dataset,\n",
    "        metrics=[\n",
    "            answer_relevancy,\n",
    "            faithfulness,\n",
    "            context_recall,\n",
    "            context_precision,\n",
    "        ],\n",
    "    )\n",
    "    df_ragas = ragas_result.to_pandas()\n",
    "    df_all_papers = pd.concat([df_all_papers, df_ragas], ignore_index=True)\n",
    "\n",
    "df_all_papers.to_csv(os.path.join(data_folder, f\"ragas_baseline_{version}.csv\"))\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae7c97a1e9ccfe8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-14T09:57:00.691207Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all_papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305865624c6f4f9c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-14T09:57:00.701453Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_score(df):\n",
    "    return df.mean(axis=0)\n",
    "\n",
    "answer_relevancy_score = mean_score(df_all_papers[\"answer_relevancy\"])\n",
    "faithfulness_score = mean_score(df_all_papers[\"faithfulness\"])\n",
    "context_recall_score = mean_score(df_all_papers[\"context_recall\"])\n",
    "context_precision_score = mean_score(df_all_papers[\"context_precision\"])\n",
    "\n",
    "ragas_baseline_dict = {\n",
    "    \"Metric\": [\"answer_relevancy\", \"faithfulness\", \"context_recall\", \"context_precision\"],\n",
    "    \"BGE_Embeddings\": [answer_relevancy_score, faithfulness_score, context_recall_score, context_precision_score],\n",
    "}\n",
    "df_ragas_baseline_results = pd.DataFrame(ragas_baseline_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7621a7bd63d999",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-14T09:57:00.702006Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ragas_baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T12:21:23.371959Z",
     "start_time": "2023-11-09T12:21:23.367542Z"
    }
   },
   "source": [
    "### RAG Base Reranker Evaluation\n",
    "`BAAI/bge-small-en` Embedding + `cross-encoder/ms-marco-MiniLM-L-12-v2` as reranker\n",
    "\n",
    "#### Eval Method:-\n",
    "1. Iterate over each row of the test dataset:-\n",
    "    1. For the current row being iterated, create a vector index using the paper document provided in the paper column of the dataset\n",
    "    2. Query the vector index with a top_k value of top 5 nodes.\n",
    "    3. Use cross-encoder/ms-marco-MiniLM-L-12-v2 as a reranker as a NodePostprocessor to get top_k value of top 3 nodes out of the 8 nodes\n",
    "    3. Compare the generated answers with the reference answers of the respective sample using Ragas Evaluator and add the scores to a list\n",
    "5. Repeat 1 untill all the rows have been iterated\n",
    "6. Calculate avg scores over all samples/ rows"
   ],
   "id": "a9e0a50192224d39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ad357a071fa92",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-14T09:57:00.703938Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index import (VectorStoreIndex, ServiceContext, Document)\n",
    "\n",
    "reranker = SentenceTransformerRerank(model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\", top_n=3)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-Ei89D8xbv2ve98FT6e629a55Ca1341518f1238BaB53dAf20\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://ai-yyds.com/v1\"\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "data_folder = \"data\"\n",
    "version = 1\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size=256, llm=\"local\", embed_model=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-14T09:57:00.704141Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(data_folder, f\"test_{version}.csv\"), index_col=0)\n",
    "\n",
    "import ast\n",
    "\n",
    "df_test[\"questions\"] = df_test[\"questions\"].apply(ast.literal_eval)\n",
    "df_test[\"answers\"] = df_test[\"answers\"].apply(ast.literal_eval)\n",
    "\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "]\n",
    "\n",
    "df_ragas_base_reranker_result = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "for index, row in tqdm(df_test.iterrows()):\n",
    "    documents = [Document(text=row[\"paper\"])]\n",
    "    query_list = row[\"questions\"]\n",
    "    reference_answers_list = row[\"answers\"]\n",
    "    response_list = []\n",
    "\n",
    "    number_of_accepted_queries = 0\n",
    "    \n",
    "    # Create a vector index from the documents\n",
    "    vector_index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    \n",
    "    # Query the vector index with a top_k value of top 8 nodes with reranker\n",
    "    # as cross-encoder/ms-marco-MiniLM-L-12-v2\n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=3, node_postprocessors=[reranker]\n",
    "    )\n",
    "    assert len(query_list) == len(reference_answers_list)\n",
    "    \n",
    "    reference_answers_list = list(map(lambda x: x.split(\",\"), row[\"answers\"]))\n",
    "    for query in query_list:\n",
    "        response = query_engine.query(query)\n",
    "        response_list.append(response)\n",
    "    \n",
    "    answers = []\n",
    "    contexts = []\n",
    "    for r in response_list:\n",
    "        answers.append(r.response)\n",
    "        contexts.append([c.node.get_content() for c in r.source_nodes])\n",
    "        \n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"question\": query_list,\n",
    "            \"answer\": answers,\n",
    "            \"contexts\": contexts,\n",
    "            \"ground_truths\": reference_answers_list\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    ragas_result = evaluate(\n",
    "        dataset,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    df_ragas = ragas_result.to_pandas()\n",
    "    df_ragas_base_reranker_result = pd.concat([df_ragas_base_reranker_result, df_ragas], ignore_index=True)\n",
    "\n",
    "df_ragas_base_reranker_result.to_csv(os.path.join(data_folder, f\"ragas_base_reranker_{version}.csv\"))\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()\n",
    "\n"
   ],
   "id": "982b357f047a8f62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-14T09:57:00.704466Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ragas_base_reranker_result.head()"
   ],
   "id": "e3e78792dcc5e26a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mean_score(df):\n",
    "    return df.mean(axis=0)\n",
    "\n",
    "answer_relevancy_score = mean_score(df_ragas_base_reranker_result[\"answer_relevancy\"])\n",
    "faithfulness_score = mean_score(df_ragas_base_reranker_result[\"faithfulness\"])\n",
    "context_recall_score = mean_score(df_ragas_base_reranker_result[\"context_recall\"])\n",
    "context_precision_score = mean_score(df_ragas_base_reranker_result[\"context_precision\"])\n",
    "\n",
    "ragas_base_reranker_dict = {\n",
    "    \"Metric\": [\"answer_relevancy\", \"faithfulness\", \"context_recall\", \"context_precision\"],\n",
    "    \"Base_Cross_Encoder\": [answer_relevancy_score, faithfulness_score, context_recall_score, context_precision_score],\n",
    "}\n",
    "df_ragas_base_reranker_results = pd.DataFrame(ragas_base_reranker_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-14T09:57:00.704577Z"
    }
   },
   "id": "8494d1596436cec8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ragas_base_reranker_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-14T09:57:00.704683Z"
    }
   },
   "id": "e79c77f2f48e8144"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate with Fine-Tuned re-ranker\n",
    "`BAAI/bge-small-en` Embedding + `cross_encoder` fine-tuned on the fine-tuning dataset as reranker\n",
    "\n",
    "#### Eval Method:-\n",
    "1. Iterate over each row of the test dataset:-\n",
    "    1. For the current row being iterated, create a vector index using the paper document provided in the paper column of the dataset\n",
    "    2. Query the vector index with a top_k value of top 5 nodes.\n",
    "    3. Use cross_encoder fine-tuned on the fine-tuning dataset as a reranker as a NodePostprocessor to get top_k value of top 3 nodes out of the 8 nodes\n",
    "    3. Compare the generated answers with the reference answers of the respective sample using Ragas Evaluator and add the scores to a list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d76d7b7e696b96ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index import (VectorStoreIndex, ServiceContext, Document)\n",
    "\n",
    "reranker = SentenceTransformerRerank(model=\"../models/cross_encoder\", top_n=3)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-Ei89D8xbv2ve98FT6e629a55Ca1341518f1238BaB53dAf20\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://ai-yyds.com/v1\"\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "data_folder = \"data\"\n",
    "version = 1\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size=256, llm=\"local\", embed_model=\"local\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-14T09:57:00.704747Z"
    }
   },
   "id": "a9798e8731128d35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from ragas.llama_index import evaluate\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(data_folder, f\"test_{version}.csv\"), index_col=0)\n",
    "\n",
    "import ast\n",
    "\n",
    "df_test[\"questions\"] = df_test[\"questions\"].apply(ast.literal_eval)\n",
    "df_test[\"answers\"] = df_test[\"answers\"].apply(ast.literal_eval)\n",
    "\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "]\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "df_ragas_finetuned_reranker_result = pd.DataFrame()\n",
    "\n",
    "for index, row in tqdm(df_test.iterrows()):\n",
    "    documents = [Document(text=row[\"paper\"])]\n",
    "    query_list = row[\"questions\"]\n",
    "    reference_answers_list = row[\"answers\"]\n",
    "    response_list = []\n",
    "\n",
    "    number_of_accepted_queries = 0\n",
    "    \n",
    "    # Create a vector index from the documents\n",
    "    vector_index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    \n",
    "    # Query the vector index with a top_k value of top 8 nodes with reranker\n",
    "    # as cross-encoder/ms-marco-MiniLM-L-12-v2\n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=3, node_postprocessors=[reranker]\n",
    "    )\n",
    "    assert len(query_list) == len(reference_answers_list)\n",
    "    \n",
    "    reference_answers_list = list(map(lambda x: x.split(\",\"), row[\"answers\"]))\n",
    "    \n",
    "    result = evaluate(query_engine, metrics, query_list, reference_answers_list)\n",
    "    \n",
    "    df_ragas = result.to_pandas()\n",
    "    df_ragas_finetuned_reranker_result = pd.concat([df_ragas_finetuned_reranker_result, df_ragas], ignore_index=True)\n",
    "\n",
    "df_ragas_finetuned_reranker_result.to_csv(os.path.join(data_folder, f\"ragas_finetuned_reranker_{version}.csv\"))\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-14T09:57:00.705066Z"
    }
   },
   "id": "d0037b3fe3e66577"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ragas_finetuned_reranker_result.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-14T09:57:00.705139Z"
    }
   },
   "id": "cc750e0f51bfeaea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mean_score(df):\n",
    "    return df.mean(axis=0)\n",
    "\n",
    "answer_relevancy_score = mean_score(df_ragas_finetuned_reranker_result[\"answer_relevancy\"])\n",
    "faithfulness_score = mean_score(df_ragas_finetuned_reranker_result[\"faithfulness\"])\n",
    "context_recall_score = mean_score(df_ragas_finetuned_reranker_result[\"context_recall\"])\n",
    "context_precision_score = mean_score(df_ragas_finetuned_reranker_result[\"context_precision\"])\n",
    "\n",
    "ragas_finetuned_reranker_dict = {\n",
    "    \"Metric\": [\"answer_relevancy\", \"faithfulness\", \"context_recall\", \"context_precision\"],\n",
    "    \"Finetuned_Cross_Encoder\": [answer_relevancy_score, faithfulness_score, context_recall_score, context_precision_score],\n",
    "}\n",
    "\n",
    "df_ragas_finetuned_reranker_results = pd.DataFrame(ragas_finetuned_reranker_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-14T09:57:00.705236Z"
    }
   },
   "id": "8eccfc8d3b9c1263"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ragas_finetuned_reranker_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-14T09:57:00.705908Z"
    }
   },
   "id": "3ae696d272cb6ef9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
