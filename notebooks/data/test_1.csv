,paper,questions,answers
0,"Identifying Condition-Action Statements in Medical Guidelines Using Domain-Independent Features	This paper advances the state of the art in text understanding of medical guidelines by releasing two new annotated clinical guidelines datasets, and establishing baselines for using machine learning to extract condition-action pairs. In contrast to prior work that relies on manually created rules, we report experiment with several supervised machine learning techniques to classify sentences as to whether they express conditions and actions. We show the limitations and possible extensions of this work on text mining of medical guidelines.	Introduction	Clinical decision-support system (CDSS) is any computer system intended to provide decision support for healthcare professionals, and using clinical data or knowledge BIBREF0 . The classic problem of diagnosis is only one of the clinical decision problems. Deciding which questions to ask, tests to order, procedures to perform, treatment to indicate, or which alternative medical care to try, are other examples of clinical decisions. CDSSs generally fall into two categories BIBREF0 Most of the questions physicians need to consult about with CDSSs are from the latter category. Medical guidelines (also known as clinical guidelines, clinical protocols or clinical practice guidelines) are most useful at the point of care and answering to ""what to do"" questions.Medical guidelines are systematically developed statements to assist with practitioners' and patients' decisions. They establish criteria regarding diagnosis, management, and treatment in specific areas of healthcare. For example, a sentence such as ""if the A1C is 7.0% and a repeat result is 6.8%, the diagnosis of diabetes is confirmed"" in medical guidelines determines what is true about a patient. Sentences such as ""Topical and oral decongestants and antihistamines should be avoided in patients with ABRS"" guide what to do or not to do with a patient. These examples illustrate conditions, criteria applicable to patients, and consequences of the conditions. The consequences may refer to activities, effects, intentions, or events. If a guideline-based CDSS needs to answer ""what to do"" questions, it has to have access to condition-action statements describing under what circumstances an action can be performed.Medical guidelines contain many condition-action statements. Condition-action statements provide information about expected process flow. If a guideline-based CDSS could extract and formalize these statements, it could help practitioners in the decision-making process. For example, it could help automatically asses the relationship between therapies, guidelines and outcomes, and in particular could help the impact of changing guidelines.However, completely automated extraction of condition-action statements does not seem possible. This is due among other things to the variety of linguistic expressions used in condition-action sentences. For example, they are not always in form of ""{if} condition {then} action”. In the sentence ""Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation”, we have a condition-action sentence without an ""{if}"" term.We propose a supervised machine learning model classifying sentences as to whether they express a condition or not. After we determine a sentence contain a condition, we use natural language processing and information extraction tools to extract conditions and resulting activities.With the help of a domain expert, we annotated three sets of guidelines to create gold standards to measure the performance of our condition-action extracting models. The sets of guidelines are: hypertension BIBREF1 , chapter4 of asthma BIBREF2 , and rhinosinusitis BIBREF3 . Chapter 4 of asthma guidelines was selected for comparison with prior work of Wenzina and Kaiser BIBREF4 . We have annotated the guidelines for the conditions, consequences, modifiers of conditions, and type of consequences. These annotate sets of guidelines are available for experiments https://www.dropbox.com/.Related Work	We will briefly discuss the modeling and annotation of condition-action for medical usage in this section. Our corpus and method of identifying conditions in clinical guidelines is explained in section 3.Research on CIGs started about 20 years ago and became more popular in the late-1990s and early 2000s. Different approaches have been developed to represent and execute clinical guidelines over patient-specific clinical data. They include document-centric models, decision trees and probabilistic models, and ""Task-Network Models""(TNMs) BIBREF5 , which represent guideline knowledge in hierarchical structures containing networks of clinical actions and decisions that unfold over time. Serban et. al BIBREF6 developed a methodology for extracting and using linguistic patterns in guideline formalization, to aid the human modellers in guideline formalization and reduce the human modelling effort. Kaiser et. al BIBREF7 developed a method to identify activities to be performed during a treatment which are described in a guideline document. They used relations of the UMLS Semantic Network BIBREF8 to identify these activities in a guideline document. Wenzina and Kaiser BIBREF4 developed a rule-based method to automatically identifying conditional activities in guideline documents.They achieved a recall of 75% and a precision of 88% on chapter 4 of asthma guidelines which was mentioned before.Condition-Action Extraction	Medical guidelines’ condition-action statements provide information to determine ""what to do"" with a patient. Other types of consequences of a condition in a sentence may help practitioner to find what is true about a patient. In this paper, we propose an automated process to find and extract condition-action statements from medical guidelines. We employed NLP tools and concepts in the process to achieve more general models.We define the task as classification task. Given an input statement, classify it to one of the three categories: NC (no condition) if the statement doesn’t have a condition; CA if the statement is a condition-action sentence; and CC (condition-consequence) if the statement has a condition which has a non-action consequence. For a CDSS, to determine both ""what is true"" about a patient and ""what to do"" with a patient, CC and CA statements can be merged to one category.There are limitations in this specification of classification categories. For example, guidelines may contain statements with a condition referring to a consequence in another statement. Or, we can see condition and effect in two different sentences: ""However, there are some cases for which the results for black persons were different from the results for the general population (question 3, evidence statements 2, 10, and 17). In those cases, separate evidence statements were developed.""In this work we focus only on statements that follow the above sentence categorization rules. This allows us to make clear comparison to prior work e.g. by Wenzina and Kaiser BIBREF4 . They annotated chapter 4 of asthma and other guidelines. They used information extraction rules and semantic pattern rules to extract conditional activities, condition-action statements. We use POS tags as features in the classification models. In our opinion, using POS tags instead of semantic pattern rules makes our model more domain-independent, and therefore more suitable for establishing baselines, not only for text mining of medical guidelines but also in other domains, such as text mining of business rules. But we also expect to improve the performance of our extraction programs by adding semantic and discourse information (this work is ongoing).Classification	Most of the condition-action sentences have a modifier in the sentences. For example, in ""In the population aged 18 years or older with CKD and hypertension, initial (or add-on) antihypertensive treatment should include an ACEI or ARB to improve kidney outcomes"", we have ""the population aged 18 years or older with CKD and hypertension"" as a condition and ""{in}"" is the modifier. ""If"", ""in"", ""for"", ""to"", ""which"", and ""when"" are the most frequent modifiers in our guidelines.We used CoreNLP BIBREF9 Shift-Reduce Constituency Parser to parse sentences in guidelines. As we mentioned, ""if"", ""in"", ""for"", ""to"", ""which"", and ""when"" are the most frequent modifiers in our guidelines. ""If"", ""in"", and ""for"" are tagged as ""IN"" which represents preposition or subordinating conjunction. ""To"" is tagged as ""TO"" and ""when"" and ""which"" are tagged as ""WHADV"". We used regular expressions to find those parses which are promising candidates for extraction of condition-action pairs; for example, we selected sentences which include these tags: IN, TO and WHADVP.We extracted part of speech (POS) tags as our features for our model. Each candidate sentence has at least one candidate condition part. We extract these parts by regular expressions. Each part of sentence which starts with below patterns is a candidate condition part:""\((SBAR|PP) \(IN""""\(SBAR \(WHADVP""""\(PP \(TO""For example, ""(ROOT (S (PP (IN In) (NP (NP (NNS adults)) (PP (IN with) (NP (NN hypertension))))) (, ,) (VP (VBZ does) (S (VP (VBG initiating) (S (NP (NP (JJ antihypertensive) (JJ pharmacologic) (NN therapy)) (PP (IN at) (NP (JJ specific) (NN BP) (NNS thresholds)))) (VP (VBP improve) (NP (NN health) (NNS outcomes))))))) (. ?)))"" is the constituent parsed tree of ""In adults with hypertension, does initiating antihypertensive pharmacologic therapy at specific BP thresholds improve health outcomes?"". ""(PP (IN In) (NP (NP (NNS adults)) (PP (IN with) (NP (NN hypertension)))))"" and ""(PP (IN at) (NP (JJ specific) (NN BP) (NNS thresholds)))"" are two candidate condition parts in this example.We created features for our model based on POS tags and their combinations. The sets of features and the combinations are learned automatically from annotated examples. We used these novel features to make our model more domain-independent.For each sentence, we extracted POS tags, sequences of 3 POS tags, and combination of all POS tags of candidate conditions as features. For example, ""PP IN NP NP NNS PP IN NP NN PPINNP INNPNP NPNPNNS NPNNSPP NNSPPIN PPINNP INNPNN PPINNPNPNNSPPINNPNN PP IN NP NN PPINNP INNPNN PPINNPNN PP IN NP JJ NN NNS PPINNP INNPJJ NPJJNN JJNNNNS PPINNPJJNNNNS"" represents ""In adults with hypertension, does initiating antihypertensive pharmacologic therapy at specific BP thresholds improve health outcomes?"" in our model. Note that the glued together part of speech tags are not a formatting error but features automatically derived by our model (from consecutive part of speech tags).Gold Standard Datasets	We use three medical guidelines documents to create gold standard datasets. They provide statements, tables, and figures about hypertension, rhinosinusitis, and asthma. The creation of the gold standard datasets is described below in detail.Our data preparation process proceeded as follows: We started by converting the guidelines from PDF or html to text format, editing sentences only to manage conversion errors, the majority of which were bullet points. Tables and some figures pose a problem, and we are simply treating them as unstructured text. We are not dealing at this time with the ambiguities introduced by this approach; we do have plans to address it in future work.Using regular expressions, as described above, we selected candidate sentences from text files. Note that candidate sentences do not always include a modifier such as ""if"" or ""in"". For example, in ""Patients on long-term steroid tablets (e.g. longer than three months) or requiring frequent courses of steroid tablets (e.g. three to four per year) will be at risk of systemic side-effects"", there is no modifier in the sentence.The annotation of the guidelines text (the next step), focused on determining whether there were condition statements in the candidate sentences or not. The instruction to the annotators were to try to paraphrase candidate sentences as sentences with ""if condition, then consequence"". If the transformed/paraphrased sentence conveyed the same meaning as the original, we considered to be a condition-consequence sentence. Then we we could annotate condition and consequence parts. For example, we paraphrased ""Beta-blockers, including eye drops, are contraindicated in patients with asthma"" to ""If patients have asthma, then beta-blockers, including eye drops, are contraindicated"". The paraphrased sentence conveys same meaning. So it became a condition-consequence sentence in our dataset. On the other hand, for example, we cannot paraphrase ""Further, the diagnostic criteria for CKD do not consider age-related decline in kidney function as reflected in estimated GFR"" to an if-then sentence.We also annotated the type of sentences based on their semantics: We classified them into three classes: condition-action, condition-consequence(effect, intention, and event) and action. Examples are shown in table 1.Each sentence was annotated by one domain expert and us (and the disagreements where less than 10 percent). Table 2 shows the statistics of the annotated sentences for 3 different medical guidelines.Model Performance	Hypertension, asthma, and rhinosinusitis guidelines and gold standard datasets were applied to evaluate our model. Since two of these annotated corpora are new, our model is establishing a baseline. The asthma corpus was investigated previously by BIBREF4 .We extracted candidate statements by applying aforementioned regex on POS tags. Hypertension, asthma, and rhinosinusitis guidelines had 278, 172, and 761 candidate statements respectively. By applying this filtering subtask, we get rid of 38, 116, and 5 no condition statement respectively from guidelines. We used Weka BIBREF10 classifiers to create our models. ZeroR, Naïve Bayes, J48, and random forest classifiers were applied in our project. Table 3 , 4 , and 5 show the results of classifiers for each guidelines.The results are based on 10-fold cross-validation on respective datasets.The results show that generally random forest classifier seems to work best in extracting Condition-Action statements.Notice that these results are lower than previously reported by BIBREF4 . The difference is due to our using of completely automated feature selection when training on an annotated corpus, and not relying on manually created extraction rules. In addition, their results demonstrate recalls on activities with specific patterns. If we consider all activities in their annotated corpus, their recall will be 56%. And if we apply their approach on our annotated corpus, the recall will be 39%. In ongoing work we hope to reduce or close this gap by adding semantic and discourse information to our feature sets.Conclusions and Future Work	We investigated the problem of automated extraction of condition-action from clinical guidelines based on an annotated corpus. We proposed a simple supervised model which classifies statements based on combinations of part of speech tags used as features. We showed results of classifiers using this model on three different annotated datasets which we created. We release these dataset for others to use.Obviously, this is very preliminary work. Our work established baselines for automated extraction of condition-action rules from medical guidelines, but its performance is still inferior to a collection of manually created extraction rules. To close this gap we are currently augmenting our model with semantic information along the lines of BIBREF7 and BIBREF4 . In addition, we are beginning to experiment with some discourse relations – these are important, for example, in understanding of lists and tables. We also plan to make our annotated datasets more convenient to use by re-annotating them with standard annotation tools e.g. BRAT BIBREF11 .","['What supervised machine learning models do they use?', 'Does the supervised machine learning approach outperform previous work?', 'How large is the released data set?', 'What is an example of a condition-action pair?']","['Unacceptable', 'Unacceptable', '1470 sentences', 'Unacceptable']"
1,"Multilingual is not enough: BERT for Finnish	Deep learning-based language models pretrained on large unannotated text corpora have been demonstrated to allow efficient transfer learning for natural language processing, with recent approaches such as the transformer-based BERT model advancing the state of the art across a variety of tasks. While most work on these models has focused on high-resource languages, in particular English, a number of recent efforts have introduced multilingual models that can be fine-tuned to address tasks in a large number of different languages. However, we still lack a thorough understanding of the capabilities of these models, in particular for lower-resourced languages. In this paper, we focus on Finnish and thoroughly evaluate the multilingual BERT model on a range of tasks, comparing it with a new Finnish BERT model trained from scratch. The new language-specific model is shown to systematically and clearly outperform the multilingual. While the multilingual model largely fails to reach the performance of previously proposed methods, the custom Finnish BERT model establishes new state-of-the-art results on all corpora for all reference tasks: part-of-speech tagging, named entity recognition, and dependency parsing. We release the model and all related resources created for this study with open licenses at this https URL .	Introduction	Transfer learning approaches using deep neural network architectures have recently achieved substantial advances in a range of natural language processing (NLP) tasks ranging from sequence labeling tasks such as part-of-speech (POS) tagging and named entity recognition (NER) BIBREF0 to dependency parsing BIBREF1 and natural language understanding (NLU) tasks BIBREF2. While the great majority of this work has focused primarily on English, a number of studies have also targeted other languages, typically through multilingual models.The BERT model of devlin2018bert has been particularly influential, establishing state-of-the-art results for English for a range of NLU tasks and NER when it was released. For most languages, the only currently available BERT model is the multilingual model (M-BERT) trained on pooled data from 104 languages. While M-BERT has been shown to have a remarkable ability to generalize across languages BIBREF3, several studies have also demonstrated that monolingual BERT models, where available, can notably outperform M-BERT. Such results include the evaluation of the recently released French BERT model BIBREF4, the preliminary results accompanying the release of a German BERT model, and the evaluation of ronnqvist-etal-2019-multilingual comparing M-BERT with English and German monolingual models.In this paper, we study the application of language-specific and multilingual BERT models to Finnish NLP. We introduce a new Finnish BERT model trained from scratch and perform a comprehensive evaluation comparing its performance to M-BERT on established datasets for POS tagging, NER, and dependency parsing as well as a range of diagnostic text classification tasks. The results show that 1) on most tasks the multilingual model does not represent an advance over previous state of the art, indicating that multilingual models may fail to deliver on the promise of deep transfer learning for lower-resourced languages, and 2) the custom Finnish BERT model systematically outperforms the multilingual as well as all previously proposed methods on all benchmark tasks, showing that language-specific deep transfer learning models can provide comparable advances to those reported for much higher-resourced languages.Related Work	The current transfer learning methods have evolved from word embedding techniques, such as word2vec BIBREF5, GLoVe BIBREF6 and fastText BIBREF7, to take into account the textual context of words. Crucially, incorporating the context avoids the obvious limitations stemming from the one-vector-per-unique-word assumption inherent to the previous word embedding methods. The current successful wave of work proposing and applying different contextualized word embeddings was launched with ELMo BIBREF0, a context embedding method based on bidirectional LSTM networks. Another notable example is the ULMFit model BIBREF8, which specifically focuses on techniques for domain adaptation of LSTM-based language models. Following the introduction of the attention-based (as opposed to recurrent) Transformer architecture BIBREF9, BERT was proposed by BIBREF2, demonstrating superior performance on a broad array of tasks. The BERT model has been further refined in a number of follow-up studies BIBREF10, BIBREF11 and, presently, BERT and related models form the de facto standard approach to embedding text segments as well as individual words in context.Unlike the previous generation of models, training BERT is a computationally intensive task, requiring substantial resources. As of this writing, Google has released English and Chinese monolingual BERT models and the multilingual M-BERT model covering 104 languages. Subsequently, monolingual BERT models have been published for German and French BIBREF4. In a separate line of work, a cross-lingual BERT model for 15 languages was published by BIBREF12, leveraging also cross-lingual signals. Finally, a number of studies have introduced monolingual models focusing on particular subdomains of English, such as BioBERT BIBREF13 and SciBERT BIBREF14 for biomedical publications and scientific text.Pretraining	We next introduce the sources of unlabeled data used to pretrain FinBERT and present the data filtering and cleanup, vocabulary generation, and pretraining processes.Pretraining ::: Pretraining Data	To provide a sufficiently large and varied unannotated corpus for pretraining, we compiled Finnish texts from three primary sources: news, online discussion, and an internet crawl. All of the unannotated texts were split into sentences, tokenized, and parsed using the Turku Neural Parser pipeline BIBREF15. Table TABREF4 summarizes the initial statistics of the three sources prior to cleanup and filtering.Pretraining ::: Pretraining Data ::: News	We combine two major sources of Finnish news: the Yle corpus, an archive of news published by Finland's national public broadcasting company in the years 2011-2018, and The STT corpus of newswire articles sent to media outlets by the Finnish News Agency (STT) between 1992 and 2018. The combined resources contain approx. 900 million tokens, with 20% originating from the Yle corpus and 80% from STT.Pretraining ::: Pretraining Data ::: Online discussion	The Suomi24 corpus (version 2017H2) contains all posts to the Suomi24 online discussion website from 2001 to 2017. Suomi24 is one of the largest social networking forums in Finland and covers a broad range of topics and levels of style and formality in language. The corpus is also roughly five times the size of the available news resources.Pretraining ::: Pretraining Data ::: Internet crawl	Two primary sources were used to create pretraining data from unrestricted crawls. First, we compiled documents from the dedicated internet crawl of the Finnish internet of luotolahti2015towards run between 2014 and 2016 using the SpiderLing crawler BIBREF16. Second, we selected texts from the Common Crawl project by running a a map-reduce language detection job on the plain text material from Common Crawl. These sources were supplemented with plain text extracted from the Finnish Wikipedia using the mwlib library. Following initial compilation, this text collection was analyzed for using the Onion deduplication tool. Duplicate documents were removed, and remaining documents grouped by their level of duplication.Pretraining ::: Pretraining Data ::: Cleanup and filtering	As quality can be more important than quantity for pretraining data BIBREF17, we applied a series of custom cleaning and filtering steps to the raw textual data. Initial cleaning removed header and tag material from newswire documents. In the first filtering step, machine translated and generated texts were removed using a simple support vector machine (SVM) classifier with lexical features trained on data from the FinCORE corpus BIBREF18. The remaining documents were then aggressively filtered using language detection and hand-written heuristics, removing documents that e.g. had too high a ratio of digits, uppercase or non-Finnish alphabetic characters, or had low average sentence length. A delexicalized SVM classifier operating on parse-derived features was then trained on news (positives) and heuristically filtered documents (negatives) and applied to remove documents that were morphosyntactically similar to the latter. Finally, all internet crawl-sourced documents featuring 25% or more duplication were removed from the data. The statistics of the final pretraining data produced in this process are summarized in Table TABREF10. We note that even with this aggressive filtering, this data is roughly 30 times the size of the Finnish Wikipedia included in M-BERT pretraining data.Pretraining ::: Vocabulary generation	To generate dedicated BERT vocabularies for Finnish, a sample of cleaned and filtered sentences were first tokenized using BERT BasicTokenizer, generating both a cased version where punctuation is separated, and an uncased version where characters are additionally mapped to lowercase and accents stripped. We then used the SentencePiece BIBREF19 implementation of byte-pair-encoding (BPE) BIBREF20 to generate cased and uncased vocabularies of 50,000 word pieces each.To assess the coverage of the generated cased and uncased vocabularies and compare these to previously introduced vocabularies, we sampled a random 1% of tokens extracted using WikiExtractor from the English and Finnish Wikipedias and tokenized the texts using various vocabularies to determine the number of word pieces and unknown pieces per basic token. Table TABREF15 shows the results of this evaluation. For English, both BERT and M-BERT generate less than 1.2 WordPieces per token, meaning that the model will represent the great majority of words as a single piece. For Finnish, this ratio is nearly 2 for M-BERT. While some of this difference is explained by the morphological complexity of the language, it also reflects that only a small part of the M-BERT vocabulary is dedicated to Finnish: using the language-specific FinBERT vocabularies, this ratio remains notably lower even though the size of these vocabularies is only half of the M-BERT vocabularies. Table TABREF16 shows examples of tokenization using the FinBERT and M-BERT vocabularies.Pretraining ::: Pretraining example generation	We used BERT tools to create pretraining examples using the same masked language model and next sentence prediction tasks used for the original BERT. Separate duplication factors were set for news, discussion and crawl texts to create a roughly balanced number of examples from each source. We also used whole-word masking, where all pieces of a word are masked together rather than selecting masked word pieces independently. We otherwise matched the parameters and process used to create pretraining data for the original BERT, including generating separate examples with sequence lengths 128 and 512 and setting the maximum number of masked tokens per sequence separately for each (20 and 77, respectively).Pretraining ::: Pretraining process	We pretrained cased and uncased models configured similarly to the base variants of BERT, with 110M parameters for each. The models were trained using 8 Nvidia V100 GPUs across 2 nodes on the Puhti supercomputer of CSC, the Finnish IT Center for Science. Following the approach of devlin2018bert, each model was trained for 1M steps, where the initial 90% used a maximum sequence length of 128 and the last 10% the full 512. A batch size of 140 per GPU was used for primary training, giving a global batch size of 1120. Due to memory constraints, the batch size was dropped to 20 per GPU for training with sequence length 512. We used the LAMB optimizer BIBREF21 with warmup over the first 1% of steps to a peak learning rate of 1e-4 followed by decay. Pretraining took approximately 12 days to complete per model variant.Evaluation	We next present an evaluation of the M-BERT and FinBERT models on a series of Finnish datasets representing both downstream NLP tasks and diagnostic evaluation tasks.Unless stated otherwise, all experiments follow the basic setup used in the experiments of devlin2018bert, selecting the learning rate, batch size and the number of epochs used for fine-tuning separately for each model and dataset combination using a grid search with evaluation on the development data. Other model and optimizer parameters were kept at the BERT defaults. Excepting for the parsing experiments, we repeat each experiment 5-10 times and report result mean and standard deviation.Evaluation ::: Part of Speech Tagging	Part of speech tagging is a standard sequence labeling task and several Finnish resources are available for the task.Evaluation ::: Part of Speech Tagging ::: Data	To assess POS tagging performance, we use the POS annotations of the three Finnish treebanks included in the Universal Dependencies (UD) collection BIBREF24: the Turku Dependency Treebank (TDT) BIBREF25, FinnTreeBank (FTB) BIBREF26 and Parallel UD treebank (PUD) BIBREF27. A broad range of methods were applied to tagging these resources as a subtask in the recent CoNLL shared tasks in 2017 and 2018 BIBREF28, and we use the CoNLL 2018 versions (UD version 2.2) of these corpora to assure comparability with their results. The statistics of these resources are shown in Table TABREF17. As the PUD corpus only provides a test set, we train and select parameters on the training and development sets of the compatibly annotated TDT corpus for evaluation on PUD. The CoNLL shared task proceeds from raw text and thus requires sentence splitting and tokenization in order to assign POS tags. To focus on tagging performance while maintaining comparability, we predict tags for the tokens predicted by the Uppsala system BIBREF29, distributed as part of the CoNLL'18 shared task system outputs BIBREF30.Evaluation ::: Part of Speech Tagging ::: Methods	We implement the BERT POS tagger straightforwardly by attaching a time-distributed dense output layer over the top layer of BERT and using the first piece of each wordpiece-tokenized input word to represent the word. The implementation and data processing tools are openly available. We compare POS tagging results to the best-performing methods for each corpus in the CoNLL 2018 shared task, namely that of che2018towards for TDT and FTB and lim2018sex for PUD. We report performance for the UPOS metric as implemented by the official CoNLL 2018 evaluation script.Evaluation ::: Part of Speech Tagging ::: Results	Table TABREF25 summarizes the results for POS tagging. We find that neither M-BERT model improves on the previous state of the art for any of the three resources, with results ranging 0.1-0.8% points below the best previously published results. By contrast, both language-specific models outperform the previous state of the art, with absolute improvements for FinBERT cased ranging between 0.4 and 1.7% points. While these improvements over the already very high reference results are modest in absolute terms, the relative reductions in errors are notable: in particular, the FinBERT cased error rate on FTB is less than half of the best CoNLL'18 result BIBREF22. We also note that the uncased models are surprisingly competitive with their cased equivalents for a task where capitalization has long been an important feature: for example, FinBERT uncased performance is within approx. 0.1% points of FinBERT cased for all corpora.Evaluation ::: Named Entity Recognition	Like POS tagging, named entity recognition is conventionally cast as a sequence labeling task. During the development of FinBERT, only one corpus was available for Finnish NER.Evaluation ::: Named Entity Recognition ::: Data	FiNER, a manually annotated NER corpus for Finnish, was recently introduced by ruokolainen2019finnish. The corpus annotations cover five types of named entities – person, organization, location, product and event – as well as dates. The primary corpus texts are drawn from a Finnish technology news publication, and it additionally contains an out-of-domain test set of documents drawn from the Finnish Wikipedia. In addition to conventional CoNLL-style named entity annotation, the corpus includes a small number of nested annotations (under 5% of the total). As ruokolainen2019finnish report results also for top-level (non-nested) annotations and the recognition of nested entity mentions would complicate evaluation, we here consider only the top-level annotations of the corpus. Table TABREF26 summarizes the statistics of these annotations.Evaluation ::: Named Entity Recognition ::: Methods	Our NER implementation is based on the approach proposed for CoNLL English NER by devlin2018bert. A dense layer is attached on top of the BERT model to predict IOB tags independently, without a CRF layer. To include document context for each sentence, we simply concatenate as many of the following sentences as can fit in the 512 wordpiece sequence. The FiNER data does not identify document boundaries, and therefore not all these sentences are necessarily from the same document. We make the our implementation available under an open licence.We compare NER results to the rule-based FiNER-tagger BIBREF32 developed together with the FiNER corpus and to the neural network-based model of gungor2018improving targeted specifically toward morphologically rich languages. The former achieved the highest results on the corpus and the latter was the best-performing machine learning-based method in the experiments of ruokolainen2019finnish. Named entity recognition performance is evaluated in terms of exact mention-level precision, recall and F-score as implemented by the standard conlleval script, and F-score is used to compare performance.Evaluation ::: Named Entity Recognition ::: Results	The results for named entity recognition are summarized in Table TABREF34 for the in-domain (technology news) test set and Table TABREF35 for the out-of-domain (Wikipedia) test set. We find that while M-BERT is able to outperform the best previously published results on the in-domain test set, it fails to reach the performance of FiNER-tagger on the out-of-domain test set. As for POS tagging, the language-specific FinBERT model again outperforms both M-BERT as well as all previously proposed methods, establishing new state-of-the-art results for Finnish named entity recognition.Evaluation ::: Dependency Parsing	Dependency parsing involves the prediction of a directed labeled graph over tokens. Finnish dependency parsing has a long history and several established resources are available for the task.Evaluation ::: Dependency Parsing ::: Data	The CoNLL 2018 shared task addressed end-to-end parsing from raw text into dependency structures on 82 different corpora representing 57 languages BIBREF28. We evaluate the pre-trained BERT models on the dependency parsing task using the three Finnish UD corpora introduced in Section SECREF27: the Turku Dependency Treebank (TDT), FinnTreeBank (FTB) and the Parallel UD treebank (PUD). To allow direct comparison with CoNLL 2018 results, we use the same versions of the corpora as used in the shared task (UD version 2.2) and evaluate performance using the official script provided by the task organizers. These corpora are the same used in the part-of-speech tagging experiments, and their key statistics were summarized above in Table TABREF17.Evaluation ::: Dependency Parsing ::: Methods	We evaluate the models using the Udify dependency parser recently introduced by BIBREF1. Udify is a multi-task model that support supporting multi- or monolingual fine-tuning of pre-trained BERT models on UD treebanks. Udify implements a multi-task network where a separate prediction layer for each task is added on top of the pre-trained BERT encoder. Additionally, instead of using only the top encoder layer representation in prediction, Udify adds a layers-wise dot-product attention, which calculates a weighted sum of all intermediate representation of 12 BERT layers for each token. All prediction layers as well as layer-wise attention are trained simultaneously, while also fine-tuning the pre-trained BERT weights.We train separate Udify parsing models using monolingual fine-tuning for TDT and FTB. The TDT models are used to evaluate performance also on PUD, which does not include a training set. We report parser performance in terms of Labeled Attachment Score (LAS). Each parser model is fine-tuned for 160 epochs with BERT weights kept frozen during the first epoch and subsequently updated along with other weights. The learning rate scheduler warm-up period is defined to be approximately one epoch. Otherwise, parameters are the same as used in BIBREF1. As the Udify model does not implement sentence or token segmentation, we use UDPipe BIBREF34 to pre-segment the text when reporting LAS on predicted segmentation.We compare our results to the best-performing system in the CoNLL 2018 shared task for the LAS metric, HIT-SCIR BIBREF22. In addition to having the highest average score over all treebanks for this metric, the system also achieved the highest LAS among 26 participants for each of the three Finnish treebanks. The dependency parser used in the HIT-SCIR system is the biaffine graph-based parser of BIBREF35 with deep contextualized word embeddings (ELMo) BIBREF36 trained monolingually on web crawl and Wikipedia data provided by BIBREF37. The final HIT-SCIR model is an ensemble over three parser models trained with different parameter initializations, where the final prediction is calculated by averaging the softmaxed output scores.We also compare results to the recent work of BIBREF33, where the merits of two parsing architectures, graph-based BIBREF38 and transition-based BIBREF39, are studied with two different deep contextualized embeddings, ELMo and BERT. We include results for their best-performing combination on the Finnish TDT corpus, the transition-based parser with monolingual ELMo embeddings.Evaluation ::: Dependency Parsing ::: Results	Table TABREF41 shows LAS results for predicted and gold segmentation. While Udify initialized with M-BERT fails to outperform our strongest baseline BIBREF22, Udify initialized with FinBERT achieves notably higher performance on all three treebanks, establishing new state-of-the-art parsing results for Finnish with a large margin. Depending on the treebank, Udify with cased FinBERT LAS results are 2.3–3.6% points above the previous state of the art, decreasing errors by 24%–31% relatively.Casing seem to have only a moderate impact in parsing, as the performance of cased and uncased models falls within 0.1–0.6% point range in each treebank. However, in each case the trend is that with FinBERT the cased version always outperforms the uncased one, while with M-BERT the story is opposite, the uncased always outperforming the cased one.To relate the high LAS of 93.56 achieved with the combination of the Udify parser and our pre-trained FinBERT model to human performance, we refer to the original annotation of the TDT corpus BIBREF40, where individual annotators were measured against the double-annotated and resolved final annotations. The comparison is reported in terms of LAS. Here, one must take into account that the original TDT corpus was annotated in the Stanford Dependencies (SD) annotation scheme BIBREF41, slightly modified to be suitable for the Finnish language, while the work reported in this paper uses the UD version of the corpus. Thus, the reported numbers are not directly comparable, but keeping in mind the similarities of SD and UD annotation schemes, give a ballpark estimate of human performance in the task. BIBREF40 report the average LAS of the five human annotators who participated in the treebank construction as 91.3, with individual LAS scores ranging from 95.9 to 71.8 (or 88.0 ignoring an annotator who only annotated 2% of the treebank and was still in the training phrase). Based on these numbers, the achieved parser LAS of 93.56 seems to be on par with or even above average human level performance and approaching the level of a well-trained and skilled annotator.Evaluation ::: Text classification	Finnish lacks the annotated language resources to construct a comprehensive collection of classification tasks such as those available for English BIBREF42, BIBREF43, BIBREF44. To assess model performance at text classification, we create two datasets based on Finnish document collections with topic information, one representing formal language (news) and the other informal (online discussion).Evaluation ::: Text classification ::: Data	Documents in the Yle news corpus (Section SECREF3) are annotated using a controlled vocabulary to identify subjects such as sports, politics, and economy. We identified ten such upper-level topics that were largely non-overlapping in the data and sampled documents annotated with exactly one selected topic to create a ten-class classification dataset. As the Yle corpus is available for download under a license that does not allow redistribution, we release tools to recreate this dataset. The Ylilauta corpus consists of the text of discussions on the Finnish online discussion forum Ylilauta from 2012 to 2014. Each posted message belongs to exactly one board, with topics such as games, fashion and television. We identified the ten most frequent topics and sampled messages consisting of at least ten tokens to create a text classification dataset from the Ylilauta data.To facilitate analysis and comparison, we downsample both corpora to create balanced datasets with 10000 training examples as well as 1000 development and 1000 test examples of each class. To reflect generalization performance to new documents, both resources were split chronologically, drawing the training set from the oldest texts, the test set from the newest, and the development set from texts published between the two. To assess classifier performance across a range of training dataset sizes, we further downsampled the training sets to create versions with 100, 316, 1000, and 3162 examples of each class ($10^2, 10^{2.5}, \ldots $). Finally, we truncated each document to a maximum of 256 basic tokens to minimize any advantage the language-specific model might have due to its more compact representation of Finnish.Evaluation ::: Text classification ::: Methods	We implement the text classification methods following devlin2018bert, minimizing task-specific architecture and simply attaching a dense output layer to the initial ([CLS]) token of the top layer of BERT. We establish baseline text classification performance using fastText BIBREF7. We evaluated a range of parameter combinations and different pretrained word vectors for the method using the development data, selecting character n-gram features of lengths 3–7, training for 25 epochs, and initialization with subword-enriched embeddings induced from Wikipedia texts BIBREF45 for the final experiments.Evaluation ::: Text classification ::: Results	The text classification results for various training set sizes are shown in Table TABREF45 for Yle news and in Table TABREF46 for Ylilauta online discussion and illustrated in Figure FIGREF47. We first note that performance is notably higher for the news corpus, with error rates for a given method and data set size more than doubling when moving from news to the discussion corpus. As both datasets represent 10-class classification tasks with balanced classes, this suggests that the latter task is inherently more difficult, perhaps in part due to the incidence of spam and off-topic messages on online discussion boards.The cased and uncased variants of FinBERT perform very similarly for both datasets and all training set sizes, while for M-BERT the uncased model consistently outperforms the cased – as was also found for parsing – with a marked advantage for small dataset sizes.Comparing M-BERT and FinBERT, we find that the language-specific models outperform the multilingual models across the full range of training data sizes for both datasets. For news, the four BERT variants have broadly similar learning curves, with the absolute advantage for FinBERT models ranging from 3% points for 1K examples to just over 1% point for 100K examples, and relative reductions in error from 20% to 13%. For online discussion, the differences are much more pronounced, with M-BERT models performing closer to the FastText baseline than to FinBERT. Here the language-specific BERT outperforms the multilingual by over 20% points for the smallest training data and maintains a 5% point absolute advantage even with 100,000 training examples, halving the error rate of the multilingual model for the smallest training set and maintaining an over 20% relative reduction for the largest.These contrasting results for the news and discussion corpora may be explained in part by domain mismatch: while the news texts are written in formal Finnish resembling the Wikipedia texts included as pretraining data for all BERT models as well as the FastText word vectors, only FinBERT pretraining material included informal Finnish from online discussions. This suggests that in pretraining BERT models care should be taken to assure that not only the targeted language but also the targeted text domains are sufficiently represented in the data.Evaluation ::: Probing Tasks	Finally, we explored the ability of the models to capture linguistic properties using the probing tasks proposed by BIBREF46. We use the implementation and Finnish data introduced for these tasks by BIBREF47, which omit the TopConst task defined in the original paper. We also left out the Semantic odd-man-out (SOMO) task, as we found the data to have errors making the task impossible to perform correctly. All of the tasks involve freezing the BERT layers and training a dense layer on top of it to function as a diagnostic classifier. The only information passed from BERT to the classifier is the state represented by the [CLS] token.In brief, the tasks can be roughly categorized into 3 different groups: surface, syntactic and semantic information.Evaluation ::: Probing Tasks ::: Surface tasks	In the sentence length (SentLen) task, sentences are classified into 6 classes depending on their length. The word content (WC) task measures the model's ability to determine which of 1000 mid-frequency words occurs in a sentence, where only one of the words is present in any one sentence.Evaluation ::: Probing Tasks ::: Syntactic tasks	The tree depth (TreeDepth) task is used to test how well the model can identify the depth of the syntax tree of a sentence. We used dependency trees to maintain comparability with the work of BIBREF47, whereas the original task used constituency trees. Bigram shift (BiShift) tests the model's ability to recognize when two adjacent words have had their positions swapped.Evaluation ::: Probing Tasks ::: Semantic tasks	In the subject number (SubjNum) task the number of the subject, i.e. singular or plural, connected to the main verb of a sentence is predicted. Object number (ObjNum) is similar to the previous task but for objects of the main verb. The Coordination inversion (CoordInv) has the order of two clauses joined by a coordinating conjunction reversed in half the examples. The model then has to predict whether or not a given example was inverted. In the Tense task the classifier has to predict whether a main verb of a sentence is in the present or past tense.Evaluation ::: Probing Tasks ::: Results	Table TABREF57 presents results comparing the FinBERT models to replicated M-BERT results from BIBREF47. We find that the best performance is achieved by either the cased or uncased language-specific model for all tasks except TreeDepth, where M-BERT reaches the highest performance. The differences between the results for the language-specific and multilingual models are modest for most tasks with the exception of the BiShift task, where the FinBERT models are shown to be markedly better at identifying sentences with inverted words. While this result supports the conclusion of our other experiments that FinBERT is the superior language model, results for the other tasks offer only weak support at best. We leave for future work the question whether these tasks measure aspects where the language-specific model does not have a clear advantage over the multilingual or if the results reflect limitations in the implementation or data of the probing tasks.Discussion	We have demonstrated that it is possible to create a language-specific BERT model for a lower-resourced language, Finnish, that clearly outperforms the multilingual BERT at a range of tasks and advances the state of the art in many NLP tasks. These findings raise the question whether it would be possible to realize similar advantages for other languages that currently lack dedicated models of this type. It is likely that the feasibility of training high quality deep transfer learning models hinges on the availability of pretraining data.As of this writing, Finnish ranks 24th among the different language editions of Wikipedia by article count, and 25th in Common Crawl by page count. There are thus dozens of languages for which unannotated corpora of broadly comparable size or larger than that used to pretrain FinBERT could be readily assembled from online resources. Given that language-specific BERT models have been shown to outperform multilingual ones also for high-resource languages such as French BIBREF4 – ranked 3rd by Wikipedia article count – it is further likely that the benefits of a language-specific model observed here extend at least to languages with more resources than Finnish. (We are not aware of efforts to establish the minimum amount of unannotated text required to train high-quality models of this type.)The methods we applied to collect and filter texts for training FinBERT have only few language dependencies, such as the use of UD parsing results for filtering. As UD resources are already available for over 70 languages, the specific approach and tools introduced in this work could be readily applied to a large number of languages. To facilitate such efforts, we also make all of the supporting tools developed in this work available under open licenses.Conclusions	In this work, we compiled and carefully filtered a large unannotated corpus of Finnish, trained language-specific FinBERT models, and presented evaluations comparing these to multilingual BERT models at a broad range of natural language processing tasks. The results indicate that the multilingual models fail to deliver on the promises of deep transfer learning for lower-resourced languages, falling behind the performance of previously proposed methods for most tasks. By contrast, the newly introduced FinBERT model was shown not only to outperform multilingual BERT for all downstream tasks, but also to establish new state-of-the art results for three different Finnish corpora for part-of-speech tagging and dependency parsing as well as for named entity recognition.The FinBERT models and all of the tools and resources introduced in this paper are available under open licenses from https://turkunlp.org/finbert.Acknowledgments	We gratefully acknowledge the support of CSC – IT Center for Science through its Grand Challenge program, the Academy of Finland, the Google Digital News Innovation Fund and collaboration of the Finnish News Agency STT, as well as the NVIDIA Corporation GPU Grant Program.","['By how much did the new model outperform multilingual BERT?', 'What previous proposed methods did they explore?', 'What was the new Finnish model trained on?']","['For POS,  improvements for cased BERT are 1.26 2.52  0.5 for TDT,  FTB and PUD datasets respectively.\nFor NER in-domain test set, improvement is  2.11 F1 and for NER out-of-domain test set, improvement is  5.32 F1.\nFor Dependency parsing, improvements are in range from 3.35 to 6.64 LAS for cased BERT.', 'Unacceptable', 'Unacceptable']"
2,"Efficient Dynamic WFST Decoding for Personalized Language Models	We propose a two-layer cache mechanism to speed up dynamic WFST decoding with personalized language models. The first layer is a public cache that stores most of the static part of the graph. This is shared globally among all users. A second layer is a private cache that caches the graph that represents the personalized language model, which is only shared by the utterances from a particular user. We also propose two simple yet effective pre-initialization methods, one based on breadth-first search, and another based on a data-driven exploration of decoder states using previous utterances. Experiments with a calling speech recognition task using a personalized contact list demonstrate that the proposed public cache reduces decoding time by factor of three compared to decoding without pre-initialization. Using the private cache provides additional efficiency gains, reducing the decoding time by a factor of five.	Introduction	Speech input is now a common feature for smart devices. In many cases, the user's query involves entities such as a name from a contact list, a location, or a music title. Recognizing entities is particularly challenging for speech recognition because many entities are infrequent or out of the main vocabulary of the system. One way to improve performance is such cases is through the use of a personal language model (LM) which contains the expected user-specific entities. Because each user can have their own personalized LM, it is vital that the speech decoder be able to efficiently load the model on the fly, so it can be used in decoding, without any noticeable increase in latency.Many state-of-the-art speech recognition decoders are based on the weighted finite state transducer (WFST) paradigm BIBREF0, BIBREF1. A conventional WFST decoder searches a statically composed $H C L G$ graph, where $H$ is the graph that translates HMM states to CD phones, $C$ translates CD phones to graphemes, $L$ translates graphemes to words and $G$ is graph that represents the language model. Using a statically composed graph has two limitations. First, it is both compute and memory intensive when the vocabulary and LM are large. Second, the static graph approach makes it hard to handle personalized language models BIBREF2. Many common tasks a user may want to perform with a voice assistant such as making phone calls, messaging to a specific contact or playing favorite music require a personalized language model. A dynamic WFST decoder is better suited for such cases. As denoted in Eq (DISPLAY_FORM1), in a dynamic WFST decoder, $HCL$ is composed and optimized offline, while $G$ is composed on the fly with lazy (on-demand) composition, denoted as $\circ $.To handle dynamic entities, a class LM $G_c$ is normally used as background $G$ and a personalized LM $G_p$ is replaced on-the-fly, before applying lazy composition.Since the non-terminal states are composed on-the-fly, it means the states of recognition FST will also contain personalized information that cannot be used by other users or service threads.In previous work, a method was proposed to do a pre-initialized composition for a non-class LM BIBREF3. However, it the dynamic part is still expanded on-the-fly. In this work, we propose two improvements in order to best leverage class language models. First, we use simpler methods for pre-initialization which do not need to pre-generate decoder state statistics. Second, we propose a two-layer pre-initialization mechanism that also avoids performing dynamic expansion on per user basis. In the two-layer pre-initialization method, we make use of a class LM with class tag. We build a personalized FST that contains the members of the class for each user. Using the FST replacement algorithm, we obtain a personalized language transducer BIBREF4. We perform a pre-composition for all FST states whose transitions do not contain class tags. By doing so, the actual on-demand composition is only required for the states in personalized FST. For a multi-threaded service, the pre-composed FST can be shared by all threads, since it does not contain personalized FST states (non-terminals). The personalized part will be shared for all utterances from the same user, which will take full advantage of memory usage.Unlike the previous pre-initialization approach that is based on calculating the state statistics BIBREF3, our simplified pre-initialization methods do not rely on pre-calculated state frequencies. Instead, we directly expand the graph with breadth-first search or through a data-driven approach where a small numbers of utterances are processed by the decoder offline. We found that both methods are effective, but the data-driven approach outperforms the breadth first search algorithm. Both methods can be combined to achieve the best performance. Through a series of experiments on a speech recognition task for the calling domain, we found that pre-initialization on the public graph speeds up the decoding time by a factor of three. Futhermore, sharing the private graph further reduces decoding time and results in factor of five improvement in efficiency.Architecture and Algorithm	The general composition algorithm is well-explained in BIBREF5, BIBREF6 and a pre-composition algorithm with a non-class LM is described in BIBREF3. Here we will only present our new algorithm focusing on how to pre-compose the graph while avoiding non-terminal states. In this work, we use the same mathematical notation as BIBREF0.Architecture and Algorithm ::: Two-layer cached FST during decoding	A WFST can be written aswhere $\mathcal {A}$, $\mathcal {B}$ are finite label sets for input and output. $Q$ is the finite state set. $I\subseteq Q$ is the initial state set, $F\subseteq Q$ is final state set. $E\subseteq Q\times (\mathcal {A} \cup \lbrace \epsilon \rbrace ) \times (\mathcal {B} \cup \lbrace \epsilon \rbrace ) \times \mathbb {K} \times Q$ is a set of transitional mapping between states in $Q$ with weighted input/output label pair, where $\mathbb {K}$ is a semiring $(\mathbb {K}, \oplus , \otimes , \overline{0}, \overline{1})$.The composition of two weighted FSTs is defined aswhere $\mathcal {B} = \mathcal {B}_1 \cap \mathcal {A}_2$ is the intersection of output label set of $T_1$ and input label set of $T_2$. For $a, b, c\ne \epsilon $, two transitions $(q_1, a, b, w_1, q_1^{\prime })$ in $T_1$ and $(q2, b, c, w_2, q_2^{\prime })$, the composed transition will be $((q_1, q_2), a, c, w_1 \bigotimes w_2, (q_1^{\prime }, q_2^{\prime }))$.For two FSTs $T_1$, $T_2$ over semiring $\mathbb {K}$,is the class language model transducer obtained by replacing the class labels in generic root FST $G_c$ with class FSTs $G_p$ for different classes, where $\mathcal {C}$ denotes the set of all supported classes.The calculation for composition is very slow for LM with large vocabulary size. Naive on-the-fly composition is very time-consuming. In BIBREF3, the authors proposed a pre-initialized composition algorithm, which does a partial composition based on the state frequency. This one-time cost calculation can do some composition in advance. During decoding search, the FST will skip the composition of pre-initialized states. However, extending this algorithm to class LMs is non-trivial in practice. For a class LM, the non-terminal states cannot be composed during pre-initialization since we need a pre-initialization that is applicable to all users, which means we need to apply some restrictions to prevent composition of the personalized part.We define $T_P$ as a partial composed FST structure for $T=T_1 \circ T_2$, where $P \subseteq Q$ is the set of pre-composed states. In real time decoding, the on-the-fly composition will be performed on top of the pre-initialized $T_P$, which is similar to previous work BIBREF3. In a production environment, multiple threads will share the same pre-composed FST $T_P$ structure, while each thread will own a private FST structure.where $T_D$ is the dynamic cache built on top of $T_P$. $T_D$ may need to copy some states from $T_P$ if we need to update information for those states in $T_P$.In order to support this mechanism, we use a two-layered cached FST for decoding. The first layer is public cache which represents $T_P$. It is a static cache created by pre-initialization. The second layer is the private cache, which is owned by a particular user and constructed on-the-fly. Figure FIGREF9 shows the architecture of our two-layer FST. The solid box denotes the static graph and the dashed ones show the dynamic graph. Personalized states will appear only in $T_D$.The static public cache stores the most frequent states, which greatly reduces the run time factor (RTF) of online decoding. Since $T_D$ has a smaller size than a fully dynamic graph, the marginal memory efficiency for multi-threaded service will be better.Furthermore, the private cache will not be freed after decoding a single utterance. The lifetime of a private cache actually can last for the entire dialog section for a specific user. The private cache keeps updating during the dialog session, making processing the subsequent utterances faster as more states are composed and stored in $T_D$. With this accumulated dynamic cache, a longer dialog can expect a better RTF in theory. In general, the static public cache serves all threads, while the private cache boosts the performance within a dialog session. The private cache will be freed at the end of the dialog.Architecture and Algorithm ::: Pre-composition algorithm for class language models	Based on the algorithm described in BIBREF3, we allow the states $(q_1, q_2)$ such that $q_2 = (q_c, q_p), q_c \in Q_c, q_p=0 $ to be pre-composed, where $q_c$ and $q_p$ denote states in $G_c$ and $G_p$, respectively. States in $G_c$ with a class label transition will be ignored during pre-composition.By applying this restriction, the states in the pre-composed recognition FST $T_P$ will not contain any personalized states, and thus, can be shared by all users and threads.Note that care must taken to account for the special case when the initial states could have transitions with a class label. In this case, the entire graph is blocked (Figure FIGREF12(a)), so we need to add an extra $\epsilon $ transition before class label in the root FST, which will guarantee all the initial states are composed (Figure FIGREF12(b)). In the pre-composition stage, we don't need the actual class FSTs for each class, so $G_p$ is simply a placeholder FST which only contains a placeholder word $\left\langle temp \right\rangle $. This means all the transitions following the placeholder transition may be blocked if there is no other path that skips over the placeholder transition. In practice, for a large LM graph with a large vocabulary, the connectivity is usually very high, once the initial states are guaranteed to be composed.This pre-composition algorithm can be applied with lookahead filter BIBREF7. We implemented this algorithm using OpenFst framework BIBREF4, which supports such a lookahead filter in both the pre-composition and decoding stages. In our implementation, the decoding FST has a two-layered cache and state table. The state table is necessary since the add-on composition during decoding must be based on the same state map.Architecture and Algorithm ::: Pre-composition methods	In general, we can pre-compose all the states of the decoding FST that are applied to all users, i.e. those unrelated to the personalized language model. However, this full set pre-composition could be very slow and memory consuming. In fact, most of the states are rarely composed during real data traffic, and therefore, performing partial pre-composition is sufficient. Here we propose two simple methods for pre-composition.Architecture and Algorithm ::: Pre-composition methods ::: Distance based method	Naive breath-first-search (BFS) is the most obvious way to perform pre-composition. We iterate over all states within a specific distance from the start state of decoding FST. It generalizes to a full set pre-composition when the search depth is large.Architecture and Algorithm ::: Pre-composition methods ::: Data-driven warm-up	Our goal is to pre-compose the most frequently encountered states. However, if some frequent states are far from the start state, they may not be identified by naive BFS. In this case, it is very time and memory consuming to increase the depth of the BFS. Moreover, if we simply use a offline corpus of utterances to analyze the frequency of all states, some highly frequent states could be blocked by less frequent states. Thus, the easiest way is to do pre-composition using real utterances.The decoding FST can be expanded while decoding utterances. We utilize a special decoder in the warm-up stage. This warm-up decoder will apply the same restriction discussed in the previous section. We use an empty contact FST in the warm-up stage to avoid expanding any personalization-related states. This data driven pre-composition will expand most frequent states which are visited during warm-up decoding, especially for some specific patterns.Architecture and Algorithm ::: Out-Of-Vocabulary recognition	Handling out-of-vocabulary (OOV) words in speech recognition is very important especially for contact name recognition. We replace the normal class (contact) FST with a mono-phone FST by adding monophone words in the lexicon BIBREF2, BIBREF8, BIBREF9. By using s monophone FST, we avoid the necessity of adding new words into lexicon on-the-fly, which significantly simplifies the system. We use silence phone ""SIL"" to represent the word boundary. These monophone words will not be applied with silence phone in lexicon since they are not real words.In Figure FIGREF17, the contact name is represented as monophone words using IPA phone set. SIL is added after each name in contact FST. Names with the same pronunciation also need to be handled using disambiguation symbols. In practice, because of accent and pronunciation variability, we have found that multiple pronunciations of OOV names are required in the personalized class FST.Experiments	We performed a series of experiments on different data sets in order to evaluate the impact on real-time factor (RTF) and word error rate (WER) of the proposed approach. In theory, the pre-composition algorithm will not change the WER, since the search algorithm does not change.Experiments ::: Experimental Setup	In these experiments, speech recognition was performed using a hybrid LSTM-HMM framework. The acoustic model is an LSTM that consumes 40-dimensional log filterbank coefficients as the input and generates the posterior probabilities of 8000 tied context-dependent states as the output. The LM is a pruned 4-gram model trained using various semantic patterns that include a class label as well as a general purpose text corpus. The LM contains $@contact$ as an entity word, which will be replaced by the personalized contact FST. After pruning, the LM has 26 million n-grams.The personalized class FST (contact FST) only contains monophone words. Determinization and minimization are applied to the contact FST with disambiguation symbols. The disambiguation symbols are removed after graph optimization. The decoding experiments are performed on a server with 110 GB memory and 24 processors.Experiments are performed on two data sets. The first contains 7,500 utterances from the calling domain from Facebook employees. This includes commands like “Please call Jun Liu now"". The second consists of approximately 10,000 utterances from other common domains, such as weather, time, and music. Note that we include the contact FST for both calling and non-calling utterances, as we do not assume knowledge of the user's intent a priori. Each user has a contact FST containing 500 contacts on average. We keep up to five pronunciations for each name, generated by a grapheme-to-phoneme model.We experiment with both the naive BFS and the proposed data-driven pre-composition methods. For the data-driven approach, we randomly picked 500 utterances from the evaluation data set as warm up utterances. We use an empty contact FST to be replaced into the root LM to avoid personalized states during warm-up decoding. In order to evaluate the benefit of the proposed private cache to store the personalized language model, we group multiple utterances from a user into virtual dialog sessions of one, two, or five turns.Experiments ::: Results	Table TABREF19 shows the WER and RTF for two corpora with different pre-composition methods with ten concurrent speech recognition client requests. The private cache is freed after decoding each utterance. RTF is calculated by $t_{decode}/t_{wav}$, where $t_{decode}$ is the decoding time and $t_{wav}$ is the audio duration. We use 50th and 95th percentile values for the RTF comparison. As expected, the WER remains unchanged for the same data set. With pre-composition, the RTF for both calling and non-calling is reduced by a factor of three.Table TABREF21 shows the additional RTF improvement that can be obtained during multi-turn dialogs from the proposed private cache. When the dialog session is only a single turn, the RTF remains unchanged. However, for multi-turn sessions, additional RTF reductions are obtained for both the calling and non-calling corpora. The decoding time is reduced by a factor of five compared to a fully dynamic graph for dialog sessions of five turns.Figure FIGREF22 shows the RTF and memory usage for teh different pre-composition approaches. The upper graph shows the RTF for different steps of naive BFS using the calling data set. The figure shows that additional BFS steps improves RTF for both 50 and 95 percentiles. However, no improvement is observed beyond five steps, because the most frequent states close to the start state have already been pre-composed. The additional BFS steps only result in more memory usage. With the data-driven warmup, the RTF shows additional improvement. Furthermore, the difference in the p50 and p95 RTF values becomes much smaller than in the BFS approach.The lower graph of Figure FIGREF22 shows the memory usage as a function of the number of concurrent requests. Though the pre-composed graph may use more memory when we have only a small number of threads, the marginal memory cost for additional requests for a fully dynamic graph is roughly 1.5 times larger than for the pre-composed graph. The data-driven method has the best marginal memory efficiency for a large number of concurrent requests.Conclusions	In this work, we propose new methods for improving the efficiency of dynamic WFST decoding with personalized language models. Experimental results show that using a pre-composed graph can reduce the RTF by a factor of three compared with a fully dynamic graph. Moreover, in multi-utterance dialog sessions, the RTF can be reduced by a factor of 5 using the proposed private cache without harming WER. Though a fully dynamic graph uses less memory for the graph, the pre-composed graph has a better marginal memory cost, which is more memory efficient in large-scale production services that need to support a large number of concurrent requests.Our results also show that increasing the steps of naive BFS will not help the RTF, since it may compose infrequently encountered states, resulting in unnecessary memory usage. Using the proposed data-driven warm-up performs better in both marginal memory efficiency and RTF than naive BFS. Both pre-composition methods can also be combined.Acknoledgements	We would like to thank Mike Seltzer, Christian Fuegen, Julian Chan, and Dan Povey for useful discussions about the work.","['What does the cache consist of?', 'What languages is the model tested on?', 'What is a personalized language model?']","['Unacceptable', 'Unacceptable', 'A model that contains the expected user-specific entities.']"
3,"A system for the 2019 Sentiment, Emotion and Cognitive State Task of DARPAs LORELEI project	During the course of a Humanitarian Assistance-Disaster Relief (HADR) crisis, that can happen anywhere in the world, real-time information is often posted online by the people in need of help which, in turn, can be used by different stakeholders involved with management of the crisis. Automated processing of such posts can considerably improve the effectiveness of such efforts; for example, understanding the aggregated emotion from affected populations in specific areas may help inform decision-makers on how to best allocate resources for an effective disaster response. However, these efforts may be severely limited by the availability of resources for the local language. The ongoing DARPA project Low Resource Languages for Emergent Incidents (LORELEI) aims to further language processing technologies for low resource languages in the context of such a humanitarian crisis. In this work, we describe our submission for the 2019 Sentiment, Emotion and Cognitive state (SEC) pilot task of the LORELEI project. We describe a collection of sentiment analysis systems included in our submission along with the features extracted. Our fielded systems obtained the best results in both English and Spanish language evaluations of the SEC pilot task.	Introduction	The growing adoption of online technologies has created new opportunities for emergency information propagation BIBREF0 . During crises, affected populations post information about what they are experiencing, what they are witnessing, and relate what they hear from other sources BIBREF1 . This information contributes to the creation and dissemination of situational awareness BIBREF2 , BIBREF3 , BIBREF4 , BIBREF0 , and crisis response agencies such as government departments or public health-care NGOs can make use of these channels to gain insight into the situation as it unfolds BIBREF2 , BIBREF5 . Additionally, these organizations might also post time-sensitive crisis management information to help with resource allocation and provide status reports BIBREF6 . While many of these organizations recognize the value of the information found online—specially during the on-set of a crisis—they are in need of automatic tools that locate actionable and tactical information BIBREF7 , BIBREF0 .Opinion mining and sentiment analysis techniques offer a viable way of addressing these needs, with complementary insights to what keyword searches or topic and event extraction might offer BIBREF8 . Studies have shown that sentiment analysis of social media during crises can be useful to support response coordination BIBREF9 or provide information about which audiences might be affected by emerging risk events BIBREF10 . For example, identifying tweets labeled as “fear” might support responders on assessing mental health effects among the affected population BIBREF11 . Given the critical and global nature of the HADR events, tools must process information quickly, from a variety of sources and languages, making it easily accessible to first responders and decision makers for damage assessment and to launch relief efforts accordingly BIBREF12 , BIBREF13 . However, research efforts in these tasks are primarily focused on high resource languages such as English, even though such crises may happen anywhere in the world.The LORELEI program provides a framework for developing and testing systems for real-time humanitarian crises response in the context of low-resource languages. The working scenario is as follows: a sudden state of danger requiring immediate action has been identified in a region which communicates in a low resource language. Under strict time constraints, participants are expected to build systems that can: translate documents as necessary, identify relevant named entities and identify the underlying situation BIBREF14 . Situational information is encoded in the form of Situation Frames — data structures with fields identifying and characterizing the crisis type. The program's objective is the rapid deployment of systems that can process text or speech audio from a variety of sources, including newscasts, news articles, blogs and social media posts, all in the local language, and populate these Situation Frames. While the task of identifying Situation Frames is similar to existing tasks in literature (e.g., slot filling), it is defined by the very limited availability of data BIBREF15 . This lack of data requires the use of simpler but more robust models and the utilization of transfer learning or data augmentation techniques.The Sentiment, Emotion, and Cognitive State (SEC) evaluation task was a recent addition to the LORELEI program introduced in 2019, which aims to leverage sentiment information from the incoming documents. This in turn may be used in identifying severity of the crisis in different geographic locations for efficient distribution of the available resources. In this work, we describe our systems for targeted sentiment detection for the SEC task. Our systems are designed to identify authored expressions of sentiment and emotion towards a HADR crisis. To this end, our models are based on a combination of state-of-the-art sentiment classifiers and simple rule-based systems. We evaluate our systems as part of the NIST LoREHLT 2019 SEC pilot task.Previous Work	Social media has received a lot of attention as a way to understand what people communicate during disasters BIBREF16 , BIBREF11 . These communications typically center around collective sense-making BIBREF17 , supportive actions BIBREF18 , BIBREF19 , and social sharing of emotions and empathetic concerns for affected individuals BIBREF20 . To organize and make sense of the sentiment information found in social media, particularly those messages sent during the disaster, several works propose the use of machine learning models (e.g., Support Vector Machines, Naive Bayes, and Neural Networks) trained on a multitude of linguistic features. These features include bag of words, part-of-speech tags, n-grams, and word embeddings; as well as previously validated sentiment lexica such as Linguistic Inquiry and Word Count (LIWC) BIBREF22 , AFINN BIBREF23 , and SentiWordNet BIBREF24 . Most of the work is centered around identifying messages expressing sentiment towards a particular situation as a way to distinguish crisis-related posts from irrelevant information BIBREF25 . Either in a binary fashion (positive vs. negative) (e.g., BIBREF25 ) or over fine-grained emotional classes (e.g., BIBREF16 ).In contrast to social media posts, sentiment analysis of news articles and blogs has received less attention BIBREF26 . This can be attributed to a more challenging task due to the nature of the domain since, for example, journalists will often refrain from using clearly positive or negative vocabulary when writing news articles BIBREF27 . However, certain aspects of these communication channels are still apt for sentiment analysis, such as column pieces BIBREF28 or political news BIBREF27 , BIBREF29 .In the context of leveraging the information found online for HADR emergencies, approaches for languages other than English have been limited. Most of which are done by manually constructing resources for a particular language (e.g., in tweets BIBREF30 , BIBREF31 , BIBREF32 and in disaster-related news coverage BIBREF33 ), or by applying cross-language text categorization to build language-specific models BIBREF31 , BIBREF34 .In this work, we develop systems that identify positive and negative sentiments expressed in social media posts, news articles and blogs in the context of a humanitarian emergency. Our systems work for both English and Spanish by using an automatic machine translation system. This makes our approach easily extendable to other languages, bypassing the scalability issues that arise from the need to manually construct lexica resources.Problem Definition	This section describes the SEC task in the LORELEI program along with the dataset, evaluation conditions and metrics.The Sentiment, Emotion and Cognitive State (SEC) Task	Given a dataset of text documents and manually annotated situation frames, the task is to automatically detect sentiment polarity relevant to existing frames and identify the source and target for each sentiment instance. The source is defined as a person or a group of people expressing the sentiment, and can be either a PER/ORG/GPE (person, organization or geo political entity) construct in the frame, the author of the text document, or an entity not explicitly expressed in the document. The target toward which the sentiment is expressed, is either the frame or an entity in the document.Situation awareness information is encoded into situation frames in the LORELEI program BIBREF35 . Situation Frames (SF) are similar in nature to those used in Natural Language Understanding (NLU) systems: in essence they are data structures that record information corresponding to a single incident at a single location BIBREF15 . A SF frame includes a situation Type taken from a fixed inventory of 11 categories (e.g., medical need, shelter, infrastructure), Location where the situation exists (if a location is mentioned) and additional variables highlighting the Status of the situation (e.g., entities involved in resolution, time and urgency). An example of a SF can be found in table 1 . A list of situation frames and documents serve as input for our sentiment analysis systems.Data	Training data provided for the task included documents were collected from social media, SMS, news articles, and news wires. This consisted of 76 documents in English and 47 in Spanish. The data are relevant to the HADR domain but are not grounded in a common HADR incident. Each document is annotated for situation frames and associated sentiment by 2 trained annotators from the Linguistic Data Consortium (LDC). Sentiment annotations were done at a segment (sentence) level, and included Situation Frame, Polarity (positive / negative), Sentiment Score, Emotion, Source and Target. Sentiment labels were annotated between the values of -3 (very negative) and +3 (very positive) with 0.5 increments excluding 0. Additionally, the presence or absence of three specific emotions: fear, anger, and joy/happiness was marked. If a segment contains sentiment toward more than one target, each will be annotated separately. Summary of the training data is given in Table 2 .Evaluation	Systems participating in the task were expected to produce outputs with sentiment polarity, emotion, sentiment source and target, and the supporting segment from the input document. This output is evaluated against a ground truth derived from two or more annotations. For the SEC pilot evaluation, a reference set with dual annotations from two different annotators was provided. The system's performance was measured using variants of precision, recall and f1 score, each modified to take into account the multiple annotations. The modified scoring is as follows: let the agreement between annotators be defined as two annotations with the same sentiment polarity, source, and target. That is, consider two annotators in agreement even if their judgments vary on sentiment values or perceived emotions. Designate those annotations with agreement as “D” and those which were not agreed upon as “S”. When computing precision, recall and f measure, each of the sentiment annotations in D will count as two occurrences in the reference, and likewise a system match on a sentiment annotation in D will count as two matches. Similarly, a match on a sentiment annotation in S will count as a single match. The updated precision, recall and f-measure were defined as follows: $
\text{precision} &= \frac{2 * \text{Matches in D} + \text{Matches in S}}{2 * \text{Matches in D} + \text{Matches in S} + \text{Unmatched}}\\[10pt]
\text{recall} &= \frac{2 * \text{Matches in D} + \text{Matches in S}}{2|D| + |S|}\\[10pt]
\text{f1} &= \frac{2 * \text{precision} * \text{recall}}{(\text{precision} + \text{recall})}
$ Method	We approach the SEC task, particularly the polarity and emotion identification, as a classification problem. Our systems are based on English, and are extended to other languages via automatic machine translation (to English). In this section we present the linguistic features and describe the models using for the evaluation.Machine Translation	Automatic translations from Spanish to English were obtained from Microsoft Bing using their publicly available API. For the pilot evaluation, we translated all of the Spanish documents into English, and included them as additional training data. At this time we do not translate English to Spanish, but plan to explore this thread in future work.Linguistic Features	We extract word unigrams and bigrams. These features were then transformed using term frequencies (TF) and Inverse document-frequency (IDF).Word embeddings pretrained on large corpora allow models to efficiently leverage word semantics as well as similarities between words. This can help with vocabulary generalization as models can adapt to words not previously seen in training data. In our feature set we include a 300-dimensional word2vec word representation trained on a large news corpus BIBREF36 . We obtain a representation for each segment by averaging the embedding of each word in the segment. We also experimented with the use of GloVe BIBREF37 , and Sent2Vec BIBREF38 , an extension of word2vec for sentences.We use two sources of sentiment features: manually constructed lexica, and pre-trained sentiment embeddings. When available, manually constructed lexica are a useful resource for identifying expressions of sentiment BIBREF21 . We obtained word percentages across 192 lexical categories using Empath BIBREF39 , which extends popular tools such as the Linguistic Inquiry and Word Count (LIWC) BIBREF22 and General Inquirer (GI) BIBREF40 by adding a wider range of lexical categories. These categories include emotion classes such as surprise or disgust.Neural networks have been shown to capture specific task related subtleties which can complement the manually constructed sentiment lexica described in the previous subsection. For this work, we learn sentiment representations using a bilateral Long Short-Term Memory model BIBREF41 trained on the Stanford Sentiment Treebank BIBREF42 . This model was selected because it provided a good trade off between simplicity and performance on a fine-grained sentiment task, and has been shown to achieve competitive results to the state-of-the-art BIBREF43 .Models	We now describe the models used for this work. Our models can be broken down into two groups: our first approach explores state-of-the-art models in targeted and untargeted sentiment analysis to evaluate their performance in the context of the SEC task. These models were pre-trained on larger corpora and evaluated directly on the task without any further adaptation. In a second approach we explore a data augmentation technique based on a proposed simplification of the task. In this approach, traditional machine learning classifiers were trained to identify which segments contain sentiment towards a SF regardless of sentiment polarity. For the classifiers, we explored the use of Support Vector Machines and Random Forests. Model performance was estimated through 10-fold cross validation on the train set. Hyper-parameters, such as of regularization, were selected based on the performance on grid-search using an 10-fold inner-cross validation loop. After choosing the parameters, models were re-trained on all the available data.We consider some of the most popular baseline models in the literature: (i) minority class baseline (due to the heavily imbalanced dataset), (ii) Support Vector Machines trained on TF-IDF bi-gram language model, (iii) and Support Vector Machines trained on word2vec representations. These models were trained using English documents only.Two types of targeted sentiment are evaluated for the task: those expressed towards either a situation frame or those towards an entity. To identify sentiment expressed towards an SF, we use the pretrained model described in BIBREF44 , in which a multiplicative LSTM cell is trained at the character level on a corpus of 82 million Amazon reviews. The model representation is then fed to a logistic regression classifier to predict sentiment. This model (which we will refer to as OpenAI) was chosen since at the time of our system submission it was one of the top three performers on the binary sentiment classification task on the Stanford Sentiment Treebank. In our approach, we first map the text associated with the SF annotation with a segment from the document and pass the full segment to the pretrained OpenAI model identify the sentiment polarity for that segment.To identify sentiment targeted towards an entity, we use the recently released Target-Based Sentiment Analysis (TBSA) model from BIBREF45 . In TBSA, two stacked LSTM cells are trained to predict both sentiment and target boundary tags (e.g., predicting S-POS to indicate the start of the target towards which the author is expressing positive sentiment, I-POS and E-POS to indicate intermediate and end of the target). In our submission, since input text documents can be arbitrarily long, we only consider sentences which include a known and relevant entity; these segments are then fed to the TBSA model to predict targeted sentiment. If the target predicted by this model matched with any of the known entities, the system would output the polarity and the target.In this model we limit our focus on the task of correctly identifying those segments with sentiment towards a SF. That is, given a pair of SF and segment, we train models to identify if this segment contains any sentiment towards that SF. This allows us to expand our dataset from 123 documents into one with $\sum _d |SF_d| \times |d|$ number of samples, where $|d|$ is the length of the document (i.e., number of segments) and $|SF_d|$ is the number of SF annotations for document $d$ . Summary of the training dataset after augmentation is given in Table 3 .Given the highly skewed label distribution in the training data, a majority of the constructed pairs do not have any sentiment towards a SF. Hence, our resulting dataset has a highly imbalanced distribution which we address by training our models after setting the class weights to be the inverse class frequency. To predict polarity, we assume the majority class of negative sentiment. We base this assumption on the fact that the domain we are working with doesn't seem to support the presence of positive sentiment, as made evident by the highly imbalanced dataset.Owing to the nature of the problem domain, there is considerable variance in the source of the text documents and their structure. For example, tweets only have one segment per sample whereas news articles contain an average of $7.07\pm 4.96$ and $6.31\pm 4.93$ segments for English and Spanish documents respectively. Moreover, studies suggest that sentiments expressed in social media tend to differ significantly from those in the news BIBREF26 . Table 4 presents a breakdown of the train set for each sentiment across domains, as is evident tweets form a sizeable group of the training set. Motivated by this, we train different models for tweets and non-tweet documents in order to capture the underlying differences between the data sources.Initial experiments showed that our main source of error was not being able to correctly identify the supporting segment. Even if polarity, source and target were correctly identified, missing the correct segment was considered an error, and thus lowered our models' precision. To address this, we decided to use a model which only produced results for tweets given that these only contain one segment, making the segment identification sub-task trivial.Results	Model performance during train is presented in Table 5 . While all the models outperformed the baselines, not all of them did so with a significant margin due to the robustness of the baselines selected. The ones found to be significantly better than the baselines were models IIb (Domain-specific) and IIc (Twitter-only) (permutation test, $n = 10^5$ both $p < 0.05$ ). The difference in precision between model IIb and IIc points out to the former making the wrong predictions for news articles. These errors are most likely in selecting the wrong supporting segment. Moreover, even though models IIa-c only produce negative labels, they still achieve improved performance over the state-of-the-art systems, highlighting the highly skewed nature of the training dataset.Table 6 present the official evaluation results for English and Spanish. Some information is missing since at the time of submission only partial score had been made public. As previously mentioned, the pre-trained state-of-the-art models (model I) were directly applied to the evaluation data without any adaptation. These performed reasonably well for the English data. Among the submissions of the SEC Task pilot, our systems outperformed the other competitors for both languages.Conclusion	Understanding the expressed sentiment from an affected population during the on-set of a crisis is a particularly difficult task, especially in low-resource scenarios. There are multiple difficulties beyond the limited amount of data. For example, in order to provide decision-makers with actionable and usable information, it is not enough for the system to correctly classify sentiment or emotional state, it also ought to identify the source and target of the expressed sentiment. To provide a sense of trust and accountability on the system's decisions, it makes sense to identify a justifying segment. Moreover, these systems should consider a variety of information sources to create a broader and richer picture on how a situation unfolds. Thus, it is important that systems take into account the possible differences in the way sentiment is expressed in each one of these sources. In this work, we presented two approaches to the task of providing actionable and useful information. Our results show that state-of-the-art sentiment classifiers can be leveraged out-of-the-box for a reasonable performance on English data. By identifying possible differences coming from the information sources, as well as by exploiting the information communicated as the situation unfolds, we showed significant performance gains on both English and Spanish.","['Did they pre-train on existing sentiment corpora?', 'What were the most salient features extracted by the models?', 'How many languages are in the dataset?', 'Did the system perform well on low-resource languages?']","['Unacceptable', 'Unacceptable', '2', 'Unacceptable']"
4,"Towards Automatic Bot Detection in Twitter for Health-related Tasks	With the increasing use of social media data for health-related research, the credibility of the information from this source has been questioned as the posts may originate from automated accounts or ""bots"". While automatic bot detection approaches have been proposed, there are none that have been evaluated on users posting health-related information. In this paper, we extend an existing bot detection system and customize it for health-related research. Using a dataset of Twitter users, we first show that the system, which was designed for political bot detection, underperforms when applied to health-related Twitter users. We then incorporate additional features and a statistical machine learning classifier to significantly improve bot detection performance. Our approach obtains F_1 scores of 0.7 for the ""bot"" class, representing improvements of 0.339. Our approach is customizable and generalizable for bot detection in other health-related social media cohorts.	Introduction	In recent years, social media has evolved into an important source of information for various types of health-related research. Social networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth. Twitter, for example, has 330 million monthly active users worldwide that generate almost 500 million micro-blogs (tweets) per day. For some years, the use of the platform to share personal health information has been growing, particularly amongst people living with one or more chronic conditions and those living with disability. Twenty percent of social network site users living with chronic conditions gather and share health information on the sites, compared with 12% of social network site users who report no chronic conditions. Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject recruitment for cancer trials BIBREF2, and characterizing drug abuse BIBREF3, to name a few. Twitter is particularly popular in research due to the availability of the public streaming API, which releases a sample of publicly posted data in real time. While early health-related research from social media focused almost exclusively on population-level studies, some very recent research tasks have focused on performing longitudinal data analysis at the user level, such as mining health-related information from cohorts of pregnant women BIBREF4.When conducting user-level studies from social media, one challenge is to ascertain the credibility of the information posted. Particularly, it is important to verify, when deriving statistical estimates from user cohorts, that the user accounts represent humans and not bots (accounts that can be controlled to automatically produce content and interact with other profiles)BIBREF5, BIBREF6. Bots may spread false information by automatically retweeting posts without a human verifying the facts or to influence public opinions on particular topics on purpose BIBREF5, BIBREF7, BIBREF8. For example, a recent study BIBREF9 showed that the highest proportion of anti-vaccine content is generated by accounts with unknown or intermediate bot scores, meaning that the existing methods were not able to fully determine if they were indeed bots. Automatic bot detection techniques mostly rely on extracting features from users' profiles and their social networks BIBREF10, BIBREF11. Some studies have used Honeypot profiles on Twitter to identify and analyze bots BIBREF12, while other studies have analyzed social proximity BIBREF13 or both social and content proximities BIBREF10, tweet timing intervals BIBREF14, or user-level content-based and graph-based features BIBREF15. However, in response to efforts towards keeping Twitter bot-free, bots have evolved and changed to overcome the detection techniques BIBREF16.The objectives of this study are to (i) evaluate an existing bot detection system on user-level datasets selected for their health-related content, and (ii) extend the bot detection system for effective application within the health realm. Bot detection approaches have been published in the past few years, but most of the code and data necessary for reproducing the published results were not made available BIBREF17, BIBREF18, BIBREF19. The only system for which we found both operational code and data available, Botometer BIBREF20 (formerly BotOrNot), was chosen as the benchmark system for this study. To the best of our knowledge, this paper presents the first study on health-related bot detection. We have made the classification code and training set of annotated users available at (we will provide a URL with the camera-ready version of the paper).Methods ::: Corpus	To identify bots in health-related social media data, we retrieved a sample of $10,417$ users from a database containing more than 400 million publicly available tweets posted by more than $100,000$ users who have announced their pregnancy on Twitter BIBREF4. This sample is based on related work for detecting users who have mentioned various pregnancy outcomes in their tweets. Two professional annotators manually categorized the $10,417$ users as ""bot,"" ""non-bot,"" or ""unavailable,"" based on their publicly available Twitter sites. Users were annotated broadly as ""bot"" if, in contrast to users annotated as ""non-bot,"" they do not appear to be posting personal information. Users were annotated as ""unavailable"" if their Twitter sites could not be viewed at the time of annotation, due to modifying their privacy settings or being removed or suspended from Twitter. Based on 1000 overlapping annotations, their inter-annotator agreement (IAA) was $\kappa $ = $0.93$ (Cohen’s kappa BIBREF21), considered ""almost perfect agreement"" BIBREF22. Their IAA does not include disagreements resulting from the change of a user's status to or from ""unavailable"" in the time between the first and second annotations. Upon resolving the disagreements, 413 $(4\%)$ users were annotated as ""bot,"" 7849 $(75.35\%)$ as ""non-bot,"" and $20.69$ $(19.9\%)$ as ""unavailable"".Methods ::: Classification	We used the 8262 ""bot"" and ""non-bot"" users in experiments to train and evaluate three classification systems. We split the users into $80\%$ (training) and $20\%$ (test) sets, stratified based on the distribution of ""bot"" and ""non-bot"" users. The training set includes $61,160,686$ tweets posted by 6610 users, and the held-out test set includes $15,703,735$ tweets posted by 1652 users. First, we evaluated Botometer on our held-out test set. Botometer is a publicly available bot detection system designed for political dot detection. It outputs a score between 0 and 1 for a user, representing the likelihood that a user is a bot. Second, we used the Botometer score for each user as a feature in training a gradient boosting classifier which is a decision tree-based ensemble machine learning algorithm with gradient boosting BIBREF23 and can be used to address class imbalance. To adapt the Botometer scores to our binary classification task, we set the threshold to $0.47$, based on performing 5-fold cross validation over the training set. To further address the class imbalance, we used the Synthetic Minority Over-sampling Technique (SMOTE)BIBREF24 to create artificial instances of ""bot"" users in the training set. We also performed 5-fold cross validation over the training set to optimize parameters for the classifier; we used exponential as the loss function, set the number of estimators to 200, and set the learning rate to $0.1$. Third, we used the classifier with an extended set of features that are not used by Botometer. Based on our manual annotation, we consider the following features to be potentially informative for distinguishing ""bot"" and ""non-bot"" users in health-related data:Tweet Diversity. Considering that ""bot"" users may re-post the same tweets, we used the ratio of a user's unique tweets to the total number of tweets posted by the user, where 0 indicates that the user has posted only the same tweet multiple times, and 1 indicates that each tweet is unique and has been posted only once. As Figure 1 illustrates, a subset of ""bot"" users (in the training set) have posted more of the same tweets than ""non-bot"" users.URL score. During manual annotation, we found that ""bot"" users' tweets frequently contain URLs (e.g., advertisements for health-related products, such as medications), so we use the ratio of the number of a user's tweets containing a URL to the total number of tweets posted by the user.Mean Daily Posts. Considering that ""bot"" users may post tweets more frequently than ""non-bot"" users, we measured the average and standard deviation of the number of tweets posted daily by a user. As Figure 1 illustrates, a subset of ""bot"" users post, on average, more tweets daily than ""non-bot"" users.Topics. Considering that ""bot"" users may post tweets about a limited number of targeted topics, we used topic modeling to the measure the heterogeneity of topics in a user's tweets. We used Latent Dirichlet Allocation (LDA)BIBREF25 to extract the top five topics from all of the users' 1000 most recent tweets (or all the tweets if a user has posted less than 1000 tweets), and used the mean of the weights of each topic across all of a user's tweets.Mean Post Length. Considering that the length of tweets may be different between ""bot"" and ""non-bot"" users, we used the mean word length and standard deviation of a user's tweets.Profile Picture. In addition to tweet-related features, we used features based on information in users' profiles. Considering that a ""non-bot"" user's profile picture may be more likely to contain a face, we used a publicly available system to detect the number of faces in a profile picture. As Figure 2, illustrates a face was not detected in the profile picture of the majority of ""non-bot"" users (in the training set), whereas at least one face was detected in the profile picture of the majority of ""bot"" users.User Name. Finally, we used a publicly available lexicon to detect the presence or absence of a person's name in a user name. As Figure 2 illustrates, the name of a person is present (1) in approximately half of ""non-bot"" user names, whereas the name of a person is absent (0) in the majority of ""bot"" user names.Results	Table 1 presents the precision, recall, and F$_1$-scores for the three bot detection systems evaluated on the held-out test set. The F$_1$-score for the ""bot"" class indicates that Botometer ($0.361$), designed for political bot detection, does not generalize well for detecting ""bot"" users in health-related data. Although the classifier with only the Botometer score as a feature ($0.286$) performs even worse than the default Botometer system, our extended feature set significantly improves performance ($0.700$). For imbalanced data, a higher F$_1$-score for the majority class is typical; in this case, it reflects that we have modeled the detection of ""bot"" users based on their natural distribution in health-related data.Discussion	Our results demonstrate that (i) a publicly available bot detection system, designed for political bot detection, underperforms when applied to health-related data, and (ii) extending the system with simple features derived from health-related data significantly improves performance. An F$_1$-score of $0.700$ for the ""bot"" class represents a promising benchmark for automatic classification of highly imbalanced Twitter data and, in this case, for detecting users who are not reporting information about their own pregnancy on Twitter. Detecting such users is particularly important in the process of automatically selecting cohortsBIBREF26 from a population of social media users for user-level observational studiesBIBREF27.A brief error analysis of the 25 false negatives users (in the held-out test set of 1652 users) from the classifier with the extended feature set reveals that, while only one of the users is an account that automatically re-posts other users' tweets, the majority of the errors can be attributed to our broad definition of ""bot"" users, which includes health-related companies, organizations, forums, clubs, and support groups that are not posting personal information. These users are particularly challenging to automatically identify as ""bot"" users because, with humans posting on behalf of an online maternity store, or to a pregnancy forum, for example, their tweets resemble those posted by ""non-bot"" users. In future work, we will focus on deriving features for modeling the nuances that distinguish such ""bot"" users.Conclusion	As the use of social networks, such as Twitter, in health research is increasing, there is a growing need to validate the credibility of the data prior to making conclusions. The presence of bots in social media presents a crucial problem, particularly because bots may be customized to perpetuate specific biased or false information, or to execute advertising or marketing goals. We demonstrate that, while existing systems have been successful in detecting bots in other domains, they do not perform as well for detecting health-related bots. Using a machine learning algorithm on top of an existing bot detection system, and a set of simple derived features, we were able to significantly improve bot detection performance in health-related data. Introducing more features would likely contribute to further improving performance, which we will explore in future work.Acknowledgments	This study was funded in part by the National Library of Medicine (NLM) (grant number: R01LM011176) and the National Institute on Drug Abuse (NIDA) (grant number: R01DA046619) of the National Institutes of Health (NIH). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.","['Do the authors report results on only English datasets?', 'What are the characteristics of the dataset of Twitter users?', 'How can an existing bot detection system by customized for health-related research?', 'What type of health-related research takes place in social media?']","['Unacceptable', 'Unacceptable', 'An existing bot detection score for each user  can be used as a feature in training', 'Unacceptable']"
5,"A Simple Joint Model for Improved Contextual Neural Lemmatization	English verbs have multiple forms. For instance, talk may also appear as talks, talked or talking, depending on the context. The NLP task of lemmatization seeks to map these diverse forms back to a canonical one, known as the lemma. We present a simple joint neural model for lemmatization and morphological tagging that achieves state-of-the-art results on 20 languages from the Universal Dependencies corpora. Our paper describes the model in addition to training and decoding procedures. Error analysis indicates that joint morphological tagging and lemmatization is especially helpful in low-resource lemmatization and languages that display a larger degree of morphological complexity. Code and pre-trained models are available at https://sigmorphon.github.io/sharedtasks/2019/task2/.	Introduction	* Equal contribution. Listing order is random.Lemmatization is a core NLP task that involves a string-to-string transduction from an inflected word form to its citation form, known as the lemma. More concretely, consider the English sentence: The bulls are running in Pamplona. A lemmatizer will seek to map each word to a form you may find in a dictionary—for instance, mapping running to run. This linguistic normalization is important in several downstream NLP applications, especially for highly inflected languages. Lemmatization has previously been shown to improve recall for information retrieval BIBREF0 , BIBREF1 , to aid machine translation BIBREF2 , BIBREF3 and is a core part of modern parsing systems BIBREF4 , BIBREF5 .However, the task is quite nuanced as the proper choice of the lemma is context dependent. For instance, in the sentence A running of the bulls took place in Pamplona, the word running is its own lemma, since, here, running is a noun rather than an inflected verb. Several counter-examples exist to this trend, as discussed in depth in haspelmath2013understanding. Thus, a good lemmatizer must make use of some representation of each word's sentential context. The research question in this work is, then, how do we design a lemmatization model that best extracts the morpho-syntax from the sentential context?Recent work BIBREF7 has presented a system that directly summarizes the sentential context using a recurrent neural network to decide how to lemmatize. As N18-1126's system currently achieves state-of-the-art results, it must implicitly learn a contextual representation that encodes the necessary morpho-syntax, as such knowledge is requisite for the task. We contend, however, that rather than expecting the network to implicitly learn some notion of morpho-syntax, it is better to explicitly train a joint model to morphologically disambiguate and lemmatize. Indeed, to this end, we introduce a joint model for the introduction of morphology into a neural lemmatizer. A key feature of our model is its simplicity: Our contribution is to show how to stitch existing models together into a joint model, explaining how to train and decode the model. However, despite the model's simplicity, it still achieves a significant improvement over the state of the art on our target task: lemmatization.Experimentally, our contributions are threefold. First, we show that our joint model achieves state-of-the-art results, outperforming (on average) all competing approaches on a 20-language subset of the Universal Dependencies (UD) corpora BIBREF8 . Second, by providing the joint model with gold morphological tags, we demonstrate that we are far from achieving the upper bound on performance—improvements on morphological tagging could lead to substantially better lemmatization. Finally, we provide a detailed error analysis indicating when and why morphological analysis helps lemmatization. We offer two tangible recommendations: one is better off using a joint model (i) for languages with fewer training data available and (ii) languages that have richer morphology.Our system and pre-trained models on all languages in the latest version of the UD corpora are released at https://sigmorphon.github.io/sharedtasks/2019/task2/.Background: Lemmatization	Most languages BIBREF11 in the world exhibit a linguistic phenomenon known as inflectional morphology, which causes word forms to mutate according to the syntactic category of the word. The syntactic context in which the word form occurs determines which form is properly used. One privileged form in the set of inflections is called the lemma. We regard the lemma as a lexicographic convention, often used to better organize dictionaries. Thus, the choice of which inflected form is the lemma is motivated by tradition and convenience, e.g., the lemma is the infinitive for verbs in some Indo-European languages, Not in Latin. rather than by linguistic or cognitive concerns. Note that the stem differs from the lemma in that the stem may not be an actual inflection. In the NLP literature, the syntactic category that each inflected form encodes is called the morphological tag. The morphological tag generalizes traditional part-of-speech tags, enriching them with further linguistic knowledge such as tense, mood, and grammatical case. We call the individual key–attribute pairs morphological attributes.An example of a sentence annotated with morphological tags and lemmata in context is given in fig:sentence. The task of mapping a sentence to a sequence of morphological tags is known as morphological tagging.A Joint Neural Model	The primary contribution of this paper is a joint model of morphological tagging and lemmatization. The intuition behind the joint model is simple: high-accuracy lemmatization requires a representation of the sentential context, in which the word occurs (this behind has been evinced in sec:introduction)—a morphological tag provides the precise summary of the context required to choose the correct lemma. Armed with this, we define our joint model of lemmatization and morphological tagging as: DISPLAYFORM0  fig:model illustrates the structure of our model in the form of a graphical model. We will discuss the lemmatization factor and the morphological tagging factor following two subsections, separately. We caution the reader that the discussion of these models will be brief: Neither of these particular components is novel with respect to the literature, so the formal details of the two models is best found in the original papers. The point of our paper is to describe a simple manner to combine these existing parts into a state-of-the-art lemmatizer.Morphological Tagger: p(𝐦∣𝐰)p(\mathbf {m}\mid \mathbf {w})	We employ a simple LSTM-based tagger to recover the morphology of a sentence BIBREF12 , BIBREF13 . We also experimented with the neural conditional random field of P18-1247, but E17-1048 gave slightly better tagging scores on average and is faster to train. Given a sequence of INLINEFORM0 words INLINEFORM1 , we would like to obtain the morphological tags INLINEFORM2 for each word, where INLINEFORM3 . The model first obtains a word representation for each token using a character-level biLSTM BIBREF14 embedder, which is then input to a word-level biLSTM tagger that predicts tags for each word. Given a function cLSTM that returns the last hidden state of a character-based LSTM, first we obtain a word representation INLINEFORM4 for word INLINEFORM5 as, DISPLAYFORM0 where INLINEFORM0 is the character sequence of the word. This representation INLINEFORM1 is then input to a word-level biLSTM tagger. The word-level biLSTM tagger predicts a tag from INLINEFORM2 . A full description of the model is found in E17-1048[author=ryan,color=violet!40,size=,fancyline,caption=,]For camera ready, add citation to my paper. I removed it for anonymity.. We use standard cross-entropy loss for training this model and decode greedily while predicting the tags during test-time. Note that greedy decoding is optimal in this tagger as there is no interdependence between the tags INLINEFORM3 .A Lemmatizer: p(ℓ i ∣m i ,w i )p(\ell _i \mid m_i, w_i)	Neural sequence-to-sequence models BIBREF15 , BIBREF16 have yielded state-of-the-art performance on the task of generating morphological variants—including the lemma—as evinced in several recent shared tasks on the subject BIBREF17 , BIBREF18 , BIBREF19 . Our lemmatization factor in eq:joint is based on such models. Specifically, we make use of a hard-attention mechanism BIBREF20 , BIBREF21 , rather than the original soft-attention mechanism. Our choice of hard attention is motivated by the performance of K18-3008's system at the CoNLL-SIGMORPHON task. We use a nearly identical model, but opt for an exact dynamic-programming-based inference scheme BIBREF22 .We briefly describe the model here. Given an inflected word INLINEFORM0 and a tag INLINEFORM1 , we would like to obtain the lemma INLINEFORM2 , dropping the subscript for simplicity. Moreover, for the remainder of this section the subscripts will index into the character string INLINEFORM3 , that is INLINEFORM4 , where each INLINEFORM5 . A character-level biLSTM encoder embeds INLINEFORM6 to INLINEFORM7 . The decoder LSTM produces INLINEFORM8 , reading the concatenation of the embedding of the previous character INLINEFORM9 and the tag embedding INLINEFORM10 , which is produced by an order-invariant linear function. In contrast to soft attention, hard attention models the alignment distribution explicitly.We denote INLINEFORM0 as the set of all monotonic alignments from INLINEFORM1 to INLINEFORM2 where an alignment aligns each target character INLINEFORM3 to exactly one source character in INLINEFORM4 and for INLINEFORM5 , INLINEFORM6 refers to the event that the INLINEFORM7 character of INLINEFORM8 is aligned to the INLINEFORM9 character of INLINEFORM10 . We factor the probabilistic lemmatizer as, DISPLAYFORM0 The summation is computed with dynamic programming—specifically, using the forward algorithm for hidden Markov models BIBREF23 . INLINEFORM0 is a two-layer feed-forward network followed by a softmax. The transition INLINEFORM1 is the multiplicative attention function with INLINEFORM2 and INLINEFORM3 as input. To enforce monotonicity, INLINEFORM4 if INLINEFORM5 .Decoding	We consider two manners, by which we decode our model. The first is a greedy decoding scheme. The second is a crunching BIBREF24 scheme. We describe each in turn.In the greedy scheme, we select the best morphological tag sequence DISPLAYFORM0 and then decode each lemmata DISPLAYFORM0 Note that we slightly abuse notation since the argmax here is approximate: exact decoding of our neural lemmatizer is hard. This sort of scheme is also referred to as pipeline decoding.In the crunching scheme, we first extract a INLINEFORM0 -best list of taggings from the morphological tagger. For an input sentence INLINEFORM1 , call the INLINEFORM2 -best tags for the INLINEFORM3 word INLINEFORM4 . Crunching then says we should decode in the following manner DISPLAYFORM0  Crunching is a tractable heuristic that approximates true joint decoding and, as such, we expect it to outperform the more naïve greedy approach.Training with Jackknifing	In our model, a simple application of maximum-likelihood estimation (MLE) is unlikely to work well. The reason is that our model is a discriminative directed graphical model (as seen in fig:model) and, thus, suffers from exposure bias BIBREF25 . The intuition behind the poor performance of MLE is simple: the output of the lemmatizer depends on the output of the morphological tagger; as the lemmatizer has only ever seen correct morphological tags, it has never learned to adjust for the errors that will be made at the time of decoding. To compensate for this, we employ jackknifing BIBREF26 , which is standard practice in many NLP pipelines, such as dependency parsing.Jackknifing for training NLP pipelines is quite similar to the oft-employed cross-validation. We divide our training data into INLINEFORM0 splits. Then, for each split INLINEFORM1 , we train the morphological tagger on the INLINEFORM2 split, and then decode it, using either greedy decoding or crunching, on the remaining INLINEFORM3 splits. This technique helps avoid exposure bias and improves the lemmatization performance, which we will demonstrate empirically in sec:exp. Indeed, the model is quite ineffective without this training regime. Note that we employ jackknifing for both the greedy decoding scheme and the crunching decoding scheme.Dataset	To enable a fair comparison with N18-1126, we use the Universal Dependencies Treebanks BIBREF8 for all our experiments. Following previous work, we use v2.0 of the treebanks for all languages, except Dutch, for which v2.1 was used due to inconsistencies in v2.0. The standard splits are used for all treebanks.Training Setup and Hyperparameters	For the morphological tagger, we use the baseline implementation from P18-1247. This implementation uses an input layer and linear layer dimension of 128 and a 2-layer LSTM with a hidden layer dimension of 256. The Adam BIBREF27 optimizer is used for training and a dropout rate BIBREF28 of 0.3 is enforced during training. The tagger was trained for 10 epochs.For the lemmatizer, we use a 2-layer biLSTM encoder and a 1-layer LSTM decoder with 400 hidden units. The dimensions of character and tag embedding are 200 and 40, respectively. We enforce a dropout rate of 0.4 in the embedding and encoder LSTM layers. The lemmatizer is also trained with Adam and the learning rate is 0.001. We halve the learning rate whenever the development log-likelihood increases and we perform early-stopping when the learning rate reaches INLINEFORM0 . We apply gradient clipping with a maximum gradient norm of 5.Baselines (and Related Work)	We compare our approach against recent competing methods that report results on UD datasets.The current state of the art is held by N18-1126, who, as discussed in sec:introduction, provide a direct context-to-lemma approach, avoiding the use of morphological tags. We remark that N18-1126 assume a setting where lemmata are annotated at the token level, but morphological tags are not available; we contend, however, that such a setting is not entirely realistic as almost all corpora annotated with lemmata at the token level include morpho-syntactic annotation, including the vast majority of the UD corpora. Thus, we do not consider it a stretch to assume the annotation of morphological tags to train our joint model.Our next baseline is the UDPipe system of K17-3009. Their system performs lemmatization using an averaged perceptron tagger that predicts a (lemma rule, UPOS) pair. Here, a lemma rule generates a lemma by removing parts of the word prefix/suffix and prepending and appending a new prefix/suffix. A guesser first produces correct lemma rules and the tagger is used to disambiguate from them.The strongest non-neural baseline we consider is the system of D15-1272, who, like us, develop a joint model of morphological tagging lemmatization. In contrast to us, however, their model is globally normalized BIBREF29 . Due to their global normalization, they directly estimate the parameters of their model with MLE without worrying about exposure bias. However, in order to efficiently normalize the model, they heuristically limit the set of possible lemmata through the use of edit trees BIBREF30 , which makes the computation of the partition function tractable.Much like D15-1272, Morfette relies on the concept of edit trees. However, a simple perceptron is used for classification with hand-crafted features. A full description of the model is given in grzegorz2008learning.Results and Discussion	Experimentally, we aim to show three points. i) Our joint model (eq:joint) of morphological tagging and lemmatization achieves state-of-the-art accuracy; this builds on the findings of N18-1126, who show that context significantly helps neural lemmatization. Moreover, the upper bound for contextual lemmatizers that make use of morphological tags is much higher, indicating room for improved lemmatization with better morphological taggers. ii) We discuss a number of error patterns that the model seems to make on the languages, where absolute accuracy is lowest: Latvian, Estonian and Arabic. We suggest possible paths forward to improve performance. iii) We offer an explanation for when our joint model does better than the context-to-lemma baseline. We show through a correlational study that our joint approach with morphological tagging helps the most in two cases: low-resource languages and morphologically rich languages.Main Results	The first experiment we run focuses on pure performance of the model. Our goal is to determine whether joint morphological tagging and lemmatization improves average performance in a state-of-the-art neural model.For measuring lemmatization performance, we measure the accuracy of guessing the lemmata correctly over an entire corpus. To demonstrate the effectiveness of our model in utilizing context and generalizing to unseen word forms, we follow N18-1126 and also report accuracies on tokens that are i) ambiguous, i.e., more than one lemmata exist for the same inflected form, ii) unseen, i.e., where the inflected form has not been seen in the training set, and iii) seen unambiguous, i.e., where the inflected form has only one lemma and is seen in the training set.The results showing comparisons with all other methods are summarized in fig:results. Each bar represents the average accuracy across 20 languages. Our method achieves an average accuracy of INLINEFORM0 and the strongest baseline, N18-1126, achieves an average accuracy of INLINEFORM1 . The difference in performance ( INLINEFORM2 ) is statistically significant with INLINEFORM3 under a paired permutation test. We outperform the strongest baseline in 11 out of 20 languages and underperform in only 3 languages with INLINEFORM4 . The difference between our method and all other baselines is statistical significant with INLINEFORM5 in all cases. We highlight two additional features of the data. First, decoding using gold morphological tags gives an accuracy of INLINEFORM6 for a difference in performance of INLINEFORM7 . We take the large difference between the upper bound and the current performance of our model to indicate that improved morphological tagging is likely to significantly help lemmatization. Second, it is noteworthy that training with gold tags, but decoding with predicted tags, yields performance that is significantly worse than every baseline except for UDPipe. This speaks for the importance of jackknifing in the training of joint morphological tagger-lemmatizers that are directed and, therefore, suffer from exposure bias.In fig:crunching, we observed crunching further improves performance of the greedy decoding scheme. In 8 out of 20 languages, the improvement is statistical significant with INLINEFORM0 . We select the best INLINEFORM1 for each language based on the development set.In fig:error-analysis, we provide a language-wise breakdown of the performance of our model and the model of N18-1126. Our strongest improvements are seen in Latvian, Greek and Hungarian. When measuring performance solely over unseen inflected forms, we achieve even stronger gains over the baseline method in most languages. This demonstrates the generalization power of our model beyond word forms seen in the training set. In addition, our accuracies on ambiguous tokens are also seen to be higher than the baseline on average, with strong improvements on highly inflected languages such as Latvian and Russian. Finally, on seen unambiguous tokens, we note improvements that are similar across all languages.Error Patterns	We attempt to identify systematic error patterns of our model in an effort to motivate future work. For this analysis, we compare predictions of our model and the gold lemmata on three languages with the weakest absolute performance: Estonian, Latvian and Arabic. First, we note the differences in the average lengths of gold lemmata in the tokens we guess incorrectly and all the tokens in the corpus. The lemmata we guess incorrectly are on average 1.04 characters longer than the average length of all the lemmata in the corpus. We found that the length of the incorrect lemmata does not correlate strongly with their frequency. Next, we identify the most common set of edit operations in each language that would transform the incorrect hypothesis to the gold lemma. This set of edit operations was found to follow a power-law distribution.For the case of Latvian, we find that the operation {replace: s INLINEFORM0 a} is the most common error made by our model. This operation corresponds to a possible issue in the Latvian treebank, where adjectives were marked with gendered lemmas. This issue has now been resolved in the latest version of the treebank. For Estonian, the operation {insert: m, insert: a} is the most common error. The suffix -ma in Estonian is used to indicate the infinitive form of verbs. Gold lemmata for verbs in Estonian are marked in their infinitive forms whereas our system predicts the stems of these verbs instead. These inflected forms are usually ambiguous and we believe that the model doesn't generalize well to different form-lemma pairs, partly due to fewer training data available for Estonian. This is an example of an error pattern that could be corrected using improved morphological information about the tokens. Finally, in Arabic, we find that the most common error pattern corresponds to a single ambiguous word form, 'an , which can be lemmatized as 'anna (like “that” in English) or 'an (like “to” in English) depending on the usage of the word in context. The word 'anna must be followed by a nominal sentence while 'an is followed by a verb. Hence, models that can incorporate rich contextual information would be able to avoid such errors.Why our model performs better?	Simply presenting improved results does not entirely satiate our curiosity: we would also like to understand why our model performs better. Specifically, we have assumed an additional level of supervision—namely, the annotation of morphological tags. We provide the differences between our method and our retraining of the Lematus system presented in tab:diffs. In addition to the performance of the systems, we also list the number of tokens in each treebank and the number of distinct morphological tags per language. We perform a correlational study, which is shown in tab:correlations.We see that there is a moderate positive correlation ( INLINEFORM0 ) between the number of morphological tags in a language and the improvement our model obtains. As we take the number of tags as a proxy for the morphological complexity in the language, we view this as an indication that attempting to directly extract the relevant morpho-syntactic information from the corpus is not as effective when there is more to learn. In such languages, we recommend exploiting the additional annotation to achieve better results.The second correlation we find is a stronger negative correlation ( INLINEFORM0 ) between the number of tokens available for training in the treebank and the gains in performance of our model over the baseline. This is further demonstrated by the learning curve plot in fig:learning, where we plot the validation accuracy on the Polish treebank for different sizes of the training set. The gap between the performance of our model and Lematus-ch20 is larger when fewer training data are available, especially for ambiguous tokens. This indicates that the incorporation of morphological tags into a model helps more in the low-resource setting. Indeed, this conclusion makes sense—neural networks are good at extracting features from text when there is a sufficiently large amount of data. However, in the low-resource case, we would expect direct supervision on the sort of features we desire to extract to work better. Thus, our second recommendation is to model tags jointly with lemmata when fewer training tokens are available. As we noted earlier, it is almost always the case that token-level annotation of lemmata comes with token-level annotation of morphological tags. In low-resource scenarios, a data augmentation approach such as the one proposed by BIBREF31 can be helpful and serve complementary to our approach.Conclusion	We have presented a simple joint model for morphological tagging and lemmatization and discussed techniques for training and decoding. Empirically, we have shown that our model achieves state-of-the-art results, hinting that explicitly modeling morphological tags is a more effective manner for modeling context. In addition to strong numbers, we tried to explain when and why our model does better. Specifically, we show a significant correlation between our scores and the number of tokens and tags present in a treebank. We take this to indicate that our method improves performance more for low-resource languages as well as morphologically rich languages.Acknowledgments	We thank Toms Bergmanis for his detailed feedback on the accepted version of the manuscript. Additionally, we would like to thank the three anonymous reviewers for their valuable suggestions. The last author would like to acknowledge support from a Facebook Fellowship.Additional Results	We present the exact numbers on all languages to allow future papers to compare to our results in tab:dev and tab:test.","['what previous work do they also look at?', 'what languages did they experiment with?']","['Unacceptable', 'They experiment with: arabic, basque, croatian, dutch, estonian, finnish, german, greek, hindi, hungarian, italian, latvian, polish, portuguese, romanian, russian, slovak, slovenian, turkish and urdu.']"
6,"Sequence Labeling Parsing by Learning Across Representations	We use parsing as sequence labeling as a common framework to learn across constituency and dependency syntactic abstractions. To do so, we cast the problem as multitask learning (MTL). First, we show that adding a parsing paradigm as an auxiliary loss consistently improves the performance on the other paradigm. Secondly, we explore an MTL sequence labeling model that parses both representations, at almost no cost in terms of performance and speed. The results across the board show that on average MTL models with auxiliary losses for constituency parsing outperform single-task ones by 1.05 F1 points, and for dependency parsing by 0.62 UAS points.	Introduction	Constituency BIBREF0 and dependency grammars BIBREF1 , BIBREF2 are the two main abstractions for representing the syntactic structure of a given sentence, and each of them has its own particularities BIBREF3 . While in constituency parsing the structure of sentences is abstracted as a phrase-structure tree (see Figure FIGREF6 ), in dependency parsing the tree encodes binary syntactic relations between pairs of words (see Figure FIGREF6 ).When it comes to developing natural language processing (nlp) parsers, these two tasks are usually considered as disjoint tasks, and their improvements therefore have been obtained separately BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 .Despite the potential benefits of learning across representations, there have been few attempts in the literature to do this. klein2003fast considered a factored model that provides separate methods for phrase-structure and lexical dependency trees and combined them to obtain optimal parses. With a similar aim, ren2013combine first compute the n best constituency trees using a probabilistic context-free grammar, convert those into dependency trees using a dependency model, compute a probability score for each of them, and finally rerank the most plausible trees based on both scores. However, these methods are complex and intended for statistical parsers. Instead, we propose a extremely simple framework to learn across constituency and dependency representations.Learning across representations	To learn across representations we cast the problem as multi-task learning. mtl enables learning many tasks jointly, encapsulating them in a single model and leveraging their shared representation BIBREF12 , BIBREF22 . In particular, we will use a hard-sharing architecture: the sentence is first processed by stacked bilstms shared across all tasks, with a task-dependent feed-forward network on the top of it, to compute each task's outputs. In particular, to benefit from a specific parsing abstraction we will be using the concept of auxiliary tasks BIBREF23 , BIBREF24 , BIBREF25 , where tasks are learned together with the main task in the mtl setup even if they are not of actual interest by themselves, as they might help to find out hidden patterns in the data and lead to better generalization of the model. For instance, BIBREF26 have shown that semantic parsing benefits from that approach.The input is the same for both types of parsing and the same number of timesteps are required to compute a tree (equal to the length of the sentence), which simplifies the joint modeling. In this work, we focus on parallel data (we train on the same sentences labeled for both constituency and dependency abstractions). In the future, we plan to explore the idea of exploiting joint training over disjoint treebanks BIBREF27 .Baselines and models	We test different sequence labeling parsers to determine whether there are any benefits in learning across representations. We compare: (i) a single-task model for constituency parsing and another one for dependency parsing, (ii) a multi-task model for constituency parsing (and another for dependency parsing) where each element of the 3-tuple is predicted as a partial label in a separate subtask instead of as a whole, (iii) different mtl models where the partial labels from a specific parsing abstraction are used as auxiliary tasks for the other one, and (iv) an mtl model that learns to produce both abstractions as main tasks.For constituency parsing, we use the single-task model by BIBREF10 . The input is the raw sentence and the output for each token a single label of the form INLINEFORM0 = INLINEFORM1 . For dependency parsing we use the model by BIBREF11 to predict a single dependency label of the form INLINEFORM2 = INLINEFORM3 for each token.For constituency parsing, instead of predicting a single label output of the form INLINEFORM0 , we generate three partial and separate labels INLINEFORM1 , INLINEFORM2 and INLINEFORM3 through three task-dependent feed-forward networks on the top of the stacked bilstms. This is similar to BIBREF28 . For dependency parsing, we propose in this work a mtl version too. We observed in preliminary experiments, as shown in Table TABREF14 , that casting the problem as 3-task learning led to worse results. Instead, we cast it as a 2-task learning problem, where the first task consists in predicting the head of a word INLINEFORM4 , i.e. predicting the tuple INLINEFORM5 , and the second task predicts the type of the relation INLINEFORM6 . The loss is here computed as INLINEFORM7 = INLINEFORM8 , where INLINEFORM9 is the partial loss coming from the subtask INLINEFORM10 .We predict the partial labels from one of the parsing abstractions as main tasks. The partial labels from the other parsing paradigm are used as auxiliary tasks. The loss is computed as INLINEFORM0 = INLINEFORM1 , where INLINEFORM2 is an auxiliary loss and INLINEFORM3 its specific weighting factor. Figure FIGREF17 shows the architecture used in this and the following multi-paradigm model.All tasks are learned as main tasks instead.Data	In the following experiments we use two parallel datasets that provide syntactic analyses for both dependency and constituency parsing.For the evaluation on English language we use the English Penn Treebank BIBREF40 , transformed into Stanford dependencies BIBREF41 with the predicted PoS tags as in BIBREF32 .We also use the spmrl datasets, a collection of parallel dependency and constituency treebanks for morphologically rich languages BIBREF42 . In this case, we use the predicted PoS tags provided by the organizers. We observed some differences between the constituency and dependency predicted input features provided with the corpora. For experiments where dependency parsing is the main task, we use the input from the dependency file, and the converse for constituency, for comparability with other work. d-mtl models were trained twice (one for each input), and dependency and constituent scores are reported on the model trained on the corresponding input.We use bracketing F-score from the original evalb and eval_spmrl official scripts to evaluate constituency trees. For dependency parsing, we rely on las and uas scores where punctuation is excluded in order to provide a homogeneous setup for PTB and SPMRL.Results	Table TABREF18 compares single-paradigm models against their double-paradigm mtl versions. On average, mtl models with auxiliary losses achieve the best performance for both parsing abstractions. They gain INLINEFORM0 F1 points on average in comparison with the single model for constituency parsing, and INLINEFORM1 uas and INLINEFORM2 las points for dependency parsing. In comparison to the single-paradigm MTL models, the average gain is smaller: 0.05 f1 points for constituency parsing, and 0.09 uas and 0.21 las points for dependency parsing.mtl models that use auxiliary tasks (d-mtl-aux) consistently outperform the single-task models (s-s) in all datasets, both for constituency parsing and for dependency parsing in terms of uas. However, this does not extend to las. This different behavior between uas and las seems to be originated by the fact that 2-task dependency parsing models, which are the basis for the corresponding auxiliary task and mtl models, improve uas but not las with respect to single-task dependency parsing models. The reason might be that the single-task setup excludes unlikely combinations of dependency labels with PoS tags or dependency directions that are not found in the training set, while in the 2-task setup, both components are treated separately, which may be having a negative influence on dependency labeling accuracy.In general, one can observe different range of gains of the models across languages. In terms of uas, the differences between single-task and mtl models span between INLINEFORM0 (Basque) and INLINEFORM1 (Hebrew); for las, INLINEFORM2 and INLINEFORM3 (both for Hebrew); and for F1, INLINEFORM4 (Hebrew) and INLINEFORM5 (Korean). Since the sequence labeling encoding used for dependency parsing heavily relies on PoS tags, the result for a given language can be dependent on the degree of the granularity of its PoS tags.In addition, Table TABREF19 provides a comparison of the d-mtl-aux models for dependency and constituency parsing against existing models on the PTB test set. Tables TABREF20 and TABREF21 shows the results for various existing models on the SPMRL test sets.Table TABREF22 shows the speeds (sentences/second) on a single core of a CPU. The d-mtl setup comes at almost no added computational cost, so the very good speed-accuracy tradeoff already provided by the single-task models is improved.Conclusion	We have described a framework to leverage the complementary nature of constituency and dependency parsing. It combines multi-task learning, auxiliary tasks, and sequence labeling parsing, so that constituency and dependency parsing can benefit each other through learning across their representations. We have shown that mtl models with auxiliary losses outperform single-task models, and mtl models that treat both constituency and dependency parsing as main tasks obtain strong results, coming almost at no cost in terms of speed. Source code will be released upon acceptance.Acknowlegments	This work has received funding from the European Research Council (ERC), under the European Union's Horizon 2020 research and innovation programme (FASTPARSE, grant agreement No 714150), from the ANSWER-ASAP project (TIN2017-85160-C2-1-R) from MINECO, and from Xunta de Galicia (ED431B 2017/01).Model parameters	The models were trained up to 150 iterations and optimized with Stochastic Gradient Descent (SGD) with a batch size of 8. The best model for constituency parsing was chosen with the highest achieved F1 score on the development set during the training and for dependency parsing with the highest las score. The best double paradigm, multi-task model was chosen based on the highest harmonic mean among las and F1 scores.Table TABREF30 shows model hyperparameters.","['Which labeling scheme do they use?', 'What parts of their multitask model are shared?', 'Which dataset do they use?']","['Unacceptable', 'Unacceptable', 'Unacceptable']"
7,"Prototype-to-Style: Dialogue Generation with Style-Aware Editing on Retrieval Memory	The ability of a dialog system to express prespecified language style during conversations has a direct, positive impact on its usability and on user satisfaction. We introduce a new prototype-to-style (PS) framework to tackle the challenge of stylistic dialogue generation. The framework uses an Information Retrieval (IR) system and extracts a response prototype from the retrieved response. A stylistic response generator then takes the prototype and the desired language style as model input to obtain a high-quality and stylistic response. To effectively train the proposed model, we propose a new style-aware learning objective as well as a de-noising learning strategy. Results on three benchmark datasets from two languages demonstrate that the proposed approach significantly outperforms existing baselines in both in-domain and cross-domain evaluations	Introduction	Most early research on dialogue response generation focused on generating grammatical and contextually relevant responses BIBREF0, BIBREF1, BIBREF2. While promising results have been demonstrated BIBREF3, BIBREF4, syntactically coherent responses alone do not guarantee an engaging and attractive dialogue system. Expressing a unique and consistent speaking style has been shown to be crucial for increasing the user's engagement with dialogue systems BIBREF5. There are various definitions of language style BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10. In this work, from a purely computational standpoint, we refer to language style as any characteristic style of expression. Hence, our work is in line with previous work on dialogue generation with emotion BIBREF11, BIBREF12, BIBREF13, BIBREF14; response attitude BIBREF15, and speaker personality BIBREF16.The aforementioned approaches explicitly incorporate the language style information into the model configuration either via embeddings or memory modules to control the process of response generation. In our replication experiments, we found that these approaches tend to overemphasise the importance of the language style. As a result, the generated responses tend to be generic and non-informative BIBREF17, but they do express a distinct style; e.g., they generate a generic response: “I am happy to hear that."" that conveys a `happy' emotion to different queries.In this work, we propose a novel prototype-to-style (PS) framework to tackle the challenge of stylistic dialogue generation. Our motivation is two-fold: (1) Human-written responses are informative and diverse, which could be leveraged as guidance for the generation model; (2) However, the retrieved response is not guaranteed to express the desired language style. Moreover, the quality of the retrieved response varies among different queries due to the instability of the IR system. Therefore, to transform the retrieved result into a relevant and stylistic response, an adequate editing process is necessary.An illustration of the proposed framework is shown in Figure FIGREF2, where a prototype is first extracted from the retrieved response. The stylistic response generator then takes the desired language style and the extracted prototype as additional input to obtain an adequate and stylistic response. The proposed stylistic response generator mainly inherits from the GPT-2 model BIBREF18 which is pre-trained with a large unlabeled text corpus. However, the GPT-2 model does not naturally fit the task of dialogue generation. To this end, we design various adaptations to the model architecture to extend the GPT-2 model to address the task of dialogue generation. Furthermore, in order to control the style of the generated responses, we train the model with a novel style-aware maximum likelihood estimation (MLE) objective that encodes additional style knowledge into the model's parameters. Finally, to mitigate the possible effect that the retrieved response containing irrelevant and inappropriate information with respect to the input query, we adopt a de-noising learning strategy BIBREF19, BIBREF20 to prevent the model from uncritically copying the prototype.To fully evaluate the proposed approach, we conduct extensive experiments on three benchmark datasets. Results of both human and automatic evaluation show that the proposed approach significantly outperforms several strong baselines. In addition, we also conduct an extensive cross-domain experiment to demonstrate that the proposed approach is more robust than such baselines.It should be noted that stylistic dialogue generation is different from the task of text style transfer. Text style transfer aims to rewrite the input sentences such that they possess certain language styles, while rigorously preserving their semantic meaning BIBREF21. On the other hand, stylistic dialogue generation does not aim at preserving the semantic meaning of the input sentences. Instead, it aims at generating sentences that are adequate and relevant responses to the input sentences, while expressing the prespecified language styles.In summary, the contributions of this work are: (1) We propose a novel framework that tackles the challenge of stylistic dialogue generation by leveraging useful information contained in the retrieved responses; (2) We propose a new stylistic response generator by making proper adaptations to a large-scale pre-trained language model. We train our model with a new style-aware learning objective in a de-noising manner. Experiments show that the proposed model outperforms many strong baselines on three benchmark datasets on both in-domain and cross-domain evaluations.Related Work	We summarize three categories of relevant work in the following.Related Work ::: Text Style Transfer:	The task of text style transfer aims to transfer the style contained in a sentence while preserving its meaning. BIBREF22 proposed a DRG framework to tackle this task with the help of external knowledge. Recently, based on the pre-trained language model, BIBREF23 further improved the system performance under the same DRG framework.Related Work ::: Retrieval Guided Dialogue Generation:	Many prior works BIBREF24, BIBREF25, BIBREF26, BIBREF27 proposed to leverage information from the retrieved responses to improve the system performance on non-task oriented dialogue generation. It should be noted that all these approaches aim to improve the content quality of the generated responses but do not take the style aspect into consideration.Related Work ::: Stylistic Dialogue Generation:	Extensive research has tried to tackle the task of stylistic dialogue generation. BIBREF16 proposed to represent the user's personality with embeddings and incorporated them into the decoder structure to control the response generation process. BIBREF15 used reinforcement learning to train the generation model via the interaction with a pre-trained classifier to generate responses with specified attitude. BIBREF11, BIBREF12, BIBREF13, BIBREF14 incorporated external knowledge into the model architecture either via embeddings or internal and external memory modules, such that during the generation process, emotion-based styles can be dynamically controlled. BIBREF28 proposed to use a shared latent space for stylistic dialogue generation.Methodology	The proposed framework leverages the results acquired from an IR system, A major challenge is that the retrieved response is not guaranteed to express the desired language style. At the first step, a neutral response prototype is extracted by masking all stylistic words contained in the retrieved response. A stylistic response generator then takes the desired language style and the extracted prototype as additional input to generate an adequate and stylistic response to the input query. To better emphasize the generation of stylistic expressions, we propose a style-aware learning objective. Finally, to prevent the model from learning to uncritically copy the prototype, we adopt a de-noising learning strategy BIBREF19, BIBREF20 to train the generator.Methodology ::: Prototype Extraction	The response prototype is constructed from the retrieved response by masking the stylistic words. To determine whether a word is stylistic, we use the pointwise mutual information (PMI) BIBREF29 metric. The relevance between the word $x$ and the style $s$ is measured aswhere $p(x, s)$ is the frequency that the word $x$ appears in a response with style $s$ in the training corpus. And a word $x$ is stylistic given the style $s$ if $\textup {PMI}(x,s)\ge t_s$. In our experiments, we empirically set $t_s$ as $t_s = \frac{3}{4}\times \max _{v\in \mathcal {V}}\textup {PMI}(v; s)$, where $\mathcal {V}$ is the vocabulary set of the training corpus. Given the set of all possible language styles $\mathcal {S}$, the stylistic vocabulary $\mathcal {SV}$ is defined as all words that express any style $s\in \mathcal {S}$. An example is provided in Figure FIGREF2 where the prototype: “That's _ . I will go with my _ together !” is extracted from the retrieved response by masking the stylistic words great, bro and buddies.Methodology ::: Stylistic Response Generator	The proposed Stylistic Response Generator inherits from the GPT-2 BIBREF18 model which consists of a 12-layer decoder-only Transformer BIBREF30. To make use of the GPT-2 model, the input tokens must be a consecutive natural sequence (e.g. sentence, document). Based on the input sequence, the input representation is constructed by adding up the token embeddings and the corresponding position embeddings.To achieve the goal of adapting the GPT-2 model under the proposed PS framework, we first make modifications to the form of the input sequence. As shown in Figure FIGREF6, we construct the input sequence as the concatenation of the input query, the response prototype and the reference response. Then we introduce a special token $[B]$ to indicate the boundary between these three parts. To further ensure the model can identify the different parts of the input sequence, we introduce a new segment level input which consists of three learnable segment embeddings $E_Q$, $E_P$ and $E_R$ to indicate the positions of the input query, the response prototype and the response history. To control the language style of the generated response, we propose to incorporate learnable style embeddings into the input representation. Specifically, we add the style embeddings to the entire part of the response history. This way, the model is constantly aware of the desired language style through the entire generation process.Methodology ::: Learning ::: Style-Aware Learning Objective	We propose to use a new style-aware learning objective to train the stylistic response generator. Consider a training instance consists of the input query ${\bf X} = (x_1, ..., x_N)$, the reference response ${\bf Y} = (y_1, ..., y_T)$, the reference language style $s$ and the response prototype ${\bf C} = (c_1, ..., c_T)$, the proposed objective is defined aswhere $\theta $ are the model parameters and $\mathcal {SV}$ is the stylistic vocabulary introduced in SV. By increasing $\alpha $, the proposed objective encodes more knowledge about stylistic expressions into the model parameters.We find that including the language model as an auxiliary objective in addition to the supervised style-aware learning objective helps to improve generalization as well as accelerate convergence. This observation is in line with BIBREF31, BIBREF32. In this work, the language model objective is defined as the reconstruction loss of the input query based on itself:The final learning objective is then defined aswhere $\beta $ regulates the importance of the auxiliary objective.Methodology ::: Learning ::: De-noising Training	We use a de-noising training strategy similar to DBLP:conf/nips/JainS08, DBLP:conf/cvpr/KrullBJ19 for training data construction, as shown in Figure FIGREF17. Specifically, during training, the response prototype is extracted from the reference response by the following steps. First, we mask all the stylistic words in the reference response. Second, we randomly select some words (40%) and replace it with a special token [MASK] or a random word drawn from the vocabulary.The second step is necessary otherwise the model will learn to generate a response by uncritically copying the response prototype, since the prototype after the first step is always an integral part of the golden response. This copy mechanism is undesirable since during testing the retrieved response is likely to contain information that is irrelevant to the input query. Thus, we deliberately train the response generator with noisy input to let the model learn to filter out the inappropriate information contained in the response prototype.Datasets	We conduct extensive experiments on three dialogue datasets: gender-specific (Chinese) dataset, emotion-specific (Chinese) dataset, and sentiment-specific (English) dataset. For each dataset, we randomly select 200 instances as a held-out test set for evaluation.Datasets ::: Gender-Specific Dialogue Dataset	We use a publicly available gender-specific dialogue dataset BIBREF33. In this dataset, each response contains one specific gender preference including Female, Male and Neutral.Datasets ::: Emotion-Specific Dialogue Dataset	We use a publicly available emotion-specific dataset BIBREF11 which contains responses with 6 different emotions including Like, Disgust, Happy, Anger, Sad and Other.Datasets ::: Sentiment-Specific Dialogue Dataset	To construct this dataset, we first build a classifier on the basis of BERT BIBREF34 and finetuned it on the the SemEval-2017 Subtask A dataset BIBREF35. This dataset consists of twitter instances with different sentiments including Positive, Negative and Neutral.The sentiment classifier attains 81.4% classification accuracy which is further used to annotate the OpenSubtitles dataset BIBREF36. The data statistic of the resulting sentiment-specific dialogue dataset is shown in Table TABREF21.Experiments ::: Pretraining and Implementation Details	As there is no off-the-shelf pre-trained word-level language model in Chinese, we manually pre-trained one. The corpus collection and model pre-training details are presented in the supplementary material. For the English pre-trained language model, we use the PyTorch adaptation released by the HuggingFace team.To optimize the model, we use the Adam optimizer BIBREF37 with a batch size of 64 and learning rate of 2e-5. During inference, the retrieval system is built from the training corpus, and the retrieved responses are selected using the Jaccard similarity BIBREF38 between queries.During the inference stage, we retrieve the candidates from the training set. Specifically, we employ Jacquard Similarity to calculate the similarity between the input query q and queries in training set and find the most similar query q$^\prime $. Then we directly adopt the response of the retrieved query q$^\prime $ to construct the response prototype.Experiments ::: Model Comparison	We compare the proposed approach with several competitive baselines that can be categorized into two classes: generative approaches and retrieval-based approaches.Experiments ::: Model Comparison ::: Generative Approaches ::: Seq2seq:	Standard sequence-to-sequence model with attention mechanism BIBREF39, BIBREF40.Experiments ::: Model Comparison ::: Generative Approaches ::: GPT2-FT:	To examine the effect of leveraging the pre-trained language model for the task of dialogue generation, we directly fine-tune the GPT-2 model on the dialogue data without any designed adaptations.Experiments ::: Model Comparison ::: Generative Approaches ::: Speaker:	Model proposed by BIBREF16 which incorporates distributed style embeddings into the structure of decoding cells to control the generation process.Experiments ::: Model Comparison ::: Generative Approaches ::: ECM:	Model proposed by BIBREF11 which uses memory modules to control the stylistic expressions in the generated responses.Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Skeleton-to-Response (SR):	Model proposed by BIBREF27 which modifies the retrieved response based on the lexical difference between the input and the retrieved query. This approach does not take the style aspect into consideration.Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Style Transfer (RST):	For this approach, we apply the state-of-the-art style transfer BIBREF23 model on the retrieved response. This approach does not consider the input query information during the transfer process.Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Reranking (RRe):	Given the input query, a style classifier is used to rerank the top 10 retrieved responses. The response with the highest score on the desired style is selected.Experiments ::: Model Comparison ::: Ablation Study ::: PS:	The full model proposed in this work.Experiments ::: Model Comparison ::: Ablation Study ::: PS w/o R:	In the ablated model, we examine how the retrieved prototype effects our model's performance. To this end, we remove the response prototype from the input representation.Experiments ::: Evaluation Metrics	The quality of dialogue responses is known to be difficult to measure automatically BIBREF41; we therefore rely on human evaluation. To evaluate the responses, we hire five annotators from a commercial annotation company. To prevent introducing potential bias to the annotators, all results are randomly shuffled before being evaluated. All results are evaluated by the annotators following the metrics below.Experiments ::: Evaluation Metrics ::: Quality:	This metric evaluates the content quality of the generated responses. The annotators are asked to give a score within 5-point scale where 5 means perfectly human-like response (relevant, fluent and informative), 3 means marginally acceptable and 1 means unreadable and impossible to understand.Experiments ::: Evaluation Metrics ::: Style Expression:	This metric measures how well the generated responses express the desired style. The annotators give a score ranging from 1 to 5 to this metric, where 5 means very strong style, 3 means no obvious style and 1 means very conflicted style. The style conflict means the generated style is conflicted to the desired one (e.g. female to male, positive to negative emotion).Experiments ::: Evaluation Metrics ::: Ranking:	The annotators are further asked to jointly evaluate the content quality and the style expression of the generated responses from different approaches. Then the annotators give a ranking to each result where top 1 means the best.Experiments ::: Main Results	Both human and automatic evaluation results on the three benchmark datasets are shown in Table TABREF25, TABREF26 and TABREF27. For each dataset, we present results on individual styles as well as the overall results. We observe that the proposed model achieves the top performance results on most of the metrics. It generates responses with both intense style and high response quality. In addition, we also measure the diversity of the generated responses with two automatic metrics: Distinct-1 and Distinct-2 BIBREF16. The results show that the proposed model achieves the closest performance to that of the RRe approach whose responses are all written by human. On the ranking metric which jointly evaluates the content quality and the style expression, the proposed model outperforms other approaches by a substantial margin.From the results in Table TABREF26 and TABREF27, we can observe that ECM obtains the highest style expression scores on the emotion and sentiment dialogue datasets. This is because ECM directly incorporates the style information into its model architecture to force the generation of stylistic expressions. However, as shown in the quality scores, this behavior also undermines the quality of the generated responses. Therefore, the overall performance of ECM is not optimal as shown in the results of the ranking metric.From the experiment results, we observe that removing retrieved information (PS w/o R) from the proposed model causes a drastic drop on the quality score. This demonstrates that the retrieved information is indispensable for the model to generate a stylistic response and maintain a high response quality. In addition, comparing with GPT2-FT baseline, the ablated model (PS w/o R) shows similar content quality and much stronger stylistic expression, which is gained from the model architectural design and the new training strategy.Experiments ::: Further Analysis	We present further discussions and empirical analysis of the proposed approach.Experiments ::: Further Analysis ::: Balance between Quality and Style	In practice, a satisfactory stylistic dialogue system should express the desired style on the premise of the response quality. Based on the criterion of human evaluation metric, 3 is the marginal score of acceptance. So we deem a response as marginally acceptable by actual users when both quality and style expression scores are greater or equal to 3. On the other hand, 4 is the score that well satisfies the users, so responses with both scores greater or equal to 4 are deemed as satisfying to actual users.The ratios of both scores $\ge 3$ and $\ge 4$ are shown in Figure FIGREF47, from which we can see that the proposed approach outperforms all other approaches on $\ge 3$-ratio and $\ge 4$-ratio. The proposed model best balances the trade-off between the response quality and style expression and therefore generating most acceptable and satisfying responses.Experiments ::: Further Analysis ::: Cross-Domain Evaluation	To evaluate the robustness of different approaches, we further analyze their performances when there is a notable difference between the data distribution of the training and testing set. Specifically, we use the models trained on gender-specific dataset to conduct inference on the test set of emotion-specific dataset and vise versa, which is regarded as domain variation. In Figure FIGREF50, we show the data distributions of these two datasets from which we can observe a notable distribution discrepancy. For evaluation, all results are evaluated with the same metrics as in the previous experiments. The averages response quality scores before and after domain variation are shown in Figure FIGREF55. For a direct comparison, the in-domain performance of each model can be found in Table TABREF25 and TABREF26.As shown in Figure FIGREF55, some of the strong baselines exhibit a drastic drop in response quality after domain variation such as GPT2-FT and PS w/o R. In contrast, the PS model successfully maintains high response quality in spite of domain variation. The model seems to benefit from leveraging retrieved results to bridge the gap between the two different domains. This can also be observed in the results of RST and RRe which also use the retrieved results and get a even higher performance when facing domain variation.Experiments ::: Case Study	We present several examples of generated responses by the proposed PS approach. Table TABREF51 shows responses with different gender and emotion styles, and Table TABREF52 shows responses with different sentiments. Examples in Table TABREF51 show that the proposed approach is able to extract informative details such as “have nightmares” and “higher salary” that are relevant to the queries from the retrieved responses. By taking the desired style as input, the proposed model generates adequate and stylistic responses while producing the informative details. Examples in Table TABREF52 also demonstrate that the proposed model is able to generate responses with desired sentiments based on the informative details (e.g. “_ want us to target _ ones _”, “_ can make _ decision.” and “_ sound _ to me _”) contained in the retrieved response.Conclusion	In this work, we propose a novel PS framework to tackle the task of stylistic dialogue generation. Additionally, we propose a new stylistic response generator which works coherently with the proposed framework. We conduct extensive experiments on three benchmark datasets from two languages. Results of human and automatic evaluation show that the proposed approach outperforms many strong baselines by a substantial margin.","['Is there a metric that also rewards good stylistic response?', 'What are existing baseline models on these benchmark datasets?', 'On what two languages is experimented on?', 'What three benchmark datasets are used?']","['Unacceptable', 'Unacceptable', 'Chinese and English', 'Unacceptable']"
8,"Modeling Conversation Structure and Temporal Dynamics for Jointly Predicting Rumor Stance and Veracity	Automatically verifying rumorous information has become an important and challenging task in natural language processing and social media analytics. Previous studies reveal that people's stances towards rumorous messages can provide indicative clues for identifying the veracity of rumors, and thus determining the stances of public reactions is a crucial preceding step for rumor veracity prediction. In this paper, we propose a hierarchical multi-task learning framework for jointly predicting rumor stance and veracity on Twitter, which consists of two components. The bottom component of our framework classifies the stances of tweets in a conversation discussing a rumor via modeling the structural property based on a novel graph convolutional network. The top component predicts the rumor veracity by exploiting the temporal dynamics of stance evolution. Experimental results on two benchmark datasets show that our method outperforms previous methods in both rumor stance classification and veracity prediction.	Introduction	Social media websites have become the main platform for users to browse information and share opinions, facilitating news dissemination greatly. However, the characteristics of social media also accelerate the rapid spread and dissemination of unverified information, i.e., rumors BIBREF0. The definition of rumor is “items of information that are unverified at the time of posting” BIBREF1. Ubiquitous false rumors bring about harmful effects, which has seriously affected public and individual lives, and caused panic in society BIBREF2, BIBREF3. Because online content is massive and debunking rumors manually is time-consuming, there is a great need for automatic methods to identify false rumors BIBREF4.Previous studies have observed that public stances towards rumorous messages are crucial signals to detect trending rumors BIBREF5, BIBREF6 and indicate the veracity of them BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11. Therefore, stance classification towards rumors is viewed as an important preceding step of rumor veracity prediction, especially in the context of Twitter conversations BIBREF12.The state-of-the-art methods for rumor stance classification are proposed to model the sequential property BIBREF13 or the temporal property BIBREF14 of a Twitter conversation thread. In this paper, we propose a new perspective based on structural property: learning tweet representations through aggregating information from their neighboring tweets. Intuitively, a tweet's nearer neighbors in its conversation thread are more informative than farther neighbors because the replying relationships of them are closer, and their stance expressions can help classify the stance of the center tweet (e.g., in Figure FIGREF1, tweets “1”, “4” and “5” are the one-hop neighbors of the tweet “2”, and their influences on predicting the stance of “2” are larger than that of the two-hop neighbor “3”). To achieve this, we represent both tweet contents and conversation structures into a latent space using a graph convolutional network (GCN) BIBREF15, aiming to learn stance feature for each tweet by aggregating its neighbors' features. Compared with the sequential and temporal based methods, our aggregation based method leverages the intrinsic structural property in conversations to learn tweet representations.After determining the stances of people's reactions, another challenge is how we can utilize public stances to predict rumor veracity accurately. We observe that the temporal dynamics of public stances can indicate rumor veracity. Figure FIGREF2 illustrates the stance distributions of tweets discussing $true$ rumors, $false$ rumors, and $unverified$ rumors, respectively. As we can see, $supporting$ stance dominates the inception phase of spreading. However, as time goes by, the proportion of $denying$ tweets towards $false$ rumors increases quite significantly. Meanwhile, the proportion of $querying$ tweets towards $unverified$ rumors also shows an upward trend. Based on this observation, we propose to model the temporal dynamics of stance evolution with a recurrent neural network (RNN), capturing the crucial signals containing in stance features for effective veracity prediction.Further, most existing methods tackle stance classification and veracity prediction separately, which is suboptimal and limits the generalization of models. As shown previously, they are two closely related tasks in which stance classification can provide indicative clues to facilitate veracity prediction. Thus, these two tasks can be jointly learned to make better use of their interrelation.Based on the above considerations, in this paper, we propose a hierarchical multi-task learning framework for jointly predicting rumor stance and veracity, which achieves deep integration between the preceding task (stance classification) and the subsequent task (veracity prediction). The bottom component of our framework classifies the stances of tweets in a conversation discussing a rumor via aggregation-based structure modeling, and we design a novel graph convolution operation customized for conversation structures. The top component predicts rumor veracity by exploiting the temporal dynamics of stance evolution, taking both content features and stance features learned by the bottom component into account. Two components are jointly trained to utilize the interrelation between the two tasks for learning more powerful feature representations.The contributions of this work are as follows.$\bullet $ We propose a hierarchical framework to tackle rumor stance classification and veracity prediction jointly, exploiting both structural characteristic and temporal dynamics in rumor spreading process.$\bullet $ We design a novel graph convolution operation customized to encode conversation structures for learning stance features. To our knowledge, we are the first to employ graph convolution for modeling the structural property of Twitter conversations.$\bullet $ Experimental results on two benchmark datasets verify that our hierarchical framework performs better than existing methods in both rumor stance classification and veracity prediction.Related Work	Rumor Stance Classification Stance analysis has been widely studied in online debate forums BIBREF17, BIBREF18, and recently has attracted increasing attention in different contexts BIBREF19, BIBREF20, BIBREF21, BIBREF22. After the pioneering studies on stance classification towards rumors in social media BIBREF7, BIBREF5, BIBREF8, linguistic feature BIBREF23, BIBREF24 and point process based methods BIBREF25, BIBREF26 have been developed.Recent work has focused on Twitter conversations discussing rumors. BIBREF12 proposed to capture the sequential property of conversations with linear-chain CRF, and also used a tree-structured CRF to consider the conversation structure as a whole. BIBREF27 developed a novel feature set that scores the level of users' confidence. BIBREF28 designed affective and dialogue-act features to cover various facets of affect. BIBREF29 proposed a semi-supervised method that propagates the stance labels on similarity graph. Beyond feature-based methods, BIBREF13 utilized an LSTM to model the sequential branches in a conversation, and their system ranked the first in SemEval-2017 task 8. BIBREF14 adopted attention to model the temporal property of a conversation and achieved the state-of-the-art performance.Rumor Veracity Prediction Previous studies have proposed methods based on various features such as linguistics, time series and propagation structures BIBREF30, BIBREF31, BIBREF32, BIBREF33. Neural networks show the effectiveness of modeling time series BIBREF34, BIBREF35 and propagation paths BIBREF36. BIBREF37's model adopted recursive neural networks to incorporate structure information into tweet representations and outperformed previous methods.Some studies utilized stance labels as the input feature of veracity classifiers to improve the performance BIBREF9, BIBREF38. BIBREF39 proposed to recognize the temporal patterns of true and false rumors' stances by two hidden Markov models (HMMs). Unlike their solution, our method learns discriminative features of stance evolution with an RNN. Moreover, our method jointly predicts stance and veracity by exploiting both structural and temporal characteristics, whereas HMMs need stance labels as the input sequence of observations.Joint Predictions of Rumor Stance and Veracity Several work has addressed the problem of jointly predicting rumor stance and veracity. These studies adopted multi-task learning to jointly train two tasks BIBREF40, BIBREF41, BIBREF42 and learned shared representations with parameter-sharing. Compared with such solutions based on “parallel” architectures, our method is deployed in a hierarchical fashion that encodes conversation structures to learn more powerful stance features by the bottom component, and models stance evolution by the top component, achieving deep integration between the two tasks' feature learning.Problem Definition	Consider a Twitter conversation thread $\mathcal {C}$ which consists of a source tweet $t_1$ (originating a rumor) and a number of reply tweets $\lbrace t_2,t_3,\ldots ,t_{|\mathcal {C}|}\rbrace $ that respond $t_1$ directly or indirectly, and each tweet $t_i$ ($i\in [1, |\mathcal {C}|]$) expresses its stance towards the rumor. The thread $\mathcal {C}$ is a tree structure, in which the source tweet $t_1$ is the root node, and the replying relationships among tweets form the edges.This paper focuses on two tasks. The first task is rumor stance classification, aiming to determine the stance of each tweet in $\mathcal {C}$, which belongs to $\lbrace supporting,denying,querying,commenting\rbrace $. The second task is rumor veracity prediction, with the aim of identifying the veracity of the rumor, belonging to $\lbrace true,false,unverified\rbrace $.Proposed Method	We propose a Hierarchical multi-task learning framework for jointly Predicting rumor Stance and Veracity (named Hierarchical-PSV). Figure FIGREF4 illustrates its overall architecture that is composed of two components. The bottom component is to classify the stances of tweets in a conversation thread, which learns stance features via encoding conversation structure using a customized graph convolutional network (named Conversational-GCN). The top component is to predict the rumor's veracity, which takes the learned features from the bottom component into account and models the temporal dynamics of stance evolution with a recurrent neural network (named Stance-Aware RNN).Proposed Method ::: Conversational-GCN: Aggregation-based Structure Modeling for Stance Prediction	Now we detail Conversational-GCN, the bottom component of our framework. We first adopt a bidirectional GRU (BGRU) BIBREF43 layer to learn the content feature for each tweet in the thread $\mathcal {C}$. For a tweet $t_i$ ($i\in [1,|\mathcal {C}|]$), we run the BGRU over its word embedding sequence, and use the final step's hidden vector to represent the tweet. The content feature representation of $t_i$ is denoted as $\mathbf {c}_i\in \mathbb {R}^{d}$, where $d$ is the output size of the BGRU.As we mentioned in Section SECREF1, the stance expressions of a tweet $t_i$'s nearer neighbors can provide more informative signals than farther neighbors for learning $t_i$'s stance feature. Based on the above intuition, we model the structural property of the conversation thread $\mathcal {C}$ to learn stance feature representation for each tweet in $\mathcal {C}$. To this end, we encode structural contexts to improve tweet representations by aggregating information from neighboring tweets with a graph convolutional network (GCN) BIBREF15.Formally, the conversation $\mathcal {C}$'s structure can be represented by a graph $\mathcal {C}_{G}=\langle \mathcal {T}, \mathcal {E} \rangle $, where $\mathcal {T}=\lbrace t_i\rbrace _{i=1}^{|\mathcal {C}|}$ denotes the node set (i.e., tweets in the conversation), and $\mathcal {E}$ denotes the edge set composed of all replying relationships among the tweets. We transform the edge set $\mathcal {E}$ to an adjacency matrix $\mathbf {A}\in \mathbb {R}^{|\mathcal {C}|\times |\mathcal {C}|}$, where $\mathbf {A}_{ij}=\mathbf {A}_{ji}=1$ if the tweet $t_i$ directly replies the tweet $t_j$ or $i=j$. In one GCN layer, the graph convolution operation for one tweet $t_i$ on $\mathcal {C}_G$ is defined as:where $\mathbf {h}_i^{\text{in}}\in \mathbb {R}^{d_{\text{in}}}$ and $\mathbf {h}_i^{\text{out}}\in \mathbb {R}^{d_{\text{out}}}$ denote the input and output feature representations of the tweet $t_i$ respectively. The convolution filter $\mathbf {W}\in \mathbb {R}^{d_{\text{in}}\times d_{\text{out}}}$ and the bias $\mathbf {b}\in \mathbb {R}^{d_{\text{out}}}$ are shared over all tweets in a conversation. We apply symmetric normalized transformation $\hat{\mathbf {A}}={\mathbf {D}}^{-\frac{1}{2}}\mathbf {A}{\mathbf {D}}^{-\frac{1}{2}}$ to avoid the scale changing of feature representations, where ${\mathbf {D}}$ is the degree matrix of $\mathbf {A}$, and $\lbrace j\mid \hat{\mathbf {A}}_{ij}\ne 0\rbrace $ contains $t_i$'s one-hop neighbors and $t_i$ itself.In this original graph convolution operation, given a tweet $t_i$, the receptive field for $t_i$ contains its one-hop neighbors and $t_i$ itself, and the aggregation level of two tweets $t_i$ and $t_j$ is dependent on $\hat{\mathbf {A}}_{ij}$. In the context of encoding conversation structures, we observe that such operation can be further improved for two issues. First, a tree-structured conversation may be very deep, which means that the receptive field of a GCN layer is restricted in our case. Although we can stack multiple GCN layers to expand the receptive field, it is still difficult to handle conversations with deep structures and increases the number of parameters. Second, the normalized matrix $\hat{\mathbf {A}}$ partly weakens the importance of the tweet $t_i$ itself. To address these issues, we design a novel graph convolution operation which is customized to encode conversation structures. Formally, it is implemented by modifying the matrix $\hat{\mathbf {A}}$ in Eq. (DISPLAY_FORM6):where the multiplication operation expands the receptive field of a GCN layer, and adding an identity matrix elevates the importance of $t_i$ itself.After defining the above graph convolution operation, we adopt an $L$-layer GCN to model conversation structures. The $l^{\text{th}}$ GCN layer ($l\in [1, L]$) computed over the entire conversation structure can be written as an efficient matrix operation:where $\mathbf {H}^{(l-1)}\in \mathbb {R}^{|\mathcal {C}|\times d_{l-1}}$ and $\mathbf {H}^{(l)}\in \mathbb {R}^{|\mathcal {C}|\times d_l}$ denote the input and output features of all tweets in the conversation $\mathcal {C}$ respectively.Specifically, the first GCN layer takes the content features of all tweets as input, i.e., $\mathbf {H}^{(0)}=(\mathbf {c}_1,\mathbf {c}_2,\ldots ,\mathbf {c}_{|\mathcal {C}|})^{\top }\in \mathbb {R}^{|\mathcal {C}|\times d}$. The output of the last GCN layer represents the stance features of all tweets in the conversation, i.e., $\mathbf {H}^{(L)}=(\mathbf {s}_1,\mathbf {s}_2,\ldots ,\mathbf {s}_{|\mathcal {C}|})^{\top }\in \mathbb {R}^{|\mathcal {C}|\times 4}$, where $\mathbf {s}_i$ is the unnormalized stance distribution of the tweet $t_i$.For each tweet $t_i$ in the conversation $\mathcal {C}$, we apply softmax to obtain its predicted stance distribution:The ground-truth labels of stance classification supervise the learning process of Conversational-GCN. The loss function of $\mathcal {C}$ for stance classification is computed by cross-entropy criterion:where $s_i$ is a one-hot vector that denotes the stance label of the tweet $t_i$. For batch-wise training, the objective function for a batch is the averaged cross-entropy loss of all tweets in these conversations.In previous studies, GCNs are used to encode dependency trees BIBREF44, BIBREF45 and cross-document relations BIBREF46, BIBREF47 for downstream tasks. Our work is the first to leverage GCNs for encoding conversation structures.Proposed Method ::: Stance-Aware RNN: Temporal Dynamics Modeling for Veracity Prediction	The top component, Stance-Aware RNN, aims to capture the temporal dynamics of stance evolution in a conversation discussing a rumor. It integrates both content features and stance features learned from the bottom Conversational-GCN to facilitate the veracity prediction of the rumor.Specifically, given a conversation thread $\mathcal {C}=\lbrace t_1,t_2,\ldots ,t_{|\mathcal {C}|}\rbrace $ (where the tweets $t_*$ are ordered chronologically), we combine the content feature and the stance feature for each tweet, and adopt a GRU layer to model the temporal evolution:where $[\cdot ;\cdot ]$ denotes vector concatenation, and $(\mathbf {v}_1,\mathbf {v}_2,\ldots ,\mathbf {v}_{|\mathcal {C}|})$ is the output sequence that represents the temporal feature. We then transform the sequence to a vector $\mathbf {v}$ by a max-pooling function that captures the global information of stance evolution, and feed it into a one-layer feed-forward neural network (FNN) with softmax normalization to produce the predicted veracity distribution $\hat{\mathbf {v}}$:The loss function of $\mathcal {C}$ for veracity prediction is also computed by cross-entropy criterion:where $v$ denotes the veracity label of $\mathcal {C}$.Proposed Method ::: Jointly Learning Two Tasks	To leverage the interrelation between the preceding task (stance classification) and the subsequent task (veracity prediction), we jointly train two components in our framework. Specifically, we add two tasks' loss functions to obtain a joint loss function $\mathcal {L}$ (with a trade-off parameter $\lambda $), and optimize $\mathcal {L}$ to train our framework:In our Hierarchical-PSV, the bottom component Conversational-GCN learns content and stance features, and the top component Stance-Aware RNN takes the learned features as input to further exploit temporal evolution for predicting rumor veracity. Our multi-task framework achieves deep integration of the feature representation learning process for the two closely related tasks.Experiments	In this section, we first evaluate the performance of Conversational-GCN on rumor stance classification and evaluate Hierarchical-PSV on veracity prediction (Section SECREF21). We then give a detailed analysis of our proposed method (Section SECREF26).Experiments ::: Data & Evaluation Metric	To evaluate our proposed method, we conduct experiments on two benchmark datasets.The first is SemEval-2017 task 8 BIBREF16 dataset. It includes 325 rumorous conversation threads, and has been split into training, development and test sets. These threads cover ten events, and two events of that only appear in the test set. This dataset is used to evaluate both stance classification and veracity prediction tasks.The second is PHEME dataset BIBREF48. It provides 2,402 conversations covering nine events. Following previous work, we conduct leave-one-event-out cross-validation: in each fold, one event's conversations are used for testing, and all the rest events are used for training. The evaluation metric on this dataset is computed after integrating the outputs of all nine folds. Note that only a subset of this dataset has stance labels, and all conversations in this subset are already contained in SemEval-2017 task 8 dataset. Thus, PHEME dataset is used to evaluate veracity prediction task.Table TABREF19 shows the statistics of two datasets. Because of the class-imbalanced problem, we use macro-averaged $F_1$ as the evaluation metric for two tasks. We also report accuracy for reference.Experiments ::: Implementation Details	In all experiments, the number of GCN layers is set to $L=2$. We list the implementation details in Appendix A.Experiments ::: Experimental Results ::: Results: Rumor Stance Classification	Baselines We compare our Conversational-GCN with the following methods in the literature:$\bullet $ Affective Feature + SVM BIBREF28 extracts affective and dialogue-act features for individual tweets, and then trains an SVM for classifying stances.$\bullet $ BranchLSTM BIBREF13 is the winner of SemEval-2017 shared task 8 subtask A. It adopts an LSTM to model the sequential branches in a conversation thread. Before feeding branches into the LSTM, some additional hand-crafted features are used to enrich the tweet representations.$\bullet $ TemporalAttention BIBREF14 is the state-of-the-art method. It uses a tweet's “neighbors in the conversation timeline” as the context, and utilizes attention to model such temporal sequence for learning the weight of each neighbor. Extra hand-crafted features are also used.Performance Comparison Table TABREF20 shows the results of different methods for rumor stance classification. Clearly, the macro-averaged $F_1$ of Conversational-GCN is better than all baselines.Especially, our method shows the effectiveness of determining $denying$ stance, while other methods can not give any correct prediction for $denying$ class (the $F_{\text{D}}$ scores of them are equal to zero). Further, Conversational-GCN also achieves higher $F_1$ score for $querying$ stance ($F_{\text{Q}}$). Identifying $denying$ and $querying$ stances correctly is crucial for veracity prediction because they play the role of indicators for $false$ and $unverified$ rumors respectively (see Figure FIGREF2). Meanwhile, the class-imbalanced problem of data makes this a challenge. Conversational-GCN effectively encodes structural context for each tweet via aggregating information from its neighbors, learning powerful stance features without feature engineering. It is also more computationally efficient than sequential and temporal based methods. The information aggregations for all tweets in a conversation are worked in parallel and thus the running time is not sensitive to conversation's depth.Experiments ::: Experimental Results ::: Results: Rumor Veracity Prediction	To evaluate our framework Hierarchical-PSV, we consider two groups of baselines: single-task and multi-task baselines.Single-task Baselines In single-task setting, stance labels are not available. Only veracity labels can be used to supervise the training process.$\bullet $ TD-RvNN BIBREF37 models the top-down tree structure using a recursive neural network for veracity classification.$\bullet $ Hierarchical GCN-RNN is the single-task variant of our framework: we optimize $\mathcal {L}_{\rm {veracity}}$ (i.e., $\lambda =0$ in Eq. (DISPLAY_FORM16)) during training. Thus, the bottom Conversational-GCN only has indirect supervision (veracity labels) to learn stance features.Multi-task Baselines In multi-task setting, both stance labels and veracity labels are available for training.$\bullet $ BranchLSTM+NileTMRG BIBREF41 is a pipeline method, combining the winner systems of two subtasks in SemEval-2017 shared task 8. It first trains a BranchLSTM for stance classification, and then uses the predicted stance labels as extra features to train an SVM for veracity prediction BIBREF38.$\bullet $ MTL2 (Veracity+Stance) BIBREF41 is a multi-task learning method that adopts BranchLSTM as the shared block across tasks. Then, each task has a task-specific output layer, and two tasks are jointly learned.Performance Comparison Table TABREF23 shows the comparisons of different methods. By comparing single-task methods, Hierarchical GCN-RNN performs better than TD-RvNN, which indicates that our hierarchical framework can effectively model conversation structures to learn high-quality tweet representations. The recursive operation in TD-RvNN is performed in a fixed direction and runs over all tweets, thus may not obtain enough useful information. Moreover, the training speed of Hierarchical GCN-RNN is significantly faster than TD-RvNN: in the condition of batch-wise optimization for training one step over a batch containing 32 conversations, our method takes only 0.18 seconds, while TD-RvNN takes 5.02 seconds.Comparisons among multi-task methods show that two joint methods outperform the pipeline method (BranchLSTM+NileTMRG), indicating that jointly learning two tasks can improve the generalization through leveraging the interrelation between them. Further, compared with MTL2 which uses a “parallel” architecture to make predictions for two tasks, our Hierarchical-PSV performs better than MTL2. The hierarchical architecture is more effective to tackle the joint predictions of rumor stance and veracity, because it not only possesses the advantage of parameter-sharing but also offers deep integration of the feature representation learning process for the two tasks. Compared with Hierarchical GCN-RNN that does not use the supervision from stance classification task, Hierarchical-PSV provides a performance boost, which demonstrates that our framework benefits from the joint learning scheme.Experiments ::: Further Analysis and Discussions	We conduct additional experiments to further demonstrate the effectiveness of our model.Experiments ::: Further Analysis and Discussions ::: Effect of Customized Graph Convolution	To show the effect of our customized graph convolution operation (Eq. (DISPLAY_FORM7)) for modeling conversation structures, we further compare it with the original graph convolution (Eq. (DISPLAY_FORM6), named Original-GCN) on stance classification task.Specifically, we cluster tweets in the test set according to their depths in the conversation threads (e.g., the cluster “depth = 0” consists of all source tweets in the test set). For BranchLSTM, Original-GCN and Conversational-GCN, we report their macro-averaged $F_1$ on each cluster in Figure FIGREF28.We observe that our Conversational-GCN outperforms Original-GCN and BranchLSTM significantly in most levels of depth. BranchLSTM may prefer to “shallow” tweets in a conversation because they often occur in multiple branches (e.g., in Figure FIGREF1, the tweet “2” occurs in two branches and thus it will be modeled twice). The results indicate that Conversational-GCN has advantage to identify stances of “deep” tweets in conversations.Experiments ::: Further Analysis and Discussions ::: Ablation Tests	Effect of Stance Features To understand the importance of stance features for veracity prediction, we conduct an ablation study: we only input the content features of all tweets in a conversation to the top component RNN. It means that the RNN only models the temporal variation of tweet contents during spreading, but does not consider their stances and is not “stance-aware”. Table TABREF30 shows that “– stance features” performs poorly, and thus the temporal modeling process benefits from the indicative signals provided by stance features. Hence, combining the low-level content features and the high-level stance features is crucial to improve rumor veracity prediction.Effect of Temporal Evolution Modeling We modify the Stance-Aware RNN by two ways: (i) we replace the GRU layer by a CNN that only captures local temporal information; (ii) we remove the GRU layer. Results in Table TABREF30 verify that replacing or removing the GRU block hurts the performance, and thus modeling the stance evolution of public reactions towards a rumorous message is indeed necessary for effective veracity prediction.Experiments ::: Further Analysis and Discussions ::: Interrelation of Stance and Veracity	We vary the value of $\lambda $ in the joint loss $\mathcal {L}$ and train models with various $\lambda $ to show the interrelation between stance and veracity in Figure FIGREF31. As $\lambda $ increases from 0.0 to 1.0, the performance of identifying $false$ and $unverified$ rumors generally gains. Therefore, when the supervision signal of stance classification becomes strong, the learned stance features can produce more accurate clues for predicting rumor veracity.Experiments ::: Case Study	Figure FIGREF33 illustrates a $false$ rumor identified by our model. We can observe that the stances of reply tweets present a typical temporal pattern “$supporting\rightarrow querying\rightarrow denying$”. Our model captures such stance evolution with RNN and predicts its veracity correctly. Further, the visualization of tweets shows that the max-pooling operation catches informative tweets in the conversation. Hence, our framework can notice salience indicators of rumor veracity in the spreading process and combine them to give correct prediction.Conclusion	We propose a hierarchical multi-task learning framework for jointly predicting rumor stance and veracity on Twitter. We design a new graph convolution operation, Conversational-GCN, to encode conversation structures for classifying stance, and then the top Stance-Aware RNN combines the learned features to model the temporal dynamics of stance evolution for veracity prediction. Experimental results verify that Conversational-GCN can handle deep conversation structures effectively, and our hierarchical framework performs much better than existing methods. In future work, we shall explore to incorporate external context BIBREF16, BIBREF50, and extend our model to multi-lingual scenarios BIBREF51. Moreover, we shall investigate the diffusion process of rumors from social science perspective BIBREF52, draw deeper insights from there and try to incorporate them into the model design.Acknowledgments	This work was supported in part by the National Key R&D Program of China under Grant #2016QY02D0305, NSFC Grants #71621002, #71472175, #71974187 and #71602184, and Ministry of Health of China under Grant #2017ZX10303401-002. We thank all the anonymous reviewers for their valuable comments. We also thank Qianqian Dong for her kind assistance.","['How do they split the dataset when training and evaluating their models?', 'Do they demonstrate the relationship between veracity and stance over time in the Twitter dataset?', 'How much improvement does their model yield over previous methods?']","[""SemEval-2017 task 8  dataset includes 325 rumorous conversation threads, and has been split into training, development and test sets. \nThe PHEME dataset provides 2,402 conversations covering nine events - in each fold, one event's conversations are used for testing, and all the rest events are used for training. "", 'Unacceptable', 'Their model improves macro-averaged F1 by 0.017 over previous best model in Rumor Stance Classification and improves macro-averaged F1 by 0.03 and 0.015 on Multi-task Rumor Veracity Prediction on SemEval and PHEME datasets respectively']"
9,"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model	We show that BERT (Devlin et al., 2018) is a Markov random field language model. Formulating BERT in this way gives way to a natural procedure to sample sentence from BERT. We sample sentences from BERT and find that it can produce high-quality, fluent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.	Introduction	BERT BIBREF0 is a recently released sequence model used to achieve state-of-art results on a wide range of natural language understanding tasks, including constituency parsing BIBREF1 and machine translation BIBREF2 . Early work probing BERT's linguistic capabilities has found it surprisingly robust BIBREF3 .BERT is trained on a masked language modeling objective. Unlike a traditional language modeling objective of predicting the next word in a sequence given the history, masked language modeling predicts a word given its left and right context. Because the model expects context from both directions, it is not immediately obvious how to efficiently evaluate BERT as a language model (i.e., use it to evaluate the probability of a text sequence) or how to sample from it.We attempt to answer these questions by showing that BERT is a combination of a Markov random field language model BIBREF4 , BIBREF5 with pseudo log-likelihood BIBREF6 training. This formulation automatically leads to a sampling procedure based on Gibbs sampling.BERT as a Markov Random Field	Let $X=(x_1, \ldots , x_T)$ be a sequence of random variables $x_i$ 's. Each random variable is categorical in that it can take one of $M$ items from a vocabulary $V=\left\lbrace  v_1, \ldots , v_{M} \right\rbrace $ . These random variables form a fully-connected graph with undirected edges, indicating that each variable $x_i$ is dependent on all the other variables.Using BERT as an MRF-LM	The discussion so far implies that BERT is in fact a Markov random field language model (MRF-LM) and that it learns a distribution over sentences (of some given length.) This suggests that we can use BERT not only as parameter initialization for finetuning but as a generative model of sentences to either score a sentence or sample a sentence.Experiments	Our experiments demonstrate the potential of using BERT as a standalone language model rather than as a parameter initializer for transfer learning BIBREF0 , BIBREF2 , BIBREF16 . We show that sentences sampled from BERT are well-formed and are assigned high probabilities by an off-the-shelf language model. We take pretrained BERT models trained on a mix of Toronto Book Corpus BIBREF17 and Wikipedia provided by BIBREF0 and its PyTorch implementation provided by HuggingFace.Evaluation	We consider several evaluation metrics to estimate the quality and diversity of the generations.We follow BIBREF18 by computing BLEU BIBREF19 between the generations and the original data distributions to measure how similar the generations are. We use a random sample of 5000 sentences from the test set of WikiText-103 BIBREF20 and a random sample of 5000 sentences from TBC as references.We also evaluate the perplexity of a trained language model on the generations as a rough proxy for fluency. Specifically, we use the Gated Convolutional Language Model BIBREF21 pretrained on WikiText-103.Following BIBREF22 , we compute self-BLEU: for each generated sentence, we compute BLEU treating the rest of the sentences as references, and average across sentences. Self-BLEU measures how similar each generated sentence is to the other generations; high self-BLEU indicates that the model has low sample diversity.We also evaluate the percentage of $n$ -grams that are unique, when compared to the original data distribution and within the corpus of generations. We note that this metric is somewhat in opposition to BLEU between generations and data, as fewer unique $n$ -grams implies higher BLEU.We use the non-sequential sampling scheme, as empirically this led to the most coherent generations. We show generations from the sequential sampler in Table 4 in the appendix. We compare against generations from a high-quality neural language model, the OpenAI Generative Pre-Training Transformer BIBREF23 , which was trained on TBC and has approximately the same number of parameters as the base configuration of BERT. For all models, we generate 1000 uncased sequences of length 40.Results	We present sample generations, quality results, and diversity results respectively in Tables 1 , 2 , 3 .We find that, compared to GPT, the BERT generations are of worse quality, but are more diverse. Particularly telling is that the outside language model, which was trained on Wikipedia, is less perplexed by the GPT generations than the BERT generations. GPT was only trained on romance novels, whereas BERT was trained on romance novels and Wikipedia. However, we do see that the perplexity on BERT samples is not absurdly high, and in reading the samples, we find that many are fairly coherent.We find that BERT generations are more diverse than GPT generations. GPT has high $n$ -gram overlap (smaller percent of unique $n$ -grams) with TBC, but surprisingly also with WikiText-103, despite being trained on different data. BERT has lower $n$ -gram overlap with both corpora, perhaps because of worse quality generations, but also has lower self-BLEU.Conclusion	We show that BERT is a Markov random field language model. We give a practical algorithm for generating from BERT without any additional training and verify in experiments that the algorithm produces diverse and fairly fluent generations. Further work might explore sampling methods that do not need to run the model over the entire sequence each iteration and that enable conditional generation. To facilitate further investigation, we release our code on GitHub at https://github.com/kyunghyuncho/bert-gen and a demo as a Colab notebook at https://colab.research.google.com/drive/1MxKZGtQ9SSBjTK5ArsZ5LKhkztzg52RV.Acknowledgements	AW is supported by an NSF Graduate Research Fellowship. KC is partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from Pattern Recognition to AI) and Samsung Electronics (Improving Deep Learning using Latent Structure).Other Sampling Strategies	We investigated two other sampling strategies: left-to-right and generating for all positions at each time step. See Section ""Using BERT as an MRF-LM"" for an explanation of the former. For the latter, we start with an initial sequence of all masks, and at each time step, we would not mask any positions but would generate for all positions. This strategy is designed to save on computation. However, we found that this tended to get stuck in non-fluent sentences that could not be recovered from. We present sample generations for the left-to-right strategy in Table 4 .",['Which metrics are used for evaluating the quality?'],['Unacceptable']
10,"Understanding the Radical Mind: Identifying Signals to Detect Extremist Content on Twitter	The Internet and, in particular, Online Social Networks have changed the way that terrorist and extremist groups can influence and radicalise individuals. Recent reports show that the mode of operation of these groups starts by exposing a wide audience to extremist material online, before migrating them to less open online platforms for further radicalization. Thus, identifying radical content online is crucial to limit the reach and spread of the extremist narrative. In this paper, our aim is to identify measures to automatically detect radical content in social media. We identify several signals, including textual, psychological and behavioural, that together allow for the classification of radical messages. Our contribution is three-fold: (1) we analyze propaganda material published by extremist groups and create a contextual text-based model of radical content, (2) we build a model of psychological properties inferred from these material, and (3) we evaluate these models on Twitter to determine the extent to which it is possible to automatically identify online radical tweets. Our results show that radical users do exhibit distinguishable textual, psychological, and behavioural properties. We find that the psychological properties are among the most distinguishing features. Additionally, our results show that textual models using vector embedding features significantly improves the detection over TF-IDF features. We validate our approach on two experiments achieving high accuracy. Our findings can be utilized as signals for detecting online radicalization activities.	Introduction	The rise of Online Social Networks (OSN) has facilitated a wide application of its data as sensors for information to solve different problems. For example, Twitter data has been used for predicting election results, detecting the spread of flu epidemics, and a source for finding eye-witnesses during criminal incidents and crises BIBREF0 , BIBREF1 . This phenomenon is possible due to the great overlap between our online and offline worlds. Such seamless shift between both worlds has also affected the modus operandi of cyber-criminals and extremist groups BIBREF2 . They have benefited tremendously from the Internet and OSN platforms as it provides them with opportunities to spread their propaganda, widen their reach for victims, and facilitate potential recruitment opportunities. For instance, recent studies show that the Internet and social media played an important role in the increased amount of violent, right-wing extremism BIBREF3 . Similarly, radical groups such as Al-Qaeda and ISIS have used social media to spread their propaganda and promoted their digital magazine, which inspired the Boston Marathon bombers in 2010 BIBREF4 .To limit the reach of cyber-terrorists, several private and governmental organizations are policing online content and utilising big data technologies to minimize the damage and counter the spread of such information. For example, the UK launched a Counter Terrorism Internet Referral Unit in 2010 aiming to remove unlawful Internet content and it supports the police in investigating terrorist and radicalizing activities online. The Unit reports that among the most frequently referred links were those coming from several OSNs, such as Facebook and Twitter BIBREF2 . Similarly, several OSNs are constantly working on detecting and removing users promoting extremist content. In 2018, Twitter announced that over INLINEFORM0 million accounts were suspended for terrorist content BIBREF5 .Realizing the danger of violent extremism and radicalization and how it is becoming a major challenge to societies worldwide, many researchers have attempted to study the behaviour of pro-extremist users online. Looking at existing literature, we find that a number of existing studies incorporate methods to identify distinguishing properties that can aid in automatic detection of these users BIBREF6 , BIBREF7 . However, many of them depend on performing a keyword-based textual analysis which, if used alone, may have several shortcomings, such as producing a large number of false positives and having a high dependency on the data being studied. In addition, it can be evaded using automated tools to adjust the writing style.Another angle for analyzing written text is by looking at the psychological properties that can be inferred regarding their authors. This is typically called psycholinguistics, where one examines how the use of the language can be indicative of different psychological states. Examples of such psychological properties include introversion, extroversion, sensitivity, and emotions. One of the tools that automates the process of extracting psychological meaning from text is the Linguistic Inquiry and Word Count (LIWC) BIBREF8 tool. This approach has been used in the literature to study the behaviour of different groups and to predict their psychological states, such as predicting depression BIBREF9 . More recently, it has also been applied to uncover different psychological properties of extremist groups and understand their intentions behind the recruitment campaigns BIBREF10 .Building on the findings of previous research efforts, this paper aims to study the effects of using new textual and psycholinguistic signals to detect extremist content online. These signals are developed based on insights gathered from analyzing propaganda material published by known extremist groups. In this study, we focus mainly on the ISIS group as they are one of the leading terrorist groups that utilise social media to share their propaganda and recruit individuals. We analyze the propaganda material they publish in their online English magazine called Dabiq, and use data-mining techniques to computationally uncover contextual text and psychological properties associated with these groups. From our analysis of these texts, we are able to extract a set of signals that provide some insight into the mindset of the radical group. This allows us to create a general radical profile that we apply as a signal to detect pro-ISIS supporters on Twitter. Our results show that these identified signals are indeed critical to help improve existing efforts to detect online radicalization.Related Work	In recent years, there has been an increase in online accounts advocating and supporting terrorist groups such as ISIS BIBREF5 . This phenomenon has attracted researchers to study their online existence, and research ways to automatically detect these accounts and limit their spread. Ashcroft et al. BIBREF6 make an attempt to automatically detect Jihadist messages on Twitter. They adopt a machine-learning method to classify tweets as ISIS supporters or not. In the article, the authors focus on English tweets that contain a reference to a set of predefined English hashtags related to ISIS. Three different classes of features are used, including stylometric features, temporal features and sentiment features. However, one of the main limitations of their approach is that it is highly dependent on the data. Rowe and Saif BIBREF7 focused on studying Europe-based Twitter accounts in order to understand what happens before, during, and after they exhibit pro-ISIS behaviour. They define such behaviour as sharing of pro-ISIS content and/or using pro-ISIS terms. To achieve this, they use a term-based approach such that a user is considered to exhibit a radicalization behaviour if he/she uses more pro-ISIS terms than anti-ISIS terms. While such an approach seems effective in distinguishing radicalised users, it is unable to properly deal with lexical ambiguity (i.e., polysemy). Furthermore, in BIBREF11 the authors focused on detecting Twitter users who are involved with “Media Mujahideen”, a Jihadist group who distribute propaganda content online. They used a machine learning approach using a combination of data-dependent and data-independent features. Similar to BIBREF7 they used textual features as well as temporal features to classify tweets and accounts. The experiment was based on a limited set of Twitter accounts, which makes it difficult to generalize the results for a more complex and realistic scenario.Radicalization literature also looked at psychological factors involved with adopting such behaviour. Torok BIBREF12 used a grounded theory approach to develop an explanatory model for the radicalization process utilizing concepts of psychiatric power. Their findings show that the process typically starts with the social isolation of individuals. This isolation seems to be self-imposed as individuals tend to spend a long time engaging with radical content. This leads to the concept of homophily, the tendency to interact and associate with similar others. Through constant interaction with like-minded people, an individual gradually strengthens their mindset and progresses to more extreme levels. Similarly, they start to feel as being part of a group with a strong group identity which leads to group polarization. In psychology, group polarization occurs when discussion leads the group to adopt actions that are more extreme than the initial actions of the individual group members BIBREF13 . Moreover, the National Police Service Agency of the Netherlands developed a model to describe the phases a Jihadist may pass through before committing an act of terrorism BIBREF14 . These sequential phases of radicalism include strong links between the person's psychological and emotional state (e.g., social alienation, depression, lack of confidence in authority) and their susceptibility to radicalization.Methodology	As illustrated in Fig. FIGREF1 , our approach consists of two main phases: Phase 1:Radical Properties Extraction, where articles from Dabiq extremist magazines are input into this step to perform two parallel tasks. In the first task, we build a language model using (i) Term-Frequency Inverse-Document-Frequency (TF-IDF) scores of uni-, bi-, and tri-grams, and (ii) Word embeddings generated from a word2vec model BIBREF15 . The output of this task is a radical corpus of top k-grams, and a word embedding model giving a vector representation for each word in the corpus. The second task seeks to create a psychological profile based on the language used in the extremist propaganda articles, consisting of a set of emotional and topical categories using LIWC dictionary-based tool. Phase 2: Tweet classification involves the use of the models generated from Phase 1 to engineer features related to radical activities. We identify three groups of features and then train a binary classifier to detect radical tweets.Feature Engineering	Feature engineering is the process of exploring large spaces of heterogeneous features with the aim of discovering meaningful features that may aid in modeling the problem at hand. We explore three categories of information to identify relevant features to detect radical content. Some features are user-based while others are message-based. The three categories are: 1) Radical language (Textual features INLINEFORM0 ); 2) Psychological signals (Psychological features INLINEFORM1 ); and 3) Behavioural features ( INLINEFORM2 ). In the following, we detail each of these categories.In order to understand how radical messages are constructed and used, as mentioned earlier, we analyze content of ISIS propaganda material published in Dabiq magazine. Dabiq is an online magazine published by ISIS terrorist groups with the purpose of recruiting people and promoting their propaganda and ideology. Using this data source, we investigate what topics, textual properties, and linguistic cues exist in these magazines. Our intuition is that utilising these linguistic cues from the extremist propaganda would allow us to detect supporters of ISIS group who are influenced by their propaganda.We use two methods to extract the radical language from the propaganda corpus. First we calculate tf-idf scores for each gram in the propaganda corpus. We use uni-grams, bi-grams, and tri-grams to capture phrases and context in which words are being used. We then select the top scoring grams to be used as features for the language model. N-grams and words frequency have been used in the literature to classify similar problems, such as hate-speech and extremist text and have proven successful BIBREF16 . The second method we use is word embeddings to capture semantic meanings. Research in NLP has compared the effectiveness of word embedding methods for encoding semantic meaning and found that semantic relationships between words are best captured by word vectors within word embedding models BIBREF17 . Therefore, we train word2vec model on our propaganda corpus to build the lexical semantic aspects of the text using vector space models. We learn word embeddings using skip-gram word2vec model implemented in the gensim package with vector size of 100 and window size of 5. This word embedding model is used to obtain the vector representation for each word. We aggregate the vectors for each word in the tweet, and concatenate the maximum and average for each word vector dimension, such that any given tweet is represented in 200 dimension sized vector. This approach of aggregating vectors was used successfully in previous research BIBREF18 . Moreover, since ISIS supporters typically advocate for violent behaviour and tend to use offensive curse words, we use dictionaries of violent words and curse words to record the ratio of such words in the tweet. We also count the frequency of words with all capital letters as they are traditionally used to convey yelling behaviour.Research in fields such as linguistics, social science, and psychology suggest that the use of language and the word choices we make in our daily communication, can act as a powerful signal to detect our emotional and psychological states BIBREF8 . Several psychological properties are unintentionally transmitted when we communicate. Additionally, literature from the fields of terrorism and psychology suggests that terrorists may differ from non-terrorists in their psychological profiles BIBREF19 . A number of studies looked at the motivating factors surrounding terrorism, radicalization, and recruitment tactics, and found that terrorist groups tend to target vulnerable individuals who have feelings of desperation and displaced aggression. In particular research into the recruiting tactics of ISIS groups, it was found that they focus on harnessing the individual's need for significance. They seek out vulnerable people and provide them with constant attention BIBREF20 . Similarly, these groups create a dichotomy and promote the mentality of dividing the world into “us” versus “them” BIBREF21 . Inspired by previous research, we extract psychological properties from the radical corpus in order to understand the personality, emotions, and the different psychological properties conveyed in these articles.We utilise LIWC dictionaries to assign a score to a set of psychological, personality, and emotional categories. Mainly, we look at the following properties: (1) Summary variables: Analytically thinking which reflects formal, logical, and hierarchical thinking (high value), versus informal, personal, and narrative thinking (low value). Clout which reflects high expertise and confidence levels (high value), versus tentative, humble, and anxious levels (low value). Tone which reflects positive emotions (high value) versus more negative emotions such as anxiety, sadness, or anger (low value). Authentic which reflects whether the text is conveying honesty and disclosing (high value) versus more guarded, and distanced (low value). (2) Big five: Measures the five psychological properties (OCEAN), namely Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. (3) Emotional Analysis: Measures the positive emotions conveyed in the text, and the negative emotions (including anger, sadness, anxiety). (4) Personal Drives: Focuses on five personal drives, namely power, reward, risk, achievement, and affiliation. (5) Personal Pronouns: Counts the number of 1st, 2nd, and 3rd personal pronouns used. For each Twitter user, we calculate their psychological profiles across these categories. Additionally, using Minkowski distance measure, we calculate the distance between each of these profiles and the average values of the psychological properties created from the ISIS magazines.This category consists of measuring behavioural features to capture different properties related to the user and their behaviour. This includes how active the user is (frequency of tweets posted) and the followers/following ratio. Additionally, we use features to capture users' interactions with others through using hashtags, and engagement in discussions using mention action. To capture this, we construct the mention interaction graph ( INLINEFORM0 ) from our dataset, such that INLINEFORM1 = INLINEFORM2 , where INLINEFORM3 represents the user nodes and INLINEFORM4 represents the set of edges. The graph INLINEFORM5 is a directed graph, where an edge INLINEFORM6 exists between two user nodes INLINEFORM7 and INLINEFORM8 , if user INLINEFORM9 mentions user INLINEFORM10 . After constructing the graph, we measure the degree of influence each user has over their network using different centrality measures, such as degree centrality, betweenness centrality, and HITS-Hub. Such properties have been adopted in the research literature to study properties of cyber-criminal networks and their behaviour BIBREF22 , BIBREF23 .Dataset	 We acquired a publicly available dataset of tweets posted by known pro-ISIS Twitter accounts that was published during the 2015 Paris attacks by Kaggle data science community. The dataset consists of around INLINEFORM0 tweets posted by more than 100 users. These tweets were labelled as being pro-ISIS by looking at specific indicators, such as a set of keywords used (in the user's name, description, tweet text), their network of follower/following of other known radical accounts, and sharing of images of the ISIS flag or some radical leaders. To validate that these accounts are indeed malicious, we checked the current status of the users' accounts in the dataset and found that most of them had been suspended by Twitter. This suggests that they did, in fact, possess a malicious behaviour that opposes the Twitter platform terms of use which caused them to be suspended. We filter out any tweets posted by existing active users and label this dataset as known-bad.To model the normal behaviour, we collected a random sample of tweets from ten-trending topics in Twitter using the Twitter streaming API. These topics were related to news events and on-going social events (e.g., sports, music). We filter out any topics and keywords that may be connected to extremist views. This second dataset consists of around INLINEFORM0 tweets published by around INLINEFORM1 users. A random sample of 200 tweets was manually verified to ascertain it did not contain radical views. We label this dataset as our random-good data.A third dataset is used which was acquired from Kaggle community. This dataset is created to be a counterpoise to the pro-ISIS dataset (our known-bad) as it consists of tweets talking about topics concerning ISIS without being radical. It contains INLINEFORM0 tweets from around INLINEFORM1 users collected on two separate days. We verify that this dataset is indeed non radical by checking the status of users in Twitter and found that a subset ( INLINEFORM2 users) was suspended. We remove those from the dataset and only keep users that are still active on Twitter. This dataset is labelled as counterpoise data.We performed a series of preprocessing steps to clean the complete dataset and prepare it for feature extraction. These steps are: (1) We remove any duplicates and re-tweets from the dataset in order to reduce noise. (2) We remove tweets that have been authored by verified users accounts, as they are typically accounts associated with known public figures. (3) All stop words (e.g., and, or, the) and punctuation marks are removed from the text of the tweet. (4) If the tweet text contains a URL, we record the existence of the URL in a new attribute, hasURL, and then remove it from the tweet text. (5) If the tweet text contains emojis (e.g., :-), :), :P), we record the existence of the emoji in a new attribute, hasEmj, and then remove it from the tweet text. (6) If the tweet text contains any words with all capital characters, we record its existence in a new attribute, allCaps, and then normalize the text to lower-case and filter out any non-alphabetic characters. (7) We tokenize the cleansed tweet text into words, then we perform lemmatization, the process of reducing inflected words to their roots (lemma), and store the result in a vector.Experimental Set-up	We conducted two experiments using the datasets described in Section SECREF11 . Our hypothesis is that supporters of groups such as ISIS may exhibit similar textual and psychological properties when communicating in social media to the properties seen in the propaganda magazines. A tweet is considered radical if it promotes violence, racism, or supports violent behaviour. In Exp 1 we use the first two datasets, i.e., the known-bad and the random-good datasets to classify tweets to radical and normal classes. For Exp 2 we examine if our classifier can also distinguish between tweets that are discussing similar topics (ISIS related) by using the known-bad and the counterpoise datasets.The classification task is binomial (binary) classification where the output of the model predicts whether the input tweet is considered radical or normal. In order to handle the imbalanced class problem in the dataset, there are multiple techniques suggested in the literature Oversampling or undersampling of the minority/majority classes are common techniques. Another technique that is more related to the classification algorithm is cost sensitive learning, which penalizes the classification model for making a mistake on the minority class. This is achieved by applying a weighted cost on misclassifying of the minority class BIBREF24 . We will use the last approach to avoid downsampling of our dataset.Previous research investigating similar problems reported better performances for Random Forest (RF) classifiers BIBREF25 . RF usually performs very well as it is scalable and is robust to outliers. RF typically outperforms decision trees as it has a hierarchical structure and is based on multiple trees. This allows RF to be able to model non-linear decision boundaries. Moreover, Neural Networks (NN) also produced good results when applied to problems related to image recognition, text and natural language processing BIBREF26 . However, they usually tend to require very large amounts of data to train. For the purpose of this study, we experimented with multiple classification algorithms, including RF, NN, SVM, and KNN and found that RF and NN produced the best performance. Due to space limitation, we only report results obtained using RF model. We configured the model to use 100 estimators trees with a maximum depth of 50, and we selected gini impurity for the split criteria. We used the out-of-bag samples (oob) score to estimate the generalization accuracy of the model. Additionally, since RF tends to be biased towards the majority class, we apply the cost sensitive learning method described earlier to make RF more suitable for imbalanced data BIBREF24 .We divided the dataset to training set (80%) and testing set (20%), where the testing set is held out for validation. We reported validation results using different combinations of the features categories (i.e., INLINEFORM0 , INLINEFORM1 , INLINEFORM2 ) and different evaluation metrics: accuracy, recall, precision, f-measure, and area under the ROC curve. Recall measures how many radical tweets we are able to detect, while precision measures how many radical tweets we can detect without falsely accusing anyone. For instance, if we identify every single tweet as radical, we will expose all radical tweets and thus obtain high recall, but at the same time, we will call everyone in the population a radical and thus obtain low precision. F-measure is the average of both precision and recall.Results	Exp 1: The classification results using the known-bad and random-good datasets are reported in Table TABREF16 . The table shows the average accuracy, precision, recall and f-measure scores obtained from each feature category ( INLINEFORM0 , INLINEFORM1 , INLINEFORM2 ) and their combination ( INLINEFORM3 ). We also compared the two textual models, and find that results obtained from using word embedding outperforms the use of n-grams tf-idf scores. This confirms that contextual information is important in detecting radicalization activities. Furthermore, our model performed best using the INLINEFORM4 features across all metrics. This means that the model is able to distinguish between both radical and non-radical with high confidence using only INLINEFORM5 .Exp2: In this experiment, we tested the performance of our classifier in distinguishing between radical and normal tweets that discusses ISIS-related topics. Although this task is more challenging given the similarity of the topic discussed in the two classes, we find that the model still achieves high performance. Table TABREF17 shows the different metrics obtained from each feature category. The INLINEFORM0 feature group obtains 80% accuracy, and 91%, 100% for INLINEFORM1 and INLINEFORM2 feature groups, respectively. The results are consistent with the ones obtained from the first experiment with the features from INLINEFORM3 group contributing to the high accuracy of the model. The area under the Receiver Operator Characteristic (ROC) curve, which measures accuracy based on TP, and FP rates, is shown in Fig. FIGREF18 for each classification model.Features Significance	We investigated which features contribute most to the classification task to distinguish between radical and non-radical tweets. We used the mean decrease impurity method of random forests BIBREF27 to identify the most important features in each feature category. The ten most important features are shown in Table TABREF22 . We found that the most important feature for distinguishing radical tweets is the psychological feature distance measure. This measures how similar the Twitter user is to the average psychological profile calculated from the propaganda magazine articles. Following this is the Us-them dichotomy which looks at the total number of pronouns used (I,they, we, you). This finding is in line with the tactics reported in the radicalization literature with regards to emphasizing the separation between the radical group and the world.Moreover, among the top contributing features are behavioural features related to the number of mentions a single user makes, and their HITS hub and authority rank among their interaction network. This relates to how active the user is in interacting with other users and how much attention they receive from their community. This links to the objectives of those radical users in spreading their ideologies and reaching out to potential like-minded people. As for the INLINEFORM0 category, we find that the use of word2vec embedding improves the performance in comparison with using the tf-idf features. Additionally, all bi-grams and tri-grams features did not contribute much to the classification; only uni-grams did. This can be related to the differences in the writing styles when constructing sentences and phrases in articles and in the social media context (especially given the limitation of the number of words allowed by the Twitter platform). Additionally, the violent word ratio, longWords, and allCaps features are among the top contributing features from this category. This finding agrees to a large extent with observations from the literature regarding dealing with similar problems, where the use of dictionaries of violent words aids with the prediction of violent extremist narrative.Conclusion and Future Work	In this paper, we identified different signals that can be utilized to detect evidence of online radicalization. We derived linguistic and psychological properties from propaganda published by ISIS for recruitment purposes. We utilize these properties to detect pro-ISIS tweets that are influenced by their ideology. Unlike previous efforts, these properties do not only focus on lexical keyword analysis of the messages, but also add a contextual and psychological dimension. We validated our approach in different experiments and the results show that this method is robust across multiple datasets. This system can aid law enforcement and OSN companies to better address such threats and help solve a challenging real-world problem. In future work, we aim to investigate if the model is resilient to different evasion techniques that users may adopt. We will also expand the analysis to other languages.","['What languages feature in the dataset?', 'What textual, psychological and behavioural patterns are observed in radical users?', 'Where is the propaganda material sourced from?', 'Which behavioural features are used?', 'Which psychological features are used?', 'Which textual features are used?']","['Unacceptable', 'They use a lot of ""us"" and ""them"" in their vocabulary. They use a lot of mentions, and they tend to be ""central"" in their network. They use a lot of violent words. ', 'Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable']"
11,"Optimizing Differentiable Relaxations of Coreference Evaluation Metrics	Coreference evaluation metrics are hard to optimize directly as they are non-differentiable functions, not easily decomposable into elementary decisions. Consequently, most approaches optimize objectives only indirectly related to the end goal, resulting in suboptimal performance. Instead, we propose a differentiable relaxation that lends itself to gradient-based optimisation, thus bypassing the need for reinforcement learning or heuristic modification of cross-entropy. We show that by modifying the training objective of a competitive neural coreference system, we obtain a substantial gain in performance. This suggests that our approach can be regarded as a viable alternative to using reinforcement learning or more computationally expensive imitation learning.	Introduction	Coreference resolution is the task of identifying all mentions which refer to the same entity in a document. It has been shown beneficial in many natural language processing (NLP) applications, including question answering BIBREF0 and information extraction BIBREF1 , and often regarded as a prerequisite to any text understanding task.Coreference resolution can be regarded as a clustering problem: each cluster corresponds to a single entity and consists of all its mentions in a given text. Consequently, it is natural to evaluate predicted clusters by comparing them with the ones annotated by human experts, and this is exactly what the standard metrics (e.g., MUC, B INLINEFORM0 , CEAF) do. In contrast, most state-of-the-art systems are optimized to make individual co-reference decisions, and such losses are only indirectly related to the metrics.One way to deal with this challenge is to optimize directly the non-differentiable metrics using reinforcement learning (RL), for example, relying on the REINFORCE policy gradient algorithm BIBREF2 . However, this approach has not been very successful, which, as suggested by clark-manning:2016:EMNLP2016, is possibly due to the discrepancy between sampling decisions at training time and choosing the highest ranking ones at test time. A more successful alternative is using a `roll-out' stage to associate cost with possible decisions, as in clark-manning:2016:EMNLP2016, but it is computationally expensive. Imitation learning BIBREF3 , BIBREF4 , though also exploiting metrics, requires access to an expert policy, with exact policies not directly computable for the metrics of interest.In this work, we aim at combining the best of both worlds by proposing a simple method that can turn popular coreference evaluation metrics into differentiable functions of model parameters. As we show, this function can be computed recursively using scores of individual local decisions, resulting in a simple and efficient estimation procedure. The key idea is to replace non-differentiable indicator functions (e.g. the member function INLINEFORM0 ) with the corresponding posterior probabilities ( INLINEFORM1 ) computed by the model. Consequently, non-differentiable functions used within the metrics (e.g. the set size function INLINEFORM2 ) become differentiable ( INLINEFORM3 ). Though we assume that the scores of the underlying statistical model can be used to define a probability model, we show that this is not a serious limitation. Specifically, as a baseline we use a probabilistic version of the neural mention-ranking model of P15-1137, which on its own outperforms the original one and achieves similar performance to its global version BIBREF5 . Importantly when we use the introduced differentiable relaxations in training, we observe a substantial gain in performance over our probabilistic baseline. Interestingly, the absolute improvement (+0.52) is higher than the one reported in clark-manning:2016:EMNLP2016 using RL (+0.05) and the one using reward rescaling (+0.37). This suggests that our method provides a viable alternative to using RL and reward rescaling.The outline of our paper is as follows: we introduce our neural resolver baseline and the B INLINEFORM0 and LEA metrics in Section SECREF2 . Our method to turn a mention ranking resolver into an entity-centric resolver is presented in Section SECREF3 , and the proposed differentiable relaxations in Section SECREF4 . Section SECREF5 shows our experimental results.Neural mention ranking	In this section we introduce neural mention ranking, the framework which underpins current state-of-the-art models BIBREF6 . Specifically, we consider a probabilistic version of the method proposed by P15-1137. In experiments we will use it as our baseline.Let INLINEFORM0 be the list of mentions in a document. For each mention INLINEFORM1 , let INLINEFORM2 be the index of the mention that INLINEFORM3 is coreferent with (if INLINEFORM4 , INLINEFORM5 is the first mention of some entity appearing in the document). As standard in coreference resolution literature, we will refer to INLINEFORM6 as an antecedent of INLINEFORM7 . Then, in mention ranking the goal is to score antecedents of a mention higher than any other mentions, i.e., if INLINEFORM8 is the scoring function, we require INLINEFORM9 for all INLINEFORM10 such that INLINEFORM11 and INLINEFORM12 are coreferent but INLINEFORM13 and INLINEFORM14 are not.Let INLINEFORM0 and INLINEFORM1 be respectively features of INLINEFORM2 and features of pair INLINEFORM3 . The scoring function is defined by: INLINEFORM4 where INLINEFORM0  and INLINEFORM0 are real vectors and matrices with proper dimensions, INLINEFORM1 are real scalars.Unlike P15-1137, where the max-margin loss is used, we define a probabilistic model. The probability that INLINEFORM0 and INLINEFORM1 are coreferent is given by DISPLAYFORM0 Following D13-1203 we use the following softmax-margin BIBREF8 loss function: INLINEFORM0 where INLINEFORM0 are model parameters, INLINEFORM1 is the set of the indices of correct antecedents of INLINEFORM2 , and INLINEFORM3 . INLINEFORM4 is a cost function used to manipulate the contribution of different error types to the loss function: INLINEFORM5 The error types are “false anaphor”, “false new”, “wrong link”, and “no mistake”, respectively. In our experiments, we borrow their values from D13-1203: INLINEFORM0 . In the subsequent discussion, we refer to the loss as mention-ranking heuristic cross entropy.Evaluation Metrics	We use five most popular metrics,MUC BIBREF9 ,B INLINEFORM0 BIBREF10 ,CEAF INLINEFORM0 , CEAF INLINEFORM1 BIBREF11 ,BLANC BIBREF12 ,LEA BIBREF13 .for evaluation. However, because MUC is the least discriminative metric BIBREF13 , whereas CEAF is slow to compute, out of the five most popular metrics we incorporate into our loss only B INLINEFORM0 . In addition, we integrate LEA, as it has been shown to provide a good balance between discriminativity and interpretability.Let INLINEFORM0 and INLINEFORM1 be the gold-standard entity set and an entity set given by a resolver. Recall that an entity is a set of mentions. The recall and precision of the B INLINEFORM2 metric is computed by: INLINEFORM3  The LEA metric is computed as: INLINEFORM0  where INLINEFORM0 is the number of coreference links in entity INLINEFORM1 . INLINEFORM2 , for both metrics, is defined by: INLINEFORM3  INLINEFORM0 is used in the standard evaluation.From mention ranking to entity centricity	Mention-ranking resolvers do not explicitly provide information about entities/clusters which is required by B INLINEFORM0 and LEA. We therefore propose a simple solution that can turn a mention-ranking resolver into an entity-centric one.First note that in a document containing INLINEFORM0 mentions, there are INLINEFORM1 potential entities INLINEFORM2 where INLINEFORM3 has INLINEFORM4 as the first mention. Let INLINEFORM5 be the probability that mention INLINEFORM6 corresponds to entity INLINEFORM7 . We now show that it can be computed recursively based on INLINEFORM8 as follows: INLINEFORM9  In other words, if INLINEFORM0 , we consider all possible INLINEFORM1 with which INLINEFORM2 can be coreferent, and which can correspond to entity INLINEFORM3 . If INLINEFORM4 , the link to be considered is the INLINEFORM5 's self-link. And, if INLINEFORM6 , the probability is zero, as it is impossible for INLINEFORM7 to be assigned to an entity introduced only later. See Figure FIGREF13 for extra information.We now turn to two crucial questions about this formula:The first question is answered in Proposition SECREF16 . The second question is important because, intuitively, when a mention INLINEFORM0 is anaphoric, the potential entity INLINEFORM1 does not exist. We will show that the answer is “No” by proving in Proposition SECREF17 that the probability that INLINEFORM2 is anaphoric is always higher than any probability that INLINEFORM3 , INLINEFORM4 refers to INLINEFORM5 .Proposition 1 INLINEFORM0 is a valid probability distribution, i.e., INLINEFORM1 , for all INLINEFORM2 .We prove this proposition by induction.Basis: it is obvious that INLINEFORM0 .Assume that INLINEFORM0 for all INLINEFORM1 . Then, INLINEFORM2  Because INLINEFORM0 for all INLINEFORM1 , this expression is equal to INLINEFORM2 Therefore, INLINEFORM0 (according to Equation EQREF5 ).Proposition 2 INLINEFORM0 for all INLINEFORM1 .We prove this proposition by induction.Basis: for INLINEFORM0 , INLINEFORM1 Assume that INLINEFORM0 for all INLINEFORM1 and INLINEFORM2 . Then INLINEFORM3 Entity-centric heuristic cross entropy loss	Having INLINEFORM0 computed, we can consider coreference resolution as a multiclass prediction problem. An entity-centric heuristic cross entropy loss is thus given below: INLINEFORM1 where INLINEFORM0 is the correct entity that INLINEFORM1 belongs to, INLINEFORM2 . Similar to INLINEFORM3 in the mention-ranking heuristic loss in Section SECREF2 , INLINEFORM4 is a cost function used to manipulate the contribution of the four different error types (“false anaphor”, “false new”, “wrong link”, and “no mistake”): INLINEFORM5 From non-differentiable metrics to differentiable losses	There are two functions used in computing B INLINEFORM0 and LEA: the set size function INLINEFORM1 and the link function INLINEFORM2 . Because both of them are non-differentiable, the two metrics are non-differentiable. We thus need to make these two functions differentiable.There are two remarks. Firstly, both functions can be computed using the indicator function INLINEFORM0 : INLINEFORM1  Secondly, given INLINEFORM0 , the indicator function INLINEFORM1 , INLINEFORM2 is the converging point of the following softmax as INLINEFORM3 (see Figure FIGREF19 ): INLINEFORM4 where INLINEFORM0 is called temperature BIBREF14 .Therefore, we propose to represent each INLINEFORM0 as a soft-cluster: INLINEFORM1 where, as defined in Section SECREF3 , INLINEFORM0 is the potential entity that has INLINEFORM1 as the first mention. Replacing the indicator function INLINEFORM2 by the probability distribution INLINEFORM3 , we then have a differentiable version for the set size function and the link function: INLINEFORM4  INLINEFORM0 and INLINEFORM1 are computed similarly with the constraint that only mentions in INLINEFORM2 are taken into account. Plugging these functions into precision and recall of B INLINEFORM3 and LEA in Section SECREF6 , we obtain differentiable INLINEFORM4 and INLINEFORM5 , which are then used in two loss functions: INLINEFORM6  where INLINEFORM0 is the hyper-parameter of the INLINEFORM1 regularization terms.It is worth noting that, as INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . Therefore, when training a model with the proposed losses, we can start at a high temperature (e.g., INLINEFORM3 ) and anneal to a small but non-zero temperature. However, in our experiments we fix INLINEFORM4 . Annealing is left for future work.Experiments	We now demonstrate how to use the proposed differentiable B INLINEFORM0 and LEA to train a coreference resolver. The source code and trained models are available at https://github.com/lephong/diffmetric_coref.Setup	We run experiments on the English portion of CoNLL 2012 data BIBREF15 which consists of 3,492 documents in various domains and formats. The split provided in the CoNLL 2012 shared task is used. In all our resolvers, we use not the original features of P15-1137 but their slight modification described in N16-1114 (section 6.1).Resolvers	We build following baseline and three resolvers:baseline: the resolver presented in Section SECREF2 . We use the identical configuration as in N16-1114: INLINEFORM0 , INLINEFORM1 , INLINEFORM2 (where INLINEFORM3 are respectively the numbers of mention features and pair-wise features). We also employ their pretraining methodology. INLINEFORM0 : the resolver using the entity-centric cross entropy loss introduced in Section SECREF18 . We set INLINEFORM1 . INLINEFORM0 and INLINEFORM1 : the resolvers using the losses proposed in Section SECREF4 . INLINEFORM2 is tuned on the development set by trying each value in INLINEFORM3 .To train these resolvers we use AdaGrad BIBREF16 to minimize their loss functions with the learning rate tuned on the development set and with one-document mini-batches. Note that we use the baseline as the initialization point to train the other three resolvers.Results	We firstly compare our resolvers against P15-1137 and N16-1114. Results are shown in the first half of Table TABREF25 . Our baseline surpasses P15-1137. It is likely due to using features from N16-1114. Using the entity-centric heuristic cross entropy loss and the relaxations are clearly beneficial: INLINEFORM0 is slightly better than our baseline and on par with the global model of N16-1114. INLINEFORM1 outperform the baseline, the global model of N16-1114, and INLINEFORM2 . However, the best values of INLINEFORM3 are INLINEFORM4 , INLINEFORM5 respectively for INLINEFORM6 , and INLINEFORM7 . Among these resolvers, INLINEFORM8 achieves the highest F INLINEFORM9 scores across all the metrics except BLANC.When comparing to clark-manning:2016:EMNLP2016 (the second half of Table TABREF25 ), we can see that the absolute improvement over the baselines (i.e. `heuristic loss' for them and the heuristic cross entropy loss for us) is higher than that of reward rescaling but with much shorter training time: INLINEFORM0 (7 days) and INLINEFORM1 (15 hours) on the CoNLL metric for clark-manning:2016:EMNLP2016 and ours, respectively. It is worth noting that our absolute scores are weaker than these of clark-manning:2016:EMNLP2016, as they build on top of a similar but stronger mention-ranking baseline, which employs deeper neural networks and requires a much larger number of epochs to train (300 epochs, including pretraining). For the purpose of illustrating the proposed losses, we started with a simpler model by P15-1137 which requires a much smaller number of epochs, thus faster, to train (20 epochs, including pretraining).Analysis	Table TABREF28 shows the breakdown of errors made by the baseline and our resolvers on the development set. The proposed resolvers make fewer “false anaphor” and “wrong link” errors but more “false new” errors compared to the baseline. This suggests that loss optimization prevents over-clustering, driving the precision up: when antecedents are difficult to detect, the self-link (i.e., INLINEFORM0 ) is chosen. When INLINEFORM1 increases, they make more “false anaphor” and “wrong link” errors but less “false new” errors.In Figure FIGREF29 (a) the baseline, but not INLINEFORM0 nor INLINEFORM1 , mistakenly links INLINEFORM2 [it] with INLINEFORM3 [the virus]. Under-clustering, on the other hand, is a problem for our resolvers with INLINEFORM4 : in example (b), INLINEFORM5 missed INLINEFORM6 [We]. This behaviour results in a reduced recall but the recall is not damaged severely, as we still obtain a better INLINEFORM7 score. We conjecture that this behaviour is a consequence of using the INLINEFORM8 score in the objective, and, if undesirable, F INLINEFORM9 with INLINEFORM10 can be used instead. For instance, also in Figure FIGREF29 , INLINEFORM11 correctly detects INLINEFORM12 [it] as non-anaphoric and links INLINEFORM13 [We] with INLINEFORM14 [our].Figure FIGREF30 shows recall, precision, F INLINEFORM0 (average of MUC, B INLINEFORM1 , CEAF INLINEFORM2 ), on the development set when training with INLINEFORM3 and INLINEFORM4 . As expected, higher values of INLINEFORM5 yield lower precisions but higher recalls. In contrast, F INLINEFORM6 increases until reaching the highest point when INLINEFORM7 for INLINEFORM8 ( INLINEFORM9 for INLINEFORM10 ), it then decreases gradually.Discussion	Because the resolvers are evaluated on F INLINEFORM0 score metrics, it should be that INLINEFORM1 and INLINEFORM2 perform the best with INLINEFORM3 . Figure FIGREF30 and Table TABREF25 however do not confirm that: INLINEFORM4 should be set with values a little bit larger than 1. There are two hypotheses. First, the statistical difference between the training set and the development set leads to the case that the optimal INLINEFORM5 on one set can be sub-optimal on the other set. Second, in our experiments we fix INLINEFORM6 , meaning that the relaxations might not be close to the true evaluation metrics enough. Our future work, to confirm/reject this, is to use annealing, i.e., gradually decreasing INLINEFORM7 down to (but larger than) 0.Table TABREF25 shows that the difference between INLINEFORM0 and INLINEFORM1 in terms of accuracy is not substantial (although the latter is slightly better than the former). However, one should expect that INLINEFORM2 would outperform INLINEFORM3 on B INLINEFORM4 metric while it would be the other way around on LEA metric. It turns out that, B INLINEFORM5 and LEA behave quite similarly in non-extreme cases. We can see that in Figure 2, 4, 5, 6, 7 in moosavi-strube:2016:P16-1.Related work	Mention ranking and entity centricity are two main streams in the coreference resolution literature. Mention ranking BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 considers local and independent decisions when choosing a correct antecedent for a mention. This approach is computationally efficient and currently dominant with state-of-the-art performance BIBREF5 , BIBREF6 . P15-1137 propose to use simple neural networks to compute mention ranking scores and to use a heuristic loss to train the model. N16-1114 extend this by employing LSTMs to compute mention-chain representations which are then used to compute ranking scores. They call these representations global features. clark-manning:2016:EMNLP2016 build a similar resolver as in P15-1137 but much stronger thanks to deeper neural networks and “better mention detection, more effective, hyperparameters, and more epochs of training”. Furthermore, using reward rescaling they achieve the best performance in the literature on the English and Chinese portions of the CoNLL 2012 dataset. Our work is built upon mention ranking by turning a mention-ranking model into an entity-centric one. It is worth noting that although we use the model proposed by P15-1137, any mention-ranking models can be employed.Entity centricity BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 , on the other hand, incorporates entity-level information to solve the problem. The approach can be top-down as in haghighi2010coreference where they propose a generative model. It can also be bottom-up by merging smaller clusters into bigger ones as in clark-manning:2016:P16-1. The method proposed by ma-EtAl:2014:EMNLP2014 greedily and incrementally adds mentions to previously built clusters using a prune-and-score technique. Importantly, employing imitation learning these two methods can optimize the resolvers directly on evaluation metrics. Our work is similar to ma-EtAl:2014:EMNLP2014 in the sense that our resolvers incrementally add mentions to previously built clusters. However, different from both ma-EtAl:2014:EMNLP2014,clark-manning:2016:P16-1, our resolvers do not use any discrete decisions (e.g., merge operations). Instead, they seamlessly compute the probability that a mention refers to an entity from mention-ranking probabilities, and are optimized on differentiable relaxations of evaluation metrics.Using differentiable relaxations of evaluation metrics as in our work is related to a line of research in reinforcement learning where a non-differentiable action-value function is replaced by a differentiable critic BIBREF26 , BIBREF27 . The critic is trained so that it is as close to the true action-value function as possible. This technique is applied to machine translation BIBREF28 where evaluation metrics (e.g., BLUE) are non-differentiable. A disadvantage of using critics is that there is no guarantee that the critic converges to the true evaluation metric given finite training data. In contrast, our differentiable relaxations do not need to train, and the convergence is guaranteed as INLINEFORM0 .Conclusions	We have proposedExperimental results show that our approach outperforms the resolver by N16-1114, and gains a higher improvement over the baseline than that of clark-manning:2016:EMNLP2016 but with much shorter training time.Acknowledgments	We would like to thank Raquel Fernández, Wilker Aziz, Nafise Sadat Moosavi, and anonymous reviewers for their suggestions and comments. The project was supported by the European Research Council (ERC StG BroadSem 678254), the Dutch National Science Foundation (NWO VIDI 639.022.518) and an Amazon Web Services (AWS) grant.","['Do they compare against Reinforment-Learning approaches?', 'How long is the training dataset?', 'What dataset do they use?']","['Unacceptable', 'Unacceptable', 'Unacceptable']"
12,"Neural Machine Translation for Low Resource Languages using Bilingual Lexicon Induced from Comparable Corpora	Resources for the non-English languages are scarce and this paper addresses this problem in the context of machine translation, by automatically extracting parallel sentence pairs from the multilingual articles available on the Internet. In this paper, we have used an end-to-end Siamese bidirectional recurrent neural network to generate parallel sentences from comparable multilingual articles in Wikipedia. Subsequently, we have showed that using the harvested dataset improved BLEU scores on both NMT and phrase-based SMT systems for the low-resource language pairs: English--Hindi and English--Tamil, when compared to training exclusively on the limited bilingual corpora collected for these language pairs.	Introduction	Both neural and statistical machine translation approaches are highly reliant on the availability of large amounts of data and are known to perform poorly in low resource settings. Recent crowd-sourcing efforts and workshops on machine translation have resulted in small amounts of parallel texts for building viable machine translation systems for low-resource pairs BIBREF0 . But, they have been shown to suffer from low accuracy (incorrect translation) and low coverage (high out-of-vocabulary rates), due to insufficient training data. In this project, we try to address the high OOV rates in low-resource machine translation systems by leveraging the increasing amount of multilingual content available on the Internet for enriching the bilingual lexicon.Comparable corpora such as Wikipedia, are collections of topic-aligned but non-sentence-aligned multilingual documents which are rich resources for extracting parallel sentences from. For example, Figure FIGREF1 shows that there are equivalent sentences on the page about Donald Trump in Tamil and English, and the phrase alignment for an example sentence is shown in Table TABREF4 .Table TABREF2 shows that there are at least tens of thousands of bilingual articles on Wikipedia which could potentially have at least as many parallel sentences that could be mined to address the scarcity of parallel sentences as indicated in column 2 which shows the number of sentence-pairs in the largest available bilingual corpora for xx-en. As shown by BIBREF1 ( BIBREF1 ), the illustrated data sparsity can be addressed by extending the scarce parallel sentence-pairs with those automatically extracted from Wikipedia and thereby improving the performance of statistical machine translation systems.In this paper, we will propose a neural approach to parallel sentence extraction and compare the BLEU scores of machine translation systems with and without the use of the extracted sentence pairs to justify the effectiveness of this method. Compared to previous approaches which require specialized meta-data from document structure or significant amount of hand-engineered features, the neural model for extracting parallel sentences is learned end-to-end using only a small bootstrap set of parallel sentence pairs.Related Work	A lot of work has been done on the problem of automatic sentence alignment from comparable corpora, but a majority of them BIBREF2 , BIBREF1 , BIBREF3 use a pre-existing translation system as a precursor to ranking the candidate sentence pairs, which the low resource language pairs are not at the luxury of having; or use statistical machine learning approaches, where a Maximum Entropy classifier is used that relies on surface level features such as word overlap in order to obtain parallel sentence pairs BIBREF4 . However, the deep neural network model used in our paper is probably the first of its kind, which does not need any feature engineering and also does not need a pre-existing translation system. BIBREF4 ( BIBREF4 ) proposed a parallel sentence extraction system which used comparable corpora from newspaper articles to extract the parallel sentence pairs. In this procedure, a maximum entropy classifier is designed for all sentence pairs possible from the Cartesian product of a pair of documents and passed through a sentence-length ratio filter in order to obtain candidate sentence pairs. SMT systems were trained on the extracted sentence pairs using the additional features from the comparable corpora like distortion and position of current and previously aligned sentences. This resulted in a state of the art approach with respect to the translation performance of low resource languages.Similar to our proposed approach, BIBREF5 ( BIBREF5 ) showed how using parallel documents from Wikipedia for domain specific alignment would improve translation quality of SMT systems on in-domain data. In this method, similarity between all pairs of cross-language sentences with different text similarity measures are estimated. The issue of domain definition is overcome by the use of IR techniques which use the characteristic vocabulary of the domain to query a Lucene search engine over the entire corpus. The candidate sentences are defined based on word overlap and the decision whether a sentence pair is parallel or not using the maximum entropy classifier. The difference in the BLEU scores between out of domain and domain-specific translation is proved clearly using the word embeddings from characteristic vocabulary extracted using the extracted additional bitexts. BIBREF2 ( BIBREF2 ) extract parallel sentences without the use of a classifier. Target language candidate sentences are found using the translation of source side comparable corpora. Sentence tail removal is used to strip the tail parts of sentence pairs which differ only at the end. This, along with the use of parallel sentences enhanced the BLEU score and helped to determine if the translated source sentence and candidate target sentence are parallel by measuring the word and translation error rate. This method succeeds in eliminating the need for domain specific text by using the target side as a source of candidate sentences. However, this approach is not feasible if there isn't a good source side translation system to begin with, like in our case.Yet another approach which uses an existing translation system to extract parallel sentences from comparable documents was proposed by BIBREF3 ( BIBREF3 ). They describe a framework for machine translation using multilingual Wikipedia articles. The parallel corpus is assembled iteratively, by using a statistical machine translation system trained on a preliminary sentence-aligned corpus, to score sentence-level en–jp BLEU scores. After filtering out the unaligned pairs based on the MT evaluation metric, the SMT is retrained on the filtered pairs.Approach	In this section, we will describe the entire pipeline, depicted in Figure FIGREF5 , which is involved in training a parallel sentence extraction system, and also to infer and decode high-precision nearly-parallel sentence-pairs from bilingual article pages collected from Wikipedia.Bootstrap Dataset	The parallel sentence extraction system needs a sentence aligned corpus which has been curated. These sentences were used as the ground truth pairs when we trained the model to classify parallel sentence pair from non-parallel pairs.Negative Sampling	The binary classifier described in the next section, assigns a translation probability score to a given sentence pair, after learning from examples of translations and negative examples of non-translation pairs. For, this we make a simplistic assumption that the parallel sentence pairs found in the bootstrap dataset are unique combinations, which fail being translations of each other, when we randomly pick a sentence from both the sets. Thus, there might be cases of false negatives due to the reliance on unsupervised random sampling for generation of negative labels.Therefore at the beginning of every epoch, we randomly sample INLINEFORM0 negative sentences of the target language for every source sentence. From a few experiments and also from the literature, we converged on INLINEFORM1 to be performing the best, given our compute constraints.Model	Here, we describe the neural network architecture as shown in BIBREF6 ( BIBREF6 ), where the network learns to estimate the probability that the sentences in a given sentence pair, are translations of each other, INLINEFORM0 , where INLINEFORM1 is the candidate source sentence in the given pair, and INLINEFORM2 is the candidate target sentence.As illustrated in Figure FIGREF5 (d), the architecture uses a siamese network BIBREF7 , consisting of a bidirectional RNN BIBREF8 sentence encoder with recurrent units such as long short-term memory units, or LSTMs BIBREF9 and gated recurrent units, or GRUs BIBREF10 learning a vector representation for the source and target sentences and the probability of any given pair of sentences being translations of each other. For seq2seq architectures, especially in translation, we have found the that the recommended recurrent unit is GRU, and all our experiments use this over LSTM.The forward RNN reads the variable-length sentence and updates its recurrent state from the first token until the last one to create a fixed-size continuous vector representation of the sentence. The backward RNN processes the sentence in reverse. In our experiments, we use the concatenation of the last recurrent state in both directions as a final representation INLINEFORM0 DISPLAYFORM0 where INLINEFORM0 is the gated recurrent unit (GRU). After both source and target sentences have been encoded, we capture their matching information by using their element-wise product and absolute element-wise difference. We estimate the probability that the sentences are translations of each other by feeding the matching vectors into fully connected layers: DISPLAYFORM0 where INLINEFORM0 is the sigmoid function, INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , INLINEFORM4 and INLINEFORM5 are model parameters. The model is trained by minimizing the cross entropy of our labeled sentence pairs: DISPLAYFORM0 where INLINEFORM0 is the number of source sentences and INLINEFORM1 is the number of candidate target sentences being considered.For prediction, a sentence pair is classified as parallel if the probability score is greater than or equal to a decision threshold INLINEFORM0 that we need to fix. We found that to get high precision sentence pairs, we had to use INLINEFORM1 , and if we were able to sacrifice some precision for recall, a lower INLINEFORM2 of 0.80 would work in the favor of reducing OOV rates. DISPLAYFORM0 Dataset	We experimented with two language pairs: English – Hindi (en–hi) and English – Tamil (en–ta). The parallel sentence extraction systems for both en–ta and en–hi were trained using the architecture described in SECREF7 on the following bootstrap set of parallel corpora:An English-Tamil parallel corpus BIBREF11 containing a total of INLINEFORM0 sentence pairs, composed of INLINEFORM1 English Tokens and INLINEFORM2 Tamil Tokens.An English-Hindi parallel corpus BIBREF12 containing a total of INLINEFORM0 sentence pairs, from which a set of INLINEFORM1 sentence pairs were picked randomly.Subsequently, we extracted parallel sentences using the trained model, and parallel articles collected from Wikipedia. There were INLINEFORM0 bilingual English-Tamil and INLINEFORM1 English-Hindi titles on the Wikimedia dumps collected in December 2017.Evaluation Metrics	For the evaluation of the performance of our sentence extraction models, we looked at a few sentences manually, and have done a qualitative analysis, as there was no gold standard evaluation set for sentences extracted from Wikipedia. In Table TABREF13 , we can see the qualitative accuracy for some parallel sentences extracted from Tamil. The sentences extracted from Tamil, have been translated to English using Google Translate, so as to facilitate a comparison with the sentences extracted from English.For the statistical machine translation and neural machine translation evaluation we use the BLEU score BIBREF13 as an evaluation metric, computed using the multi-bleu script from Moses BIBREF14 .Sentence Alignment	Figures FIGREF16 shows the number of high precision sentences that were extracted at INLINEFORM0 without greedy decoding. Greedy decoding could be thought of as sampling without replacement, where a sentence that's already been extracted on one side of the extraction system, is precluded from being considered again. Hence, the number of sentences without greedy decoding, are of an order of magnitude higher than with decoding, as can be seen in Figure FIGREF16 .Machine Translation	We evaluated the quality of the extracted parallel sentence pairs, by performing machine translation experiments on the augmented parallel corpus.As the dataset for training the machine translation systems, we used high precision sentences extracted with greedy decoding, by ranking the sentence-pairs on their translation probabilities. Phrase-Based SMT systems were trained using Moses BIBREF14 . We used the grow-diag-final-and heuristic for extracting phrases, lexicalised reordering and Batch MIRA BIBREF15 for tuning (the default parameters on Moses). We trained 5-gram language models with Kneser-Ney smoothing using KenLM BIBREF16 . With these parameters, we trained SMT systems for en–ta and en–hi language pairs, with and without the use of extracted parallel sentence pairs.For training neural machine translation models, we used the TensorFlow BIBREF17 implementation of OpenNMT BIBREF18 with attention-based transformer architecture BIBREF19 . The BLEU scores for the NMT models were higher than for SMT models, for both en–ta and en–hi pairs, as can be seen in Table TABREF23 .Conclusion	In this paper, we evaluated the benefits of using a neural network procedure to extract parallel sentences. Unlike traditional translation systems which make use of multi-step classification procedures, this method requires just a parallel corpus to extract parallel sentence pairs using a Siamese BiRNN encoder using GRU as the activation function.This method is extremely beneficial for translating language pairs with very little parallel corpora. These parallel sentences facilitate significant improvement in machine translation quality when compared to a generic system as has been shown in our results.The experiments are shown for English-Tamil and English-Hindi language pairs. Our model achieved a marked percentage increase in the BLEU score for both en–ta and en–hi language pairs. We demonstrated a percentage increase in BLEU scores of 11.03% and 14.7% for en–ta and en–hi pairs respectively, due to the use of parallel-sentence pairs extracted from comparable corpora using the neural architecture.As a follow-up to this work, we would be comparing our framework against other sentence alignment methods described in BIBREF20 , BIBREF21 , BIBREF22 and BIBREF23 . It has also been interesting to note that the 2018 edition of the Workshop on Machine Translation (WMT) has released a new shared task called Parallel Corpus Filtering where participants develop methods to filter a given noisy parallel corpus (crawled from the web), to a smaller size of high quality sentence pairs. This would be the perfect avenue to test the efficacy of our neural network based approach of extracting parallel sentences from unaligned corpora.","['Do they evaluate their parallel sentence generation?', 'How much data do they manage to gather online?', 'Which models do they use for phrase-based SMT?', 'Which models do they use for NMT?', 'What are the BLEU performance improvements they achieve?']","['Unacceptable', 'Unacceptable', 'Phrase-Based SMT systems were trained using Moses, grow-diag-final-and heuristic were used for extracting phrases,  and lexicalised reordering and Batch MIRA for tuning.', 'Unacceptable', 'Unacceptable']"
13,"Data-driven Approach for Quality Evaluation on Knowledge Sharing Platform	In recent years, voice knowledge sharing and question answering (Q&A) platforms have attracted much attention, which greatly facilitate the knowledge acquisition for people. However, little research has evaluated on the quality evaluation on voice knowledge sharing. This paper presents a data-driven approach to automatically evaluate the quality of a specific Q&A platform (Zhihu Live). Extensive experiments demonstrate the effectiveness of the proposed method. Furthermore, we introduce a dataset of Zhihu Live as an open resource for researchers in related areas. This dataset will facilitate the development of new methods on knowledge sharing services quality evaluation.	Introduction	Knowledge sharing platforms such as Quora and Zhihu emerge as very convenient tools for acquiring knowledge. These question and answer (Q&A) platforms are newly emerged communities about knowledge acquisition, experience sharing and social networks services (SNS).Unlike many other Q&A platforms, Zhihu platform resembles a social network community. Users can follow other people, post ideas, up-vote or down-vote answers, and write their own answers. Zhihu allows users to keep track of specific fields by following related topics, such as “Education”, “Movie”, “Technology” and “Music”. Once a Zhihu user starts to follow a specific topic or a person, the related updates are automatically pushed to the user's feed timeline.Although these platforms have exploded in popularity, they face some potential problems. The key problem is that as the number of users grows, a large volume of low-quality questions and answers emerge and overwhelm users, which make users hard to find relevant and helpful information.Zhihu Live is a real-time voice-answering product on the Zhihu platform, which enables the speakers to share knowledge, experience, and opinions on a subject. The audience can ask questions and get answers from the speakers as well. It allows communication with the speakers easily and efficiently through the Internet. Zhihu Live provides an extremely useful reward mechanism (like up-votes, following growth and economic returns), to encourage high-quality content providers to generate high-level information on Zhihu platform.However, due to the lack of efficient filter mechanism and evaluation schemes, many users suffer from lots of low-quality contents, which affects the service negatively. Recently, studies on social Q&A platforms and knowledge sharing are rising and have achieved many promising results. Shah et al. BIBREF0 propose a data-driven approach with logistic regression and carefully designed hand-crafted features to predict the answer quality on Yahoo! Answers. Wang et al. BIBREF1 illustrate that heterogeneity in the user and question graphs are important contributors to the quality of Quora's knowledge base. Paul et al. BIBREF2 explore reputation mechanism in quora through detailed data analysis, their experiments indicate that social voting helps users identify and promote good content but is prone to preferential attachment. Patil et al. BIBREF3 propose a method to detect experts on Quora by their activity, quality of answers, linguistic characteristics and temporal behaviors, and achieves 97% accuracy and 0.987 AUC. Rughinis et al. BIBREF4 indicate that there are different regimes of engagement at the intersection of the technological infrastructure and users' participation in Quora.All of these works are mainly focused on answer ranking and answer quality evaluation. But there is little research achievement about quality evaluation in voice-answering areas. In this work, we present a data-driven approach for quality evaluation about Zhihu Live, by consuming the dataset we collected to gather knowledge and insightful conclusion. The proposed data-driven approach includes data collection, storage, preprocessing, data analysis, and predictive analysis via machine learning. The architecture of our data-driven method is shown in Fig. FIGREF3 . The records are crawled from Zhihu Live official website and stored in MongoDB. Data preprocessing methods include cleaning and data normalization to make the dataset satisfy our target problem. Descriptive data analysis and predictive analysis are also conducted for deeper analysis about this dataset.The main contributions of this paper are as follows: (1) We release a public benchmark dataset which contains 7242 records and 286,938 text comments about Zhihu Live. Detailed analysis about the dataset is also discussed in this paper. This dataset could help researchers verify their ideas in related fields. (2) By analyzing this dataset, we gain several insightful conclusion about Zhihu Live. (3) We also propose a multi-branched neural network (MTNet) to evaluate Zhihu Lives' scores. The superiority of our proposed model is demonstrated by comparing performance with other mainstream regressors.The rest of this paper is organized as follows: Section 2 describes detailed procedures of ZhihuLive-DB collection, and descriptive analysis. Section 3 illustrates our proposed MTNet. In section 4, we give a detailed description of experiments, and the last section discusses the conclusion of this paper and future work.Data Collection	In order to make a detailed analysis about Zhihu Live with data-driven approach, the first step is to collect Zhihu Live data. Since there is no public dataset available for research and no official APIs, we develop a web spider with python requests library to crawl data from Zhihu Live official website. Our crawling strategy is breadth-first traverse (we crawl the records one by one from the given URLs, and then extract more detailed information from sub URLs). We follow the crawler-etiquette defined in Zhihu's robots.txt. So we randomly set 2 to 5 seconds pause after per crawling to prevent from being banned by Zhihu, and avoid generating abnormal traffic as well. Our spider crawls 7242 records in total. Majority of the data are embedded in Ajax calls. In addition, we also crawl 286,938 comments of these Zhihu Lives. All of the datasets are stored in MongoDB, a widely-used NoSQL database.Statistical Analysis	The rating scores are within a range of INLINEFORM0 . We calculate min, Q1, median, Q3, max, mean, and mode about review count (see Table TABREF8 ). Because the number of received review may greatly influence the reliability of the review score. From Table TABREF8 we can see that many responses on Zhihu Live receive no review at all, which are useless for quality evaluation.One of the most challenging problems is no unique standard to evaluate a Zhihu Live as a low-quality or high-quality one. A collection of people may highly praise a Zhihu Live while others may not. In order to remove the sample bias, we delete those records whose review count is less than Q1 (11). So we get 5477 records which belong to 18 different fields.The statistics of review scores after deletion are shown in Table TABREF9 . The mean score of 5477 records is 4.51, and the variance is 0.16. It indicates that the majority of Zhihu Lives are of high quality, and the users' scores are relatively stable.Badge in Zhihu represents identity authentication of public figures and high-quality answerers. Only those who hold a Ph.D. degree or experts in a specific domain can be granted a badge. Hence, these speakers tend to host high-quality Zhihu Lives theoretically. Table TABREF10 shows that 3286 speakers hold no badge, 1475 speakers hold 1 badge, and 446 speakers hold 2 badges, respectively. The average score of Zhihu Lives given by two badges holders is slightly higher than others. We can conclude that whether the speaker holds badges does have slightly influence on the Zhihu Live quality ratings, which is consistent with our supposition.Furthermore, we calculate the average scores of different Zhihu Live types (See Table TABREF11 ). We find that Others, Art and Sports fields contain more high-quality Zhihu Lives, while Delicacy, Business and Psychology fields contain more low-quality Lives. We can conclude that the topics related to self-improvement tend to receive more positive comments.There are two types of Zhihu accounts: personal and organization. From Table TABREF12 , we can see that the majority of the Zhihu Live speakers are men with personal accounts. Organizations are less likely to give presentation and share ideas upon Zhihu Live platform.Comments Text Analysis	Apart from analyzing Zhihu Live dataset, we also adopt TextRank BIBREF5 algorithm to calculate TOP-50 hot words with wordcloud visualization (see Fig. FIGREF14 ). Bigger font denotes higher weight of the word, we can see clearly that the majority of the comments show contentment about Zhihu Lives, and the audience care more about “content”, “knowledge” and “speaker”.Performance Metric	We define the quality evaluation problem as a standard regression task since the scores we aim to predict are continuous values. Hence we use Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) to estimate the performance of diverse learning algorithms. MAE and RMSE are used to evaluate the fit quality of the learning algorithms, if they are close to zero, it means the learning algorithm fits the dataset well. DISPLAYFORM0 DISPLAYFORM1 where INLINEFORM0 denotes the number of samples, INLINEFORM1 denotes the input feature vector of a sample INLINEFORM2 , INLINEFORM3 denotes the learning algorithm, INLINEFORM4 denotes the groundtruth score of a Zhihu Live response INLINEFORM5 .The results are calculated by randomly selecting 80% in the dataset as training set, and the remaining records as test set.MTNet	In this section, we first give a brief introduction of the neural network and then present a description of our proposed MTNet to predict the quality of responses in detail.Deep Neural Network	Deep neural network (DNN) has aroused dramatically attention due to their extraordinary performance in computer vision BIBREF6 , BIBREF7 , speech recognition BIBREF8 and natural language processing (NLP) BIBREF9 tasks. We apply DNN to our Zhihu Live quality evaluation problem aiming to approximate a function INLINEFORM0 which can accurately predict a Zhihu Live's score.In our quality evaluation task, we take multiple layer perception BIBREF8 as the basic composition block of MTNet. Since we treat the Zhihu Live quality evaluation problem as a regression task, we set the output neuron equal to 1. DNNs are trained by backpropagation algorithm BIBREF8 .The calculation details of neural network can be illustrated as: DISPLAYFORM0 where INLINEFORM0 represents output of a neuron, INLINEFORM1 represents weights of the connections, INLINEFORM2 represents bias, INLINEFORM3 represents nonlinear activation function (sigmoid, tanh and ReLU are often used in practice).MTNet Architecture	The architecture of our proposed MTNet is shown in Fig. FIGREF24 . It includes 4 parts: an input layer for receiving raw data; shared layers for general feature extraction through stacked layers and non-linear transformation; branched layers for specific feature extraction; and the output layer with one neuron. The output of the last shared layer is fed into different branches. These branches are trained jointly. In the last shared layer, the information flow is split into many branches BIBREF7 , which enables feature sharing and reuse. Finally, the output result is calculated in the output layer by averaging outputs from these branches BIBREF10 . The overall neural network with different branches is trained in parallel. The detailed configuration of MTNet is listed in Tabel TABREF21 .The advantages of MTNet are as follows:With multi-branched layers, different data under diverse levels can be fed into different branches, which enables MTNet extract multi-level features for later regression.Multi-branched architecture in our MTNet can also act as an ensemble method BIBREF10 , which promotes the performance as well.We use mean square error (MSE) with INLINEFORM0 regularization as the cost function. DISPLAYFORM0 where INLINEFORM0 denotes the raw input of INLINEFORM1 -th data sample, INLINEFORM2 denotes the capacity of dataset, INLINEFORM3 denotes groundtruth score of INLINEFORM4 -th Zhihu Live. INLINEFORM5 denotes INLINEFORM6 regularization to prevent from overfitting.Experiments	We implement our method based on Scikit-Learn BIBREF11 and PyTorch , and the experiments are conducted on a server with NVIDIA Tesla K80 GPU.Data Preprocessing	Several features' types in ZhihuLive-DB are not numerical, while machine learning predictor can only support numerical values as input. We clean the original dataset through the following preprocessing methods.For categorical features, we replace them with one-hot-encoding BIBREF11 .The missing data is filled with Median of each attribute.We normalize the numerical values with minimum subtraction and range division to ensure values [0, 1] intervals.The review scores are used as labels in our experiments, our task is to precisely estimate the scores with MTNet. Since the data-driven methods are based on crowd wisdom on Zhihu Live platform, they don't need any additional labeling work, and ensure the reliability of the scores of judgment as well.Feature Selection	Since feature selection plays an import part in a data mining task, conventional feature extraction methods need domain knowledge BIBREF12 . Feature selection influences model's performance dramatically BIBREF13 .For conventional regression algorithms, we conduct feature selection by adopting the best Top K features through univariate statistical tests. The hyper-parameter such as regularization item INLINEFORM0 is determined through cross validation. For each regression algorithm mentioned above, the hyper-parameters are carefully tuned, and the hyper-parameters with the best performance are denoted as the final comparison results. The details of f_regression BIBREF14 , BIBREF11 feature selection are as follows:We calculate the correlation between each regressor and label as: INLINEFORM0 .We convert the correlation into an F score and then to a p-value.Finally, we get 15-dimension feature vector as the input for conventional (non-deep learning based) regressors.Deep neural network can learn more abstract features via stacked layers. Deep learning has empowered many AI tasks (like computer vision BIBREF6 and natural language processing BIBREF9 ) in an end-to-end fashion. We apply deep learning to our Zhihu Live quality evaluation problem. Furthermore, we also compare our MTNet algorithm with baseline models with carefully designed features.Experimental Results	We train our MTNet with Adam optimizer for 20 epochs. We set batch size as 8, and weight decay as 1e-5, we adopt 3 branched layers in MTNet. Detailed configuration is shown in Table TABREF21 . We use ReLU in shared layers, and relu6 in branched layers to prevent information loss. Our proposed MTNet achieves 0.2250 on MAE and 0.3216 on RMSE, respectively.We compare MTNet with other mainstream regression algorithms BIBREF14 (linear regression, KNN, SVR, Random Forest and MLP). The architecture of MLP is 15-16-8-8-1, where each number represents the number of neurons in each layer. We try three kinds of kernels (RBF kernel, linear kernel, and poly kernel) with SVR in our experiments for fair comparison.The results are listed in Table TABREF37 . Our method achieves the best performance in contrast to the compared baseline regressors.Conclusion	In this paper, we adopt a data-driven approach which includes data collection, data cleaning, data normalization, descriptive analysis and predictive analysis, to evaluate the quality on Zhihu Live platform. To the best of our knowledge, we are the first to research quality evaluation of voice-answering products. We publicize a dataset named ZhihuLive-DB, which contains 7242 records and 286,938 comments text for researchers to evaluate Zhihu Lives' quality. We also make a detailed analysis to reveal inner insights about Zhihu Live. In addition, we propose MTNet to accurately predict Zhihu Lives' quality. Our proposed method achieves best performance compared with the baselines.As knowledge sharing and Q&A platforms continue to gain a greater popularity, the released dataset ZhihuLive-DB could greatly help researchers in related fields. However, current data and attributes are relatively unitary in ZhihuLive-DB. The malicious comment and assessment on SNS platforms are also very important issues to be taken into consideration. In our future work, we will gather richer dataset, and integrate malicious comments detector into our data-driven approach.Acknowledgements	Supported by Foundation Research Funds for the Central Universities (Program No.2662017JC049) and State Scholarship Fund (NO.261606765054).","['Can their method be transferred to other Q&A platforms (in other languages)?', 'What measures of quality do they use for a Q&A platform?']","['Unacceptable', 'Rating scores given by users']"
14,"BAE: BERT-based Adversarial Examples for Text Classification	Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans but which get misclassified by the model. We present BAE, a powerful black box attack for generating grammatically correct and semantically coherent adversarial examples. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging a language model to generate alternatives for the masked tokens. Compared to prior work, we show that BAE performs a stronger attack on three widely used models for seven text classification datasets.	Introduction	Recent studies have shown the vulnerability of ML models to adversarial attacks, small perturbations which lead to misclassification of inputs. Adversarial example generation in NLP BIBREF0 is more challenging than in common computer vision tasks BIBREF1, BIBREF2, BIBREF3 due to two main reasons: the discrete nature of input space and ensuring semantic coherence with the original sentence. A major bottleneck in applying gradient based BIBREF4 or generator model BIBREF5 based approaches to generate adversarial examples in NLP is the backward propagation of the perturbations from the continuous embedding space to the discrete token space.Recent works for attacking text models rely on introducing errors at the character level in words BIBREF6, BIBREF7 or adding and deleting words BIBREF8, BIBREF9, BIBREF10, etc. for creating adversarial examples. These techniques often result in adversarial examples which are unnatural looking and lack grammatical correctness, and thus can be easily identified by humans.TextFooler BIBREF11 is a black-box attack, that uses rule based synonym replacement from a fixed word embedding space to generate adversarial examples. These adversarial examples do not account for the overall semantics of the sentence, and consider only the token level similarity using word embeddings. This can lead to out-of-context and unnaturally complex replacements (see Table ), which can be easily identifiable by humans.The recent advent of powerful language models BIBREF12, BIBREF13 in NLP has paved the way for using them in various downstream applications. In this paper, we present a simple yet novel technique: BAE (BERT-based Adversarial Examples), which uses a language model (LM) for token replacement to best fit the overall context. We perturb an input sentence by either replacing a token or inserting a new token in the sentence, by means of masking a part of the input and using a LM to fill in the mask (See Figure FIGREF1). BAE relies on the powerful BERT masked LM for ensuring grammatical correctness of the adversarial examples. Our attack beats the previous baselines by a large margin and confirms the inherent vulnerabilities of modern text classification models to adversarial attacks. Moreover, BAE produces more richer and natural looking adversarial examples as it uses the semantics learned by a LM.To the best of our knowledge, we are the first to use a LM for adversarial example generation. We summarize our major contributions as follows:We propose BAE, a novel strategy for generating natural looking adversarial examples using a masked language model.We introduce 4 BAE attack modes, all of which are almost always stronger than previous baselines on 7 text classification datasets.We show that, surprisingly, just a few replace/insert operations can reduce the accuracy of even a powerful BERT-based classifier by over $80\%$ on some datasets.Methodology	Problem Definition We are given a dataset $(S,Y) = \lbrace (\mathbb {S}_1,y_1),(\mathbb {S}_2,y_2)\dots (\mathbb {S}_m,y_m)\rbrace $ and a trained classification model $C:\mathbb {S}\rightarrow Y$. We assume the soft-label black-box setting where the attacker can only query the classifier for output probabilities on a given input, and does not have access to the model parameters, gradients or training data. For an input pair $(\mathbb {S},y)$, we want to generate an adversarial example $\mathbb {S}_{adv}$ such that $C(\mathbb {S}_{adv}){\ne }y$ where $\mathbb {S}_{adv}$ is natural looking, grammatically correct and semantically similar to $\mathbb {S}$ (by some pre-defined definitionof similarity).BAE For generating adversarial example $\mathbb {S}_{adv}$, we define two perturbations on the input $\mathbb {S}$:Replace a token $t \in \mathbb {S}$ with anotherInsert a new token $t^{\prime }$ in $\mathbb {S}$Some tokens in the input are more attended to by $C$ than others, and therefore contribute more towards the final prediction. Replacing these tokens or inserting a new token adjacent to them can thus have a stronger effect on altering the classifier prediction. We estimate the token importance $I_i$ of each token $t_i \in \mathbb {S}=[t_1, \dots , t_n]$, by deleting $t_i$ from $\mathbb {S}$ and computing the decrease in probability of predicting the correct label $y$, similar to BIBREF11.While the motivation for replacing tokens in decreasing order of importance is clear, we conjecture that adjacent insertions in this same order can lead to a powerful attack. This intuition stems from the fact that the inserted token changes the local context around the original token.The Replace (R) and Insert (I) operations are performed on a token $t$ by masking it and inserting a mask token adjacent to it in $\mathbb {S}$ respectively. The pre-trained BERT masked language model (MLM) is used to predict the mask tokens (See Figure FIGREF1).BERT is a powerful LM trained on a large training corpus ($\sim $ 2 billion words), and hence the predicted mask tokens fit well grammatically in $\mathbb {S}$. The BERT-MLM does not however guarantee semantic coherence to the original text $\mathbb {S}$ as demonstrated by the following simple example. Consider the sentence: `the food was good'. For replacing the token `good', BERT-MLM may predict the tokens `nice' and `bad', both of which fit well into the context of the sentence. However, replacing `good' with `bad' changes the original sentiment of the sentence.To ensure semantic similarity on introducing perturbations in the input text, we filter the set of top K masked tokens (K is a pre-defined constant) predicted by BERT-MLM using a Universal Sentence Encoder (USE) BIBREF14 based sentence similarity scorer. For the R operations we add an additional check for grammatical correctness of the generated adversarial example by filtering out predicted tokens that do not form the same part of speech (POS) as the original token $t_i$ in the sentence.To choose the token for a perturbation (R/I) that best attacks the model from the filtered set of predicted tokens:If there are multiple tokens can cause $C$ to misclassify $\mathbb {S}$ when they replace the mask, we choose the token which makes $\mathbb {S}_{adv}$ most similar to the original $\mathbb {S}$ based on the USE score.If no token causes misclassification, we choose the perturbation that decreases the prediction probability $P(C(\mathbb {S}_{adv}){=}y)$ the most.The perturbations are applied iteratively to the input tokens in decreasing order of importance, until either $C(\mathbb {S}_{adv}){\ne }y$ (successful attack) or all the tokens of $\mathbb {S}$ have been perturbed (failed attack).We present 4 attack modes for BAE based on the R and I operations, where for each token $t$ in $\mathbb {S}$:BAE-R: Replace token $t$ (See Algorithm )BAE-I: Insert a token to the left or right of $t$BAE-R/I: Either replace token $t$ or insert a token to the left or right of $t$BAE-R+I: First replace token $t$, then insert a token to the left or right of $t$Experiments	Datasets and Models We evaluate our adversarial attacks on different text classification datasets from tasks such as sentiment classification, subjectivity detection and question type classification. Amazon, Yelp, IMDB are sentence-level sentiment classification datasets which have been used in recent work BIBREF15 while MR BIBREF16 contains movie reviews based on sentiment polarity. MPQA BIBREF17 is a dataset for opinion polarity detection, Subj BIBREF18 for classifying a sentence as subjective or objective and TREC BIBREF19 is a dataset for question type classification.We use 3 popular text classification models: word-LSTM BIBREF20, word-CNN BIBREF21 and a fine-tuned BERT BIBREF12 base-uncased classifier. For each dataset we train the model on the training data and perform the adversarial attack on the test data. For complete model details refer to Appendix.As a baseline, we consider TextFooler BIBREF11 which performs synonym replacement using a fixed word embedding space BIBREF22. We only consider the top $K{=}50$ synonyms from the MLM predictions and set a threshold of 0.8 for the cosine similarity between USE based embeddings of the adversarial and input text.Results We perform the 4 modes of our attack and summarize the results in Table . Across datasets and models, our BAE attacks are almost always more effective than the baseline attack, achieving significant drops of 40-80% in test accuracies, with higher average semantic similarities as shown in parentheses. BAE-R+I is the strongest attack since it allows both replacement and insertion at the same token position, with just one exception. We observe a general trend that the BAE-R and BAE-I attacks often perform comparably, while the BAE-R/I and BAE-R+I attacks are much stronger. We observe that the BERT-based classifier is more robust to the BAE and TextFooler attacks than the word-LSTM and word-CNN models which can be attributed to its large size and pre-training on a large corpus.The baseline attack is often stronger than the BAE-R and BAE-I attacks for the BERT based classifier. We attribute this to the shared parameter space between the BERT-MLM and the BERT classifier before fine-tuning. The predicted tokens from BERT-MLM may not drastically change the internal representations learned by the BERT classifier, hindering their ability to adversarially affect the classifier prediction.Effectiveness We study the effectiveness of BAE on limiting the number of R/I operations permitted on the original text. We plot the attack performance as a function of maximum $\%$ perturbation (ratio of number of word replacements and insertions to the length of the original text) for the TREC dataset. From Figure , we clearly observe that the BAE attacks are consistently stronger than TextFooler. The classifier models are relatively robust to perturbations up to 20$\%$, while the effectiveness saturates at 40-50$\%$. Surprisingly, a 50$\%$ perturbation for the TREC dataset translates to replacing or inserting just 3-4 words, due to the short text lengths.Qualitative Examples We present adversarial examples generated by the attacks on a sentence from the IMDB and Yelp datasets in Table . BAE produces more natural looking examples than TextFooler as tokens predicted by the BERT-MLM fit well in the sentence context. TextFooler tends to replace words with complex synonyms, which can be easily detected. Moreover, BAE's additional degree of freedom to insert tokens allows for a successful attack with fewer perturbations.Human Evaluation We consider successful adversarial examples generated from the Amazon and IMDB datasets and verify their sentiment and grammatical correctness. Human evaluators annotated the sentiment and the grammar (Likert scale of 1-5) of randomly shuffled adversarial examples and original texts. From Table , BAE and TextFooler have inferior accuracies compared to the Original, showing they are not always perfect. However, BAE has much better grammar scores, suggesting more natural looking adversarial examples.Ablation Study We analyze the benefits of R/I operations in BAE in Table . From the table, the splits $\mathbb {A}$ and $\mathbb {B}$ are the $\%$ of test points which compulsorily need I and R operations respectively for a successful attack. We can observe that the split $\mathbb {A}$ is larger than $\mathbb {B}$ thereby indicating the importance of the I operation over R. Test points in split require both R and I operations for a successful attack. Interestingly, split is largest for Subj, which is the most robust to attack (Table ) and hence needs both R/I operations. Thus, this study gives positive insights towards the importance of having the flexibility to both replace and insert words.Refer to the Appendix for additional results, effectiveness graphs and details of human evaluation.Conclusion	In this paper, we have presented a novel technique for generating adversarial examples (BAE) based on a language model. The results obtained on several text classification datasets demonstrate the strength and effectiveness of our attack.","['what text classification datasets do they evaluate on?', 'which models is their approach compared to?']","['Unacceptable', 'Unacceptable']"
15,"Predicting the Role of Political Trolls in Social Media	We investigate the political roles of ""Internet trolls"" in social media. Political trolls, such as the ones linked to the Russian Internet Research Agency (IRA), have recently gained enormous attention for their ability to sway public opinion and even influence elections. Analysis of the online traces of trolls has shown different behavioral patterns, which target different slices of the population. However, this analysis is manual and labor-intensive, thus making it impractical as a first-response tool for newly-discovered troll farms. In this paper, we show how to automate this analysis by using machine learning in a realistic setting. In particular, we show how to classify trolls according to their political role ---left, news feed, right--- by using features extracted from social media, i.e., Twitter, in two scenarios: (i) in a traditional supervised learning scenario, where labels for trolls are available, and (ii) in a distant supervision scenario, where labels for trolls are not available, and we rely on more-commonly-available labels for news outlets mentioned by the trolls. Technically, we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph, from which we extract several types of learned representations, i.e.,~embeddings, for the trolls. Experiments on the ""IRA Russian Troll"" dataset show that our methodology improves over the state-of-the-art in the first scenario, while providing a compelling case for the second scenario, which has not been explored in the literature thus far.	Introduction	Internet “trolls” are users of an online community who quarrel and upset people, seeking to sow discord by posting inflammatory content. More recently, organized “troll farms” of political opinion manipulation trolls have also emerged.Such farms usually consist of state-sponsored agents who control a set of pseudonymous user accounts and personas, the so-called “sockpuppets”, which disseminate misinformation and propaganda in order to sway opinions, destabilize the society, and even influence elections BIBREF0.The behavior of political trolls has been analyzed in different recent circumstances, such as the 2016 US Presidential Elections and the Brexit referendum in UK BIBREF0, BIBREF1. However, this kind of analysis requires painstaking and time-consuming manual labor to sift through the data and to categorize the trolls according to their actions. Our goal in the current paper is to automate this process with the help of machine learning (ML). In particular, we focus on the case of the 2016 US Presidential Elections, for which a public dataset from Twitter is available. For this case, we consider only accounts that post content in English, and we wish to divide the trolls into some of the functional categories identified by BIBREF0: left troll, right troll, and news feed.We consider two possible scenarios. The first, prototypical ML scenario is supervised learning, where we want to learn a function from users to categories {left, right, news feed}, and the ground truth labels for the troll users are available. This scenario has been considered previously in the literature by BIBREF2. Unfortunately, a solution for such a scenario is not directly applicable to a real-world use case. Suppose a new troll farm trying to sway the upcoming European or US elections has just been discovered. While the identities of the accounts might be available, the labels to learn from would not be present. Thus, any supervised machine learning approach would fall short of being a fully automated solution to our initial problem.A more realistic scenario assumes that labels for troll accounts are not available. In this case, we need to use some external information in order to learn a labeling function. Indeed, we leverage more persistent entities and their labels: news media. We assume a learning scenario with distant supervision where labels for news media are available. By combining these labels with a citation graph from the troll accounts to news media, we can infer the final labeling on the accounts themselves without any need for manual labeling.One advantage of using distant supervision is that we can get insights about the behavior of a newly-discovered troll farm quickly and effortlessly. Differently from troll accounts in social media, which usually have a high churn rate, news media accounts in social media are quite stable. Therefore, the latter can be used as an anchor point to understand the behavior of trolls, for which data may not be available.We rely on embeddings extracted from social media. In particular, we use a combination of embeddings built on the user-to-user mention graph, the user-to-hashtag mention graph, and the text of the tweets of the troll accounts. We further explore several possible approaches using label propagation for the distant supervision scenario.As a result of our approach, we improve the classification accuracy by more than 5 percentage points for the supervised learning scenario. The distant supervision scenario has not previously been considered in the literature, and is one of the main contributions of the paper. We show that even by hiding the labels from the ML algorithm, we can recover 78.5% of the correct labels.The contributions of this paper can be summarized as follows:We predict the political role of Internet trolls (left, news feed, right) in a realistic, unsupervised scenario, where labels for the trolls are not available, and which has not been explored in the literature before.We propose a novel distant supervision approach for this scenario, based on graph embeddings, BERT, and label propagation, which projects the more-commonly-available labels for news media onto the trolls who cited these media.We improve over the state of the art in the traditional, fully supervised setting, where training labels are available.Related Work ::: Trolls and Opinion Manipulation	The promise of social media to democratize content creation BIBREF3 has been accompanied by many malicious attempts to spread misleading information over this new medium, which quickly got populated by sockpuppets BIBREF4, Internet water army BIBREF5, astroturfers BIBREF6, and seminar users BIBREF7. Several studies have shown that trust is an important factor in online relationships BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12, but building trust is a long-term process and our understanding of it is still in its infancy BIBREF13. This makes it easy for politicians and companies to manipulate user opinions in forums BIBREF14, BIBREF15, BIBREF16.Trolls. Social media have seen the proliferation of fake news and clickbait BIBREF17, BIBREF18, aggressiveness BIBREF19, and trolling BIBREF20. The latter often is understood to concern malicious online behavior that is intended to disrupt interactions, to aggravate interacting partners, and to lure them into fruitless argumentation in order to disrupt online interactions and communication BIBREF5. Here we are interested in studying not just any trolls, but those that engage in opinion manipulation BIBREF21, BIBREF22, BIBREF23. This latter definition of troll has also become prominent in the general public discourse recently. BIBREF24 have also suggested that the spreading of misinformation online is fostered by the presence of polarization and echo chambers in social media BIBREF25, BIBREF26, BIBREF27.Trolling behavior is present and has been studied in all kinds of online media: online magazines BIBREF28, social networking sites BIBREF20, online computer games BIBREF29, online encyclopedia BIBREF30, and online newspapers BIBREF31, among others.Troll detection was addressed using domain-adapted sentiment analysis BIBREF32, lexico-syntactic features about writing style and structure BIBREF33, BIBREF34, and graph-based approaches over signed social networks BIBREF35.Sockpuppet is a related notion, and refers to a person who assumes a false identity in an Internet community and then speaks to or about themselves while pretending to be another person. The term has also been used to refer to opinion manipulation, e.g., in Wikipedia BIBREF36. Sockpuppets have been identified by using authorship-identification techniques and link analysis BIBREF37. It has been also shown that sockpuppets differ from ordinary users in their posting behavior, linguistic traits, and social network structure BIBREF4.Internet Water Army is a literal translation of the Chinese term wangluo shuijun, which is a metaphor for a large number of people who are well organized to flood the Internet with purposeful comments and articles. Internet water army has been allegedly used in China by the government (also known as 50 Cent Party) as well as by a number of private organizations.Astroturfing is an effort to simulate a political grass-roots movement. It has attracted strong interest from political science, and research on it has focused on massive streams of microblogging data BIBREF6.Identification of malicious accounts in social media includes detecting spam accounts BIBREF38, BIBREF39, fake accounts BIBREF40, BIBREF41, compromised and phishing accounts BIBREF42. Fake profile detection has also been studied in the context of cyber-bullying BIBREF43. A related problem is that of Web spam detection, which has been addressed as a text classification problem BIBREF44, e.g., using spam keyword spotting BIBREF45, lexical affinity of arbitrary words to spam content BIBREF46, frequency of punctuation and word co-occurrence BIBREF47.Trustworthiness of online statements is an emerging topic, given the interest in fake news BIBREF48. It is related to trolls, as they often engage in opinion manipulation and spread rumors BIBREF49. Research topics include predicting the credibility of information in social media BIBREF50, BIBREF51, BIBREF52, BIBREF53 and political debates BIBREF54, BIBREF55, BIBREF56, and stance classification BIBREF57.For example, BIBREF58 leverage user reputation, author writing style, and various time-based features, BIBREF59 analyze the interaction of content and social network structure, and BIBREF60 studied how Twitter users judge truthfulness. BIBREF61 study how people handle rumors in social media, and found that users with higher reputation are more trusted, and thus can spread rumors easily. BIBREF62 use temporal patterns to detect rumors and to predict their frequency, and BIBREF61 focus on conversational threads. More recent work has focused on the credibility and the factuality in community forums BIBREF63, BIBREF64, BIBREF65, BIBREF23.Related Work ::: Understanding the Role of Political Trolls	None of the above work has focused on understanding the role of political trolls. The only closely relevant work is that of BIBREF2, who predict the roles of the Russian trolls on Twitter by leveraging social theory and Actor-Network Theory approaches. They characterize trolls using the digital traces they leave behind, which is modeled using a time-sensitive semantic edit distance.For this purpose, they use the “IRA Russian Troll” dataset BIBREF0, which we also use in our experiments. However, we have a very different approach based on graph embeddings, which we show to be superior to their method in the supervised setup. We further experiment with a new, and arguably more realistic, setup based on distant supervision, where labels are not available. To the best of our knowledge, this setup has not been explored in previous work.Related Work ::: Graph Embeddings	Graph embeddings are machine learning techniques to model and capture key features from a graph automatically. They can be trained either in a supervised or in an unsupervised manner BIBREF66. The produced embeddings are latent vector representations that map each vertex $V$ in a graph $G$ to a $d$-dimensional vector. The vectors capture the underlying structure of the graph by putting “similar” vertices close together in the vector space. By expressing our data as a graph structure, we can leverage and extract critical insights about the topology and the contextual relationships between the vertices in the graph.In mathematical terms, graph embeddings can be expressed as a function $f: V \rightarrow R^d$ from the set of vertices $V$ to a set of embeddings, where $d$ is the dimensionality of the embeddings. The function $f$ can be represented as a matrix of dimensions $|V| \times d$. In our experiments, we train Graph Embeddings in an unsupervised manner by using node2vec BIBREF67, which is based on random walks over the graph. Essentially, this is an application of the well-known skip-gram model BIBREF68 from word2vec to random walks on graphs.Besides node2vec, there have been a number of competing proposals for building graph embeddings; see BIBREF66 for an extensive overview of the topic. For example, SNE BIBREF69 model both the graph structure and some node attributes. Similarly, Line BIBREF70 represent each node as the concatenation of two embedded vectors that model first- and second-order proximity. TriDNR BIBREF71 represents nodes by coupling several neural network models. For our experiments, we use node2vec, as we do not have access to user attributes: the users have been banned from Twitter, their accounts were suspended, and we only have access to their tweets thanks to the “IRA Russian Trolls” dataset.Method	Given a set of known political troll users (each user being represented as a collection of their tweets), we aim to detect their role: left, right, or news feed. BIBREF0 describe these roles as follows:Right Trolls spread nativist and right-leaning populist messages. Such trolls support the candidacy and Presidency of Donald Trump and denigrate the Democratic Party; moreover, they often send divisive messages about mainstream and moderate Republicans.Left Trolls send socially liberal messages and discuss gender, sexual, religious, and -especially- racial identity. Many tweets are seemed intentionally divisive, attacking mainstream Democratic politicians, particularly Hillary Clinton, while supporting Bernie Sanders prior to the elections.News Feed Trolls overwhelmingly present themselves as US local news aggregators, linking to legitimate regional news sources and tweeting about issues of local interest.Technically, we leverage the community structure and the text of the messages in the social network of political trolls represented as a graph, from which we learn and extract several types of vector representations, i.e., troll user embeddings. Then, armed with these representations, we tackle the following tasks:A fully supervised learning task, where we have labeled training data with example troll and their roles;A distant supervision learning task, in which labels for the troll roles are not available at training time, and thus we use labels for news media as a proxy, from which we infer labels for the troll users.Method ::: Embeddings	We use two graph-based (user-to-hashtag and user-to-mentioned-user) and one text-based (BERT) embedding representations.Method ::: Embeddings ::: U2H	We build a bipartite, undirected User-to-Hashtag (U2H) graph, where nodes are users and hashtags, and there is an edge $(u,h)$ between a user node $u$ and a hashtag node $h$ if user $u$ uses hashtag $h$ in their tweets. This graph is bipartite as there are no edges connecting two user nodes or two hashtag nodes. We run node2vec BIBREF67 on this graph, and we extract the embeddings for the users (we ignore the hashtag embeddings). We use 128 dimensions for the output embeddings. These embeddings capture how similar troll users are based on their usage of hashtags.Method ::: Embeddings ::: U2M	We build an undirected User-to-Mentioned-User (U2M) graph, where the nodes are users, and there is an edge $(u,v)$ between two nodes if user $u$ mentions user $v$ in their tweets (i.e., $u$ has authored a tweet that contains “@$v$” ). We run node2vec on this graph and we extract the embeddings for the users. As we are interested only in the troll users, we ignore the embeddings of users who are only mentioned by other trolls. We use 128 dimensions for the output embeddings. The embeddings extracted from this graph capture how similar troll users are according to the targets of their discussions on the social network.Method ::: Embeddings ::: BERT	BERT offers state-of-the-art text embeddings based on the Transformer BIBREF72. We use the pre-trained BERT-large, uncased model, which has 24-layers, 1024-hidden, 16-heads, and 340M parameters, which yields output embeddings with 768 dimensions. Given a tweet, we generate an embedding for it by averaging the representations of the BERT tokens from the penultimate layer of the neural network. To obtain a representation for a user, we average the embeddings of all their tweets. The embeddings extracted from the text capture how similar users are according to their use of language.Method ::: Fully Supervised Learning (T1)	Given a set of troll users for which we have labels, we use the above embeddings as a representation to train a classifier. We use an L2-regularized logistic regression (LR) classifier. Each troll user is an example, and the label for the user is available for training thanks to manual labeling. We can therefore use cross-validation to evaluate the predictive performance of the model, and thus the predictive power of the features.We experiment with two ways of combining features: embedding concatenation and model ensembling. Embedding concatenation concatenates the feature vectors from different embeddings into a longer feature vector, which we then use to train the LR model. Model ensembling instead trains a separate model with each kind of embedding, and then merges the prediction of the different models by averaging the posterior probabilities for the different classes. Henceforth, we denote embedding concatenation with the symbol ${2.5mu}\Vert {2.5mu}$ and model ensembling with $\oplus $. For example, U2H ${2.5mu}\Vert {2.5mu}$ U2M is a model trained on the concatenation of U2H and U2M embeddings, while U2H $\oplus $ BERT represents the average predictions of two models, one trained on U2H embeddings and one on BERT.Method ::: Distant Supervision (T2)	In the distant supervision scenario, we assume not to have access to user labels. Given a set of troll users without labels, we use the embeddings described in Section SECREF9 together with mentions of news media by the troll users to create proxy models. We assume that labels for news media are readily available, as they are stable sources of information that have a low churn rate.We propagate labels from the given media to the troll user that mentions them according to the following media-to-user mapping:This propagation can be done in different ways: (a) by training a proxy model for media and then applying it to users, (b) by additionally using label propagation (LP) for semi-supervised learning.Let us describe the proxy model propagation for ($a$) first. Let $M$ be the set of media, and $U$ be the set of users. We say a user $u \in U$ mentions a medium $m \in M$ if $u$ posts a tweet that contains a link to the website of $m$. We denote the set of users that mention the medium $m$ as $C_m \subseteq U$.We can therefore create a representation for a medium by aggregating the embeddings of the users that mention the target medium. Such a representation is convenient as it lies in the same space as the user representation. In particular, given a medium $m \in M$, we compute its representation $R(m)$ aswhere $R(u)$ is the representation of user $u$, i.e., one (or a concatenation) of the embeddings described in Section SECREF9.Finally, we can train a LR model that uses $R(m)$ as features and the label for the medium $l(m)$. This model can be applied to predict the label of a user $u$ by using the same type of representation $R(u)$, and the label mapping in Equation DISPLAY_FORM16.Label Propagation ($b$) is a transductive, graph-based, semi-supervised machine learning algorithm that, given a small set of labeled examples, assigns labels to previously unlabeled examples. The labels of each example change in relationship to the labels of neighboring ones in a properly-defined graph.More formally, given a partially-labeled dataset of examples $X = X_u \cup X_l$, of which $X_l$ are labeled examples with labels $Y_l$, and $X_u$ are unlabeled examples, and a similarity graph $G(X,E)$, the label propagation algorithm finds the set of unknown labels $Y_u$ such that the number of discordant pairs $(u,v) \in E : y_u \ne y_v$ is minimized, where $y_z$ is the label assigned to example $z$.The algorithm works as follows: At every iteration of propagation, each unlabeled node updates its label to the most frequent one among its neighbors. LP reaches convergence when each node has the same label as the majority of its neighbors. We define two different versions of LP by creating two different versions of the similarity graph $G$.Method ::: Distant Supervision (T2) ::: LP1	Label Propagation using direct mention.In the first case, the set of edges among users $U$ in the similarity graph $G$ consists of the logical OR between the 2-hop closure of the U2H and the U2M graph. That is, for each two users $u, v \in U$, there is an edge in the similarity graph $(u,v) \in E$ if $u$ and $v$ share a common hashtag or a common user mentionThe graph therefore uses the same information that is available to the embeddings.To this graph, which currently encompasses only the set of users $U$, we add connections to the set of media $M$. We add an edge between each pair $(u,m)$ if $u \in C_m$. Then, we run the label propagation algorithm, which propagates the labels from the labeled nodes $M$ to the unlabeled nodes $U$, thanks to the mapping from Equation DISPLAY_FORM16.Method ::: Distant Supervision (T2) ::: LP2	Label Propagation based on a similarity graph.In this case, we use the same representation for the media as in the proxy model case above, as described by Equation DISPLAY_FORM17. Then, we build a similarity graph among media and users based on their embeddings. For each pair $x,y \in U \cup M$ there is an edge in the similarity graph $(x,y) \in E$ iffwhere sim is a similarity function between vectors, e.g., cosine similarity, and $\tau $ is a user-specified parameter that regulates the sparseness of the similarity graph.Finally, we perform label propagation on the similarity graph defined by the embedding similarity, with the set of nodes corresponding to $M$ starting with labels, and with the set of nodes corresponding to $U$ starting without labels.Data ::: IRA Russian Troll Tweets	Our main dataset contains 2973371 tweets by 2848 Twitter users, which the US House Intelligence Committee has linked to the Russian Internet Research Agency (IRA). The data was collected and published by BIBREF0, and then made available online. The time span covers the period from February 2012 to May 2018.The trolls belong to the following manually assigned roles: Left Troll, Right Troll, News Feed, Commercial, Fearmonger, Hashtag Gamer, Non English, Unknown. BIBREF2 have argued that the first three categories are not only the most frequent, but also the most interesting ones. Moreover, focusing on these troll types allows us to establish a connection between troll types and the political bias of the news media they mention. Table TABREF13 shows a summary of the troll role distribution, the total number of tweets per role, as well as examples of troll usernames and tweets.Data ::: Media Bias/Fact Check	We use data from Media Bias/Fact Check (MBFC) to label news media sites. MBFC divides news media into the following bias categories: Extreme-Left, Left, Center-Left, Center, Center-Right, Right, and Extreme-Right. We reduce the granularity to three categories by grouping Extreme-Left and Left as LEFT, Extreme-Right and Right as RIGHT, and Center-Left, Center-Right, and Center as CENTER.Table TABREF24 shows some basic statistics about the resulting media dataset. Similarly to the IRA dataset, the distribution is right-heavy.Experiments and Evaluation ::: Experimental Setup	For each user in the IRA dataset, we extracted all the links in their tweets, we expanded them recursively if they were shortened, we extracted the domain of the link, and we checked whether it could be found in the MBFC dataset. By grouping these relationships by media, we constructed the sets of users $C_m$ that mention a given medium $m \in M$.The U2H graph consists of 108410 nodes and 443121 edges, while the U2M graph has 591793 nodes and 832844 edges. We ran node2vec on each graph to extract 128-dimensional vectors for each node. We used these vectors as features for the fully supervised and for the distant-supervision scenarios. For Label Propagation, we used an empirical threshold for edge materialization $\tau = 0.55$, to obtain a reasonably sparse similarity graph.We used two evaluation measures: accuracy, and macro-averaged F1 (the harmonic average of precision and recall). In the supervised scenario, we performed 5-fold cross-validation. In the distant-supervision scenario, we propagated labels from the media to the users. Therefore, in the latter case the user labels were only used for evaluation.Experiments and Evaluation ::: Evaluation Results	Table TABREF25 shows the evaluation results. Each line of the table represents a different combination of features, models, or techniques. As mentioned in Section SECREF3, the symbol `${2.5mu}\Vert {2.5mu}$' denotes a single model trained on the concatenation of the features, while the symbol `$\oplus $' denotes an averaging of individual models trained on each feature separately. The tags `LP1' and `LP2' denote the two label propagation versions, by mention and by similarity, respectively.We can see that accuracy and macro-averaged F1 are strongly correlated and yield very consistent rankings for the different models. Thus, henceforth we will focus our discussion on accuracy.We can see in Table TABREF25 that it is possible to predict the roles of the troll users by using distant supervision with relatively high accuracy. Indeed, the results for T2 are lower compared to their T1 counterparts by only 10 and 20 points absolute in terms of accuracy and F1, respectively. This is impressive considering that the models for T2 have no access to labels for troll users.Looking at individual features, for both T1 and T2, the embeddings from U2M outperform those from U2H and from BERT. One possible reason is that the U2M graph is larger, and thus contains more information. It is also possible that the social circle of a troll user is more indicative than the hashtags they used. Finally, the textual content on Twitter is quite noisy, and thus the BERT embeddings perform slightly worse when used alone.All our models with a single type of embedding easily outperform the model of BIBREF2. The difference is even larger when combining the embeddings, be it by concatenating the embedding vectors or by training separate models and then combining the posteriors of their predictions.By concatenating the U2M and the U2H embeddings (U2H ${2.5mu}\Vert {2.5mu}$ U2M), we fully leverage the hashtags and the mention representations in the latent space, thus achieving accuracy of 88.7 for T1 and 78.0 for T2, which is slightly better than when training separate models and then averaging their posteriors (U2H $\oplus $ U2M): 88.3 for T1 and 77.9 for T2. Adding BERT embeddings to the combination yields further improvements, and follows a similar trend, where feature concatenation works better, yielding 89.2 accuracy for T1 and 78.2 for T2 (compared to 89.0 accuracy for T1 and 78.0 for T2 for U2H $\oplus $ U2M $\oplus $ BERT).Adding label propagation yields further improvements, both for LP1 and for LP2, with the latter being slightly superior: 89.6 vs. 89.3 accuracy for T1, and 78.5 vs. 78.3 for T2.Overall, our methodology achieves sizable improvements over previous work, reaching an accuracy of 89.6 vs. 84.0 of BIBREF2 in the fully supervised case. Moreover, it achieves 78.5 accuracy in the distant supervised case, which is only 11 points behind the result for T1, and is about 10 points above the majority class baseline.Discussion ::: Ablation Study	We performed different experiments with the hyper-parameters of the graph embeddings. With smaller dimensionality (i.e., using 16 dimensions instead of 128), we noticed 2–3 points of absolute decrease in accuracy across the board.Moreover, we found that using all of the data for learning the embeddings was better than focusing only on users that we target in this study, namely left, right, and news feed, i.e., using the rest of the data adds additional context to the embedding space, and makes the target labels more contextually distinguishable. Similarly, we observe 5–6 points of absolute drop in accuracy when training our embeddings on tweets by trolls labeled as left, right, and news feed.Discussion ::: Comparison to Full Supervision	Next, we compared to the work of BIBREF2, who had a fully supervised learning scenario, based on Tarde's Actor-Network Theory. They paid more attention to the content of the tweet by applying a text-distance metric in order to capture the semantic distance between two sequences. In contrast, we focus on critical elements of information that are salient in Twitter: hashtags and user mentions. By building a connection between users, hashtags, and user mentions, we effectively filtered out the noise and we focused only on the most sensitive type of context, thus automatically capturing features from this network via graph embeddings.Discussion ::: Reverse Classification: Media from Trolls	Table TABREF30 shows an experiment in distant supervision for reverse classification, where we trained a model on the IRA dataset with the troll labels, and then we applied that model to the representation of the media in the MBFC dataset, where each medium is represented as the average of the embeddings of the users who cited that medium. We can see that we could improve over the baseline by 20 points absolute in terms of accuracy and by 41 in terms absolute in terms of macro-averaged F1.We can see in Table TABREF30 that the relative ordering in terms or performance for the different models is consistent with that for the experiments in the previous section. This suggests that the relationship between trolls and media goes both ways, and thus we can use labels for media as a way to label users, and we can also use labels for troll users as a way to label media.Conclusion and Future Work	We have proposed a novel approach to analyze the behavior patterns of political trolls according to their political leaning (left vs. news feed vs. right) using features from social media, i.e., from Twitter. We experimented with two scenarios: (i) supervised learning, where labels for trolls are provided, and (ii) distant supervision, where such labels are not available, and we rely on more common labels for news outlets cited by the trolls. Technically, we leveraged the community structure and the text of the messages in the online social network of trolls represented as a graph, from which we extracted several types of representations, i.e., embeddings, for the trolls. Our experiments on the “IRA Russian Troll” dataset have shown improvements over the state-of-the-art in the supervised scenario, while providing a compelling case for the distant-supervision scenario, which has not been explored before.In future work, we plan to apply our methodology to other political events such as Brexit as well as to other election campaigns around the world, in connection to which large-scale troll campaigns have been revealed. We further plan experiments with other graph embedding methods, and with other social media. Finally, the relationship between media bias and troll's political role that we have highlighted in this paper is extremely interesting. We have shown how to use it to go from the media-space to the user-space and vice-versa, but so far we have just scratched the surface in terms of understanding of the process that generated these data and its possible applications.Acknowledgments	This research is part of the Tanbih project, which aims to limit the effect of “fake news”, propaganda and media bias by making users aware of what they are reading. The project is developed in collaboration between the Qatar Computing Research Institute, HBKU and the MIT Computer Science and Artificial Intelligence Laboratory.Gianmarco De Francisci Morales acknowledges support from Intesa Sanpaolo Innovation Center. The funder had no role in the study design, in the data collection and analysis, in the decision to publish, or in the preparation of the manuscript.","['What is the state-of-the-art?', 'How large is the dataset?', 'How are labels for trolls obtained?', 'Do they only look at tweets?']","['Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable']"
16,"Shallow Discourse Parsing with Maximum Entropy Model	In recent years, more research has been devoted to studying the subtask of the complete shallow discourse parsing, such as indentifying discourse connective and arguments of connective. There is a need to design a full discourse parser to pull these subtasks together. So we develop a discourse parser turning the free text into discourse relations. The parser includes connective identifier, arguments identifier, sense classifier and non-explicit identifier, which connects with each other in pipeline. Each component applies the maximum entropy model with abundant lexical and syntax features extracted from the Penn Discourse Tree-bank. The head-based representation of the PDTB is adopted in the arguments identifier, which turns the problem of indentifying the arguments of discourse connective into finding the head and end of the arguments. In the non-explicit identifier, the contextual type features like words which have high frequency and can reflect the discourse relation are introduced to improve the performance of non-explicit identifier. Compared with other methods, experimental results achieve the considerable performance.	Introduction	Automated deriving discourse relation from free text is a challenging but im-portant problem. The shallow discourse parsing is very useful in the text summariza-tion BIBREF0 , opinion analysis BIBREF1 and natural language generation. Shallow discourse parser is the system of parsing raw text into a set of discourse relations between two adjacent or non-adjacent text spans. Discourse relation is composed of a discourse connective, two arguments of the discourse connective and the sense of the discourse connective. Discourse connective signals the explicit dis-course relation, but in non-explicit discourse relation, a discourse connective is omit-ted. Two arguments of the discourse connective, Arg1 and Arg2, which are the two adjacent or non-adjacent text spans connecting in the discourse relation. The sense of the discourse connective characterizes the nature of the discourse relations. The following discourse relation annotation is taken from the document in the PDTB. Arg1 is shown in italicized, and Arg2 is shown in bold. The discourse connective is underlined.The connective identifier finds the connective word, “unless”. The arguments identifier locates the two arguments of “unless”. The sense classifier labels the dis-course relation. The non-explicit identifier checks all the pair of adjacent sentences. If the non-explicit identifier indentifies the pair of sentences as non-explicit relation, it will label it the relation sense. Though many research work BIBREF2 , BIBREF3 , BIBREF4 are committed to the shallow discourse parsing field, all of them are focus on the subtask of parsing only rather than the whole parsing process. Given all that, a full shallow discourse parser framework is proposed in our paper to turn the free text into discourse relations set. The parser includes connective identifier, arguments identifier, sense classifier and non-explicit identifier, which connects with each other in pipeline. In order to enhance the performance of the parser, the feature-based maximum entropy model approach is adopted in the experiment. Maximum entropy model offers a clean way to combine diverse pieces of contextual evidence in order to estimate the probability of a certain linguistic class occurring with a certain linguistic context in a simple and accessible manner. The three main contributions of the paper are:The rest of this paper is organized as follows. Section 2 reviews related work in discourse parsing. Section 3 describes the experimental corpus–PDTB. Section 4 de-scribes the framework and the components of the parser. Section 5 presents experi-ments and evaluations. Conclusions are presented in the Section 6.Related Work	Different from traditional shallow parsing BIBREF5 , BIBREF6 , BIBREF7 which is dealing with a single sentence, the shallow discourse parsing tries to analyze the discourse level information, which is more complicated. Since the release of second version of the Penn Discourse Treebank (PDTB), which is over the 1 million word Wall Street Journal corpus, analyzing the PDTB-2.0 is very useful for further study on shallow discourse parsing. Prasad et al. PrasadDLMRJW08 describe lexically-grounded annotations of discourse relations in PDTB. Identifying the discourse connective from ordinary words accurately is not easy because discourse words can have discourse or non-discourse usage. Pitler and Nenkova PitlerN09 use syntax feature to disambiguate explicit discourse connective in text and prove that the syntactic features can improve performance in disambiguation task. After identifying the discourse connective, there is a need to find the arguments. There are some different methods to find the arguments. Ziheng Lin et al. LinNK14 first identify the locations of Arg1, and choose sentence from prior candidate sentence if the location is before the connective. Otherwise, label arguments span by choosing the high node in the parse tree. Wellner and Pustejovsky WellnerP07 focus on identifying rela-tions between the pairs of head words. Based on such thinking, Robert Elwell and Jason Baldridge ElwellB08 improve the performance using connective specific rankers, which differentiate between specific connectives and types of connectives. Ziheng Lin et al. LinNK14 present an implicit discourse relation classifier based the Penn Discourse Treebank. All of these efforts can be viewed as the part of the full parser. More and more researcher has been devoted to the subtask of the shallow discourse parsing, like dis-ambiguating discourse connective BIBREF8 , finding implicit relation BIBREF9 . There is a need to pull these subtasks together to achieve more efforts. So in this paper, we develop a full shallow discourse parser based on the maximum entropy model using abundant features. Our parser attempts to identify connective, arguments of discourse connec-tive and the relation into right sense.The Penn Discourse Treebank	The Penn Discourse Treebank is the corpus which is over one million words from the Wall Street Journal BIBREF10 , annotated with discourse relations. The table one shows the discourse relation extracted from PDTB. Arg1 is shown in italicized, Arg2 is shown in bold. The discourse connective is underlined.Discourse connective is the signal of explicit relation. Discourse connective in the PTDB can be classified as three categories: subordinating conjunctions (e.g., because, if, etc.), coordinating conjunctions (e.g., and, but, etc.), and discourse adverbials (e.g., however, also, tec.). Different category has different discourse usage. Discourse connective word can be ambiguous between discourse or non-discourse usage. An apparent example is 'after' because it can be a VP (e.g., ""If you are after something, you are trying to get it"") or it can be a connective (e.g., “It wasn't until after Christmas that I met Paul”). In the case of explicit relation, Arg2 is the argument to which the connective is syntactically bound, and Arg1 is the other argument. But the span of the arguments of explicit relation can be clauses or sentences. In the case of implicit relation, Arg1 is before Arg2 BIBREF11 . For explicit, implicit and altLex relation, there are three-level hierarchy of relation senses. The first level consists of four major relation classes: Temporal, Contingency, Comparison, and Expansion.Shallow Discourse Parser framework	We design a complete discourse parser connecting subtasks together in pipeline. First let’s have a quick view about the procedure of the parser. The first step is pre-processing, which takes the raw text as input and generates POS tag of token, the dependency tree, constituent tree and so on. Next the parser needs to distinguish the connective between discourse usage and non-discourse usage. Then, the two argu-ments of discourse connective need to be identified. Next to above steps, the parser labels the discourse relation right sense. Until now the explicit relations already have been found fully. The last step is indentifying the non-explicit relation. The parser will handle every pair of adjacent sentences in same paragraph. The text is pre-processed by the Stanford CoreNLP tools. Stanford CoreNLP provides a series of natural language analysis tools which can tokenize the text, label tokens with their part-of-speech (POS) tag, and provides full syntactic analysis, in-cluding both constituent and dependency representation. The parser uses Stanford CoreNLP toolkit to preprocess the raw text. Next, each component of the parser will be described in detail. Connective Identifier	The main duty of this component is disambiguate the connective words which are in PDTB predefined set. Pitler and Nenkova citePitlerN09 show that syntactic features are very useful on disambiguate discourse connective, so we adopt these syntactic fea-tures as part of our features. Ziheng Lin et al. LinKN09 show that a connective’s context and part-of-speech (POS) gives a very strong indication of discourse usage. The table 1 shows the feature we use.Arguments Identifier	On this step, we adopt the head-based thinking BIBREF12 , which turns the problem of identifying arguments of discourse connective into identifying the head and end of the arguments. First, we need to extract the candidates of arguments. To reduce the Arg1 candidates space, we only consider words with appropriate part-of-speech (all verbs, common nouns, adjectives) and within 10 ”steps” between word and connec-tive as candidates, where a step is either a sentence boundary or a dependency link. Only words in the same sentence with the connective are considered for Arg2 candi-dates. Second, we need to choose the best candidate as the head of Arg1 and Arg2. In the end, we need to obtain the arguments span according head and end of argu-ments on the constituent tree. The table 2 shows the feature we use. The table 3 shows the procedure of the arguments identifier.Sense Classifier	The sense of discourse relation has three levels: class, type and subtype. There are four classes on the top level of the sense: Comparison, Temporal , Con-tingency, Expansion. Each class includes a set of different types, and some types may have different subtypes. The connective itself is a very good feature because discourse connective almost determine senses. So we train an explicit classifier using simple but effective features.Non-explicit Identifier	The non-explicit relation is the relation between adjacent sentences in same para-graph. So we just check adjacent sentences which don’t form explicit relation and then label them with non-explicit relation or nothing. In the experiment, we find that the two arguments of non-explicit relation have association with each other and also have some common words. So we introduce feature words, which indicate appear-ance of relation, like “it, them”.Experiments	In our experiments, we make use of the Section 02-21 in the PDTB as training set, Section 22 as testing set. All of components adopt maximum entropy model. In order to evaluate the performance of the discourse parser, we compare it with other approaches: (1) Baseline_1, which applies the probability information. The connective identifier predicts the connective according the frequency of the connec-tive in the train set. The arguments identifier takes the immediately previous sentence in which the connective appears as Arg1 and the text span after the connective but in the same sentence with connective as Arg2. The non-explicit identifier labels the ad-jacent sentences according to the frequency of the non-explicit relation. (2) Base-line_2, which is the parser using the Support Vector Maching as the train and predic-tion model with numeric type feature from the hashcode of the textual type feature.It is not surprised to find that Baseline_1 shows the poorest performance, which it just considers the probability information, ignores the contextual link. The perfor-mance of Baseline_2 is better than that of “Baseline_1”. This can be mainly credited to the ability of abundant lexical and syntax features. Our parser shows better per-formance than Baselin_2 because the most of features we use are textual type fea-tures, which are convenient for the maximum entropy model. Though the textual type features can turn into numeric type according to hashcode of string, it is incon-venient for Support Vector Machine because the hashcode of string is not continu-ous. According the performance of the parser, we find that the connective identifying can achieve higher precision and recall rate. In addition, the precision and recall rate of identifying Arg2 is higher than that of identifying Arg1 because Arg2 has stronger syntax link with connective compared to Arg1. The sense has three layers: class, type and subtype.Conclusion	In this paper, we design a full discourse parser to turn any free English text into discourse relation set. The parser pulls a set of subtasks together in a pipeline. On each component, we adopt the maximum entropy model with abundant lexical, syntactic features. In the non-explicit identifier, we introduce some contextual infor-mation like words which have high frequency and can reflect the discourse relation to improve the performance of non-explicit identifier. In addition, we report another two baselines in this paper, namely Baseline1 and Baseline2, which base on probabilistic model and support vector machine model, respectively. Compared with two baselines, our parser achieves the considerable improvement. As future work, we try to explore the deep learning methods BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 to improve this study. We believe that our discourse parser is very useful in many applications because we can provide the full discourse parser turning any unrestricted text into discourse structure.","['Do they manage to consistenly outperform the best performing methods?', 'Do they try to use other models aside from Maximum Entropy?', 'What methods to they compare to?', 'Which dataset to they train and evaluate on?', 'Do they attempt to jointly learn connectives, arguments, senses and non-explicit identiifers end-to-end?']","['Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable']"
17,"Question Generation from a Knowledge Base with Web Exploration	Question generation from a knowledge base (KB) is the task of generating questions related to the domain of the input KB. We propose a system for generating fluent and natural questions from a KB, which significantly reduces the human effort by leveraging massive web resources. In more detail, a seed question set is first generated by applying a small number of hand-crafted templates on the input KB, then more questions are retrieved by iteratively forming already obtained questions as search queries into a standard search engine, before finally questions are selected by estimating their fluency and domain relevance. Evaluated by human graders on 500 random-selected triples from Freebase, questions generated by our system are judged to be more fluent than those of \newcite{serban-EtAl:2016:P16-1} by human graders.	Introduction	Question generation is important as questions are useful for student assessment or coaching purposes in educational or professional contexts, and a large-scale corpus of question and answer pairs is also critical to many NLP tasks including question answering, dialogue interaction and intelligent tutoring systems. There has been much literature so far BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 studying question generation from text. Recently people are becoming interested in question generation from KB, since large-scale KBs, such as Freebase BIBREF7 and DBPedia BIBREF8 , are freely available, and entities and their relations are already present in KBs but not for texts.Question generation from KB is challenging as function words and morphological forms for entities are abstracted away when a KB is created. To tackle this challenge, previous work BIBREF9 , BIBREF10 relies on massive human-labeled data. Treating question generation as a machine translation problem, serban-EtAl:2016:P16-1 train a neural machine translation (NMT) system with 10,000 $\langle $ triple, question $\rangle $ pairs. At test time, input triples are “translated” into questions with the NMT system. On the other hand, the question part of the 10,000 pairs are human generated, which requires a large amount of human effort. In addition, the grammaticality and naturalness of generated questions can not be guaranteed (as seen in Table 1 ).We propose a system for generating questions from KB that significantly reduces the human effort by leveraging the massive web resources. Given a KB, a small set of question templates are first hand-crafted based on the predicates in the KB. These templates consist of a transcription of the predicate in the KB (e.g. performsActivity $\Rightarrow $ how to) and placeholders for the subject (#X#) and the object (#Y#). A seed question set is then generated by applying the templates on the KB. The seed question set is further expanded through a search engine (e.g., Google, Bing), by iteratively forming each generated question as a search query to retrieve more related question candidates. Finally a selection step is applied by estimating the fluency and domain relevance of each question candidate.The only human labor in this work is the question template construction. Our system does not require a large number of templates because: (1) the iterative question expansion can produce a large number of questions even with a relatively small number of seed questions, as we see in the experiments, (2) multiple entities in the KB share the same predicates. Another advantage is that our system can easily generate updated questions as web is self-updating consistently. In our experiment, we compare with serban-EtAl:2016:P16-1 on 500 random selected triples from Freebase BIBREF7 . Evaluated by 3 human graders, questions generated by our system are significantly better then serban-EtAl:2016:P16-1 on grammaticality and naturalness.Knowledge Base	A knowledge base (KB) can be viewed as a directed graph, in which nodes are entities (such as “jigsaw” and “CurveCut”) and edges are relations of entities (such as “performsActivity”). A KB can also be viewed as a list of triples in the format of $\langle $ subject, predicate, object $\rangle $ , where subjects and objects are entities, and predicates are relations.System	Shown in Figure 1 , our system contains the sub-modules of question template construction, seed question generation, question expansion and selection. Given an input KB, a small set of question templates is first constructed such that each template is associated with a predicate, then a seed question set is generated by applying the template set on the input KB, before finally more questions are generated from related questions that are iteratively retrieved from a search engine with already-obtained questions as search queries (section ""Experiments"" ). Taking our in-house KB of power tool domain as an example, template “how to use #X#” is first constructed for predicate “performsActivity”. In addition, seed question “how to use jigsaw” is generated by applying the template on triple “ $\langle $ jigsaw, performsActivity, CurveCut $\rangle $ ”, before finally questions (Figure 2 ) are retrieved from Google with the seed question.Question expansion and selection	[t] seed question set $S$ candidate questions $E$ $E \leftarrow S$ $Q \leftarrow S$ $I \leftarrow 0$ len $(Q) > 0$ and $I < I_{max}$ $I = I + 1$ $q_{cur}$ $\leftarrow $ $E$0 .Pop() $E$1 in WebExp $E$2 not $E$3 .contains $E$4 $E$5 .Append( $E$6 ) $E$7 .Push( $E$8 ) Question expansion methodShown in Algorithm ""Experiments"" , the expanded question set $E$ is initialized as the seed question set (Line 1). In each iteration, an already-obtained question is expanded from web and the retrieved questions are added to $E$ if $E$ does not contain them (Lines 6-10). As there may be a large number of questions generated in the loop, we limit the maximum number of iterations with $I_{max}$ (Line 4).The questions collected from the web search engine may not be fluent or domain relevant; especially the domain relevance drops significantly as the iteration goes on. Here we adopt a skip-gram model BIBREF11 and a language model for evaluating the domain relevance and fluency of the expanded questions, respectively. For domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as: $$\textsc {Rel}(q) = \cos (v(q),v(D_{in}))$$   (Eq. 7) where $v(\cdot )$ is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as: $$\textsc {AvgLM}(q) = \frac{\textsc {Lm}(q)}{\textsc {Len}(q)}$$   (Eq. 8) where $\textsc {Lm}(\cdot )$ is the general-domain language model score (log probability), and $\textsc {Len}(\cdot )$ is the word count. We apply thresholds $t_{rel}$ and $t_{flu}$ for domain relevance and fluency respectively, and filter out questions whose scores are below these thresholds.Experiments	We perform three experiments to evaluate our system qualitatively and quantitatively. In the first experiment, we compare our end-to-end system with the previous state-of-the-art method BIBREF10 on Freebase BIBREF7 , a domain-general KB. In the second experiment, we validate our domain relevance evaluation method on a standard dataset about short document classification. In the final experiment, we run our end-to-end system on a highly specialized in-house KB and present sample results, showing that our system is capable of generating questions from domain specific KBs.Evaluation on Freebase	We first compare our system with serban-EtAl:2016:P16-1 on 500 randomly selected triples from Freebase BIBREF7 . For the 500 triples, we hand-crafted 106 templates, as these triples share only 53 distinct predicates (we made 2 templates for each predicate on average). 991 seed questions are generated by applying the templates on the triples, and 1529 more questions are retrieved from Google. To evaluate the fluency of the candidate questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare with the results from serban-EtAl:2016:P16-1. We ask three native English speakers to evaluate the fluency and the naturalness of both results based on a 4-point scheme where 4 is the best.We show the averaged human rate in Table 2 , where we can see that our questions are more grammatical and natural than serban-EtAl:2016:P16-1. The naturalness score is less than the grammatical score for both methods. It is because naturalness is a more strict metric since a natural question should also be grammatical.Shown in Table 1 , we compare our questions with serban-EtAl:2016:P16-1 where questions in the same line describe the same entity. We can see that our questions are grammatical and natural as these questions are what people usually ask on the web. On the other hand, questions from serban-EtAl:2016:P16-1 are either ungrammatical (such as “who was someone who was involved in the leukemia ?” and “whats the title of a book of the subject of the bible ?”), unnatural (“what 's one of the mountain where can you found in argentina in netflix ?”) or confusing (“who was someone who was involved in the leukemia ?”).Domain Relevance	We test our domain-relevance evaluating method on the web snippet dataset, which is a commonly-used for domain classification of short documents. It contains 10,060 training and 2,280 test snippets (short documents) in 8 classes (domains), and each snippet has 18 words on average. There have been plenty of prior results BIBREF12 , BIBREF13 , BIBREF14 on the dataset.Shown in Table 3 , we compare our domain-relevance evaluation method (section ""Experiments"" ) with previous state-of-the-art methods: phan2008learning first derives latent topics with LDA BIBREF15 from Wikipedia, then uses the topics as appended features to expand the short text. chen2011short further expanded phan2008learning by using multi-granularity topics. ma-EtAl:2015:VSM-NLP adopts a Bayesian model that the probability a document $D$ belongs to a topic $t$ equals to the prior of $t$ times the probability each word $w$ in $D$ comes from $t$ . Our method first concatenates training documents of the same domain into one “domain document”, then calculates each document embedding by averaging word embeddings within it, before finally assigns the label of the nearest (cosine similarity) “domain document” to each test document.Simple as it is, our method outperforms all previous methods proving its effectiveness. The reason can be that word embeddings captures the similarity between distinct words (such as “finance” and “economy”), while it is hard for traditional methods. On the order hand, LDA only learns probabilities of words belonging to topics.Evaluation on the Domain-specific KB	The last experiment is on our in-house KB in the power tool domain. It contains 67 distinct predicates, 293 distinct subjects and 279 distinct objects respectively. For the 67 predicates, we hand-craft 163 templates. Here we use the same language model as in our first experiment, and learn a skip-gram model BIBREF11 on Wikipedia for evaluating domain relevance.We generate 12,228 seed questions from which 20,000 more questions are expanded with Google. Shown in Table 4 are some expanded questions from which we can see that most of them are grammatical and relevant to the power tool domain. In addition, most questions are informative and correspond to a specific answer, except the one “do I need a hammer drill” that lacks context information. Finally, in addition to the simple factoid questions, our system generates many complex questions such as “how to cut a groove in wood without a router”.Conclusion	We presented a system to generate natural language questions from a knowledge base. By leveraging rich web information, our system is able to generate domain-relevant questions in wide scope, while human effort is significantly reduced. Evaluated by human graders, questions generated by our system are significantly better than these from serban-EtAl:2016:P16-1 on 500 random-selected triples from Freebase. We also demonstrated generated questions from our in-house KB of power tool domain, which are fluent and domain-relevant in general. Our current system only generates questions without answers, leaving automatic answer mining as our future work.","['Was the filtering based on fluency and domain relevance done automatically?', 'How was domain relevance estimated?', 'How many hand-crafted templates did they have to make?', 'How was the fluency measured?']","['Unacceptable', 'Unacceptable', '269.', 'Unacceptable']"
18,"Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model	Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and release our data and code in hope that this work will promote advances in summarization in the multi-document setting.	Introduction	Summarization is a central problem in Natural Language Processing with increasing applications as the desire to receive content in a concise and easily-understood format increases. Recent advances in neural methods for text summarization have largely been applied in the setting of single-document news summarization and headline generation BIBREF0 , BIBREF1 , BIBREF2 . These works take advantage of large datasets such as the Gigaword Corpus BIBREF3 , the CNN/Daily Mail (CNNDM) dataset BIBREF4 , the New York Times dataset BIBREF5 and the Newsroom corpus BIBREF6 , which contain on the order of hundreds of thousands to millions of article-summary pairs. However, multi-document summarization (MDS), which aims to output summaries from document clusters on the same topic, has largely been performed on datasets with less than 100 document clusters such as the DUC 2004 BIBREF7 and TAC 2011 BIBREF8 datasets, and has benefited less from advances in deep learning methods.Multi-document summarization of news events offers the challenge of outputting a well-organized summary which covers an event comprehensively while simultaneously avoiding redundancy. The input documents may differ in focus and point of view for an event. We present an example of multiple input news documents and their summary in Figure TABREF2 . The three source documents discuss the same event and contain overlaps in content: the fact that Meng Wanzhou was arrested is stated explicitly in Source 1 and 3 and indirectly in Source 2. However, some sources contain information not mentioned in the others which should be included in the summary: Source 3 states that (Wanzhou) is being sought for extradition by the US while only Source 2 mentioned the attitude of the Chinese side.Recent work in tackling this problem with neural models has attempted to exploit the graph structure among discourse relations in text clusters BIBREF9 or through an auxiliary text classification task BIBREF10 . Additionally, a couple of recent papers have attempted to adapt neural encoder decoder models trained on single document summarization datasets to MDS BIBREF11 , BIBREF12 , BIBREF13 .However, data sparsity has largely been the bottleneck of the development of neural MDS systems. The creation of large-scale multi-document summarization dataset for training has been restricted due to the sparsity and cost of human-written summaries. liu18wikisum trains abstractive sequence-to-sequence models on a large corpus of Wikipedia text with citations and search engine results as input documents. However, no analogous dataset exists in the news domain. To bridge the gap, we introduce Multi-News, the first large-scale MDS news dataset, which contains 56,216 articles-summary pairs. We also propose a hierarchical model for neural abstractive multi-document summarization, which consists of a pointer-generator network BIBREF1 and an additional Maximal Marginal Relevance (MMR) BIBREF14 module that calculates sentence ranking scores based on relevancy and redundancy. We integrate sentence-level MMR scores into the pointer-generator model to adapt the attention weights on a word-level. Our model performs competitively on both our Multi-News dataset and the DUC 2004 dataset on ROUGE scores. We additionally perform human evaluation on several system outputs.Our contributions are as follows: We introduce the first large-scale multi-document summarization datasets in the news domain. We propose an end-to-end method to incorporate MMR into pointer-generator networks. Finally, we benchmark various methods on our dataset to lay the foundations for future work on large-scale MDS.Related Work	Traditional non-neural approaches to multi-document summarization have been both extractive BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 as well as abstractive BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 . Recently, neural methods have shown great promise in text summarization, although largely in the single-document setting, with both extractive BIBREF23 , BIBREF24 , BIBREF25 and abstractive methods BIBREF26 , BIBREF27 , BIBREF1 , BIBREF28 , BIBREF29 , BIBREF30 , BIBREF2 In addition to the multi-document methods described above which address data sparsity, recent work has attempted unsupervised and weakly supervised methods in non-news domains BIBREF31 , BIBREF32 . The methods most related to this work are SDS adapted for MDS data. zhang18mds adopts a hierarchical encoding framework trained on SDS data to MDS data by adding an additional document-level encoding. baumel18mds incorporates query relevance into standard sequence-to-sequence models. lebanoff18mds adapts encoder-decoder models trained on single-document datasets to the MDS case by introducing an external MMR module which does not require training on the MDS dataset. In our work, we incorporate the MMR module directly into our model, learning weights for the similarity functions simultaneously with the rest of the model.Multi-News Dataset	Our dataset, which we call Multi-News, consists of news articles and human-written summaries of these articles from the site newser.com. Each summary is professionally written by editors and includes links to the original articles cited. We will release stable Wayback-archived links, and scripts to reproduce the dataset from these links. Our dataset is notably the first large-scale dataset for MDS on news articles. Our dataset also comes from a diverse set of news sources; over 1,500 sites appear as source documents 5 times or greater, as opposed to previous news datasets (DUC comes from 2 sources, CNNDM comes from CNN and Daily Mail respectively, and even the Newsroom dataset BIBREF6 covers only 38 news sources). A total of 20 editors contribute to 85% of the total summaries on newser.com. Thus we believe that this dataset allows for the summarization of diverse source documents and summaries.Statistics and Analysis	The number of collected Wayback links for summaries and their corresponding cited articles totals over 250,000. We only include examples with between 2 and 10 source documents per summary, as our goal is MDS, and the number of examples with more than 10 sources was minimal. The number of source articles per summary present, after downloading and processing the text to obtain the original article text, varies across the dataset, as shown in Table TABREF4 . We believe this setting reflects real-world situations; often for a new or specialized event there may be only a few news articles. Nonetheless, we would like to summarize these events in addition to others with greater news coverage.We split our dataset into training (80%, 44,972), validation (10%, 5,622), and test (10%, 5,622) sets. Table TABREF5 compares Multi-News to other news datasets used in experiments below. We choose to compare Multi-News with DUC data from 2003 and 2004 and TAC 2011 data, which are typically used in multi-document settings. Additionally, we compare to the single-document CNNDM dataset, as this has been recently used in work which adapts SDS to MDS BIBREF11 . The number of examples in our Multi-News dataset is two orders of magnitude larger than previous MDS news data. The total number of words in the concatenated inputs is shorter than other MDS datasets, as those consist of 10 input documents, but larger than SDS datasets, as expected. Our summaries are notably longer than in other works, about 260 words on average. While compressing information into a shorter text is the goal of summarization, our dataset tests the ability of abstractive models to generate fluent text concise in meaning while also coherent in the entirety of its generally longer output, which we consider an interesting challenge.Diversity	We report the percentage of n-grams in the gold summaries which do not appear in the input documents as a measure of how abstractive our summaries are in Table TABREF6 . As the table shows, the smaller MDS datasets tend to be more abstractive, but Multi-News is comparable and similar to the abstractiveness of SDS datasets. Grusky:18 additionally define three measures of the extractive nature of a dataset, which we use here for a comparison. We extend these notions to the multi-document setting by concatenating the source documents and treating them as a single input. Extractive fragment coverage is the percentage of words in the summary that are from the source article, measuring the extent to which a summary is derivative of a text: DISPLAYFORM0 where A is the article, S the summary, and INLINEFORM0 the set of all token sequences identified as extractive in a greedy manner; if there is a sequence of source tokens that is a prefix of the remainder of the summary, that is marked as extractive. Similarly, density is defined as the average length of the extractive fragment to which each summary word belongs: DISPLAYFORM0 Finally, compression ratio is defined as the word ratio between the articles and its summaries: DISPLAYFORM0 These numbers are plotted using kernel density estimation in Figure FIGREF11 . As explained above, our summaries are larger on average, which corresponds to a lower compression rate. The variability along the x-axis (fragment coverage), suggests variability in the percentage of copied words, with the DUC data varying the most. In terms of y-axis (fragment density), our dataset shows variability in the average length of copied sequence, suggesting varying styles of word sequence arrangement. Our dataset exhibits extractive characteristics similar to the CNNDM dataset.Other Datasets	As discussed above, large scale datasets for multi-document news summarization are lacking. There have been several attempts to create MDS datasets in other domains. zopf18mds introduce a multi-lingual MDS dataset based on English and German Wikipedia articles as summaries to create a set of about 7,000 examples. liu18wikisum use Wikipedia as well, creating a dataset of over two million examples. That paper uses Wikipedia references as input documents but largely relies on Google search to increase topic coverage. We, however, are focused on the news domain, and the source articles in our dataset are specifically cited by the corresponding summaries. Related work has also focused on opinion summarization in the multi-document setting; angelidis18opinions introduces a dataset of 600 Amazon product reviews.Preliminaries	We introduce several common methods for summarization.Pointer-generator Network	The pointer-generator network BIBREF1 is a commonly-used encoder-decoder summarization model with attention BIBREF33 which combines copying words from source documents and outputting words from a vocabulary. The encoder converts each token INLINEFORM0 in the document into the hidden state INLINEFORM1 . At each decoding step INLINEFORM2 , the decoder has a hidden state INLINEFORM3 . An attention distribution INLINEFORM4 is calculated as in BIBREF33 and is used to get the context vector INLINEFORM5 , which is a weighted sum of the encoder hidden states, representing the semantic meaning of the related document content for this decoding time step: DISPLAYFORM0  The context vector INLINEFORM0 and the decoder hidden state INLINEFORM1 are then passed to two linear layers to produce the vocabulary distribution INLINEFORM2 . For each word, there is also a copy probability INLINEFORM3 . It is the sum of the attention weights over all the word occurrences: DISPLAYFORM0  The pointer-generator network has a soft switch INLINEFORM0 , which indicates whether to generate a word from vocabulary by sampling from INLINEFORM1 , or to copy a word from the source sequence by sampling from the copy probability INLINEFORM2 . DISPLAYFORM0 where INLINEFORM0 is the decoder input. The final probability distribution is a weighted sum of the vocabulary distribution and copy probability:P(w) = pgenPvocab(w) + (1-pgen)Pcopy(w)Transformer	The Transformer model replaces recurrent layers with self-attention in an encoder-decoder framework and has achieved state-of-the-art results in machine translation BIBREF34 and language modeling BIBREF35 , BIBREF36 . The Transformer has also been successfully applied to SDS BIBREF2 . More specifically, for each word during encoding, the multi-head self-attention sub-layer allows the encoder to directly attend to all other words in a sentence in one step. Decoding contains the typical encoder-decoder attention mechanisms as well as self-attention to all previous generated output. The Transformer motivates the elimination of recurrence to allow more direct interaction among words in a sequence.MMR	Maximal Marginal Relevance (MMR) is an approach for combining query-relevance with information-novelty in the context of summarization BIBREF14 . MMR produces a ranked list of the candidate sentences based on the relevance and redundancy to the query, which can be used to extract sentences. The score is calculated as follows:MMR=*argmax D i RS [ Sim 1 (D i ,Q)-(1-) D j S Sim2 (D i ,D j ) ] where INLINEFORM0 is the collection of all candidate sentences, INLINEFORM1 is the query, INLINEFORM2 is the set of sentences that have been selected, and INLINEFORM3 is set of the un-selected ones. In general, each time we want to select a sentence, we have a ranking score for all the candidates that considers relevance and redundancy. A recent work BIBREF11 applied MMR for multi-document summarization by creating an external module and a supervised regression model for sentence importance. Our proposed method, however, incorporates MMR with the pointer-generator network in an end-to-end manner that learns parameters for similarity and redundancy.Hi-MAP Model	In this section, we provide the details of our Hierarchical MMR-Attention Pointer-generator (Hi-MAP) model for multi-document neural abstractive summarization. We expand the existing pointer-generator network model into a hierarchical network, which allows us to calculate sentence-level MMR scores. Our model consists of a pointer-generator network and an integrated MMR module, as shown in Figure FIGREF19 .Sentence representations	To expand our model into a hierarchical one, we compute sentence representations on both the encoder and decoder. The input is a collection of sentences INLINEFORM0 from all the source documents, where a given sentence INLINEFORM1 is made up of input word tokens. Word tokens from the whole document are treated as a single sequential input to a Bi-LSTM encoder as in the original encoder of the pointer-generator network from see2017ptrgen (see bottom of Figure FIGREF19 ). For each time step, the output of an input word token INLINEFORM2 is INLINEFORM3 (we use superscript INLINEFORM4 to indicate word-level LSTM cells, INLINEFORM5 for sentence-level).To obtain a representation for each sentence INLINEFORM0 , we take the encoder output of the last token for that sentence. If that token has an index of INLINEFORM1 in the whole document INLINEFORM2 , then the sentence representation is marked as INLINEFORM3 . The word-level sentence embeddings of the document INLINEFORM4 will be a sequence which is fed into a sentence-level LSTM network. Thus, for each input sentence INLINEFORM5 , we obtain an output hidden state INLINEFORM6 . We then get the final sentence-level embeddings INLINEFORM7 (we omit the subscript for sentences INLINEFORM8 ). To obtain a summary representation, we simply treat the current decoded summary as a single sentence and take the output of the last step of the decoder: INLINEFORM9 . We plan to investigate alternative methods for input and output sentence embeddings, such as separate LSTMs for each sentence, in future work.MMR-Attention	Now, we have all the sentence-level representation from both the articles and summary, and then we apply MMR to compute a ranking on the candidate sentences INLINEFORM0 . Intuitively, incorporating MMR will help determine salient sentences from the input at the current decoding step based on relevancy and redundancy.We follow Section 4.3 to compute MMR scores. Here, however, our query document is represented by the summary vector INLINEFORM0 , and we want to rank the candidates in INLINEFORM1 . The MMR score for an input sentence INLINEFORM2 is then defined as:MMR i = Sim 1 (hs i ,ssum)-(1-) sj D, j i Sim2 (hs i ,hs j ) We then add a softmax function to normalize all the MMR scores of these candidates as a probability distribution. MMR i = ( MMR i )i( MMR i ) Now we define the similarity function between each candidate sentence INLINEFORM0 and summary sentence INLINEFORM1 to be: DISPLAYFORM0 where INLINEFORM0 is a learned parameter used to transform INLINEFORM1 and INLINEFORM2 into a common feature space.For the second term of Equation SECREF21 , instead of choosing the maximum score from all candidates except for INLINEFORM0 , which is intended to find the candidate most similar to INLINEFORM1 , we choose to apply a self-attention model on INLINEFORM2 and all the other candidates INLINEFORM3 . We then choose the largest weight as the final score: DISPLAYFORM0  Note that INLINEFORM0 is also a trainable parameter. Eventually, the MMR score from Equation SECREF21 becomes: MMR i = Sim 1 (hsi,ssum)-(1-) scoreiMMR-attention Pointer-generator	After we calculate INLINEFORM0 for each sentence representation INLINEFORM1 , we use these scores to update the word-level attention weights for the pointer-generator model shown by the blue arrows in Figure FIGREF19 . Since INLINEFORM2 is a sentence weight for INLINEFORM3 , each token in the sentence will have the same value of INLINEFORM4 . The new attention for each input token from Equation EQREF14 becomes: DISPLAYFORM0 Experiments	In this section we describe additional methods we compare with and present our assumptions and experimental process.Baseline and Extractive Methods	First We concatenate the first sentence of each article in a document cluster as the system summary. For our dataset, First- INLINEFORM0 means the first INLINEFORM1 sentences from each source article will be concatenated as the summary. Due to the difference in gold summary length, we only use First-1 for DUC, as others would exceed the average summary length.LexRank Initially proposed by BIBREF16 , LexRank is a graph-based method for computing relative importance in extractive summarization.TextRank Introduced by BIBREF17 , TextRank is a graph-based ranking model. Sentence importance scores are computed based on eigenvector centrality within a global graph from the corpus.MMR In addition to incorporating MMR in our pointer generator network, we use this original method as an extractive summarization baseline. When testing on DUC data, we set these extractive methods to give an output of 100 tokens and 300 tokens for Multi-News data.Neural Abstractive Methods	PG-Original, PG-MMR These are the original pointer-generator network models reported by BIBREF11 .PG-BRNN The PG-BRNN model is a pointer-generator implementation from OpenNMT. As in the original paper BIBREF1 , we use a 1-layer bi-LSTM as encoder, with 128-dimensional word-embeddings and 256-dimensional hidden states for each direction. The decoder is a 512-dimensional single-layer LSTM. We include this for reference in addition to PG-Original, as our Hi-MAP code builds upon this implementation.CopyTransformer Instead of using an LSTM, the CopyTransformer model used in Gehrmann:18 uses a 4-layer Transformer of 512 dimensions for encoder and decoder. One of the attention heads is chosen randomly as the copy distribution. This model and the PG-BRNN are run without the bottom-up masked attention for inference from Gehrmann:18 as we did not find a large improvement when reproducing the model on this data.Experimental Setting	Following the setting from BIBREF11 , we report ROUGE BIBREF37 scores, which measure the overlap of unigrams (R-1), bigrams (R-2) and skip bigrams with a max distance of four words (R-SU). For the neural abstractive models, we truncate input articles to 500 tokens in the following way: for each example with INLINEFORM0 source input documents, we take the first 500 INLINEFORM1 tokens from each source document. As some source documents may be shorter, we iteratively determine the number of tokens to take from each document until the 500 token quota is reached. Having determined the number of tokens per source document to use, we concatenate the truncated source documents into a single mega-document. This effectively reduces MDS to SDS on longer documents, a commonly-used assumption for recent neural MDS papers BIBREF10 , BIBREF38 , BIBREF11 . We chose 500 as our truncation size as related MDS work did not find significant improvement when increasing input length from 500 to 1000 tokens BIBREF38 . We simply introduce a special token between source documents to aid our models in detecting document-to-document relationships and leave direct modeling of this relationship, as well as modeling longer input sequences, to future work. We hope that the dataset we introduce will promote such work. For our Hi-MAP model, we applied a 1-layer bidirectional LSTM network, with the hidden state dimension 256 in each direction. The sentence representation dimension is also 256. We set the INLINEFORM2 to calculate the MMR value in Equation SECREF21 .As our focus was on deep methods for MDS, we only tested several non-neural baselines. However, other classical methods deserve more attention, for which we refer the reader to Hong14 and leave the implementation of these methods on Multi-News for future work.Analysis and Discussion	In Table TABREF30 and Table TABREF31 we report ROUGE scores on DUC 2004 and Multi-News datasets respectively. We use DUC 2004, as results on this dataset are reported in lebanoff18mds, although this dataset is not the focus of this work. For results on DUC 2004, models were trained on the CNNDM dataset, as in lebanoff18mds. PG-BRNN and CopyTransformer models, which were pretrained by OpenNMT on CNNDM, were applied to DUC without additional training, analogous to PG-Original. We also experimented with training on Multi-News and testing on DUC data, but we did not see significant improvements. We attribute the generally low performance of pointer-generator, CopyTransformer and Hi-MAP to domain differences between DUC and CNNDM as well as DUC and Multi-News. These domain differences are evident in the statistics and extractive metrics discussed in Section 3.Additionally, for both DUC and Multi-News testing, we experimented with using the output of 500 tokens from extractive methods (LexRank, TextRank and MMR) as input to the abstractive model. However, this did not improve results. We believe this is because our truncated input mirrors the First-3 baseline, which outperforms these three extractive methods and thus may provide more information as input to the abstractive model.Our model outperforms PG-MMR when trained and tested on the Multi-News dataset. We see much-improved model performances when trained and tested on in-domain Multi-News data. The Transformer performs best in terms of R-1 while Hi-MAP outperforms it on R-2 and R-SU. Also, we notice a drop in performance between PG-original, and PG-MMR (which takes the pre-trained PG-original and applies MMR on top of the model). Our PG-MMR results correspond to PG-MMR w Cosine reported in lebanoff18mds. We trained their sentence regression model on Multi-News data and leave the investigation of transferring regression models from SDS to Multi-News for future work.In addition to automatic evaluation, we performed human evaluation to compare the summaries produced. We used Best-Worst Scaling BIBREF39 , BIBREF40 , which has shown to be more reliable than rating scales BIBREF41 and has been used to evaluate summaries BIBREF42 , BIBREF32 . Annotators were presented with the same input that the systems saw at testing time; input documents were truncated, and we separated input documents by visible spaces in our annotator interface. We chose three native English speakers as annotators. They were presented with input documents, and summaries generated by two out of four systems, and were asked to determine which summary was better and which was worse in terms of informativeness (is the meaning in the input text preserved in the summary?), fluency (is the summary written in well-formed and grammatical English?) and non-redundancy (does the summary avoid repeating information?). We randomly selected 50 documents from the Multi-News test set and compared all possible combinations of two out of four systems. We chose to compare PG-MMR, CopyTransformer, Hi-MAP and gold summaries. The order of summaries was randomized per example.The results of our pairwise human-annotated comparison are shown in Table TABREF32 . Human-written summaries were easily marked as better than other systems, which, while expected, shows that there is much room for improvement in producing readable, informative summaries. We performed pairwise comparison of the models over the three metrics combined, using a one-way ANOVA with Tukey HSD tests and INLINEFORM0 value of 0.05. Overall, statistically significant differences were found between human summaries score and all other systems, CopyTransformer and the other two models, and our Hi-MAP model compared to PG-MMR. Our Hi-MAP model performs comparably to PG-MMR on informativeness and fluency but much better in terms of non-redundancy. We believe that the incorporation of learned parameters for similarity and redundancy reduces redundancy in our output summaries. In future work, we would like to incorporate MMR into Transformer models to benefit from their fluent summaries.Conclusion	 In this paper we introduce Multi-News, the first large-scale multi-document news summarization dataset. We hope that this dataset will promote work in multi-document summarization similar to the progress seen in the single-document case. Additionally, we introduce an end-to-end model which incorporates MMR into a pointer-generator network, which performs competitively compared to previous multi-document summarization models. We also benchmark methods on our dataset. In the future we plan to explore interactions among documents beyond concatenation and experiment with summarizing longer input documents.","['Do they use pretrained embeddings in their model?', 'What results are obtained by their model?', 'What sources do the news come from?', 'What is the size of Multi-news dataset?']","['Unacceptable', 'Unacceptable', '1500 news sites', '56216']"
19,"Neural Language Modeling with Visual Features	Multimodal language models attempt to incorporate non-linguistic features for the language modeling task. In this work, we extend a standard recurrent neural network (RNN) language model with features derived from videos. We train our models on data that is two orders-of-magnitude bigger than datasets used in prior work. We perform a thorough exploration of model architectures for combining visual and text features. Our experiments on two corpora (YouCookII and 20bn-something-something-v2) show that the best performing architecture consists of middle fusion of visual and text features, yielding over 25% relative improvement in perplexity. We report analysis that provides insights into why our multimodal language model improves upon a standard RNN language model.	Introduction	 INLINEFORM0 Work performed while the author was an intern at Google.Language models are vital components of a wide variety of systems for Natural Language Processing (NLP) including Automatic Speech Recognition, Machine Translation, Optical Character Recognition, Spelling Correction, etc. However, most language models are trained and applied in a manner that is oblivious to the environment in which human language operates BIBREF0 . These models are typically trained only on sequences of words, ignoring the physical context in which the symbolic representations are grounded, or ignoring the social context that could inform the semantics of an utterance.For incorporating additional modalities, the NLP community has typically used datasets such as MS COCO BIBREF1 and Flickr BIBREF2 for image-based tasks, while several datasets BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 have been curated for video-based tasks. Despite the lack of big datasets, researchers have started investigating language grounding in images BIBREF8 , BIBREF9 , BIBREF10 and to lesser extent in videos BIBREF11 , BIBREF1 . However, language grounding has focused more on obtaining better word and sentence representations or other downstream tasks, and to lesser extent on language modeling.In this paper, we examine the problem of incorporating temporal visual context into a recurrent neural language model (RNNLM). Multimodal Neural Language Models were introduced in BIBREF12 , where log-linear LMs BIBREF13 were conditioned to handle both image and text modalities. Notably, this work did not use the recurrent neural model paradigm which has now become the de facto way of implementing neural LMs.The closest work to ours is that of BIBREF0 , who report perplexity gains of around 5–6% on three languages on the MS COCO dataset (with an English vocabulary of only 16K words).Our work is distinguishable from previous work with respect to three dimensions:Model	A language model assigns to a sentence INLINEFORM0 the probability: INLINEFORM1 where each word is assigned a probability given the previous word history.For a given video segment, we assume that there is a sequence of INLINEFORM0 video frames represented by features INLINEFORM1 , and the corresponding transcription INLINEFORM2 . In practice, we assume INLINEFORM3 since we can always assign a video frame to each word by replicating the video frames the requisite number of times. Thus, our visually-grounded language model models the probability of the next word given the history of previous words as well as video frames: INLINEFORM4 Combining the text and video modalities	There are several options for combining the text and video modalities. We opt for the simplest strategy, which concatenates the representations. For a word embedding INLINEFORM0 and corresponding visual representation INLINEFORM1 , the input to our RNNLM will be the concatenated vector INLINEFORM2 . For the examples where we were unable to compute visual features (see Section § SECREF3 ), we set INLINEFORM3 to be a zero-vector.In addition to concatenating the word and visual embedding, we explore two variants of our model that allow for a finer-grained integration of the two modalities:In this case, the RNNLM is given as input a vector INLINEFORM0 that is a weighted sum of the two embeddings: INLINEFORM1 where INLINEFORM0 are learned matrices.Here, we apply the intuition that some words could provide information as to whether or not the visual context is helpful. In a simplistic example, if the word history is the article “the,"" then the visual context could provide relevant information needed for predicting the next word. For other word histories, though, the visual context might not be needed or be even irrelevant for the next word prediction: if the previous word is “carpe"", the next word is very likely to be “diem"", regardless of visual context. We implement a simple weighting mechanism that learns a scalar weight for the visual embedding prior to concatenation with the word embedding. The input to the RNNLM is now INLINEFORM0 , where: INLINEFORM1 This approach does not add any new parameters to the model, but since the word representations INLINEFORM0 are learned, this mechanism has the potential to learn word embeddings that are also appropriate for weighting the visual context.Location of combination	We explore three locations for fusing visual features in an RNNLM (Figure ). Our Early Fusion strategy merges the text and the visual features at the input to the LSTM cells. This embodies the intuition that it is best to do feature combination at the earliest possible stage. The Middle Fusion merges the visual features at the output of the 1st LSTM layer while the Late Fusion strategies merges the two features after the final LSTM layer. The idea behind the Middle and Late fusion is that we would like to minimize changes to the regular RNNLM architecture at the early stages and still be able to benefit from the visual features.Data and Experimental Setup	Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens BIBREF14 . We tokenize the training data using a vocabulary of 66K wordpieces BIBREF15 . Thus, the input to the model is a sequence of wordpieces. Using wordpieces allows us to address out-of-vocabulary (OOV) word issues that would arise from having a fixed word vocabulary. In practice, a wordpiece RNNLM gives similar performance as a word-level model BIBREF16 . For about INLINEFORM1 of the segments, we were able to obtain visual features at the frame level. The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 , BIBREF18 . For a INLINEFORM2 -second video and INLINEFORM3 wordpieces, each feature is uniformly allocated to INLINEFORM4 wordpieces.Our RNNLM models consist of 2 LSTM layers, each containing 2048 units which are linearly projected to 512 units BIBREF19 . The word-piece and video embeddings are of size 512 each. We do not use dropout. During training, the batch size per worker is set to 256, and we perform full length unrolling to a max length of 70. The INLINEFORM0 -norms of the gradients are clipped to a max norm of INLINEFORM1 for the LSTM weights and to 10,000 for all other weights. We train with Synchronous SGD with the Adafactor optimizer BIBREF20 until convergence on a development set, created by randomly selecting INLINEFORM2 of all utterances.Experiments	For evaluation we used two datasets, YouCook2 and sth-sth, allowing us to evaluate our models in cases where the visual context is relevant to the modelled language. Note that no data from these datasets are present in the YouTube videos used for training. The perplexity of our models is shown in Table .Conclusion	We present a simple strategy to augment a standard recurrent neural network language model with temporal visual features. Through an exploration of candidate architectures, we show that the Middle Fusion of visual and textual features leads to a 20-28% reduction in perplexity relative to a text only baseline. These experiments were performed using datasets of unprecedented scale, with more than 1.2 billion tokens – two orders of magnitude more than any previously published work. Our work is a first step towards creating and deploying large-scale multimodal systems that properly situate themselves into a given context, by taking full advantage of every available signal.","['what dataset was used for training?', 'what is the size of the training data?', 'what features were derived from the videos?']","['Unacceptable', '64M video segments with 1.2B tokens', '1500-dimensional vectors similar to those used for large scale image classification tasks.']"
20,"Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection	Self-attention based Transformer has demonstrated the state-of-the-art performances in a number of natural language processing tasks. Self-attention is able to model long-term dependencies, but it may suffer from the extraction of irrelevant information in the context. To tackle the problem, we propose a novel model called \textbf{Explicit Sparse Transformer}. Explicit Sparse Transformer is able to improve the concentration of attention on the global context through an explicit selection of the most relevant segments. Extensive experimental results on a series of natural language processing and computer vision tasks, including neural machine translation, image captioning, and language modeling, all demonstrate the advantages of Explicit Sparse Transformer in model performance. We also show that our proposed sparse attention method achieves comparable or better results than the previous sparse attention method, but significantly reduces training and testing time. For example, the inference speed is twice that of sparsemax in Transformer model. Code will be available at \url{this https URL}	Introduction	Understanding natural language requires the ability to pay attention to the most relevant information. For example, people tend to focus on the most relevant segments to search for the answers to their questions in mind during reading. However, retrieving problems may occur if irrelevant segments impose negative impacts on reading comprehension. Such distraction hinders the understanding process, which calls for an effective attention.This principle is also applicable to the computation systems for natural language. Attention has been a vital component of the models for natural language understanding and natural language generation. Recently, BIBREF0 proposed Transformer, a model based on the attention mechanism for Neural Machine Translation(NMT). Transformer has shown outstanding performance in natural language generation tasks. More recently, the success of BERT BIBREF1 in natural language processing shows the great usefulness of both the attention mechanism and the framework of Transformer.However, the attention in vanilla Transformer has a obvious drawback, as the Transformer assigns credits to all components of the context. This causes a lack of focus. As illustrated in Figure FIGREF1, the attention in vanilla Transformer assigns high credits to many irrelevant words, while in Explicit Sparse Transformer, it concentrates on the most relevant $k$ words. For the word “tim”, the most related words should be ""heart"" and the immediate words. Yet the attention in vanilla Transformer does not focus on them but gives credits to some irrelevant words such as “him”.Recent works have studied applying sparse attention in Transformer model. However, they either add local attention constraints BIBREF2 which break long term dependency or hurt the time efficiency BIBREF3. Inspired by BIBREF4 which introduce sparse credit assignment to the LSTM model, we propose a novel model called Explicit Sparse Transformer which is equipped with our sparse attention mechanism. We implement an explicit selection method based on top-$k$ selection. Unlike vanilla Transformer, Explicit Sparse Transformer only pays attention to the $k$ most contributive states. Thus Explicit Sparse Transformer can perform more concentrated attention than vanilla Transformer.We first validate our methods on three tasks. For further investigation, we compare our methods with previous sparse attention methods and experimentally answer how to choose k in a series of qualitative analyses. We are surprised to find that the proposed sparse attention method can also help with training as a regularization method. Visual analysis shows that Explicit Sparse Transformer exhibits a higher potential in performing a high-quality alignment. The contributions of this paper are presented below:We propose a novel model called Explicit Sparse Transformer, which enhances the concentration of the Transformer's attention through explicit selection.We conducted extensive experiments on three natural language processing tasks, including Neural Machine Translation, Image Captioning and Language Modeling. Compared with vanilla Transformer, Explicit Sparse Transformer demonstrates better performances in the above three tasks.Compared to previous sparse attention methods for transformers, our methods are much faster in training and testing, and achieves comparable results.Explicit Sparse Transformer	The review to the attention mechanism and the attention-based framework of Transformer can be found in Appendix SECREF35.Lack of concentration in the attention can lead to the failure of relevant information extraction. To this end, we propose a novel model, Explicit Sparse Transformer, which enables the focus on only a few elements through explicit selection. Compared with the conventional attention, no credit will be assigned to the value that is not highly correlated to the query. We provide a comparison between the attention of vanilla Transformer and that of Explicit Sparse Transformer in Figure FIGREF5.Explicit Sparse Transformer is still based on the Transformer framework. The difference is in the implementation of self-attention. The attention is degenerated to the sparse attention through top-$k$ selection. In this way, the most contributive components for attention are reserved and the other irrelevant information are removed. This selective method is effective in preserving important information and removing noise. The attention can be much more concentrated on the most contributive elements of value. In the following, we first introduce the sparsification in self-attention and then extend it to context attention.In the unihead self-attention, the key components, the query $Q[l_{Q}, d]$, key $K[l_{K}, d]$ and value $V[l_{V}, d]$, are the linear transformation of the source context, namely the input of each layer, where $Q = W_{Q}x$, $K = W_{K}x$ and $V = W_{V}x$. Explicit Sparse Transformer first generates the attention scores $P$ as demonstrated below:Then the model evaluates the values of the scores $P$ based on the hypothesis that scores with larger values demonstrate higher relevance. The sparse attention masking operation $\mathcal {M}(\cdot )$ is implemented upon $P$ in order to select the top-$k$ contributive elements. Specifically, we select the $k$ largest element of each row in $P$ and record their positions in the position matrix $(i, j)$, where $k$ is a hyperparameter. To be specific, say the $k$-th largest value of row $i$ is $t_{i}$, if the value of the $j$-th component is larger than $t_i$, the position $(i, j)$ is recorded. We concatenate the threshold value of each row to form a vector $t = [t_1, t_2, \cdots , t_{l_{Q}}]$. The masking functions $\mathcal {M}(\cdot , \cdot )$ is illustrated as follows:With the top-$k$ selection, the high attention scores are selected through an explicit way. This is different from dropout which randomly abandons the scores. Such explicit selection can not only guarantee the preservation of important components, but also simplify the model since $k$ is usually a small number such as 8, detailed analysis can be found in SECREF28. The next step after top-$k$ selection is normalization:where $A$ refers to the normalized scores. As the scores that are smaller than the top k largest scores are assigned with negative infinity by the masking function $\mathcal {M}(\cdot , \cdot )$, their normalized scores, namely the probabilities, approximate 0. We show the back-propagation process of Top-k selection in SECREF50. The output representation of self-attention $C$ can be computed as below:The output is the expectation of the value following the sparsified distribution $A$. Following the distribution of the selected components, the attention in the Explicit Sparse Transformer model can obtain more focused attention. Also, such sparse attention can extend to context attention. Resembling but different from the self-attention mechanism, the $Q$ is no longer the linear transformation of the source context but the decoding states $s$. In the implementation, we replace $Q$ with $W_{Q}s$, where $W_{Q}$ is still learnable matrix.In brief, the attention in our proposed Explicit Sparse Transformer sparsifies the attention weights. The attention can then become focused on the most contributive elements, and it is compatible to both self-attention and context attention. The simple implementation of this method is in the Appendix SECREF55.Results	We conducted a series of experiments on three natural language processing tasks, including neural machine translation, image captioning and language modeling. Detailed experimental settings are in Appendix SECREF42.Results ::: Neural Machine Translation ::: Dataset	To evaluate the performance of Explicit Sparse Transformer in NMT, we conducted experiments on three NMT tasks, English-to-German translation (En-De) with a large dataset, English-to-Vietnamese (En-Vi) translation and German-to-English translation (De-En) with two datasets of medium size. For En-De, we trained Explicit Sparse Transformer on the standard dataset for WMT 2014 En-De translation. The dataset consists of around 4.5 million sentence pairs. The source and target languages share a vocabulary of 32K sub-word units. We used the newstest 2013 for validation and the newstest 2014 as our test set. We report the results on the test set.For En-Vi, we trained our model on the dataset in IWSLT 2015 BIBREF20. The dataset consists of around 133K sentence pairs from translated TED talks. The vocabulary size for source language is around 17,200 and that for target language is around 7,800. We used tst2012 for validation, and tst2013 for testing and report the testing results. For De-En, we used the dataset in IWSLT 2014. The training set contains 160K sentence pairs and the validation set contains 7K sentences. Following BIBREF21, we used the same test set with around 7K sentences. The data were preprocessed with byte-pair encoding BIBREF22. The vocabulary size is 14,000.Results ::: Neural Machine Translation ::: Result	Table TABREF10 presents the results of the baselines and our Explicit Sparse Transformer on the three datasets. For En-De, Transformer-based models outperform the previous methods. Compared with the result of Transformer BIBREF0, Explicit Sparse Transformer reaches 29.4 in BLEU score evaluation, outperforming vanilla Transformer by 0.3 BLEU score. For En-Vi, vanilla Transformer reaches 30.2, outperforming previous best method BIBREF7. Our model, Explicit Sparse Transformer, achieves a much better performance, 31.1, by a margin of 0.5 over vanilla Transformer. For De-En, we demonstrate that Transformer-based models outperform the other baselines. Compared with Transformer, our Explicit Sparse Transformer reaches a better performance, 35.6. Its advantage is +0.3. To the best of our knowledge, Explicit Sparse Transformer reaches a top line performance on the dataset.Results ::: Image Captioning ::: Dataset	We evaluated our approach on the image captioning task. Image captioning is a task that combines image understanding and language generation. We conducted experiments on the Microsoft COCO 2014 dataset BIBREF23. It contains 123,287 images, each of which is paired 5 with descriptive sentences. We report the results and evaluate the image captioning model on the MSCOCO 2014 test set for image captioning. Following previous works BIBREF24, BIBREF25, we used the publicly-available splits provided by BIBREF26. The validation set and test set both contain 5,000 images.Results ::: Image Captioning ::: Result	Table TABREF17 shows the results of the baseline models and Explicit Sparse Transformer on the COCO Karpathy test split. Transformer outperforms the mentioned baseline models. Explicit Sparse Transformer outperforms the implemented Transformer by +0.4 in terms of BLEU-4, +0.3 in terms of METEOR, +0.7 in terms of CIDEr. , which consistently proves its effectiveness in Image Captioning.Results ::: Language Modeling ::: Dataset	Enwiki8 is large-scale dataset for character-level language modeling. It contains 100M bytes of unprocessed Wikipedia texts. The inputs include Latin alphabets, non-Latin alphabets, XML markups and special characters. The vocabulary size 205 tokens, including one for unknown characters. We used the same preprocessing method following BIBREF33. The training set contains 90M bytes of data, and the validation set and the test set contains 5M respectively.Results ::: Language Modeling ::: Result	Table TABREF23 shows the results of the baseline models and Explicit Sparse Transformer-XL on the test set of enwiki8. Compared with the other strong baselines, Transformer-XL can reach a better performance, and Explicit Sparse Transformer outperforms Transformer-XL with an advantage.Discussion	In this section, we performed several analyses for further discussion of Explicit Sparse Transformer. First, we compare the proposed method of topk selection before softmax with previous sparse attention method including various variants of sparsemax BIBREF3, BIBREF42, BIBREF43. Second, we discuss about the selection of the value of $k$. Third, we demonstrate that the top-k sparse attention method helps training. In the end, we conducted a series of qualitative analyses to visualize proposed sparse attention in Transformer.Discussion ::: Comparison with other Sparse Attention Methods	We compare the performance and speed of our method with the previous sparse attention methods on the basis of strong implemented transformer baseline. The training and inference speed are reported on the platform of Pytorch and IWSLT 2014 De-En translation dataset, the batch size for inference is set to 128 in terms of sentence and half precision training(FP-16) is applied.As we can see from Table TABREF25, the proposed sparse attention method achieve the comparable results as previous sparse attention methods, but the training and testing speed is 2x faster than sparsemax and 10x faster than Entmax-alpha during the inference. This is due to the fact that our method does not introduce too much computation for calculating sparse attention scores.The other group of sparse attention methods of adding local attention constraints into attention BIBREF2, BIBREF41, do not show performance on neural machine translation, so we do not compare them in Table TABREF25.Discussion ::: How to Select a Proper k?	The natural question of how to choose the optimal $k$ comes with the proposed method. We compare the effect of the value of $k$ at exponential scales. We perform experiments on En-Vi and De-En from 3 different initializations for each value of $K$, and report the mean BLEU scores on the valid set. The figure FIGREF27 shows that regardless of the value of 16 on the En-Vi dataset, the model performance generally rises first and then falls as $k$ increases. For $k\in \lbrace 4,8,16,32\rbrace $, setting the value of $k$ to 8 achieves consistent improvements over the transformer baseline.Discussion ::: Do the proposed sparse attention method helps training?	We are surprised to find that only adding the sparsification in the training phase can also bring an improvement in the performance. We experiment this idea on IWSLT En-Vi and report the results on the valid set in Table TABREF30, . The improvement of 0.3 BLEU scores shows that vanilla Transformer may be overparameterized and the sparsification encourages the simplification of the model.Discussion ::: Do the Explicit Sparse Transformer Attend better?	To perform a thorough evaluation of our Explicit Sparse Transformer, we conducted a case study and visualize the attention distributions of our model and the baseline for further comparison. Specifically, we conducted the analysis on the test set of En-Vi, and randomly selected a sample pair of attention visualization of both models.The visualization of the context attention of the decoder's bottom layer in Figure FIGREF33. The attention distribution of the left figure is fairly disperse. On the contrary, the right figure shows that the sparse attention can choose to focus only on several positions so that the model can be forced to stay focused. For example, when generating the phrase “for thinking about my heart”(Word-to-word translation from Vietnamese), the generated word cannot be aligned to the corresponding words. As to Explicit Sparse Transformer, when generating the phrase ""with all my heart"", the attention can focus on the corresponding positions with strong confidence.The visualization of the decoder's top layer is shown in Figure FIGREF34. From the figure, the context attention at the top layer of the vanilla Transformer decoder suffers from focusing on the last source token. This is a common behavior of the attention in vanilla Transformer. Such attention with wrong alignment cannot sufficiently extract enough relevant source-side information for the generation. In contrast, Explicit Sparse Transformer, with simple modification on the vanilla version, does not suffer from this problem, but instead focuses on the relevant sections of the source context. The figure on the right demonstrating the attention distribution of Explicit Sparse Transformer shows that our proposed attention in the model is able to perform accurate alignment.Related Work	Attention mechanism has demonstrated outstanding performances in a number of neural-network-based methods, and it has been a focus in the NLP studies BIBREF44. A number of studies are proposed to enhance the effects of attention mechanism BIBREF45, BIBREF0, BIBREF4, BIBREF46. BIBREF45 propose local attention and BIBREF47 propose local attention for self-attention. BIBREF48 propose hard attention that pays discrete attention in image captioning. BIBREF49 propose a combination soft attention with hard attention to construct hierarchical memory network. BIBREF8 propose a temperature mechanism to change the softness of attention distribution. BIBREF50 propose an attention which can select a small proportion for focusing. It is trained by reinforcement learning algorithms BIBREF51. In terms of memory networks, BIBREF52 propose to sparse access memory.BIBREF2 recently propose to use local attention and block attention to sparsify the transformer. Our approach differs from them in that our method does not need to block sentences and still capture long distance dependencies. Besides, we demonstrate the importance of Explicit Sparse Transformer in sequence to sequence learning. Although the variants of sparsemax BIBREF3, BIBREF42, BIBREF43 improve in machine translation tasks, we empirically demonstrate in SECREF24 that our method introduces less computation in the standard transformer and is much faster than those sparse attention methods on GPUs.Conclusion	In this paper, we propose a novel model called Explicit Sparse Transformer. Explicit Sparse Transformer is able to make the attention in vanilla Transformer more concentrated on the most contributive components. Extensive experiments show that Explicit Sparse Transformer outperforms vanilla Transformer in three different NLP tasks. We conducted a series of qualitative analyses to investigate the reasons why Explicit Sparse Transformer outperforms the vanilla Transformer. Furthermore, we find an obvious problem of the attention at the top layer of the vanilla Transformer, and Explicit Sparse Transformer can alleviate this problem effectively with improved alignment effects.Appendix ::: Background ::: Attention Mechanism	BIBREF44 first introduced the attention mechanism to learn the alignment between the target-side context and the source-side context, and BIBREF45 formulated several versions for local and global attention. In general, the attention mechanism maps a query and a key-value pair to an output. The attention score function and softmax normalization can turn the query $Q$ and the key $K$ into a distribution $\alpha $. Following the distribution $\alpha $, the attention mechanism computes the expectation of the value $V$ and finally generates the output $C$.Take the original attention mechanism in NMT as an example. Both key $K \in \mathbb {R}^{n \times d}$ and value $V \in \mathbb {R}^{n \times d} $ are the sequence of output states from the encoder. Query $Q \in \mathbb {R}^{m \times d}$ is the sequence of output states from the decoder, where $m$ is the length of $Q$, $n$ is the length of $K$ and $V$, and $d$ is the dimension of the states. Thus, the attention mechanism is formulated as:where $f$ refers to the attention score computation.Appendix ::: Background ::: Transformer	Transformer BIBREF0, which is fully based on the attention mechanism, demonstrates the state-of-the-art performances in a series of natural language generation tasks. Specifically, we focus on self-attention and multi-head attention.The ideology of self-attention is, as the name implies, the attention over the context itself. In the implementation, the query $Q$, key $K$ and value $V$ are the linear transformation of the input $x$, so that $Q = W_{Q}x$, $K = W_{K}x$ and $V = W_{V}x$ where $W_{Q}$, $W_{K}$ and $W_{V}$ are learnable parameters. Therefore, the computation can be formulated as below:where $d$ refers to the dimension of the states.The aforementioned mechanism can be regarded as the unihead attention. As to the multi-head attention, the attention computation is separated into $g$ heads (namely 8 for basic model and 16 for large model in the common practice). Thus multiple parts of the inputs can be computed individually. For the $i$-th head, the output can be computed as in the following formula:where $C^{(i)}$ refers to the output of the head, $Q^{(i)}$, $K^{(i)}$ and $V^{(i)}$ are the query, key and value of the head, and $d_k$ refers to the size of each head ($d_k = d/g$). Finally, the output of each head are concatenated for the output:In common practice, $C$ is sent through a linear transformation with weight matrix $W_c$ for the final output of multi-head attention.However, soft attention can assign weights to a lot more words that are less relevent to the query. Therefore, in order to improve concentration in attention for effective information extraction, we study the problem of sparse attention in Transformer and propose our model Explicit Sparse Transformer.Appendix ::: Experimental Details	We use the default setting in BIBREF0 for the implementation of our proposed Explicit Sparse Transformer. The hyper parameters including beam size and training steps are tuned on the valid set.Appendix ::: Experimental Details ::: Neural Machine Translation	Training： For En-Vi translation, we use default scripts and hyper-parameter setting of tensor2tensor v1.11.0 to preprocess, train and evaluate our model. We use the default scripts of fairseq v0.6.1 to preprocess the De-En and En-De dataset. We train the model on the En-Vi dataset for $35K$ steps with batch size of $4K$. For IWSLT 2015 De-En dataset, batch size is also set to $4K$, we update the model every 4 steps and train the model for 90epochs. For WMT 2014 En-De dataset, we train the model for 72 epochs on 4 GPUs with update frequency of 32 and batch size of 3584. We train all models on a single RTX2080TI for two small IWSLT datasets and on a single machine of 4 RTX TITAN for WMT14 En-De. In order to reduce the impact of random initialization, we perform experiments with three different initializations for all models and report the highest for small datasets.Evaluation： We use case-sensitive tokenized BLEU score BIBREF55 for the evaluation of WMT14 En-De, and we use case-insensitive BLEU for that of IWSLT 2015 En-Vi and IWSLT 2014 De-En following BIBREF8. Same as BIBREF0, compound splitting is used for WMT 14 En-De. For WMT 14 En-De and IWSLT 2014 De-En, we save checkpoints every epoch and average last 10 checkpoints every 5 epochs, We select the averaged checkpoint with best valid BLEU and report its BLEU score on the test set. For IWSLT 2015 En-Vi, we save checkpoints every 600 seconds and average last 20 checkpoints.Appendix ::: Experimental Details ::: Image Captioning	We still use the default setting of Transformer for training our proposed Explicit Sparse Transformer. We report the standard automatic evaluation metrics with the help of the COCO captioning evaluation toolkit BIBREF53, which includes the commonly-used evaluation metrics, BLEU-4 BIBREF55, METEOR BIBREF54, and CIDEr BIBREF56.Appendix ::: Experimental Details ::: Language Models	We follow BIBREF40 and use their implementation for our Explicit Sparse Transformer. Following the previous work BIBREF33, BIBREF40, we use BPC ($E[− log_2 P(xt+1|ht)]$), standing for the average number of Bits-Per-Character, for evaluation. Lower BPC refers to better performance. As to the model implementation, we implement Explicit Sparse Transformer-XL, which is based on the base version of Transformer-XL. Transformer-XL is a model based on Transformer but has better capability of representing long sequences.Appendix ::: The Back-propagation Process of Top-k Selection	The masking function $\mathcal {M}(\cdot , \cdot )$ is illustrated as follow:Denote $M=\mathcal {M}(P, k)$. We regard $t_i$ as constants. When back-propagating,The next step after top-$k$ selection is normalization:where $A$ refers to the normalized scores. When backpropagating,The softmax function is evidently differentiable, therefore, we have calculated the gradient involved in top-k selection.Appendix ::: Implementation	Figure FIGREF56 shows the code for the idea in case of single head self-attention, the proposed method is easy to implement and plug in the successful Transformer model.","['What do they mean by explicit selection of most relevant segments?', 'What datasets they used for evaluation?']","['It is meant that only most contributive k elements are reserved, while other elements are removed.', 'For En-De translation the newstest 2014 set from WMT 2014 En-De translation dataset, for En-Vi translation the tst2013 from IWSLT 2015 dataset. and for De-En translation the teste set from IWSLT 2014.']"
21,"End-to-End Trainable Non-Collaborative Dialog System	End-to-end task-oriented dialog models have achieved promising performance on collaborative tasks where users willingly coordinate with the system to complete a given task. While in non-collaborative settings, for example, negotiation and persuasion, users and systems do not share a common goal. As a result, compared to collaborate tasks, people use social content to build rapport and trust in these non-collaborative settings in order to advance their goals. To handle social content, we introduce a hierarchical intent annotation scheme, which can be generalized to different non-collaborative dialog tasks. Building upon TransferTransfo (Wolf et al. 2019), we propose an end-to-end neural network model to generate diverse coherent responses. Our model utilizes intent and semantic slots as the intermediate sentence representation to guide the generation process. In addition, we design a filter to select appropriate responses based on whether these intermediate representations fit the designed task and conversation constraints. Our non-collaborative dialog model guides users to complete the task while simultaneously keeps them engaged. We test our approach on our newly proposed ANTISCAM dataset and an existing PERSUASIONFORGOOD dataset. Both automatic and human evaluations suggest that our model outperforms multiple baselines in these two non-collaborative tasks.	Introduction	Considerable progress has been made building end-to-end dialog systems for collaborative tasks in which users cooperate with the system to achieve a common goal. Examples of collaborative tasks include making restaurant reservations and retrieving bus time-table information. Since users typically have clear and explicit intentions in collaborative tasks, existing systems commonly classify user utterances into pre-defined intents. In contrast, non-collaborative tasks are those where the users and the system do not strive to achieve the same goal. Examples of such tasks include deceiving attackers, persuading users to donate to a cause BIBREF1, and negotiating a product price BIBREF2, BIBREF3. In these tasks, users often perform complex actions that are beyond a simple set of pre-defined intents. In order to reach a common state, the user and the system need to build rapport and trust which naturally involves off-task content. Previous work did not model off-task content BIBREF2, which may have led to less optimal results. For example, in the persuasion task BIBREF1, users would ask the system “How do you feel about war?"" An example of an on-task system response that the system could have made is “Do you want to make a donation?"", which sticks to the task but neglects users' question. However, a better response to such an off-task question is “War is destructive and pitiless, but you can donate to help child victims of war."" This response is better, as it has been found that users are more likely to end the conversation if the system neglects their questions BIBREF4. Therefore, we need to design a system that handles both on-task and off-task information appropriately and in a way that leads back to the system's goal.To tackle the issue of incoherent system responses to off-task content, previous studies have built hybrid systems to interleave off-task and on-task content. BIBREF4 used a rule-based dialog manager for on-task content and a neural model for off-task content, and trained a reinforcement learning model to select between these two models based on the dialog context. However, such a method is difficult to train and struggles to generalize beyond the movie promotion task they considered. To tackle these problems, we propose a hierarchical intent annotation scheme that separates on-task and off-task information in order to provide detailed supervision. For on-task information, we directly use task-related intents for representation. Off-task information, on the other hand, is too general to categorize into specific intents, so we choose dialog acts that convey syntax information. These acts, such as “open question"" are general to all tasks.Previous studies use template-based methods to maintain sentence coherence. However, rigid templates lead to limited diversity, causing the user losing engagement. On the other hand, language generation models can generate diverse responses but are bad at being coherent. We propose Multiple Intents and Semantic Slots Annotation Neural Network (MISSA) to combine the advantages of both template and generation models and takes advantage from the hierarchical annotation at the same time. MISSA follows the TransferTransfo framework BIBREF0 with three modifications: (i) We first concurrently predict user's, system's intents and semantic slots; (ii) We then perform conditional generation to improve generated response's coherence. Specifically, we generate responses conditioned on the above intermediate representation (intents and slots); (iii) Finally, we generate multiple responses with the nucleus sampling strategy BIBREF5 and then apply a response filter, which contains a set of pre-defined constraints to select coherent responses. The constraints in the filter can be defined according to specific task requirements or general conversational rules.To enrich publicly available non-collaborative task datasets, we collect a new dataset AntiScam, where users defend themselves against attackers trying to collect personal information. As non-collaborative tasks are still relatively new to the study of dialog systems, there are insufficiently many meaningful datasets for evaluation and we hope this provides a valuable example. We evaluate MISSA on the newly collected AntiScam dataset and an existing PersuasionForGood dataset. Both automatic and human evaluations suggest that MISSA outperforms multiple competitive baselines.In summary, our contributions include: (i) We design a hierarchical intent annotation scheme and a semantic slot annotation scheme to annotate the non-collaborative dialog dataset, we also propose a carefully-designed AntiScam dataset to facilitate the research of non-collaborative dialog systems. (ii) We propose a model that can be applied to all non-collaborative tasks, outperforming other baselines on two different non-collaborative tasks. (iii) We develop an anti-scam dialog system to occupy attacker's attention and elicit their private information for social good. Furthermore, we also build a persuasion dialog system to persuade people to donate to charities. We release the code and data.Related Work	The interest in non-collaborative tasks has been increasing and there have already been several related datasets. For instance, BIBREF1 wang2019persuasion collected conversations where one participant persuades another to donate to a charity. BIBREF2 he2018decoupling collected negotiation dialogs where buyers and sellers bargain for items for sale on Craigslist. There are many other non-collaborative tasks, such as the turn-taking game BIBREF6, the multi-party game BIBREF7 and item splitting negotiation BIBREF8. Similar to the AntiScam dataset proposed in this paper, these datasets contain off-task content and can be used to train non-collaborative dialog systems. However, since they are not specifically collected and designed for non-collaborative tasks, it might be difficult to disentangle the on-task and off-task contents and measure the performance. Therefore, we propose the AntiScam dataset, which is designed to interleave the on-task and off-task contents in the conversation, and can serve as a benchmark dataset for similar non-collaborative tasks.To better understand user utterances and separate on-task and off-task content within a conversation, previous work has designed hierarchical annotation schemes for specific domains. BIBREF9 hardy2002multi followed the DAMSL schemeBIBREF10 and annotated a multilingual human-computer dialog corpus with a hierarchical dialog act annotation scheme. BIBREF11 gupta2018semantic used a hierarchical annotation scheme for semantic parsing. Inspired by these studies, our idea is to annotate the intent and semantic slot separately in non-collaborative tasks. We propose a hierarchical intent annotation scheme that can be adopted by all non-collaborative tasks. With this annotation scheme, MISSA is able to quickly build an end-to-end trainable dialog system for any non-collaborative task.Traditional task-oriented dialog systems BIBREF12 are usually composed of multiple independent modules, for example, natural language understanding, dialog state tracking BIBREF13, BIBREF14, dialog policy manager BIBREF15, and natural language generation BIBREF16. Conversational intent is adopted to capture the meaning of task content in these dialog systems BIBREF2, BIBREF17. In comparison to this work, we use a hierarchical intent scheme that includes off-task and on-task intents to capture utterance meaning. We also train the model in a multi-task fashion to predict decoupled intents and semantic slots. The major defect of a separately trained pipeline is the laborious dialog state design and annotation. In order to mitigate this problem, recent work has explored replacing independent modules with end-to-end neural networks BIBREF18, BIBREF19, BIBREF20. Our model also follows this end-to-end fashion.Over the last few years, we have witnessed a huge growth in non-task-oriented dialog systems BIBREF21, BIBREF22. Social chatbots such as Gunrock BIBREF23 were able to maintain a conversation for around ten minutes in an open domain. Recent improvements build on top of the transformer and pre-trained language models BIBREF24, BIBREF25, BIBREF26, obtained state-of-the-art results on the Persona-Chat dataset BIBREF0. Pre-trained language models are proposed to build task-oriented dialog systems to drive the progress on leveraging large amounts of available unannotated data. BIBREF27. Similarly, our approach is also built on top of the TransferTransfo framework BIBREF0. BIBREF27 budzianowski2019hello focused on collaborative tasks BIBREF28. We target non-collaborative tasks instead.Another line of work interleaves on-task and off-task content by building a hybrid dialog system that combines a task-oriented model and a non-task-oriented model BIBREF4, BIBREF29. In these studies, task-oriented systems and non-task-oriented systems are designed separately and both systems generate candidate responses. A selector is then designed to choose an appropriate output from the candidate responses BIBREF4 and a connector to combine two response candidates BIBREF30, BIBREF31. Compared with these works, MISSA is end-to-end trainable and thus easier to train and update.Non-Collaborative Task Annotation Scheme	To decouple syntactic and semantic information in utterances and provide detailed supervision, we design a hierarchical intent annotation scheme for non-collaborative tasks. We first separate on-task and off-task intents. As on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task. On the other hand, since off-task content is too general to design task-specific intents, we choose common dialog acts as the categories. The advantage of this hierarchical annotation scheme is apparent when starting a new non-collaborative task: we only need to focus on designing the on-task categories and semantic slots which are the same as traditional task-oriented dialog systems. Consequently, we don't have to worry about the off-task annotation design since the off-task category is universal.In the intent annotation scheme shown in Table TABREF2, we list the designed intent annotation scheme for the newly collected AntiScam dataset and the PersuasionForGood dataset. We first define on-task intents for the datasets, which are key actions in the task. Since our AntiScam focuses on understanding and reacting towards elicitations, we define elicitation, providing_information and refusal as on-task intents. In the PersuasionForGood dataset, we define nine on-task intents in Table TABREF2 based on the original PersuasionForGood dialog act annotation scheme. All these intents are related to donation actions, which are salient on-task intents in the persuasion task. The off-task intents are the same for both tasks, including six general intents and six additional social intents. General intents are more closely related to the syntactic meaning of the sentence (open_question, yes_no_question, positive_answer, negative_answer, responsive_statement, and nonresponsive_statement) while social intents are common social actions (greeting, closing, apology, thanking,respond_to_thank, and hold).For specific tasks, we also design a semantic slot annotation scheme for annotating sentences based on their semantic content. We identify 13 main semantic slots in the anti-scam task, for example, credit card numbers. We present a detailed semantic slot annotation in Table TABREF3. Following BIBREF1, we segment each conversation turn into single sentences and then annotate each sentence rather than turns.Datasets	We test our approach on two non-collaborative task datasets: the AntiScam dataset and the PersuasionForGood dataset BIBREF1. Both datasets are collected from the Amazon Mechanical Turk platform in the form of typing conversations and off-task dialog is interleaved in the dialog.Datasets ::: AntiScam Dataset	To enrich available non-collaborative task datasets, we created a corpus of human-human anti-scam dialogs in order to learn human elicitation strategies. We chose a popular Amazon customer service scam scenario to collect dialogs between users and attackers who aim to collect users information. We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable. We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value.Datasets ::: PersuasionForGood Dataset	The PersuasionForGood dataset BIBREF1 was collected from typing conversations on Amazon Mechanical Turk platform. Two workers were randomly paired, one was assigned the role of persuader, the other was persuadee. The goal of the persuader was to persuade the persuadee to donate a portion of task earning to a specific charity. The dataset consists of 1,017 dialogs, where 300 dialogs are annotated with dialog acts. The average conversation length is 10.43, the vocabulary size is 8,141. Since the original PersuasionForGood dataset is annotated with dialog acts, we select the on-task dialog acts as on-task intents shown in Table TABREF2, and categorize the other dialog acts into our pre-defined off-task intents.Model ::: Background	The TransferTransfo framework was proposed to build open domain dialog systems. BIBREF0 wolf2019transfertransfo fine-tuned the generative pre-training model (GPT) BIBREF32 with the PERSONA-CHAT dataset BIBREF33 in a multi-task fashion, where the language model objective is combined with a next-utterance classification task. The language model's objective is to maximize the following likelihood for a given sequence of tokens, $X = \lbrace x_1,\dots ,x_n\rbrace $:The authors also trained a classifier to distinguish the correct next-utterance appended to the input human utterances from a set of randomly selected utterance distractors. In addition, they introduced dialog state embeddings to indicate speaker role in the model. The model significantly outperformed previous baselines over both automatic evaluations and human evaluations in social conversations. Since the TransferTransfo framework performs well in open domain, we adapt it for non-collaborative settings. We keep all the embeddings in the framework and train the language model and next-utterance classification task in a multi-task fashion following TransferTransfo.We make two major changes: (1) To address the problem that TransferTransfo is originally designed for an open domain without explicit intents and regulations, we add two intent classifiers and two semantic slot classifiers to classify the intents and semantic slots for both human utterances and system responses as an effort to incorporate the proposed hierarchical intent and semantic slot annotation for non-collaborative tasks. (2) In dialog systems, multiple generated responses can be coherent under the current context. Generating diverse responses has proven to be an enduring challenge. To increase response diversity, we sample multiple generated responses and choose an appropriate one according to a set of pre-defined rules.Model ::: Intent and Semantic Slot Classifiers	We train MISSA in a multi-task fashion. In addition to the language model task and the next-utterance prediction task, we also use separate classifiers to predict the intents and semantic slots of both human utterances and system responses. The intent classifier and semantic slot classifier for human utterances capture the semantic and syntactic meaning of human utterances, providing information to select the appropriate response among response candidates while the classifiers for the system intents and semantic slots are designed to help select an appropriate next-sentence. We describe response filtering in the corresponding subsection. Classifiers are designed as the following equation:where $L^i_{t}$ is the intent or semantic label of $i$-th sentence at turn $t$. $h^l_{t-1}$ is the hidden states at the end of last sentence in turn $t-1$, $h^i_{t}$ is the last hidden states at the end of $i$-th sentence in turn $t$. $W_{2h}$ are weights learned during training.MISSA is able to classify multiple intents and multiple semantic slots in a single utterance with these classifiers. Figure FIGREF6 shows how it works on the AntiScam dataset. Specifically, we set a special token $<$sep$>$ at the end of each sentence in an utterance (an utterance can consist of multiple sentences). Next, we pass the token's position information to the transformer architecture and obtain the representation of the position (represented as colored position at last layer in Figure FIGREF6). After that, we concatenate the embeddings at these position with the hidden states of last sentence. We pass these concatenated representations to the intent classifier and the slot classifier to obtain an intent and a semantic slot for each sentence in the utterance. As shown in Figure FIGREF6, the loss function ${\mathcal {L}}$ for the model combines all the task losses:where ${\mathcal {L}_{LM}}$ is the language model loss, ${\mathcal {L}_{I_h}}$, ${\mathcal {L}_{S_h}}$, ${\mathcal {L}_{I_s}}$, and ${\mathcal {L}_{S_s}}$ are losses of intent and slots classifiers, ${\mathcal {L}_{nup}}$ is next-utterance classification loss. $\lambda _{LM}$, $\lambda _{I_h}$, $\lambda _{S_h}$, $\lambda _{I_s}$, $\lambda _{S_s}$, and $\lambda _{nup}$ are the hyper-parameters that control the relative importance of every loss.Model ::: Response Generation	MISSA can generate multiple sentences in a single system turn. Therefore, we perform system generation conditioned on predicted system intents. More specifically, during the training phase, in addition to inserting a special $<$sep$>$ token at the end of each sentence, we also insert the intent of the system response as special tokens at the head of each sentence in the system response. For example, in Figure FIGREF6, we insert a $<$pos_ans$>$ token at the head of $S_t^1$, which is the system response in green. We then use a cross entropy loss function to calculate the loss between the predicted token and the ground truth intent token. During the testing phase, the model first generates a special intent token, then after being conditioned on this intent token, the model keeps generating a sentence until it generates a $<$sep$>$ token. After that, the model continues to generate another intent token and another sentence until it generates an $<$eos$>$ token.Model ::: Response Filtering	Since we only perform conditional generation, a type of soft constraint on the predicted intent of system response, the system can still generate samples that violate simple conversation regulations, such as eliciting information that has already been provided. These corner cases may lead to fatal results in high-risk tasks, for example, health care and education. To improve the robustness of MISSA and improve its ability to generalize to more tasks, we add a response filtering module after the generation. With the nucleus sampling strategy BIBREF5, MISSA is able to generate multiple diverse candidate responses with different intents and semantic slots. We then adopt a task-specific response filtering policy to choose the best candidate response as the final output. In our anti-scam scenario, we set up a few simple rules to filter out some unreasonable candidates, for instance, eliciting the repeated information. The filtering module is easily adaptable to different domains or specific requirements, which makes our dialog system more controllable.Experiments	We evaluate MISSA on two non-collaborative task datasets. AntiScam aims to build a dialog system that occupies the attacker's attention and elicits the attacker's information while PersuasionForGood BIBREF1 aims to build a dialog system that persuades people to donate to a charity. We use $80\%$ data for training, $10\%$ data for validation, and $10\%$ data for testing. More training details are presented in Appendix.Experiments ::: Baseline Models	We compare MISSA mainly with two baseline models:TransferTransfo The vanilla TransferTransfo framework is compared with MISSA to show the impact and necessity of adding the intent and slot classifiers. We follow the original TransferTransfo design BIBREF0 and train with undelexicalized data.Hybrid Following BIBREF4 yu2017learning, we also build a hybrid dialog system by combining vanilla TransferTransfo and MISSA. Specifically, we first determine if the human utterances are on-task or off-task with human intent classifier. If the classifier decides that the utterance is on-task, we choose the response from MISSA; otherwise, we choose the response from vanilla TransferTransfo baseline.In addition, we perform ablation studies on MISSA to show the effects of different components.MISSA-sel denotes MISSA without response filtering.MISSA-con denotes MISSA leaving out the intent token at the start of the response generation.Experiments ::: Automatic Evaluation Metrics	Perplexity Since the canonical measure of a good language model is perplexity, which indicates the error rate of the expected word. We choose perplexity to evaluate the model performance.Response-Intent Prediction (RIP) $\&$ Response-Slot Prediction (RSP) Different from open-domain dialog systems, we care about the intents of the system response in non-collaborative tasks as we hope to know if the system response satisfies user intents. For example, in the anti-scam task, if the attacker elicits information from the system, we need to know if the system refuses or agrees to provide the information. Therefore we care about intent prediction for the generated system response. Since our baselines are more suited for social chat as they cannot produce system intents, we use the system intent and slot classifiers trained in our model to predict their responses' intents and slots. The intent predictor achieves a $84\%$ accuracy and the semantic slot predictor achieves $77\%$ on the AntiScam dataset. Then we compare the predicted values with human-annotated ground truth in the dataset to compute the response-intent prediction (RIP) and response-slot prediction (RSP).Extended Response-Intent Prediction (ERIP) $\&$ Extended Response-Slot Prediction (ERSP) With Response-Intent Prediction, we verify the predicted intents to evaluate the coherence of the dialog. However, the real mapping between human-intent and system-intent is much more complicated as there might be multiple acceptable system-intents for the same human-intent. Therefore, we also design a metric to evaluate if the predicted system-intent is in the set of acceptable intents. Specifically, we estimate the transition probability $p(I_i|I_j)$ by counting the frequency of all the bi-gram human-intent and system-intent pairs in the training data. During the test stage, if the predicted intent matches the ground truth, we set the score as 1, otherwise we set the score as $p(I_{predict}|I_i)$ where $I_i$ is the intent of the input human utterance. We then report the average value of those scores over turns as the final extended response-intent prediction result.Experiments ::: Human Evaluation Metrics	Automatic metrics only validate the system’s performance on a single dimension at a time. The ultimate holistic evaluation should be conducted by having the trained system interact with human users. Therefore we also conduct human evaluations for the dialog system built on AntiScam. We test our models and baselines with 15 college-student volunteers. Each of them is asked to pretend to be an attacker and interact with all the models for at least three times to avoid randomness. We in total collect 225 number of dialogs. Each time, volunteers are required to use similar sentences and strategies to interact with all five models and score each model based on the metrics listed below at the end of the current round. Each model receives a total of 45 human ratings, and the average score is reported as the final human-evaluation score. In total, we design five different metrics to assess the models' conversational ability whilst interacting with humans. The results are shown in Table TABREF19.Fluency Fluency is used to explore different models' language generation quality.Coherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn.Engagement In the anti-scam scenario, one of our missions is to keep engaging with the attackers to waste their time. So we directly ask volunteers (attackers) to what extend they would like to continue chatting with the system.Dialog length (Length) Engagement is a subjective metric. Anti-scam system's goal is to engage user in the conversation longer in order to limit their harm to other potential victims. So we count the dialog length as another metric to evaluate system performance.Task Success Score (TaskSuc) The other goal of the anti-scam system is to elicit attacker's personal information. We count the average type of information (name, address and phone number) that the system obtained from attackers as the task success score.Results and Analysis	Table TABREF19 presents the main experiment results on AntiScam dataset, for both automatic evaluation metrics and human evaluation metrics. The experiment results on PersuasionForGood are shown in Table TABREF23. We observe that MISSA outperforms two baseline models (TransferTransfo and hybrid model) on almost all the metrics on both datasets. For further analysis, examples of real dialogs from the human evaluation are presented in Table TABREF21.Compared to the first TransferTransfo baseline, MISSA outperforms the TransferTransfo baseline on the on-task contents. From Table TABREF19, we observe that MISSA maintains longer conversations (14.9 turns) compared with TransferTransfo (8.5 turns), which means MISSA is better at maintaining the attacker's engagement. MISSA also has a higher task success score (1.294) than TransferTransfo (1.025), which indicates that it elicits information more strategically. In the top two dialogs (A and B) that are shown in Table TABREF21, both attackers were eliciting a credit card number in their first turns. TransferTransfo directly gave away the information, while MISSA replied with a semantically-related question “why would you need my credit card number?"" Furthermore, in the next turn, TransferTransfo ignored the context and asked an irrelevant question “what is your name?” while MISSA was able to generate the response “why can't you use my address?”, which is consistent to the context. We suspect the improved performance of MISSA comes from our proposed annotation scheme: the semantic slot information enables MISSA to keep track of the current entities, and the intent information helps MISSA to maintain coherency and prolong conversations.Compared to the hybrid model baseline, MISSA performs better on off-task content. As shown in the bottom two dialogs in Table TABREF21, attackers in both dialogs introduced their names in their first utterances. MISSA recognized attacker's name, while the hybrid model did not. We suspect it is because the hybrid model does not have the built-in semantic slot predictor. In the second turn, both attackers were explaining the reason of requesting the billing address previously. With semantic slot information, MISSA can easily understand the attacker; but the hybrid model misunderstands that the attacker was talking about the order number, possibly because the token “order” appeared in the attacker's utterance. We suspect that the hybrid model's bad performance on the off-task content leads to its low coherence rating (2.76) and short dialog length (8.2).To explore the influence of the intent-based conditional response generation method and the designed response filter, we perform an ablation study. The results are shown in Table TABREF19. We find that MISSA has higher fluency score and coherence score than MISSA-con (4.18 vs 3.78 for fluency, and 3.75 vs 3.68 for coherence), which suggests that conditioning on the system intent to generate responses improves the quality of the generated sentences. Compared with MISSA-sel, MISSA achieves better performance on all the metrics. For example, the engagement score for MISSA is 3.69 while MISSA-sel only has 2.87. This is because the response filter removed all the incoherent responses, which makes the attacker more willing to keep chatting. The ablation study shows both the conditional language generation mechanism and the response filter are essential to MISSA's good performance.We also apply our method to the PersuasionForGood dataset. As shown in Table TABREF23, MISSA and its variants outperform the TransferTransfo and the hybrid models on all evaluation metrics. Such good performance indicates MISSA can be easily applied to a different non-collaborative task and achieve good performance. Particularly, MISSA achieves the lowest perplexity, which confirms that using conditional response generation leads to high quality responses. Compared with the result on AntiScam dataset, MISSA-con performs the best in terms of RIP and ERIP. We suspect the underlying reason is that there are more possible responses with the same intent in PersuasionForGood than in AntiScam. This also suggests that we should adjust the model structure according to the nature of the dataset.Conclusion and Future Work	We propose a general dialog system pipeline to build non-collaborative dialog systems, including a hierarchical annotation scheme and an end-to-end neural response generation model called MISSA. With the hierarchical annotation scheme, we can distinguish on-task and off-task intents. MISSA takes both on and off-task intents as supervision in its training and thus can deal with diverse user utterances in non-collaborative settings. Moreover, to validate MISSA's performance, we create a non-collaborate dialog dataset that focuses on deterring phone scammers. MISSA outperforms all baseline methods in terms of fluency, coherency, and user engagement on both the newly proposed anti-scam task and an existing persuasion task. However, MISSA still produces responses that are not consistent with their distant conversation history as GPT can only track a limited history span. In future work, we plan to address this issue by developing methods that can effectively track longer dialog context.Acknowledgements	This work was supported by DARPA ASED Program HR001117S0050. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes not withstanding any copyright notation therein. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA or the U.S. Government.Appendix ::: Anti-Scam Collection Setting	We randomly pair two workers: one is assigned the role of the attacker to elicit user information, and the other one is assigned the role of an everyday user who aims to protect her/his information and potentially elicit the attacker's information. We give both workers specific personal data. Instructions are shown in Table TABREF24. The “attacker” additionally receives training on how to elicit information from people. Workers cannot see their partners' instructions.There are two tasks for the users: firstly, users are required to chat with their partners and determine if they are attackers or not, reporting their decisions at the end of the task. If users think their partners are attackers, they are instructed to prolong the conversation and elicit information from their partners. We give a bonus to users if they detect the attackers and elicit real information from the attackers, including the attacker's name, address and phone number. Since one worker can only participate once in the task, they do not know their partners are always attackers.We provide real user information including the user's name and the task background (user purchased a product on Amazon) . Attackers are well-trained to pretend to be an Amazon customer service agent. To simulate a real-world scam, we tell attackers some details about the user, such as the user's name to stop them from being too easily identified. We give a bonus to attackers if they elicit correct information from users, including the user's address, credit card number, CVS and expiration date. Each worker can only participate once to prevent workers from knowing their partner's information and goals in advance. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words. Only 172 out of 220 users successfully identified their partner as an attacker, suggesting that the attackers are well trained and not too easily identifiable.We recruited two expert annotators who have linguistic training to annotate 3,044 sentences in 100 dialogs, achieving a 0.874 averaged weighted kappa value. Table TABREF2 shows that there is a vast amount of off-task content in the dataset, which confirms the necessity of a hierarchical on-task/off-task annotation scheme. We observe that sentences from the attacker and user have different intent distributions. Compared to attackers, users produce more refusal (74 vs 19), because users are more likely to refuse to provide requested information if they have detected the attacker. Moreover, users also ask more open_questions (173 vs 54) and yes_no_questions (165 vs 117) for off-task content because they are instructed to prolong the conversation after detecting the attacker. Furthermore, attackers and users both have a massive amount of social content (292 in total and 252 in total), suggesting that it is important to have social intent sentences to maintain the conversation.Appendix ::: Training details	MISSA is based on the generative pre-trained transformer BIBREF32. We use an Adam optimizer with a learning rate of 6.25e-5 and $L2$ weight decay of $0.01$, we set the coefficient of language modeling loss to be 2, the coefficient of intent and slot classifiers to be 1, and the coefficient of next-utterance classifier to be 1. We first pre-train the model on the PERSONA-CHAT dataset. When fine-tuning on the AntiScam and the PersuasionForGood datasets, we use $80\%$ data for training, $10\%$ data for validation, and $10\%$ data for testing. Since the original PersuasionForGood dataset is annotated with intents, we separate the original on-task and off-task intents, which are shown in Table TABREF2. To deal with the words out of the vocabulary, we conduct delexicalization to replace slot values with corresponding slot tokens during the training phase, and replace the slot tokens with pre-defined information during testing.Appendix ::: Example Dialog	An example of human-human chat on AntiScam dataset is shown in Table TABREF25.","['How big is the ANTISCAM dataset? ', 'How is intent annotated?', 'What are the baselines outperformed by this work?', 'What are the evaluation metrics and criteria used to evaluate the model performance?']","['Unacceptable', 'using a role-playing task on the Amazon Mechanical Turk platform and collecting typed conversations', 'TransferTransfo and Hybrid ', 'Unacceptable']"
22,"Improved Abusive Comment Moderation with User Embeddings	Experimenting with a dataset of approximately 1.6M user comments from a Greek news sports portal, we explore how a state of the art RNN-based moderation method can be improved by adding user embeddings, user type embeddings, user biases, or user type biases. We observe improvements in all cases, with user embeddings leading to the biggest performance gains.	Introduction	News portals often allow their readers to comment on articles, in order to get feedback, engage their readers, and build customer loyalty. User comments, however, can also be abusive (e.g., bullying, profanity, hate speech), damaging the reputation of news portals, making them liable to fines (e.g., when hosting comments encouraging illegal actions), and putting off readers. Large news portals often employ moderators, who are frequently overwhelmed by the volume and abusiveness of comments. Readers are disappointed when non-abusive comments do not appear quickly online because of moderation delays. Smaller news portals may be unable to employ moderators, and some are forced to shut down their comments.In previous work BIBREF0 , we introduced a new dataset of approx. 1.6M manually moderated user comments from a Greek sports news portal, called Gazzetta, which we made publicly available. Experimenting on that dataset and the datasets of Wulczyn et al. Wulczyn2017, which contain moderated English Wikipedia comments, we showed that a method based on a Recurrent Neural Network (rnn) outperforms detox BIBREF1 , the previous state of the art in automatic user content moderation. Our previous work, however, considered only the texts of the comments, ignoring user-specific information (e.g., number of previously accepted or rejected comments of each user). Here we add user embeddings or user type embeddings to our rnn-based method, i.e., dense vectors that represent individual users or user types, similarly to word embeddings that represent words BIBREF2 , BIBREF3 . Experiments on Gazzetta comments show that both user embeddings and user type embeddings improve the performance of our rnn-based method, with user embeddings helping more. User-specific or user-type-specific scalar biases also help to a lesser extent.Dataset	We first discuss the dataset we used, to help acquaint the reader with the problem. The dataset contains Greek comments from Gazzetta BIBREF0 . There are approximately 1.45M training comments (covering Jan. 1, 2015 to Oct. 6, 2016); we call them g-train (Table TABREF5 ). An additional set of 60,900 comments (Oct. 7 to Nov. 11, 2016) was split to development set (g-dev, 29,700 comments) and test set (g-test, 29,700). Each comment has a gold label (`accept', `reject'). The user id of the author of each comment is also available, but user id s were not used in our previous work.When experimenting with user type embeddings or biases, we group the users into the following types. INLINEFORM0 is the number of training comments posted by user (id) INLINEFORM1 . INLINEFORM2 is the ratio of training comments posted by INLINEFORM3 that were rejected.Red: Users with INLINEFORM0 and INLINEFORM1 .Yellow: INLINEFORM0 and INLINEFORM1 .Green: INLINEFORM0 and INLINEFORM1 .Unknown: Users with INLINEFORM0 .Table TABREF6 shows the number of users per type.Methods	rnn: This is the rnn-based method of our previous work BIBREF0 . It is a chain of gru cells BIBREF4 that transforms the tokens INLINEFORM0 of each comment to the hidden states INLINEFORM1 ( INLINEFORM2 ). Once INLINEFORM3 has been computed, a logistic regression (lr) layer estimates the probability that comment INLINEFORM4 should be rejected: DISPLAYFORM0  INLINEFORM0 is the sigmoid function, INLINEFORM1 , INLINEFORM2 .uernn: This is the rnn-based method with user embeddings added. Each user INLINEFORM0 of the training set with INLINEFORM1 is mapped to a user-specific embedding INLINEFORM2 . Users with INLINEFORM3 are mapped to a single `unknown' user embedding. The lr layer is modified as follows; INLINEFORM4 is the embedding of the author of INLINEFORM5 ; and INLINEFORM6 . DISPLAYFORM0 ternn: This is the rnn-based method with user type embeddings added. Each user type INLINEFORM0 is mapped to a user type embedding INLINEFORM1 . The lr layer is modified as follows, where INLINEFORM2 is the embedding of the type of the author of INLINEFORM3 . DISPLAYFORM0 ubrnn: This is the rnn-based method with user biases added. Each user INLINEFORM0 of the training set with INLINEFORM1 is mapped to a user-specific bias INLINEFORM2 . Users with INLINEFORM3 are mapped to a single `unknown' user bias. The lr layer is modified as follows, where INLINEFORM4 is the bias of the author of INLINEFORM5 . DISPLAYFORM0 We expected ubrnn to learn higher (or lower) INLINEFORM0 biases for users whose posts were frequently rejected (accepted) in the training data, biasing the system towards rejecting (accepting) their posts.tbrnn: This is the rnn-based method with user type biases. Each user type INLINEFORM0 is mapped to a user type bias INLINEFORM1 . The lr layer is modified as follows; INLINEFORM2 is the bias of the type of the author. DISPLAYFORM0 We expected tbrnn to learn a higher INLINEFORM0 for the red user type (frequently rejected), and a lower INLINEFORM1 for the green user type (frequently accepted), with the biases of the other two types in between.In all methods above, we use 300-dimensional word embeddings, user and user type embeddings with INLINEFORM0 dimensions, and INLINEFORM1 hidden units in the gru cells, as in our previous experiments BIBREF0 , where we tuned all hyper-parameters on 2% held-out training comments. Early stopping evaluates on the same held-out subset. User and user type embeddings are randomly initialized and updated by backpropagation. Word embeddings are initialized to the word2vec embeddings of our previous work BIBREF0 , which were pretrained on 5.2M Gazzetta comments. Out of vocabulary words, meaning words not encountered or encountered only once in the training set and/or words with no initial embeddings, are mapped (during both training and testing) to a single randomly initialized word embedding, updated by backpropagation. We use Glorot initialization BIBREF5 for other parameters, cross-entropy loss, and Adam BIBREF6 .ubase: For a comment INLINEFORM0 authored by user INLINEFORM1 , this baseline returns the rejection rate INLINEFORM2 of the author's training comments, if there are INLINEFORM3 training comments of INLINEFORM4 , and 0.5 otherwise. INLINEFORM5 tbase: This baseline returns the following probabilities, considering the user type INLINEFORM0 of the author. INLINEFORM1 Results and Discussion	Table TABREF15 shows the auc scores (area under roc curve) of the methods considered. Using auc allows us to compare directly to the results of our previous work BIBREF0 and the work of Wulczyn et al. Wulczyn2017. Also, auc considers performance at multiple classification thresholds INLINEFORM0 (rejecting comment INLINEFORM1 when INLINEFORM2 , for different INLINEFORM3 values), which gives a more complete picture compared to reporting precision, recall, or F-scores for a particular INLINEFORM4 only. Accuracy is not an appropriate measure here, because of class imbalance (Table TABREF5 ). For methods that involve random initializations (all but the baselines), the results are averaged over three repetitions; we also report the standard error across the repetitions.User-specific information always improves our original rnn-based method (Table TABREF15 ), but the best results are obtained by adding user embeddings (uernn). Figure FIGREF16 visualizes the user embeddings learned by uernn. The two dimensions of Fig. FIGREF16 correspond to the two principal components of the user embeddings, obtained via pca.The colors and numeric labels reflect the rejection rates INLINEFORM0 of the corresponding users. Moving from left to right in Fig. FIGREF16 , the rejection rate increases, indicating that the user embeddings of uernn capture mostly the rejection rate INLINEFORM1 . This rate (a single scalar value per user) can also be captured by the simpler user-specific biases of ubrnn, which explains why ubrnn also performs well (second best results in Table TABREF15 ). Nevertheless, uernn performs better than ubrnn, suggesting that user embeddings capture more information than just a user-specific rejection rate bias.Three of the user types (Red, Yellow, Green) in effect also measure INLINEFORM0 , but in discretized form (three bins), which also explains why user type embeddings (ternn) also perform well (third best method). The performance of tbrnn is close to that of ternn, suggesting again that most of the information captured by user type embeddings can also be captured by simpler scalar user-type-specific biases. The user type biases INLINEFORM1 learned by tbrnn are shown in Table TABREF18 . The bias of the Red type is the largest, the bias of the Green type is the smallest, and the biases of the Unknown and Yellow types are in between, as expected (Section SECREF3 ). The same observations hold for the average user-specific biases INLINEFORM2 learned by ubrnn (Table TABREF18 ).Overall, Table TABREF15 indicates that user-specific information (uernn, ubrnn) is better than user-type information (ternn, tbrnn), and that embeddings (uernn, ternn) are better than the scalar biases (ubrnn, tbrnn), though the differences are small. All the rnn-based methods outperform the two baselines (ubase, tbase), which do not consider the texts of the comments.Let us provide a couple of examples, to illustrate the role of user-specific information. We encountered a comment saying just “Ooooh, down to Pireaus...” (translated from Greek), which the moderator had rejected, because it is the beginning of an abusive slogan. The rejection probability of rnn was only 0.34, presumably because there are no clearly abusive expressions in the comment, but the rejection probability of uernn was 0.72, because the author had a very high rejection rate. On the other hand, another comment said “Indeed, I know nothing about the filth of Greek soccer.” (translated, apparently not a sarcastic comment). The original rnn method marginally rejected the comment (rejection probability 0.57), presumably because of the `filth' (comments talking about the filth of some sport or championship are often rejected), but uernn gave it a very low rejection probability (0.15), because the author of the comment had a very low rejection rate.Related work	In previous work BIBREF0 , we showed that our rnn-based method outperforms detox BIBREF1 , the previous state of the art in user content moderation. detox uses character or word INLINEFORM0 -gram features, no user-specific information, and an lr or mlp classifier. Other related work on abusive content moderation was reviewed extensively in our previous work BIBREF0 . Here we focus on previous work that considered user-specific features and user embeddings.Dadvar et al. Dadvar2013 detect cyberbullying in YouTube comments, using an svm and features examining the content of each comment (e.g., second person pronouns followed by profane words, common bullying words), but also the profile and history of the author of the comment (e.g., age, frequency of profane words in past posts). Waseem et al. Waseem2016 detect hate speech tweets. Their best method is an lr classifier, with character INLINEFORM0 -grams and a feature indicating the gender of the author; adding the location of the author did not help.Cheng et al. Cheng2015 predict which users will be banned from on-line communities. Their best system uses a Random Forest or lr classifier, with features examining the average readability and sentiment of each user's past posts, the past activity of each user (e.g., number of posts daily, proportion of posts that are replies), and the reactions of the community to the past actions of each user (e.g., up-votes, number of posts rejected). Lee et al. Lee2014 and Napoles et al. Napoles2017b include similar user-specific features in classifiers intended to detect high quality on-line discussions.Amir et al. Amir2016 detect sarcasm in tweets. Their best system uses a word-based Convolutional Neural Network (cnn). The feature vector produced by the cnn (representing the content of the tweet) is concatenated with the user embedding of the author, and passed on to an mlp that classifies the tweet as sarcastic or not. This method outperforms a previous state of the art sarcasm detection method BIBREF8 that relies on an lr classifier with hand-crafted content and user-specific features. We use an rnn instead of a cnn, and we feed the comment and user embeddings to a simpler lr layer (Eq. EQREF10 ), instead of an mlp. Amir et al. discard unknown users, unlike our experiments, and consider only sarcasm, whereas moderation also involves profanity, hate speech, bullying, threats etc.User embeddings have also been used in: conversational agents BIBREF9 ; sentiment analysis BIBREF10 ; retweet prediction BIBREF11 ; predicting which topics a user is likely to tweet about, the accounts a user may want to follow, and the age, gender, political affiliation of Twitter users BIBREF12 .Our previous work BIBREF0 also discussed how machine learning can be used in semi-automatic moderation, by letting moderators focus on `difficult' comments and automatically handling comments that are easier to accept or reject. In more recent work BIBREF13 we also explored how an attention mechanism can be used to highlight possibly abusive words or phrases when showing `difficult' comments to moderators.Conclusions	Experimenting with a dataset of approx. 1.6M user comments from a Greek sports news portal, we explored how a state of the art rnn-based moderation method can be improved by adding user embeddings, user type embeddings, user biases, or user type biases. We observed improvements in all cases, but user embeddings were the best.We plan to compare uernn to cnn-based methods that employ user embeddings BIBREF14 , after replacing the lr layer of uernn by an mlp to allow non-linear combinations of comment and user embeddings.Acknowledgments	This work was funded by Google's Digital News Initiative (project ml2p, contract 362826). We are grateful to Gazzetta for the data they provided. We also thank Gazzetta's moderators for their feedback, insights, and advice.",['How much gain in performance was obtained with user embeddings?'],['On test set RNN that uses user embedding has AUC of 80.53 compared to base RNN 79.24.']
23,"Personalized Taste and Cuisine Preference Modeling via Images	With the exponential growth in the usage of social media to share live updates about life, taking pictures has become an unavoidable phenomenon. Individuals unknowingly create a unique knowledge base with these images. The food images, in particular, are of interest as they contain a plethora of information. From the image metadata and using computer vision tools, we can extract distinct insights for each user to build a personal profile. Using the underlying connection between cuisines and their inherent tastes, we attempt to develop such a profile for an individual based solely on the images of his food. Our study provides insights about an individual's inclination towards particular cuisines. Interpreting these insights can lead to the development of a more precise recommendation system. Such a system would avoid the generic approach in favor of a personalized recommendation system.	INTRODUCTION	A picture is worth a thousand words. Complex ideas can easily be depicted via an image. An image is a mine of data in the 21st century. With each person taking an average of 20 photographs every day, the number of photographs taken around the world each year is astounding. According to a Statista report on Photographs, an estimated 1.2 trillion photographs were taken in 2017 and 85% of those images were of food. Youngsters can't resist taking drool-worthy pictures of their food before tucking in. Food and photography have been amalgamated into a creative art form where even the humble home cooked meal must be captured in the perfect lighting and in the right angle before digging in. According to a YouGov poll, half of Americans take pictures of their food.The sophistication of smart-phone cameras allows users to capture high quality images on their hand held device. Paired with the increasing popularity of social media platforms such as Facebook and Instagram, it makes sharing of photographs much easier than with the use of a standalone camera. Thus, each individual knowingly or unknowingly creates a food log.A number of applications such as MyFitnessPal, help keep track of a user's food consumption. These applications are heavily dependent on user input after every meal or snack. They often include several data fields that have to be manually filled by the user. This tedious process discourages most users, resulting in a sparse record of their food intake over time. Eventually, this data is not usable. On the other hand, taking a picture of your meal or snack is an effortless exercise.Food images may not give us an insight into the quantity or quality of food consumed by the individual but it can tell us what he/she prefers to eat or likes to eat. We try to tackle the following research question with our work: Can we predict the cuisine of a food item based on just it's picture, with no additional text input from the user?RELATED WORK	The work in this field has not delved into extracting any information from food pictures. The starting point for most of the research is a knowledge base of recipes (which detail the ingredients) mapped to a particular cuisine.Han Su et. al.BIBREF0 have worked on investigating if the recipe cuisines can be predicted from the ingredients of recipes. They treat ingredients as features and provide insights on which cuisines are most similar to each other. Finding common ingredients for each cuisine is also an important aspect. Ueda et al. BIBREF1 BIBREF2 proposed a personalized recipe recommendation method based on users' food preferences. This is derived from his/her recipe browsing activities and cooking history.Yang et al BIBREF3 believed the key to recognizing food is exploiting the spatial relationships between different ingredients (such as meat and bread in a sandwich). They propose a new representation for food items that calculates pairwise statistics between local features computed over a soft pixel-level segmentation of the image into eight ingredient types. Then they accumulate these statistics in a multi-dimensional histogram, which is then used as a feature vector for a discriminative classifier.Existence of huge cultural diffusion among cuisines is shown by the work carried out by S Jayaraman et al in BIBREF4. They explore the performance of each classifier for a given type of dataset under unsupervised learning methods(Linear support Vector Classifier (SVC), Logistic Regression, Random Forest Classifier and Naive Bayes).H Holste et al's work BIBREF5 predicts the cuisine of a recipe given the list of ingredients. They eliminate distribution of ingredients per recipe as a weak feature. They focus on showing the difference in performance of models with and without tf-idf scoring. Their custom tf-idf scoring model performs well on the Yummly Dataset but is considerably naive.R M Kumar et al BIBREF6 use Tree Boosting algorithms(Extreme Boost and Random Forest) to predict cuisine based on ingredients. It is seen from their work that Extreme Boost performs better than Random Forest.Teng et al BIBREF7 have studied substitutable ingredients using recipe reviews by creating substitute ingredient graphs and forming clusters of such ingredients.DATASET	The YummlyBIBREF8 dataset is used to understand how ingredients can be used to determine the cuisine. The dataset consists of 39,774 recipes. Each recipe is associated with a particular cuisine and a particular set of ingredients. Initial analysis of the data-set revealed a total of 20 different cuisines and 6714 different ingredients. Italian cuisine, with 7383 recipes, overshadows the dataset.The numbers of recipes for the 19 cuisines is quite imbalanced.BIBREF9 The following graph shows the count of recipes per cuisine.User specific data is collected from social media platforms such as Facebook and Instagram with the users permission. These images are then undergo a series of pre processing tasks. This helps in cleaning the data.METHODOLOGY	The real task lies in converting the image into interpretable data that can be parsed and used. To help with this, a data processing pipeline is built. The details of the pipeline are discussed below. The data pipeline extensively uses the ClarifaiBIBREF8 image recognition model. The 3 models used extensively are:The General Model : It recognizes over 11,000 different concepts and is a great all purpose solution. We have used this model to distinguish between Food images and Non-Food images.The Food Model : It recognizes more than 1,000 food items in images down to the ingredient level. This model is used to identify the ingredients in a food image.The General Embedding Model : It analyzes images and returns numerical vectors that represent the input images in a 1024-dimensional space. The vector representation is computed by using Clarifai’s ‘General’ model. The vectors of visually similar images will be close to each other in the 1024-dimensional space. This is used to eliminate multiple similar images of the same food item.METHODOLOGY ::: DATA PRE PROCESSING ::: Distinctive Ingredients	A cuisine can often be identified by some distinctive ingredientsBIBREF10. Therefore, we performed a frequency test to find the most occurring ingredients in each cuisine. Ingredients such as salt and water tend to show up at the top of these lists quite often but they are not distinctive ingredients. Hence, identification of unique ingredients is an issue that is overcome by individual inspection. For example:METHODOLOGY ::: DATA PRE PROCESSING ::: To Classify Images as Food Images	A dataset of 275 images of different food items from different cuisines was compiled. These images were used as input to the Clarifai Food Model. The returned tags were used to create a knowledge database. When the general model labels for an image with high probability were a part of this database, the image was classified as a food image. The most commonly occurring food labels are visualized in Fig 3.METHODOLOGY ::: DATA PRE PROCESSING ::: To Remove Images with People	To build a clean database for the user, images with people are excluded. This includes images with people holding or eating food. This is again done with the help of the descriptive labels returned by the Clarifai General Model. Labels such as ""people"" or ""man/woman"" indicate the presence of a person and such images are discarded.METHODOLOGY ::: DATA PRE PROCESSING ::: To Remove Duplicate Images	Duplicate images are removed by accessing the EXIF data of each image. Images with the same DateTime field are considered as duplicates and one copy is removed from the database.METHODOLOGY ::: DATA PRE PROCESSING ::: Natural Language Processing	NLTK tools were used to remove low content adjectives from the labels/concepts returned from the Clarifai Models. This ensures that specific ingredient names are extracted without their unnecessary description. The Porter Stemmer Algorithm is used for removing the commoner morphological and inflectional endings from words.METHODOLOGY ::: Basic Observations	From the food images(specific to each user), each image's descriptive labels are obtained from the Food Model. The Clarifai Food Model returns a list of concepts/labels/tags with corresponding probability scores on the likelihood that these concepts are contained within the image. The sum of the probabilities of each of these labels occurring in each image is plotted against the label in Fig 4.The count of each of the labels occurring in each image is also plotted against each of the labels in Fig 5.METHODOLOGY ::: Rudimentary Method of Classification	Sometimes Clarifai returns the name of the dish itself. For example: ""Tacos"" which can be immediately classified as Mexican. There is no necessity to now map the ingredients to find the cuisine. Therefore, it is now necessary to maintain another database of native dishes from each cuisine. This database was built using the most popular or most frequently occurring dishes from each of the cuisines.When no particular dish name was returned by the API, the ingredients with a probability of greater than 0.75 are selected from the output of the API. These ingredients are then mapped to the unique and frequently occurring ingredients from each cuisine. If more than 10 ingredients occur from a particular cuisine, the dish is classified into that cuisine. A radar map is plotted to understand the preference of the user. In this case, we considered only 10 cuisines.METHODOLOGY ::: KNN Model for Classification	A more sophisticated approach to classify based on the ingredients was adopted by using the K Nearest Neighbors Model. The Yummly dataset from Kaggle is used to train the model. The ingredients extracted from the images are used as a test set. The model was run successfully for k-values ranging from 1-25. The radar charts for some of the k values are shown in Fig 7, 8 and 9.Thus from these charts, we see that the user likes to eat Italian and Mexican food on most occasions. This is also in sync with the rudimentary method that we had used earlier.CONCLUSIONS	In this paper, we present an effortless method to build a personal cuisine preference model. From images of food taken by each user, the data pipeline takes over, resulting in a visual representation of the user's preference. With more focus on preprocessing and natural text processing, it becomes important to realize the difficulty presented by the problem. We present a simple process to extract maximum useful information from the image. We observe that there is significant overlap between the ingredients from different cuisines and the identified unique ingredients might not always be picked up from the image. Although, this similarity is what helps when classifying using the KNN model. For the single user data used, we see that the 338 images are classified as food images. It is observed that Italian and Mexican are the most preferred cuisines. It is also seen that as K value increases, the number of food images classified into Italian increases significantly. Classification into cuisines like Filipino, Vietnamese and Cajun_Creole decreases. This may be attributed to the imbalanced Yummly Dataset that is overshadowed by a high number of Italian recipes.Limitations : The quality of the image and presentation of food can drastically affect the system. Items which look similar in shape and colour can throw the system off track. However, with a large database this should not matter much.Future Directions : The cuisine preferences determined for a user can be combined with the weather and physical activity of the user to build a more specific suggestive model. For example, if the meta data of the image were to be extracted and combined with the weather conditions for that date and time then we would be able to predict the type of food the user prefers during a particular weather. This would lead to a sophisticated recommendation system.","['Is this done in form of unsupervised (clustering) or suppervised learning?', 'Does this study perform experiments to prove their claim that indeed personalized profiles will have inclination towards particular cuisines?']","['Supervised methods are used to identify the dish and ingredients in the image, and an unsupervised method (KNN) is used to create the food profile.', 'Unacceptable']"
24,"Improving Few-shot Text Classification via Pretrained Language Representations.	Text classification tends to be difficult when the data is deficient or when it is required to adapt to unseen classes. In such challenging scenarios, recent studies have often used meta-learning to simulate the few-shot task, thus negating explicit common linguistic features across tasks. Deep language representations have proven to be very effective forms of unsupervised pretraining, yielding contextualized features that capture linguistic properties and benefit downstream natural language understanding tasks. However, the effect of pretrained language representation for few-shot learning on text classification tasks is still not well understood. In this study, we design a few-shot learning model with pretrained language representations and report the empirical results. We show that our approach is not only simple but also produces state-of-the-art performance on a well-studied sentiment classification dataset. It can thus be further suggested that pretraining could be a promising solution for few shot learning of many other NLP tasks. The code and the dataset to replicate the experiments are made available at this https URL.	Introduction	Deep learning (DL) has achieved great success in many fields such as computer vision, speech recognition, and machine translation BIBREF0 , BIBREF1 , BIBREF2 thanks to the advancements in optimization techniques, larger datasets, and streamlined designs of deep neural architectures. However, DL is notorious for requiring large labeled datasets, which limits the scalability of a deep model to new classes owing to the cost of annotation. Humans, however, are readily able to learn and distinguish new classes rapidly with only a few examples. This gap between human and machine learning provides opportunities for DL development and applications.Few-shot learning generally resolves the data deficiency problem by recognizing novel classes from very few labeled examples. This limitation in the size of samples (only one or very few examples) challenges the standard fine-tuning method in DL. Early studies in this field BIBREF3 applied data augmentation and regularization techniques to alleviate the overfitting problem caused by data scarcity but only to a limited extent. Instead, researchers have been inspired by human learning to explore meta-learning BIBREF4 to leverage the distribution over similar tasks. Contemporary approaches to few-shot learning often decompose the training procedure into an auxiliary meta-learning phase, which includes many sub-tasks, following the principle that the testing and training conditions must match. They extract some transferable knowledge by switching the task from one mini-batch to the next. Moreover, the few-shot model is able to classify data into new classes with just a small labeled support set.Existing approaches for few-shot learning are still plagued by problems, including imposed strong priors BIBREF5 , complex gradient transfer between tasks BIBREF6 , and fine-tuning of the target problem BIBREF7 . The approaches proposed by BIBREF8 and BIBREF9 , which combine non-parametric methods and metric learning, may provide possible solutions to these problems. The non-parametric methods allow novel examples to be rapidly assimilated without suffering from the effects of catastrophic overfitting. Such non-parametric models only need to learn the representation of the samples and the metric measure.Recently, a variety of techniques were proposed for training general-purpose language representation models using an enormous amount of unannotated text, such as ELMo BIBREF10 and generative pretrained transformer (GPT) BIBREF11 . Pretrained models can be fine-tuned on natural language processing (NLP) tasks and have achieved significant improvements over training on task-specific annotated data. More recently, a pretraining technique named bidirectional encoder representations from transformers (BERT) BIBREF12 was proposed and has enabled the creation of state-of-the-art models for a wide variety of NLP tasks, including question answering (SQuAD v1.1) and natural language inference, among others.However, there have not been many efforts in exploring pretrained language representations for few-shot text classification. The technical contributions of this work are two-fold: 1) we explore the pretrained model to address the poor generalization capability of text classification, and 2) we propose a meta-learning model based on model-agnostic meta-learning (MAML) which explicitly disentangles the task-agnostic feature learning and task-specific feature learning to demonstrate that the proposed model achieves significant improvement on text classification accuracy on public benchmark datasets. To the best of our knowledge, we are the first to bridge the pretraining strategy with meta-learning methods for few-shot text classification.Background: Meta-Learning	Our work is built on the recently proposed MAML framework BIBREF4 , which we describe briefly here. MAML aims to learn the learners (for the tasks) and the meta-learner in the few-shot meta-learning setup BIBREF13 , BIBREF14 , BIBREF15 . Formally, it considers a model that is represented by a function INLINEFORM0 with parameters INLINEFORM1 . When the model adapts to a new task INLINEFORM2 , the model changes the parameters from one INLINEFORM3 to the next, where a task contains INLINEFORM4 training examples and one or more test examples (K-shot learning). MAML updates the parameters INLINEFORM5 via one or a few iterations of gradient descent based on the training examples of task INLINEFORM6 . For example, for one gradient update, INLINEFORM0 ,where the step size INLINEFORM0 is a hyperparameter; INLINEFORM1 is a loss function that evaluates the error between the prediction INLINEFORM2 and target INLINEFORM3 , where INLINEFORM4 , INLINEFORM5 are input–output pairs sampled from the training examples of task INLINEFORM6 . Model parameters INLINEFORM7 are trained to optimize the performance of INLINEFORM8 on the unseen test examples from INLINEFORM9 across tasks. The meta-objective is as follows: INLINEFORM0 The goal of MAML is to optimize the model parameters INLINEFORM0 such that the model can learn to adapt to new tasks with parameters via a few gradient steps on the training examples of the new tasks. The model is improved by considering how the test errors on the unseen test data from INLINEFORM1 change with respect to the parameters. The meta-objective across tasks is optimized using stochastic gradient descent (SGD). The model parameters INLINEFORM2 are updated as follows: INLINEFORM0 Problem definition	Few-shot classification is a task in which a classifier must adapt and accommodate new classes that are not seen in training, given only a few examples of each of these new classes. We have a large labeled training set with a set of defined classes INLINEFORM0 . However, after training, our ultimate goal is to produce classifiers on the testing set with a disjoint set of new classes INLINEFORM1 for which only a small labeled support set will be available. If the support set contains INLINEFORM2 labeled examples for each of the INLINEFORM3 unique classes, the target few-shot problem is called a INLINEFORM4 -way INLINEFORM5 -shot problem. Usually, INLINEFORM6 is a too small sample set to train a supervised classification model. Therefore, we aim to perform meta-learning on the training set in order to extract transferrable knowledge that will allow us to perform better few-shot learning on the support set to classify the test set more successfully.Training Procedure	The training procedure of our approach consists of two parts. Language Representation Pretraining. Given all the training samples, we first utilize pretraining strategies to learn task-agnostic contextualized features that capture linguistic properties to benefit downstream few-shot text classification tasks.Episode-based Meta Training. Given the pretrained language representations, we construct episodes to compute gradients and update the model in each training iteration with MAML.Language Representation Pretraining	While the pretraining tasks have been designed with particular downstream tasks in mind BIBREF16 , we focus on those training tasks that seek to induce universal representations suitable for downstream few-shot learning tasks. We utilize BERT BIBREF12 as a recent study BIBREF17 has shown its potential to achieve state-of-the-art performance when fine-tuned in NLP tasks. BERT combines both word and sentence representations (via masked language model and next sentence prediction objectives) in a single very large pretrained transformer BIBREF18 . It is adapted to both word- and sentence-level tasks with task-specific layers. We feed the sentence representation into a softmax layer for text classification based on BIBREF12 .Episode-Based Meta Training	Given the pretrained language representations, we construct episodes to compute the gradients and update our model in each training iteration. The training episode is formed by randomly selecting a subset of classes from the training set, then choosing a subset of examples within each selected class to act as the support set INLINEFORM0 with a subset of the remaining examples to serve as the query set INLINEFORM1 . Training with such episodes is achieved by feeding the support set INLINEFORM2 to the model and updating its parameters to minimize the loss in the query set INLINEFORM3 . We call this strategy as episode-based meta training. The adaptation of meta-learning using the MAML framework with pretrained language representations is summarized in Algorithm SECREF4 , called P-MAML. The use of episodes makes the training procedure more faithful to the test environment, thereby improving generalization. It is worth noting that there are exponentially many possible meta tasks to train the model with, thus making it difficult to overfit.[th] [1] P-MAML Algorithm Training Datapoints INLINEFORM0 Construct a task INLINEFORM1 with training examples using a support set INLINEFORM2 and a test example INLINEFORM3 Randomly initialize INLINEFORM4 Pre-train INLINEFORM5 with BERT Denote INLINEFORM6 as distribution over tasks not done Sample batch of tasks INLINEFORM7 : all INLINEFORM8 Evaluate INLINEFORM9 using INLINEFORM10 Compute adapted parameters with gradient descent: INLINEFORM11 Update INLINEFORM12 using each INLINEFORM13 from INLINEFORM14 and INLINEFORM15 Datasets and Evaluation	We use the multiple tasks with the multi-domain sentiment classification BIBREF19 dataset ARSC. This dataset comprises English reviews for 23 types of products on Amazon. For each product domain, there are three different binary classification tasks. These buckets then form 23 INLINEFORM0 3 = 69 tasks in total. Following BIBREF20 , we select 12 (4 INLINEFORM1 3) tasks from four domains (i.e., Books, DVDs, Electronics, and Kitchen) as the test set, with only five examples as support set for each label in the test set. We thus create 5-shot learning models on this dataset. We evaluate the performance by few-shot classification accuracy following previous studies in few-shot learning BIBREF8 , BIBREF9 . To evaluate the proposed model objectively with the baselines, note that for ARSC, the support set for testing is fixed by BIBREF20 ; therefore, we need to run the test episode once for each of the target tasks. The mean accuracy from the 12 target tasks is compared to those of the baseline models in accordance with BIBREF20 . We use pretrained BERT-Base for the ARSC dataset. All model parameters are updated by backpropagation using Adam with a learning rate of 0.01. We regularize our network using dropout with a rate of 0.3 tuned using the development set.Evaluation Results	To evaluate the performance of our model, we compared it with various baseline models. The evaluation results are shown in Table TABREF9 : P-MAML is our current approach, Match Network BIBREF13 is a few-shot learning model using metric-based attention method, Prototypical Network BIBREF8 is a deep matrix-based method using sample averages as class prototypes, MAML BIBREF4 is an MAML method that is compatible with any model trained with gradient descent and applicable to a variety of learning problems, Relation Network BIBREF9 is a metric-based few-shot learning model that uses a neural network as the distance measurement and calculate class vectors by summing sample vectors in the support set, ROBUSTTC-FSL BIBREF20 is an approach that combines adaptive metric methods by clustering the tasks, Induction-Network-Routing BIBREF21 is a recent state-of-the-art method which learn generalized classwise representations by combining the dynamic routing algorithm with a typical meta-learning framework. From the results shown in Table TABREF9 , we observe that our approach achieves the best results amongst all meta-learning models. Compared with ROBUSTTC-FSL and Induction-Network-Routing, which adopt several metric methods and dynamic routing algorithms, our approach still provides more advantages. We believe the performance of our model can be further improved by adopting additional mechanisms like adaptive metrics, which will be part of our future work. Note that, our approach is very simple and independent of the encoder choices, and can, therefore, be easily adapted to fit other encoder architectures for sophisticated NLP tasks.Ablation Study	To analyze the contributions and effects of language representation pretraining in our approach, we perform ablation tests. GloVe is the method with pretrained GloVe BIBREF22 word embeddings; w/o pretrain is our method without pre-trained embeddings (random initialization). From the evaluation results in Table TABREF11 , we observe the performance drop significantly without pretraining, which proves the effectiveness of explicit common linguistic features learning. We also notice that our model with GloVe does not achieve good performance even compared with the random initialization, which indicates that the poor generalization capability for few-shot text classification.Discussions	It should be noted that human beings are intelligent to leverage learned knowledge about the world in understanding language. BIBREF23 think human beings have a universal grammar, and our daily language system is only a formal expression of this universal grammar. In other words, there are deep structures related to concepts and superficial structures related to speech and symbols in a language. Moreover, neuroscience research has proposed a prominent idea that language processing may offer such a principle that the brain contains partially separate systems for processing syntax and semantics. The part of the prefrontal cortex responsible for language production, called Broca’s area, is thought to be important for parsing syntactic information and applying selective attention to help a separate comprehension system interpret the semantics BIBREF24 . Our idea for few-shot learning in NLP is somewhat similar to this assumption as the pretraining stage may learn common syntax information across tasks, and the meta-learning stage may learn semantic knowledge, which is task specific.Conclusion	In this study, we attempt to analyze language representation pretraining for few-shot text classification empirically. We combine the MAML algorithm with the pretraining strategy to disentangle the task-agnostic and task-specific representation learning. Results show that our model outperforms conventional state-of-the-art few-shot text classification models. In the future, we plan to apply our method to other NLP scenarios.","['What pretrained language representations are used?', 'How many instances are explored in the few-shot experiments?', 'What tasks are explored?']","['Unacceptable', 'Unacceptable', 'Unacceptable']"
25,"Scaling in Words on Twitter	Scaling properties of language are a useful tool for understanding generative processes in texts. We investigate the scaling relations in citywise Twitter corpora coming from the Metropolitan and Micropolitan Statistical Areas of the United States. We observe a slightly superlinear urban scaling with the city population for the total volume of the tweets and words created in a city. We then find that a certain core vocabulary follows the scaling relationship of that of the bulk text, but most words are sensitive to city size, exhibiting a super- or a sublinear urban scaling. For both regimes we can offer a plausible explanation based on the meaning of the words. We also show that the parameters for Zipf's law and Heaps law differ on Twitter from that of other texts, and that the exponent of Zipf's law changes with city size.	Introduction	The recent increase in digitally available language corpora made it possible to extend the traditional linguistic tools to a vast amount of often user-generated texts. Understanding how these corpora differ from traditional texts is crucial in developing computational methods for web search, information retrieval or machine translation BIBREF0 . The amount of these texts enables the analysis of language on a previously unprecedented scale BIBREF1 , BIBREF2 , BIBREF3 , including the dynamics, geography and time scale of language change BIBREF4 , BIBREF5 , social media cursing habits BIBREF6 , BIBREF7 , BIBREF8 or dialectal variations BIBREF9 .From online user activity and content, it is often possible to infer different socio-economic variables on various aggregation scales. Ranging from showing correlation between the main language features on Twitter and several demographic variables BIBREF10 , through predicting heart-disease rates of an area based on its language use BIBREF11 or relating unemployment to social media content and activity BIBREF12 , BIBREF13 , BIBREF14 to forecasting stock market moves from search semantics BIBREF15 , many studies have attempted to connect online media language and metadata to real-world outcomes. Various studies have analyzed spatial variation in the text of OSN messages and its applicability to several different questions, including user localization based on the content of their posts BIBREF16 , BIBREF17 , empirical analysis of the geographic diffusion of novel words, phrases, trends and topics of interest BIBREF18 , BIBREF19 , measuring public mood BIBREF20 .While many of the above cited studies exploit the fact that language use or social media activity varies in space, it is hard to capture the impact of the geographic environment on the used words or concepts. There is a growing literature on how the sheer size of a settlement influences the number of patents, GDP or the total road length driven by universal laws BIBREF21 . These observations led to the establishment of the theory of urban scaling BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 , where scaling laws with city size have been observed in various measures such as economic productivity BIBREF31 , human interactions BIBREF32 , urban economic diversification BIBREF33 , election data BIBREF34 , building heights BIBREF35 , crime concentration BIBREF36 , BIBREF37 or touristic attractiveness BIBREF38 .In our paper, we aim to capture the effect of city size on language use via individual urban scaling laws of words. By examining the so-called scaling exponents, we are able to connect geographical size effects to systematic variations in word use frequencies. We show that the sensitivity of words to population size is also reflected in their meaning. We also investigate how social media language and city size affects the parameters of Zipf's law BIBREF39 , and how the exponent of Zipf's law is different from that of the literature value BIBREF39 , BIBREF40 . We also show that the number of new words needed in longer texts, the Heaps law BIBREF1 exhibits a power-law form on Twitter, indicating a decelerating growth of distinct tokens with city size.Twitter and census data	We use data from the online social network Twitter, which freely provides approximately 1% of all sent messages via their streaming API. For mobile devices, users have an option to share their exact location along with the Twitter message. Therefore, some messages contain geolocation information in the form of GPS-coordinates. In this study, we analyze 456 millions of these geolocated tweets collected between February 2012 and August 2014 from the area of the United States. We construct a geographically indexed database of these tweets, permitting the efficient analysis of regional features BIBREF41 . Using the Hierarchical Triangular Mesh scheme for practical geographic indexing, we assigned a US county to each tweet BIBREF42 , BIBREF43 . County borders are obtained from the GAdm database BIBREF44 . Counties are then aggregated into Metropolitan and Micropolitan Areas using the county to metro area crosswalk file from BIBREF45 . Population data for the MSA areas is obtained from BIBREF46 .There are many ways a user can post on Twitter. Because a large amount of the posts come from third-party apps such as Foursquare, we filter the messages according to their URL field. We only leave messages that have either no source URL, or their URL after the 'https://' prefix matches one of the following SQL patterns: 'twit%', 'tl.gd%' or 'path.com%'. These are most likely text messages intended for the original use of Twitter, and where automated texts such as the phrase 'I'm at' or 'check-in' on Foursquare are left out.For the tokenization of the Twitter messages, we use the toolkit published on https://github.com/eltevo/twtoolkit. We leave out words that are less than three characters long, contain numbers or have the same consecutive character more than twice. We also filter hashtags, characters with high unicode values, usernames and web addresses BIBREF41 .Urban scaling	Most urban socioeconomic indicators follow the certain relation for a certain urban system: DISPLAYFORM0 where INLINEFORM0 denotes a quantity (economic output, number of patents, crime rate etc.) related to the city, INLINEFORM1 is a multiplication factor, and INLINEFORM2 is the size of the city in terms of its population, and INLINEFORM3 denotes a scaling exponent, that captures the dynamics of the change of the quantity INLINEFORM4 with city population INLINEFORM5 . INLINEFORM6 describes a linear relationship, where the quantity INLINEFORM7 is linearly proportional to the population, which is usually associated with individual human needs such as jobs, housing or water consumption. The case INLINEFORM8 is called superlinear scaling, and it means that larger cities exhibit disproportionately more of the quantity INLINEFORM9 than smaller cities. This type of scaling is usually related to larger cities being disproportionately the centers of innovation and wealth. The opposite case is when INLINEFORM10 , that is called sublinear scaling, and is usually related to infrastructural quantities such as road network length, where urban agglomeration effects create more efficiency. BIBREF26 Here we investigate scaling relations between urban area populations and various measures of Twitter activity and the language on Twitter. When fitting scaling relations on aggregate metrics or on the number of times a certain word appears in a metropolitan area, we always assume that the total number of tweets, or the total number of a certain word INLINEFORM0 must be conserved in the law. That means that we have only one parameter in our fit, the value of INLINEFORM1 , while the multiplication factor INLINEFORM2 determined by INLINEFORM3 and INLINEFORM4 as follows: INLINEFORM5 where the index INLINEFORM0 denotes different cities, the total number of cities is INLINEFORM1 , and INLINEFORM2 is the population of the city with index INLINEFORM3 .We use the 'Person Model' of Leitao et al. BIBREF47 , where this conservation is ensured by the normalization factor, and where the assumption is that out of the total number of INLINEFORM0 units of output that exists in the whole urban system, the probability INLINEFORM1 for one person INLINEFORM2 to obtain one unit of output depends only on the population INLINEFORM3 of the city where person INLINEFORM4 lives as INLINEFORM5 where INLINEFORM0 is the normalization constant, i.e. INLINEFORM1 , if there are altogether INLINEFORM2 people in all of the cities. Formally, this model corresponds to a scaling relationship from ( EQREF3 ), where INLINEFORM3 . But it can also be interpreted as urban scaling being the consequence of the scaling of word choice probabilities for a single person, which has a power-law exponent of INLINEFORM4 .To assess the validity of the scaling fits for the words, we confirm nonlinear scaling, if the difference between the likelihoods of a model with a INLINEFORM0 (the scaling exponent of the total number of words) and INLINEFORM1 given by the fit is big enough. It means that the difference between the Bayesian Information Criterion (BIC) values of the two models INLINEFORM2 is sufficiently large BIBREF47 : INLINEFORM3 . Otherwise, if INLINEFORM4 , the linear model fits the scaling better, and between the two values, the fit is inconclusive.Zipf's law	We use the following form for Zipf's law that is proposed in BIBREF48 , and that fits the probability distribution of the word frequencies apart from the very rare words: INLINEFORM0 We fit the probability distribution of the frequencies using the powerlaw package of Python BIBREF49 , that uses a Maximum Likelihood method based on the results of BIBREF50 , BIBREF51 , BIBREF52 . INLINEFORM0 is the frequency for which the power-law fit is the most probable with respect to the Kolmogorov-Smirnov distance BIBREF49 .A perhaps more common form of the law connects the rank of a word and its frequency: INLINEFORM0 We use the previous form because the fitting method of BIBREF49 can only reliably tell the exponent for the tail of a distribution. In the rank-frequency case, the interesting part of the fit would be at the first few ranks, while the most common words are in the tail of the INLINEFORM0 distribution.The two formulations can be easily transformed into each other (see BIBREF48 , which gives us INLINEFORM0 This enables us to compare our result to several others in the literature.Scaling of aggregate metrics	First, we checked how some aggregate metrics: the total number of users, the total number of individual words and the total number of tweets change with city size. Figures FIGREF6 , FIGREF7 and FIGREF8 show the scaling relationship data on a log-log scale, and the result of the fitted model. In all cases, INLINEFORM0 was greater than 6, which confirmed nonlinear scaling. The the total count of tweets and words both have a slightly superlinear exponents around 1.02. The deviation from the linear exponent may seem small, but in reality it means that for a tenfold increase in city size, the abundance of the quantity INLINEFORM1 measured increases by 5%, which is already a significant change. The number of users scales sublinearly ( INLINEFORM2 ) with the city population, though.It has been shown in BIBREF32 that total communication activity in human interaction networks grows superlinearly with city size. This is in line with our findings that the total number of tweets and the total word count scales superlinearly. However, the exponents are not as big as that of the number of calls or call volumes in the previously mentioned article ( INLINEFORM0 ), which suggests that scaling exponents obtained from a mobile communication network cannot automatically be translated to a social network such as Twitter.Individual scaling of words	For the 11732 words that had at least 10000 occurrences in the dataset, we fitted scaling relationships using the Person Model. The distribution of the fitted exponents is visible in Figure FIGREF11 . There is a most probable exponent of approximately 1.02, which corresponds roughly to the scaling exponent of the overall word count. This is the exponent which we use as an alternative model for deciding nonlinearity, because a word that has a scaling law with the same exponent as the total number of words has the same relative frequency in all urban areas. The linear and inconclusive cases calculated from INLINEFORM0 values are located around this maximum, as shown in different colors in Figure FIGREF11 . In this figure, linearly and nonlinearly classified fits might appear in the same exponent bin, because of the similarity in the fitted exponents, but a difference in the goodness of fit. Words with a smaller exponent, that are ""sublinear"" do not follow the text growth, thus, their relative frequency decreases as city size increases. Words with a greater exponent, that are ""superlinear"" will relatively be more prevalent in texts in bigger cities. There are slightly more words that scale sublinearly (5271, 57% of the nonlinear words) than superlinearly (4011, 43% of the nonlinear words). Three example fits from the three scaling regime are shown in Figure FIGREF10 .We sorted the words falling into the ""linear"" scaling category according to their INLINEFORM0 values showing the goodness of fit for the fixed INLINEFORM1 model. The first 50 words in Table TABREF12 according to this ranking are some of the most common words of the English language, apart from some swearwords and abbreviations (e.g. lol) that are typical for Twitter language BIBREF10 . These are the words that are most homogeneously present in the text of all urban areas.From the first 5000 words according to word rank by occurrence, the most sublinearly and superlinearly scaling words can be seen in Table TABREF13 . Their exponent differs significantly from that of the total word count, and their meaning can usually be linked to the exponent range qualitatively. The sublinearly scaling words mostly correspond to weather services reporting (flood 0.54, thunderstorm 0.61, wind 0.85), some certain slang and swearword forms (shxt 0.81, dang 0.88, damnit 0.93), outdoor-related activities (fishing 0.82, deer 0.81, truck 0.90, hunting 0.87) and certain companies (walmart 0.83). There is a longer tail in the range of superlinearly scaling words than in the sublinear regime in Figure FIGREF11 . This tail corresponds to Spanish words (gracias 1.41, por 1.40, para 1.39 etc.), that could not be separated from the English text, since the shortness of tweets make automated language detection very noisy. Apart from the Spanish words, again some special slang or swearwords (deadass 1.52, thx 1.16, lmfao 1.17, omfg 1.16), flight-reporting (flight 1.25, delayed 1.24 etc.) and lifestyle-related words (fitness 1.15, fashion 1.15, restaurant 1.14, traffic 1.22) dominate this end of the distribution.Thus, when compared to the slightly nonlinear scaling of total amount of words, not all words follow the growth homogeneously with this same exponent. Though a significant amount remains in the linear or inconclusive range according to the statistical model test, most words are sensitive to city size and exhibit a super- or sublinear scaling. Those that fit the linear model the best, correspond to a kind of 'core-Twitter' vocabulary, which has a lot in common with the most common words of the English language, but also shows some Twitter-specific elements. A visible group of words that are amongst the most super- or sublinearly scaling words are related to the abundance or lack of the elements of urban lifestyle (e.g. deer, fitness). Thus, the imprint of the physical environment appears in a quantifiable way in the growths of word occurrences as a function of urban populations. Swearwords and slang, that are quite prevalent in this type of corpus BIBREF7 , BIBREF6 , appear at both ends of the regime that suggests that some specific forms of swearing disappear with urbanization, but the share of overall swearing on Twitter grows with city size. The peak consisting of Spanish words at the superlinear end of the exponent distribution marks the stronger presence of the biggest non-English speaking ethnicity in bigger urban areas. This is confirmed by fitting the scaling relationship to the Hispanic or Latino population BIBREF53 of the MSA areas ( INLINEFORM0 , see SI), which despite the large error, is very superlinear.Zipf's law on Twitter	Figure FIGREF15 shows the distribution of word counts in the overall corpus. The power-law fit gave a minimum count INLINEFORM0 , and an exponent INLINEFORM1 . To check whether this law depends on city size, we fitted the same distribution for the individual cities, and according to Figure FIGREF16 , the exponent gradually decreases with city size, that is, it decreases with the length of the text.That the relative frequency of some words changes with city size means that the frequency of words versus their rank, Zipf's law, can vary from metropolitan area to metropolitan area. We obtained that the exponent of Zipf's law depends on city size, namely that the exponent decreases as text size increases. It means that with the growth of a city, rarer words tend to appear in greater numbers. The values obtained for the Zipf exponent are in line with the theoretical bounds 1.6-2.4 of BIBREF54 . In the communication efficiency framework BIBREF54 , BIBREF55 , decreasing INLINEFORM0 can be understood as decreased communication efficiency due to the increased number of different tokens, that requires more effort in the process of understanding from the reader. Using more specific words can also be a result of the 140 character limit, that was the maximum length of a tweet at the time of the data collection, and it may be a similar effect to that of texting BIBREF56 . This suggests that the carrying medium has a huge impact on the exact values of the parameters of linguistic laws.The Zipf exponent measured in the overall corpus is also much lower than the INLINEFORM0 from the original law BIBREF39 . We do not observe the second power-law regime either, as suggested by BIBREF57 and BIBREF48 . Because most observations so far hold only for books or corpora that contain longer texts than tweets, our results suggest that the nature of communication, in our case Twitter itself affects the parameters of linguistic laws.Vocabulary size change	Figure FIGREF18 shows the vocabulary size as a function of the metropolitan area population, and the power-law fit. It shows that in contrary to the previous aggregate metrics, the vocabulary size grows very sublinearly ( INLINEFORM0 ) with the city size. This relationship can also be translated to the dependency on the total word count, which would give a INLINEFORM1 , another sublinear scaling.The decrease in INLINEFORM0 for bigger cities (or bigger Twitter corpora) suggesting a decreasing number of words with lower frequencies is thus confirmed. There is evidence, that as languages grow, there is a decreasing marginal need for new words BIBREF58 . In this sense, the decelerated extension of the vocabulary in bigger cities can also be regarded as language growth.Conclusion	In this paper, we investigated the scaling relations in citywise Twitter corpora coming from the Metropolitan and Micropolitan Statstical Areas of the United States. We could observe a slightly superlinear scaling decreasing with the city population for the total volume of the tweets and words created in a city. When observing the scaling of individual words, we found that a certain core vocabulary follows the scaling relationship of that of the bulk text, but most words are sensitive to city size, and their frequencies either increase at a higher or a lower rate with city size than that of the total word volume. At both ends of the spectrum, the meaning of the most superlinearly or most sublinearly scaling words is representative of their exponent. We also examined the increase in the number of words with city size, which has an exponent in the sublinear range. This shows that there is a decreasing amount of new words introduced in larger Twitter corpora.","['Is this analysis performed only on English data?', ""Do they authors offer any hypothesis for why the parameters of Zipf's law and Heaps' law differ on Twitter?"", 'What explanation do the authors offer for the super or sublinear urban scaling?', 'Do the authors give examples of the core vocabulary which follows the scaling relationship of the bulk text?']","['Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable']"
26,"Question Answering on Freebase via Relation Extraction and Textual Evidence	Existing knowledge-based question answering systems often rely on small annotated training data. While shallow methods like relation extraction are robust to data scarcity, they are less expressive than the deep meaning representation methods like semantic parsing, thereby failing at answering questions involving multiple constraints. Here we alleviate this problem by empowering a relation extraction method with additional evidence from Wikipedia. We first present a neural network based relation extractor to retrieve the candidate answers from Freebase, and then infer over Wikipedia to validate these answers. Experiments on the WebQuestions question answering dataset show that our method achieves an F_1 of 53.3%, a substantial improvement over the state-of-the-art.	Introduction	Since the advent of large structured knowledge bases (KBs) like Freebase BIBREF0 , YAGO BIBREF1 and DBpedia BIBREF2 , answering natural language questions using those structured KBs, also known as KB-based question answering (or KB-QA), is attracting increasing research efforts from both natural language processing and information retrieval communities.The state-of-the-art methods for this task can be roughly categorized into two streams. The first is based on semantic parsing BIBREF3 , BIBREF4 , which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem BIBREF4 , BIBREF5 , BIBREF6 .On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 or distributed representations BIBREF11 , BIBREF12 . Designing large training datasets for these methods is relatively easy BIBREF7 , BIBREF13 , BIBREF14 . These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America because of the lack of sophisticated representation for the mathematical function highest. To select the correct answer, one has to retrieve all the heights of the mountains, and sort them in descending order, and then pick the first entry. We propose a method based on textual evidence which can answer such questions without solving the mathematic functions implicitly.Knowledge bases like Freebase capture real world facts, and Web resources like Wikipedia provide a large repository of sentences that validate or support these facts. For example, a sentence in Wikipedia says, Denali (also known as Mount McKinley, its former official name) is the highest mountain peak in North America, with a summit elevation of 20,310 feet (6,190 m) above sea level. To answer our example question against a KB using a relation extractor, we can use this sentence as external evidence, filter out wrong answers and pick the correct one.Using textual evidence not only mitigates representational issues in relation extraction, but also alleviates the data scarcity problem to some extent. Consider the question, who was queen isabella's mother. Answering this question involves predicting two constraints hidden in the word mother. One constraint is that the answer should be the parent of Isabella, and the other is that the answer's gender is female. Such words with multiple latent constraints have been a pain-in-the-neck for both semantic parsing and relation extraction, and requires larger training data (this phenomenon is coined as sub-lexical compositionality by wang2015). Most systems are good at triggering the parent constraint, but fail on the other, i.e., the answer entity should be female. Whereas the textual evidence from Wikipedia, ...her mother was Isabella of Barcelos ..., can act as a further constraint to answer the question correctly.We present a novel method for question answering which infers on both structured and unstructured resources. Our method consists of two main steps as outlined in sec:overview. In the first step we extract answers for a given question using a structured KB (here Freebase) by jointly performing entity linking and relation extraction (sec:kb-qa). In the next step we validate these answers using an unstructured resource (here Wikipedia) to prune out the wrong answers and select the correct ones (sec:refine). Our evaluation results on a benchmark dataset WebQuestions show that our method outperforms existing state-of-the-art models. Details of our experimental setup and results are presented in sec:experiments. Our code, data and results can be downloaded from https://github.com/syxu828/QuestionAnsweringOverFB.Our Method	fig:qaframework gives an overview of our method for the question “who did shaq first play for”. We have two main steps: (1) inference on Freebase (KB-QA box); and (2) further inference on Wikipedia (Answer Refinement box). Let us take a close look into step 1. Here we perform entity linking to identify a topic entity in the question and its possible Freebase entities. We employ a relation extractor to predict the potential Freebase relations that could exist between the entities in the question and the answer entities. Later we perform a joint inference step over the entity linking and relation extraction results to find the best entity-relation configuration which will produce a list of candidate answer entities. In the step 2, we refine these candidate answers by applying an answer refinement model which takes the Wikipedia page of the topic entity into consideration to filter out the wrong answers and pick the correct ones.While the overview in fig:qaframework works for questions containing single Freebase relation, it also works for questions involving multiple Freebase relations. Consider the question who plays anakin skywalker in star wars 1. The actors who are the answers to this question should satisfy the following constraints: (1) the actor played anakin skywalker; and (2) the actor played in star wars 1. Inspired by msra14, we design a dependency tree-based method to handle such multi-relational questions. We first decompose the original question into a set of sub-questions using syntactic patterns which are listed in Appendix. The final answer set of the original question is obtained by intersecting the answer sets of all its sub-questions. For the example question, the sub-questions are who plays anakin skywalker and who plays in star wars 1. These sub-questions are answered separately over Freebase and Wikipedia, and the intersection of their answers to these sub-questions is treated as the final answer.Inference on Freebase	Given a sub-question, we assume the question word that represents the answer has a distinct KB relation $r$ with an entity $e$ found in the question, and predict a single KB triple $(e,r,?)$ for each sub-question (here $?$ stands for the answer entities). The QA problem is thus formulated as an information extraction problem that involves two sub-tasks, i.e., entity linking and relation extraction. We first introduce these two components, and then present a joint inference procedure which further boosts the overall performance.Entity Linking	For each question, we use hand-built sequences of part-of-speech categories to identify all possible named entity mention spans, e.g., the sequence NN (shaq) may indicate an entity. For each mention span, we use the entity linking tool S-MART BIBREF15 to retrieve the top 5 entities from Freebase. These entities are treated as candidate entities that will eventually be disambiguated in the joint inference step. For a given mention span, S-MART first retrieves all possible entities of Freebase by surface matching, and then ranks them using a statistical model, which is trained on the frequency counts with which the surface form occurs with the entity.Relation Extraction	We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering BIBREF16 , BIBREF12 , and the success of syntactic dependencies for relation extraction BIBREF17 , BIBREF18 , we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction.In MCCNN, we use two channels, one for syntactic information and the other for sentential information. The network structure is illustrated in Figure 2 . Convolution layer tackles an input of varying length returning a fixed length vector (we use max pooling) for each channel. These fixed length vectors are concatenated and then fed into a softmax classifier, the output dimension of which is equal to the number of predefined relation types. The value of each dimension indicates the confidence score of the corresponding relation.We use the shortest path between an entity mention and the question word in the dependency tree as input to the first channel. Similar to xu-EtAl:2015:EMNLP1, we treat the path as a concatenation of vectors of words, dependency edge directions and dependency labels, and feed it to the convolution layer. Note that, the entity mention and the question word are excluded from the dependency path so as to learn a more general relation representation in syntactic level. As shown in Figure 2 , the dependency path between who and shaq is $\leftarrow $ dobj – play – nsubj $\rightarrow $ .This channel takes the words in the sentence as input excluding the question word and the entity mention. As illustrated in Figure 2 , the vectors for did, first, play and for are fed into this channel.The model is learned using pairs of question and its corresponding gold relation from the training data. Given an input question $x$ with an annotated entity mention, the network outputs a vector $o(x)$ , where the entry $o_{k}(x)$ is the probability that there exists the k-th relation between the entity and the expected answer. We denote $t(x) \in \mathbb {R}^{K\times 1}$ as the target distribution vector, in which the value for the gold relation is set to 1, and others to 0. We compute the cross entropy error between $t(x)$ and $o(x)$ , and further define the objective function over the training data as: $
J(\theta ) = - \sum _{x} \sum _{k=1}^{K} t_k(x) \log o_k(x) + \lambda ||\theta ||^{2}_{2}
$ where $\theta $ represents the weights, and $\lambda $ the $L2$ regularization parameters. The weights $\theta $ can be efficiently computed via back-propagation through network structures. To minimize $J(\theta )$ , we apply stochastic gradient descent (SGD) with AdaGrad BIBREF20 .Joint Entity Linking & Relation Extraction	A pipeline of entity linking and relation extraction may suffer from error propagations. As we know, entities and relations have strong selectional preferences that certain entities do not appear with certain relations and vice versa. Locally optimized models could not exploit these implicit bi-directional preferences. Therefore, we use a joint model to find a globally optimal entity-relation assignment from local predictions. The key idea behind is to leverage various clues from the two local models and the KB to rank a correct entity-relation assignment higher than other combinations. We describe the learning procedure and the features below.Suppose the pair $(e_{gold}, r_{gold})$ represents the gold entity/relation pair for a question $q$ . We take all our entity and relation predictions for $q$ , create a list of entity and relation pairs $\lbrace (e_{0}, r_{0}), (e_{1}, r_{1}), ..., (e_{n}, r_{n})\rbrace $ from $q$ and rank them using an svm rank classifier BIBREF21 which is trained to predict a rank for each pair. Ideally higher rank indicates the prediction is closer to the gold prediction. For training, svm rank classifier requires a ranked or scored list of entity-relation pairs as input. We create the training data containing ranked input pairs as follows: if both $e_{pred} = e_{gold}$ and $r_{pred} = r_{gold}$ , we assign it with a score of 3. If only the entity or relation equals to the gold one (i.e., $e_{pred}=e_{gold}$ , $r_{pred}\ne r_{gold}$ or $e_{pred}\ne e_{gold}$ , $q$0 ), we assign a score of 2 (encouraging partial overlap). When both entity and relation assignments are wrong, we assign a score of 1.For a given entity-relation pair, we extract the following features which are passed as an input vector to the svm ranker above:We use the score of the predicted entity returned by the entity linking system as a feature. The number of word overlaps between the entity mention and entity's Freebase name is also included as a feature. In Freebase, most entities have a relation fb:description which describes the entity. For instance, in the running example, shaq is linked to three potential entities m.06_ttvh (Shaq Vs. Television Show), m.05n7bp (Shaq Fu Video Game) and m.012xdf (Shaquille O'Neal). Interestingly, the word play only appears in the description of Shaquille O'Neal and it occurs three times. We count the content word overlap between the given question and the entity's description, and include it as a feature.The score of relation returned by the MCCNNs is used as a feature. Furthermore, we view each relation as a document which consists of the training questions that this relation is expressed in. For a given question, we use the sum of the tf-idf scores of its words with respect to the relation as a feature. A Freebase relation $r$ is a concatenation of a series of fragments $r~=~r_1.r_2.r_3$ . For instance, the three fragments of people.person.parents are people, person and parents. The first two fragments indicate the Freebase type of the subject of this relation, and the third fragment indicates the object type, in our case the answer type. We use an indicator feature to denote if the surface form of the third fragment (here parents) appears in the question.The above two feature classes indicate local features. From the entity-relation $(e,r)$ pair, we create the query triple $(e,r,?)$ to retrieve the answers, and further extract features from the answers. These features are non-local since we require both $e$ and $r$ to retrieve the answer. One such feature is using the co-occurrence of the answer type and the question word based on the intuition that question words often indicate the answer type, e.g., the question word when usually indicates the answer type type.datetime. Another feature is the number of answer entities retrieved.Inference on Wikipedia	We use the best ranked entity-relation pair from the above step to retrieve candidate answers from Freebase. In this step, we validate these answers using Wikipedia as our unstructured knowledge resource where most statements in it are verified for factuality by multiple people.Our refinement model is inspired by the intuition of how people refine their answers. If you ask someone: who did shaq first play for, and give them four candidate answers (Los Angeles Lakers, Boston Celtics, Orlando Magic and Miami Heat), as well as access to Wikipedia, that person might first determine that the question is about Shaquille O'Neal, then go to O'Neal's Wikipedia page, and search for the sentences that contain the candidate answers as evidence. By analyzing these sentences, one can figure out whether a candidate answer is correct or not.Finding Evidence from Wikipedia	As mentioned above, we should first find the Wikipedia page corresponding to the topic entity in the given question. We use Freebase API to convert Freebase entity to Wikipedia page. We extract the content from the Wikipedia page and process it with Wikifier BIBREF22 which recognizes Wikipedia entities, which can further be linked to Freebase entities using Freebase API. Additionally we use Stanford CoreNLP BIBREF19 for tokenization and entity co-reference resolution. We search for the sentences containing the candidate answer entities retrieved from Freebase. For example, the Wikipedia page of O'Neal contains a sentence “O'Neal was drafted by the Orlando Magic with the first overall pick in the 1992 NBA draft”, which is taken into account by the refinement model (our inference model on Wikipedia) to discriminate whether Orlando Magic is the answer for the given question.Refinement Model	We treat the refinement process as a binary classification task over the candidate answers, i.e., correct (positive) and incorrect (negative) answer. We prepare the training data for the refinement model as follows. On the training dataset, we first infer on Freebase to retrieve the candidate answers. Then we use the annotated gold answers of these questions and Wikipedia to create the training data. Specifically, we treat the sentences that contain correct/incorrect answers as positive/negative examples for the refinement model. We use libsvm BIBREF23 to learn the weights for classification.Note that, in the Wikipedia page of the topic entity, we may collect more than one sentence that contain a candidate answer. However, not all sentences are relevant, therefore we consider the candidate answer as correct if at least there is one positive evidence. On the other hand, sometimes, we may not find any evidence for the candidate answer. In these cases, we fall back to the results of the KB-based approach.Lexical Features	Regarding the features used in libsvm, we use the following lexical features extracted from the question and a Wikipedia sentence. Formally, given a question $q$ = $<$ $q_1$ , ... $q_{n}$ $>$ and an evidence sentence $s$ = $<$ $s_1$ , ... $s_{m}$ $>$ , we denote the tokens of $<$0 and $<$1 by $<$2 and $<$3 , respectively. For each pair ( $<$4 , $<$5 ), we identify a set of all possible token pairs ( $<$6 , $<$7 ), the occurrences of which are used as features. As learning proceeds, we hope to learn a higher weight for a feature like (first, drafted) and a lower weight for (first, played).Experiments	In this section we introduce the experimental setup, the main results and detailed analysis of our system.Training and Evaluation Data	We use the WebQuestions BIBREF3 dataset, which contains 5,810 questions crawled via Google Suggest service, with answers annotated on Amazon Mechanical Turk. The questions are split into training and test sets, which contain 3,778 questions (65%) and 2,032 questions (35%), respectively. We further split the training questions into 80%/20% for development.To train the MCCNNs and the joint inference model, we need the gold standard relations of the questions. Since this dataset contains only question-answer pairs and annotated topic entities, instead of relying on gold relations we rely on surrogate gold relations which produce answers that have the highest overlap with gold answers. Specifically, for a given question, we first locate the topic entity $e$ in the Freebase graph, then select 1-hop and 2-hop relations connected to the topic entity as relation candidates. The 2-hop relations refer to the $n$ -ary relations of Freebase, i.e., first hop from the subject to a mediator node, and the second from the mediator to the object node. For each relation candidate $r$ , we issue the query ( $e$ , $r$ , $?$ ) to the KB, and label the relation that produces the answer with minimal $F_1$ -loss against the gold answer, as the surrogate gold relation. From the training set, we collect 461 relations to train the MCCNN, and the target prediction during testing time is over these relations.Experimental Settings	We have 6 dependency tree patterns based on msra14 to decompose the question into sub-questions (See Appendix). We initialize the word embeddings with DBLP:conf/acl/TurianRB10's word representations with dimensions set to 50. The hyper parameters in our model are tuned using the development set. The window size of MCCNN is set to 3. The sizes of the hidden layer 1 and the hidden layer 2 of the two MCCNN channels are set to 200 and 100, respectively. We use the Freebase version of DBLP:conf/emnlp/BerantCFL13, containing 4M entities and 5,323 relations.Results and Discussion	We use the average question-wise $F_1$ as our evaluation metric. To give an idea of the impact of different configurations of our method, we compare the following with existing methods.This method involves inference on Freebase only. First the entity linking (EL) system is run to predict the topic entity. Then we run the relation extraction (RE) system and select the best relation that can occur with the topic entity. We choose this entity-relation pair to predict the answer.In this method instead of the above pipeline, we perform joint EL and RE as described in sec:jointInference.We use the pipelined EL and RE along with inference on Wikipedia as described in sec:refine.This is our main model. We perform inference on Freebase using joint EL and RE, and then inference on Wikipedia to validate the results. Specifically, we treat the top two predictions of the joint inference model as the candidate subject and relation pairs, and extract the corresponding answers from each pair, take the union, and filter the answer set using Wikipedia.Table 1 summarizes the results on the test data along with the results from the literature. We can see that joint EL and RE performs better than the default pipelined approach, and outperforms most semantic parsing based models, except BIBREF24 which searches partial logical forms in strategic order by combining imitation learning and agenda-based parsing. In addition, inference on unstructured data helps the default model. The joint EL and RE combined with inference on unstructured data further improves the default pipelined model by 9.2% (from 44.1% to 53.3%), and achieves a new state-of-the-art result beating the previous reported best result of yih-EtAl:2015:ACL-IJCNLP (with one-tailed t-test significance of $p < 0.05$ ).From Table 1 , we can see that the joint EL & RE gives a performance boost of 3% (from 44.1 to 47.1). We also analyze the impact of joint inference on the individual components of EL & RE.We first evaluate the EL component using the gold entity annotations on the development set. As shown in Table 2 , for 79.8% questions, our entity linker can correctly find the gold standard topic entities. The joint inference improves this result to 83.2%, a 3.4% improvement. Next we use the surrogate gold relations to evaluate the performance of the RE component on the development set. As shown in Table 2 , the relation prediction accuracy increases by 9.4% (from 45.9% to 55.3%) when using the joint inference.Table 3 presents the results on the impact of individual and joint channels on the end QA performance. When using a single-channel network, we tune the parameters of only one channel while switching off the other channel. As seen, the sentential features are found to be more important than syntactic features. We attribute this to the short and noisy nature of WebQuestions questions due to which syntactic parser wrongly parses or the shortest dependency path does not contain sufficient information to predict a relation. By using both the channels, we see further improvements than using any one of the channels. As shown in Table 1 , when structured inference is augmented with the unstructured inference, we see an improvement of 2.9% (from 44.1% to 47.0%). And when Structured + Joint uses unstructured inference, the performance boosts by 6.2% (from 47.1% to 53.3%) achieving a new state-of-the-art result. For the latter, we manually analyzed the cases in which unstructured inference helps. Table 4 lists some of these questions and the corresponding answers before and after the unstructured inference. We observed the unstructured inference mainly helps for two classes of questions: (1) questions involving aggregation operations (Questions 1-3); (2) questions involving sub-lexical compositionally (Questions 4-5). Questions 1 and 2 contain the predicate $largest$ an aggregation operator. A semantic parsing method should explicitly handle this predicate to trigger $max(.)$ operator. For Question 3, structured inference predicts the Freebase relation fb:teams..from retrieving all the years in which Ray Allen has played basketball. Note that Ray Allen has joined Connecticut University's team in 1993 and NBA from 1996. To answer this question a semantic parsing system would require a min( $\cdot $ ) operator along with an additional constraint that the year corresponds to the NBA's term. Interestingly, without having to explicitly model these complex predicates, the unstructured inference helps in answering these questions more accurately. Questions 4-5 involve sub-lexical compositionally BIBREF25 predicates father and college. For example in Question 5, the user queries for the colleges that John Steinbeck attended. However, Freebase defines the relation fb:education..institution to describe a person's educational information without discriminating the specific periods such as high school or college. Inference using unstructured data helps in alleviating these representational issues. We analyze the errors of Structured + Joint + Unstructured model. Around 15% of the errors are caused by incorrect entity linking, and around 50% of the errors are due to incorrect relation predictions. The errors in relation extraction are due to (i) insufficient context, e.g., in what is duncan bannatyne, neither the dependency path nor sentential context provides enough evidence for the MCCNN model; (ii) unbalanced distribution of relations (3022 training examples for 461 relations) heavily influences the performance of MCCNN model towards frequently seen relations. The remaining errors are the failure of unstructured inference due to insufficient evidence in Wikipedia or misclassification.In the entity linking component, we had handcrafted POS tag patterns to identify entity mentions, e.g., DT-JJ-NN (noun phrase), NN-IN-NN (prepositional phrase). These patterns are designed to have high recall. Around 80% of entity linking errors are due to incorrect entity prediction even when the correct mention span was found.Around 136 questions (15%) of dev data contains compositional questions, leading to 292 sub-questions (around 2.1 subquestions for a compositional question). Since our question decomposition component is based on manual rules, one question of interest is how these rules perform on other datasets. By human evaluation, we found these rules achieves 95% on a more general but complex QA dataset QALD-5.While our unstructured inference alleviates representational issues to some extent, we still fail at modeling compositional questions such as who is the mother of the father of prince william involving multi-hop relations and the inter alia. Our current assumption that unstructured data could provide evidence for questions may work only for frequently typed queries or for popular domains like movies, politics and geography. We note these limitations and hope our result will foster further research in this area.Related Work	Over time, the QA task has evolved into two main streams – QA on unstructured data, and QA on structured data. TREC QA evaluations BIBREF26 were a major boost to unstructured QA leading to richer datasets and sophisticated methods BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 . While initial progress on structured QA started with small toy domains like GeoQuery BIBREF34 , recent focus has shifted to large scale structured KBs like Freebase, DBPedia BIBREF35 , BIBREF36 , BIBREF3 , BIBREF4 , BIBREF37 , and on noisy KBs BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 . An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly BIBREF43 , BIBREF44 , BIBREF45 . QALD tasks and linked data initiatives are contributing to this trend.Our model combines the best of both worlds by inferring over structured and unstructured data. Though earlier methods exploited unstructured data for KB-QA BIBREF40 , BIBREF3 , BIBREF7 , BIBREF6 , BIBREF16 , these methods do not rely on unstructured data at test time. Our work is closely related to joshi:2014 who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is limited in answering single relation queries. Our work also has similarities to sun2015open who does question answering on unstructured data but enrich it with Freebase, a reversal of our pipeline. Other line of very recent related work include Yahya:2016:RQE:2835776.2835795 and savenkovknowledge.Our work also intersects with relation extraction methods. While these methods aim to predict a relation between two entities in order to populate KBs BIBREF46 , BIBREF47 , BIBREF48 , we work with sentence level relation extraction for question answering. krishnamurthy2012weakly and fader2014open adopt open relation extraction methods for QA but they require hand-coded grammar for parsing queries. Closest to our extraction method is yao-jacana-freebase-acl2014 and yao-scratch-qa-naacl2015 who also uses sentence level relation extraction for QA. Unlike them, we can predict multiple relations per question, and our MCCNN architecture is more robust to unseen contexts compared to their logistic regression models.dong-EtAl:2015:ACL-IJCNLP1 were the first to use MCCNN for question answering. Yet our approach is very different in spirit to theirs. Dong et al. aim to maximize the similarity between the distributed representation of a question and its answer entities, whereas our network aims to predict Freebase relations. Our search space is several times smaller than theirs since we do not require potential answer entities beforehand (the number of relations is much smaller than the number of entities in Freebase). In addition, our method can explicitly handle compositional questions involving multiple relations, whereas Dong et al. learn latent representation of relation joins which is difficult to comprehend. Moreover, we outperform their method by 7 points even without unstructured inference.Conclusion and Future Work	We have presented a method that could infer both on structured and unstructured data to answer natural language questions. Our experiments reveal that unstructured inference helps in mitigating representational issues in structured inference. We have also introduced a relation extraction method using MCCNN which is capable of exploiting syntax in addition to sentential features. Our main model which uses joint entity linking and relation extraction along with unstructured inference achieves the state-of-the-art results on WebQuestions dataset. A potential application of our method is to improve KB-question answering using the documents retrieved by a search engine.Since we pipeline structured inference first and then unstructured inference, our method is limited by the coverage of Freebase. Our future work involves exploring other alternatives such as treating structured and unstructured data as two independent resources in order to overcome the knowledge gaps in either of the two resources.Acknowledgments	We would like to thank Weiwei Sun, Liwei Chen, and the anonymous reviewers for their helpful feedback. This work is supported by National High Technology R&D Program of China (Grant No. 2015AA015403, 2014AA015102), Natural Science Foundation of China (Grant No. 61202233, 61272344, 61370055) and the joint project with IBM Research. For any correspondence, please contact Yansong Feng.Appendix	The syntax-based patterns for question decomposition are shown in fig:patterns. The first four patterns are designed to extract sub-questions from simple questions, while the latter two are designed for complex questions involving clauses.","['Are experiments conducted on multiple datasets?', 'What baselines is the neural relation extractor compared to?', 'What additional evidence they use?', 'How much improvement they get from the previous state-of-the-art?', 'What is the previous state-of-the-art?']","['Unacceptable', 'Unacceptable', 'Wikipedia sentences that validate or support KB facts', '0.8 point improvement', 'F1 score of 39.9 for semantic-based parsing methods. For information extraction methods, 49.4 using relation extraction, 40.8 using distributed representations, and 52.5 using neural networks models']"
27,"DuTongChuan: Context-aware Translation Model for Simultaneous Interpreting	In this paper, we present DuTongChuan, a novel context-aware translation model for simultaneous interpreting. This model allows to constantly read streaming text from the Automatic Speech Recognition (ASR) model and simultaneously determine the boundaries of Information Units (IUs) one after another. The detected IU is then translated into a fluent translation with two simple yet effective decoding strategies: partial decoding and context-aware decoding. In practice, by controlling the granularity of IUs and the size of the context, we can get a good trade-off between latency and translation quality easily. Elaborate evaluation from human translators reveals that our system achieves promising translation quality (85.71% for Chinese-English, and 86.36% for English-Chinese), specially in the sense of surprisingly good discourse coherence. According to an End-to-End (speech-to-speech simultaneous interpreting) evaluation, this model presents impressive performance in reducing latency (to less than 3 seconds at most times). Furthermore, we successfully deploy this model in a variety of Baidu's products which have hundreds of millions of users, and we release it as a service in our AI platform.	Introduction	Recent progress in Automatic Speech Recognition (ASR) and Neural Machine Translation (NMT), has facilitated the research on automatic speech translation with applications to live and streaming scenarios such as Simultaneous Interpreting (SI). In contrast to non-real time speech translation, simultaneous interpreting involves starting translating source speech, before the speaker finishes speaking (translating the on-going speech while listening to it). Because of this distinguishing feature, simultaneous interpreting is widely used by multilateral organizations (UN/EU), international summits (APEC/G-20), legal proceedings, and press conferences. Despite of recent advance BIBREF0 , BIBREF1 , the research on simultaneous interpreting is notoriously difficult BIBREF0 due to well known challenging requirements: high-quality translation and low latency.Many studies present methods to improve the translation quality by enhancing the robustness of translation model against ASR errors BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 . On the other hand, to reduce latency, some researchers propose models that start translating after reading a few source tokens BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF1 . As one representative work related to this topic, recently, we present a translation model using prefix-to-prefix framework with INLINEFORM0 policy BIBREF0 . This model is simple yet effective in practice, achieving impressive performance both on translation quality and latency.However, existing work pays less attention to the fluency of translation, which is extremely important in the context of simultaneous translation. For example, we have a sub-sentence NMT model that starts to translate after reading a sub-sentence rather than waiting until the end of a sentence like the full-sentence models does. This will definitely reduce the time waiting for the source language speech. However, as shown in the Figure FIGREF2 , the translation for each sub-sentence is barely adequate, whereas the translation of the entire source sentence lacks coherence and fluency. Moreover, it is clear that the model produces an inappropriate translation “your own” for the source token “自己” due to the absence of the preceding sub-sentence.To make the simultaneous machine translation more accessible and producible, we borrow SI strategies used by human interpreters to create our model. As shown in Figure FIGREF3 , this model is able to constantly read streaming text from the ASR model, and simultaneously determine the boundaries of Information Units (IUs) one after another. Each detected IU is then translated into a fluent translation with two simple yet effective decoding strategies: partial decoding and context-aware decoding. Specifically, IUs at the beginning of each sentence are sent to the partial decoding module. Other information units, either appearing in the middle or at the end of a sentence, are translated into target language by the context-aware decoding module. Notice that this module is able to exploit additional context from the history so that the model can generate coherent translation. This method is derived from the “salami technique” BIBREF13 , BIBREF14 , or “chunking”, one of the most commonly used strategies by human interpreters to cope with the linearity constraint in simultaneous interpreting. Having severely limited access to source speech structure in SI, interpreters tend to slice up the incoming speech into smaller meaningful pieces that can be directly rendered or locally reformulated without having to wait for the entire sentence to unfold.In general, there are several remarkable novel advantages that differ our model from the previous work:For a comprehensive evaluation of our system, we use two evaluation metrics: translation quality and latency. According to the automatic evaluation metric, our system presents excellent performance both in translation quality and latency. In the speech-to-speech scenario, our model achieves an acceptability of 85.71% for Chinese-English translation, and 86.36% for English-Chinese translation in human evaluation. Moreover, the output speech lags behind the source speech by an average of less than 3 seconds, which presents surprisingly good experience for machine translation users BIBREF15 , BIBREF16 , BIBREF17 . We also ask three interpreters with SI experience to simultaneously interpret the test speech in a mock conference setting. However, the target texts transcribed from human SI obtain worse BLEU scores as the reference in the test set are actually from written translating rather than simultaneous interpreting. More importantly, when evaluated by human translators, the performance of NMT model is comparable to the professional human interpreter.The contributions of this paper can be concluded into the following aspects:Context-aware Translation Model	As shown in Figure FIGREF7 , our model consists of two key modules: an information unit boundary detector and a tailored NMT model. In the process of translation, the IU detector will determine the boundary for each IU while constantly reading the steaming input from the ASR model. Then, different decoding strategies are applied to translate IUs at the different positions.In this section, we use “IU” to denote one sub-sentence for better description. But in effect, our translation model is a general solution for simultaneous interpreting, and is compatible to IUs at arbitrary granularity, i.e., clause-level, phrase-level, and word-level, etc.For example, by treating a full-sentence as an IU, the model is reduced to the standard translation model. When the IU is one segment, it is reduced to the segment-to-segment translation model BIBREF18 , BIBREF12 . Moreover, if we treat one token as an IU, it is reduced to our previous wait-k model BIBREF0 . The key point of our model is to train the IU detector to recognize the IU boundary at the corresponding granularity.In the remain of this section, we will introduce above two components in details.Dynamic Context Based Information Unit Boundary Detector	Recent success on pre-training indicates that a pre-trained language representation is beneficial to downstream natural language processing tasks including classification and sequence labeling problems BIBREF19 , BIBREF20 , BIBREF21 . We thus formulate the IU boundary detection as a classification problem, and fine-tune the pre-trained model on a small size training corpus. Fine-tuned in several iterations, the model learns to recognize the boundaries of information units correctly.As shown in Figure FIGREF13 , the model tries to predict the potential class for the current position. Once the position is assigned to a definitely positive class, its preceding sequence is labeled as one information unit. One distinguishing feature of this model is that we allow it to wait for more context so that it can make a reliable prediction. We call this model a dynamic context based information unit boundary detector.Definition 1 Assuming the model has already read a sequence INLINEFORM0 with INLINEFORM1 tokens, we denote INLINEFORM2 as the anchor, and the subsequence INLINEFORM3 with INLINEFORM4 tokens as dynamic context.For example, in Figure FIGREF13 , the anchor in both cases is “姬”, and the dynamic context in the left side case is “这”, and in the right side case is “这个”.Definition 2 If the normalized probability INLINEFORM0 for the prediction of the current anchor INLINEFORM1 is larger than a threshold INLINEFORM2 , then the sequence INLINEFORM3 is a complete sequence, and if INLINEFORM4 is smaller than a threshold INLINEFORM5 ( INLINEFORM6 ), it is an incomplete sequence, otherwise it is an undetermined sequence.For a complete sequence INLINEFORM0 , we will send it to the corresponding translation model . Afterwards, the detector will continue to recognize boundaries in the rest of the sequence ( INLINEFORM1 ). For an incomplete sequence, we will take the INLINEFORM2 as the new anchor for further detection. For an undetermined sequence, which is as shown in Figure FIGREF13 , the model will wait for a new token INLINEFORM3 , and take ( INLINEFORM4 ) as dynamic context for further prediction.In the training stage, for one common sentence including two sub-sequences, INLINEFORM0 and INLINEFORM1 . We collect INLINEFORM2 plus any token in INLINEFORM3 as positive training samples, and the other sub-sequences in INLINEFORM4 as negative training samples. We refer readers to Appendix for more details.In the decoding stage, we begin with setting the size of the dynamic context to 0, and then determine whether to read more context according to the principle defined in definition SECREF15 .Partial Decoding	Traditional NMT models are usually trained on bilingual corpora containing only complete sentences. However in our context-aware translation model, information units usually are sub-sentences. Intuitively, the discrepancy between the training and the decoding will lead to a problematic translation, if we use the conventional NMT model to translate such information units. On the other hand, conventional NMT models rarely do anticipation. Whereas in simultaneous interpreting, human interpreters often have to anticipate the up-coming input and render a constituent at the same time or even before it is uttered by the speaker.In our previous work BIBREF0 , training a wait-k policy slightly differs from the traditional method. When predicting the first target token, we mask the source content behind the INLINEFORM0 token, in order to make the model learn to anticipate. The prediction of other tokens can also be obtained by moving the mask-window token-by-token from position INLINEFORM1 to the end of the line. According to our practical experiments, this training strategy do help the model anticipate correctly most of the time.Following our previous work, we propose the partial decoding model, a tailored NMT model for translating the IUs that appear at the beginning of each sentence. As depicted in Figure FIGREF17 , in the training stage, we mask the second sub-sentence both in the source and target side. While translating the first sub-sentence, the model learns to anticipate the content after the comma, and produces a temporary translation that can be further completed with more source context. Clearly, this method relies on the associated sub-sentence pairs in the training data (black text in Figure FIGREF17 ). In this paper, we propose an automatic method to acquire such sub-sentence pairs.Definition 3 Given a source sentence INLINEFORM0 with INLINEFORM1 tokens, a target sentence INLINEFORM2 with INLINEFORM3 tokens, and a word alignment set INLINEFORM4 where each alignment INLINEFORM5 is a tuple indicating a word alignment existed between the source token INLINEFORM6 and target token INLINEFORM7 , a sub-sentence pair INLINEFORM8 holds if satisfying the following conditions: DISPLAYFORM0 To acquire the word alignment, we run the open source toolkit fast_align , and use a variety of standard symmetrization heuristics to generate the alignment matrix. In the training stage, we perform training by firstly tuning the model on a normal bilingual corpus, and then fine-tune the model on a special training corpus containing sub-sentence pairs.Context-aware Decoding	For IUs that have one preceding sub-sentence, the context-aware decoding model is applied to translate them based on the pre-generated translations. The requirements of this model are obvious:The model is required to exploit more context to continue the translation.The model is required to generate the coherent translation given partial pre-generated translations.Intuitively, the above requirements can be easily satisfied using a force decoding strategy. For example, when translating the second sub-sentence in “这点也是以前让我非常地诧异，也是非常纠结的地方”, given the already-produced translation of the first sub-sentence “It also surprised me very much before .”, the model finishes the translation by adding “It's also a very surprising , tangled place .”. Clearly, translation is not that accurate and fluent with the redundant constituent “surprising”. We ascribe this to the discrepancy between training and decoding. In the training stage, the model learns to predict the translation based on the full source sentence. In the decoding stage, the source contexts for translating the first-subsentence and the second-subsentence are different. Forcing the model to generate identical translation of the first sub-sentence is very likely to cause under-translation or over-translation.To produce more adequate and coherent translation, we make the following refinements:During training, we force the model to focus on learning how to continue the translation without over-translation and under-translation.During decoding, we discard a few previously generated translations, in order to make more fluent translations.As shown in Figure FIGREF19 , during training, we do not mask the source input, instead we mask the target sequence aligned to the first sub-sentence. This strategy will force the model to learn to complete the half-way done translation, rather than to concentrate on generating a translation of the full sentence.Moreover, in the decoding stage, as shown in Figure FIGREF28 , we propose to discard the last INLINEFORM0 tokens from the generated partial translation (at most times, discarding the last token brings promising result). Then the context-aware decoding model will complete the rest of the translation. The motivation is that the translation of the tail of a sub-sentence is largely influenced by the content of the succeeding sub-sentence. By discarding a few tokens from previously generated translation, the model is able to generate a more appropriate translation. In the practical experiment, this slight modification is proved to be effective in generating fluent translation.Latency Metric: Equilibrium Efficiency	In the work of DBLP:journals/corr/abs-1810-08398 and arivazhagan2019monotonic, they used the average lagging as the metric for evaluating the latency. However, there are two major flaws of this metric:1) This metric is unsuitable for evaluating the sub-sentence model. Take the sentence in Figure FIGREF3 for example. As the model reads four tokens “她说 我 错了 那个”, and generates six target tokens “She said I was wrong ,”, the lag of the last target token is one negative value ( INLINEFORM0 ) according to its original definition.2) This metric is unsuitable for evaluating latency in the scenario of speech-to-speech translation. DBLP:journals/corr/abs-1810-08398 considered that the target token generated after the cut-off point doesn't cause any lag. However, this assumption is only supported in the speech-to-text scenario. In the speech-to-speech scenario, it is necessary to consider the time for playing the last synthesized speech.Therefore, we instead propose a novel metric, Equilibrium Efficiency (EE), which measures the efficiency of equilibrium strategy.Definition 4 Consider a sentence with INLINEFORM0 subsequences, and let INLINEFORM1 be the length of INLINEFORM2 source subsequence that emits a target subsequence with INLINEFORM3 tokens. Then the equilibrium efficiency is: INLINEFORM4 , where INLINEFORM5 is defined as: DISPLAYFORM0 and INLINEFORM0 , INLINEFORM1 is an empirical factor.In practice, we set INLINEFORM0 to 0.3 for Chinese-English translation (reading about 200 English tokens in one minute). The motivation of EE is that one good model should equilibrate the time for playing the target speech to the time for listening to the speaker. Assuming playing one word takes one second, the EE actually measures the latency from the audience hearing the final target word to the speaker finishing the speech. For example, the EE of the sentence in Figure FIGREF7 is equal to INLINEFORM1 , since the time for playing the sequence “She said I was wrong” is equilibrated to the time for speaker speaking the second sub-sentence “那个 叫 什么 什么 呃 妖姬”.Evaluation	We conduct multiple experiments to evaluate the effectiveness of our system in many ways.Data Description	We use a subset of the data available for NIST OpenMT08 task . The parallel training corpus contains approximate 2 million sentence pairs. We choose NIST 2006 (NIST06) dataset as our development set, and the NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets as our test sets. We will use this dataset to evaluate the performance of our partial decoding and context-aware decoding strategy from the perspective of translation quality and latency.Recently, we release Baidu Speech Translation Corpus (BSTC) for open research . This dataset covers speeches in a wide range of domains, including IT, economy, culture, biology, arts, etc. We transcribe the talks carefully, and have professional translators to produce the English translations. This procedure is extremely difficult due to the large number of domain-specific terminologies, speech redundancies and speakers' accents. We expect that this dataset will help the researchers to develop robust NMT models on the speech translation. In summary, there are many features that distinguish this dataset to the previously related resources:Speech irregularities are kept in transcription while omitted in translation (eg. filler words like “嗯, 呃, 啊”, and unconscious repetitions like “这个这个呢”), which can be used to evaluate the robustness of the NMT model dealing with spoken language.Each talk's transcription is translated into English by a single translator, and then segmented into bilingual sentence pairs according to the sentence boundaries in the English translations. Therefore, every sentence is translated based on the understanding of the entire talk and is translated faithfully and coherently in global sense.We use the streaming multi-layer truncated attention model (SMLTA) trained on the large-scale speech corpus (more than 10,000 hours) and fine-tuned on a number of talk related corpora (more than 1,000 hours), to generate the 5-best automatic recognized text for each acoustic speech.The test dataset includes interpretations produced by simultaneous interpreters with professional experience. This dataset contributes an essential resource for the comparison between translation and interpretation.We randomly extract several talks from the dataset, and divide them into the development and test set. In Table TABREF34 , we summarize the statistics of our dataset. The average number of utterances per talk is 152.6 in the training set, 59.75 in the dev set, and 162.5 in the test set.We firstly run the standard Transformer model on the NIST dataset. Then we evaluate the quality of the pre-trained model on our proposed speech translation dataset, and propose effective methods to improve the performance of the baseline. In that the testing data in this dataset contains ASR errors and speech irregularities, it can be used to evaluate the robustness of novel methods.In the final deployment, we train our model using a corpus containing approximately 200 million bilingual pairs both in Chinese-English and English-Chinese translation tasks.Data Preprocess	To preprocess the Chinese and the English texts, we use an open source Chinese Segmenter and Moses Tokenizer . After tokenization, we convert all English letters into lower case. And we use the “multi-bleu.pl” script to calculate BLEU scores. Except in the large-scale experiments, we conduct byte-pair encoding (BPE) BIBREF22 for both Chinese and English by setting the vocabulary size to 20K and 18K for Chinese and English, respectively. But in the large-scale experiments, we utilize a joint vocabulary for both Chinese-English and English-Chinese translation tasks, with a vocabulary size of 40K.Model Settings	We implement our models using PaddlePaddle , an end-to-end open source deep learning platform developed by Baidu. It provides a complete suite of deep learning libraries, tools and service platforms to make the research and development of deep learning simple and reliable. For training our dynamic context sequence boundary detector, we use ERNIE BIBREF20 as our pre-trained model.For fair comparison, we implement the following models:baseline: A standard Transformer based model with big version of hyper parameters.sub-sentence: We split a full sentence into multiple sub-sentences by comma, and translate them using the baseline model. To evaluate the translation quality, we concatenate the translation of each sub-sentence into one sentence.wait-k: This is our previous work BIBREF0 .context-aware: This is our proposed model using context-aware decoding strategy, without fine-tuning on partial decoding model.partial decoding: This is our proposed model using partial decoding.discard INLINEFORM0 tokens: The previously generated INLINEFORM1 tokens are removed to complete the rest of the translation by the context-aware decoding model.Experiments	We firstly conduct our experiments on the NIST Chinese-English translation task.To validate the effectiveness of our translation model, we run two baseline models, baseline and sub-sentence. We also compare the translation quality as well as latency of our models with the wait-k model.Effectiveness on Translation Quality. As shown in Table TABREF49 , there is a great deal of difference between the sub-sentence and the baseline model. On an average the sub-sentence shows weaker performance by a 3.08 drop in BLEU score (40.39 INLINEFORM0 37.31). Similarly, the wait-k model also brings an obvious decrease in translation quality, even with the best wait-15 policy, its performance is still worse than the baseline system, with a 2.15 drop, averagely, in BLEU (40.39 INLINEFORM1 38.24). For a machine translation product, a large degradation in translation quality will largely affect the use experience even if it has low latency.Unsurprisingly, when treating sub-sentences as IUs, our proposed model significantly improves the translation quality by an average of 2.35 increase in BLEU score (37.31 INLINEFORM0 39.66), and its performance is slightly lower than the baseline system with a 0.73 lower average BLEU score (40.39 INLINEFORM1 39.66). Moreover, as we allow the model to discard a few previously generated tokens, the performance can be further improved to 39.82 ( INLINEFORM2 0.16), at a small cost of longer latency (see Figure FIGREF58 ). It is consistent with our intuition that our novel partial decoding strategy can bring stable improvement on each testing dataset. It achieves an average improvement of 0.44 BLEU score (39.22 INLINEFORM3 39.66) compared to the context-aware system in which we do not fine-tune the trained model when using partial decoding strategy. An interesting finding is that our translation model performs better than the baseline system on the NIST08 testing set. We analyze the translation results and find that the sentences in NIST08 are extremely long, which affect the standard Transformer to learn better representation BIBREF23 . Using context-aware decoding strategy to generate consistent and coherent translation, our model performs better by focusing on generating translation for relatively shorter sub-sentences.Investigation on Decoding Based on Segment. Intuitively, treating one segment as an IU will reduce the latency in waiting for more input to come. Therefore, we split the testing data into segments according to the principle in Definition SECREF20 (if INLINEFORM0 in Definition SECREF20 is a comma, then the data is sub-sentence pair, otherwise it is a segment-pair.) .As Table TABREF49 shows, although the translation quality of discard 1 token based on segment is worse than that based on sub-sentence (37.96 vs. 39.66), the performance can be significantly improved by allowing the model discarding more previously generated tokens. Lastly, the discard 6 tokens obtains an impressive result, with an average improvement of 1.76 BLEU score (37.96 INLINEFORM0 39.72).Effects of Discarding Preceding Generated Tokens. As mentioned and depicted in Figure FIGREF28 , we discard one token in the previously generated translation in our context-aware NMT model. One may be interested in whether discarding more generated translation leads to better translation quality. However, when decoding on the sub-sentence, even the best discard 4 tokens model brings no significant improvement (39.66 INLINEFORM0 39.82) but a slight cost of latency (see in Figure FIGREF58 for visualized latency). While decoding on the segment, even discarding two tokens can bring significant improvement (37.96 INLINEFORM1 39.00). This finding proves that our partial decoding model is able to generate accurate translation by anticipating the future content. It also indicates that the anticipation based on a larger context presents more robust performance than the aggressive anticipation in the wait-k model, as well as in the segment based decoding model.Effectiveness on latency. As latency in simultaneous machine translation is essential and is worth to be intensively investigated, we compare the latency of our models with that of the previous work using our Equilibrium Efficiency metric. As shown in Figure FIGREF58 , we plot the translation quality and INLINEFORM0 on the NIST06 dev set. Clearly, compared to the baseline system, our model significantly reduce the time delay while remains a competitive translation quality. When treating segments as IUs, the latency can be further reduced by approximate 20% (23.13 INLINEFORM1 18.65), with a slight decrease in BLEU score (47.61 INLINEFORM2 47.27). One interesting finding is that the granularity of information units largely affects both the translation quality and latency. It is clear the decoding based on sub-sentence and based on segment present different performance in two metrics. For the former model, the increase of discarded tokens results in an obvious decrease in translation quality, but no definite improvement in latency. The latter model can benefit from the increasing of discarding tokens both in translation quality and latency.The latency of the wait-k models are competitive, their translation quality, however, is still worse than context-aware model. Improving the translation quality for the wait-k will clearly brings a large cost of latency (36.53 INLINEFORM0 46.14 vs. 10.94 INLINEFORM1 22.63). Even with a best k-20 policy, its performance is still worse than most context-aware models. More importantly, the intermediately generated target token in the wait-k policy is unsuitable for TTS due to the fact that the generated token is often a unit in BPE, typically is an incomplete word. One can certainly wait more target tokens to synthesize the target speech, however, this method will reduce to the baseline model. In general, experienced human interpreters lag approximately 5 seconds (15 INLINEFORM2 25 words) behind the speaker BIBREF15 , BIBREF16 , BIBREF17 , which indicates that the latency of our model is accessible and practicable ( INLINEFORM3 = 25 indicates lagging 25 words).In our context-sensitive model, the dynamic context based information unit boundary detector is essential to determine the IU boundaries in the steaming input. To measure the effectiveness of this model, we compare its precision as well as latency against the traditional language model based methods, a 5-gram language model trained by KenLM toolkit , and an in-house implemented RNN based model. Both of two contrastive models are trained on approximate 2 million monolingual Chinese sentences. As shown in Table TABREF60 , it is clear that our model beats the previous work with an absolute improvement of more than 15 points in term of F-score (62.79 INLINEFORM0 78.26) and no obvious burden in latency (average latency). This observation indicates that with bidirectional context, the model can learn better representation to help the downstream tasks. In the next experiments, we will evaluate models given testing data with IU boundaries detected by our detector.To our knowledge, almost all of the previous related work on simultaneous translation evaluate their models upon the clean testing data without ASR errors and with explicit sentence boundaries annotated by human translators. Certainly, testing data with real ASR errors and without explicit sentence boundaries is beneficial to evaluate the robustness of translation models. To this end, we perform experiments on our proposed BSTC dataset.The testing data in BSTC corpus consists of six talks. We firstly employ our ASR model to recognize the acoustic waves into Chinese text, which will be further segmented into small pieces of sub-sentences by our IU detector. To evaluate the contribution of our proposed BSTC dataset, we firstly train all models on the NIST dataset, and then check whether the performance can be further improved by fine-tuning them on the BSTC dataset.From the results shown in Table TABREF64 , we conclude the following observations:Due to the relatively lower CER in ASR errors (10.32 %), the distinction between the clean input and the noisy input results in a BLEU score difference smaller than 2 points (15.85 vs. 14.60 for pre-train, and 21.98 vs. 19.91 for fine-tune).Despite the small size of the training data in BSTC, fine-tuning on this data is essential to improve the performance of all models.In all settings, the best system in context-aware model beats the wait-15 model.Pre-trained models are not sensitive to errors from Auto IU, while fine-tuned models are.Another interesting work is to compare machine translation with human interpretation. We request three simultaneous interpreters (S, A and B) with years of interpreting experience ranging from three to seven years, to interpret the talks in BSTC testing dataset, in a mock conference setting .We concatenate the translation of each talk into one big sentence, and then evaluate it by BLEU score. From Table TABREF69 , we find that machine translation beats the human interpreters significantly. Moreover, the length of interpretations are relatively short, and results in a high length penalty provided by the evaluation script. The result is unsurprising, because human interpreters often deliberately skip non-primary information to keep a reasonable ear-voice span, which may bring a loss of adequacy and yet a shorter lag time, whereas the machine translation model translates the content adequately. We also use human interpreting results as references. As Table TABREF69 indicates, our model achieves a higher BLEU score, 28.08.Furthermore, we ask human translators to evaluate the quality between interpreting and machine translation. To evaluate the performance of our final system, we select one Chinese talk as well as one English talk consisting of about 110 sentences, and have human translators to assess the translation from multiple aspects: adequacy, fluency and correctness. The detailed measurements are:Bad: Typically, the mark Bad indicates that the translation is incorrect and unacceptable.OK: If a translation is comprehensible and adequate, but with minor errors such as incorrect function words and less fluent phrases, then it will be marked as OK.Good: A translation will be marked as Good if it contains no obvious errors.As shown in Table TABREF70 , the performance of our model is comparable to the interpreting. It is worth mentioning that both automatic and human evaluation criteria are designed for evaluating written translation and have a special emphasis on adequacy and faithfulness. But in simultaneous interpreting, human interpreters routinely omit less-important information to overcome their limitations in working memory. As the last column in Table 6 shows, human interpreters' oral translations have more omissions than machine's and receive lower acceptability. The evaluation results do not mean that machines have exceeded human interpreters in simultaneous interpreting. Instead, it means we need machine translation criteria that suit simultaneous interpreting. We also find that the BSTC dataset is extremely difficult as the best human interpreter obtains a lower Acceptability 73.04%. Although the NMT model obtains impressive translation quality, we do not compare the latency of machine translation and human interpreting in this paper, and leave it to the future work.To better understand the contribution of our model on generating coherent translation, we select one representative running example for analysis. As the red text in Figure FIGREF73 demonstrates that machine translation model generates coherent translation “its own grid” for the sub-sentence “这个网络”, and “corresponds actually to” for the subsequence “...对应的,就是每个...”. Compared to the human interpretation, our model presents comparable translation quality. In details, our model treats segments as IUs, and generates translation for each IU consecutively. While the human interpreter splits the entire source text into two sub-sentences, and produces the translation respectively.In the final deployment, we train DuTongChuan on the large-scale training corpus. We also utilize techniques to enhance the robustness of the translation model, such as normalization of the speech irregularities, dealing with abnormal ASR errors, and content censorship, etc (see Appendix). We successfully deploy DuTongChuan in the Baidu Create 2019 (Baidu AI Developer Conference) .As shown in Table TABREF74 , it is clear that DuTongChuan achieves promising acceptability on both translation tasks (85.71% for Chinese-English, and 86.36 % for English-Chinese). We also elaborately analyze the error types in the final translations, and we find that apart from errors occurring in translation and ASR, a majority of errors come from IU boundary detection, which account for nearly a half of errors. In the future, we should concentrate on improving the translation quality by enhancing the robustness of our IU boundary detector. We also evaluate the latency of our model in an End-to-End manner (speech-to-speech), and we find that the target speech slightly lags behind the source speech in less than 3 seconds at most times. The overall performance both on translation quality and latency reveals that DuTongChuan is accessible and practicable in an industrial scenario.Related Work	The existing research on speech translation can be divided into two types: the End-to-End model BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 and the cascaded model. The former approach directly translates the acoustic speech in one language, into text in another language without generating the intermediate transcription for the source language. Depending on the complexity of the translation task as well as the scarce training data, previous literatures explore effective techniques to boost the performance. For example pre-training BIBREF29 , multi-task learning BIBREF24 , BIBREF27 , attention-passing, BIBREF30 , and knowledge distillation BIBREF28 etc.,. However, the cascaded model remains the dominant approach and presents superior performance practically, since the ASR and NMT model can be optimized separately training on the large-scale corpus.Many studies have proposed to synthesize realistic ASR errors, and augment them with translation training data, to enhance the robustness of the NMT model towards ASR errors BIBREF2 , BIBREF3 , BIBREF4 . However, most of these approaches depend on simple heuristic rules and only evaluate on artificially noisy test set, which do not always reflect the real noises distribution on training and inference BIBREF5 , BIBREF6 , BIBREF7 .Beyond the research on translation models, there are many research on the other relevant problems, such as sentence boundary detection for realtime speech translation BIBREF31 , BIBREF18 , BIBREF32 , BIBREF33 , BIBREF34 , low-latency simultaneous interpreting BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF35 , BIBREF36 , automatic punctuation annotation for speech transcription BIBREF37 , BIBREF38 , and discussion about human and machine in simultaneous interpreting BIBREF39 .Focus on the simultaneous translation task, there are some work referring to the construction of the simultaneous interpreting corpus BIBREF40 , BIBREF41 , BIBREF42 . Particularly, BIBREF42 deliver a collection of a simultaneous translation corpus for comparative analysis on Japanese-English and English-Japanese speech translation. This work analyze the difference between the translation and the interpretations, using the interpretations from human simultaneous interpreters.For better generation of coherent translations, gong2011cache propose a memory based approach to capture contextual information to make the statistical translation model generate discourse coherent translations. kuang2017cache,tu2018learning,P18-1118 extend similar memory based approach to the NMT framework. wang2017exploiting present a novel document RNN to learn the representation of the entire text, and treated the external context as the auxiliary context which will be retrieved by the hidden state in the decoder. tiedemann2017neural and P18-1117 propose to encode global context through extending the current sentence with one preceding adjacent sentence. Notably, the former is conducted on the recurrent based models while the latter is implemented on the Transformer model. Recently, we also propose a reinforcement learning strategy to deliberate the translation so that the model can generate more coherent translations BIBREF43 .Conclusion and Future Work	In this paper, we propose DuTongChuan, a novel context-aware translation model for simultaneous interpreting. This model is able to constantly read streaming text from the ASR model, and simultaneously determine the boundaries of information units one after another. The detected IU is then translated into a fluent translation with two simple yet effective decoding strategies: partial decoding and context-aware decoding. We also release a novel speech translation corpus, BSTC, to boost the research on robust speech translation task.With elaborate comparison, our model obtains superior translation quality against the wait-k model, but also presents competitive performance in latency. Assessment from human translators reveals that our system achieves promising translation quality (85.71% for Chinese-English, and 86.36% for English-Chinese), specially in the sense of surprisingly good discourse coherence. Our system also presents superior performance in latency (delayed in less 3 seconds at most times) in a speech-to-speech simultaneous translation. We also deploy our simultaneous machine translation model in our AI platform, and welcome the other users to enjoy it.In the future, we will conduct research on novel method to evaluate the interpreting.Acknowledgement	We thank Ying Chen for improving the written of this paper. We thank Yutao Qu for developing partial modules of DuTongChuan. We thank colleagues in Baidu for their efforts on construction of the BSTC. They are Zhi Li, Ying Chen, Xuesi Song, Na Chen, Qingfei Li, Xin Hua, Can Jin, Lin Su, Lin Gao, Yang Luo, Xing Wan, Qiaoqiao She, Jingxuan Zhao, Can Jin, Wei Jin, Xiao Yang, Shuo Liu, Yang Zhang, Jing Ma, Junjin Zhao, Yan Xie, Minyang Zhang, Niandong Du, etc.We also thank tndao.com and zaojiu.com for contributing their speech corpora.Training Samples for Information Unit Detector	For example, for a sentence “她说我错了，那个叫什么什么呃妖姬。”, there are some representative training samples:Techniques for Robust Translation	To develop an industrial simultaneous machine translation system, it is necessary to deal with problems that affect the translation quality in practice such as large number of speech irregularities, ASR errors, and topics that allude to violence, religion, sex and politics.Speech Irregularities Normalization	In the real talk, the speaker tends to express his opinion using irregularities rather than regular written language utilized to train prevalent machine translation relevant models. For example, as depicted in Figure FIGREF3 , the spoken language in the real talk often contains unconscious repetitions (i.e., “什么(shénmē) 什么(shénmē)), and filler words (“呃”, “啊”), which will inevitably affects the downstream models, especially the NMT model. The discrepancy between training and decoding is not only existed in the corpus, but also occurs due to the error propagation from ASR model (e.g. recognize the “饿 (è)” into filler word “呃 (è) ” erroneously), which is related to the field of robust speech NMT research.In the study of robust speech translation, there are many methods can be applied to alleviate the discrepancy mostly arising from the ASR errors such as disfluency detection, fine-tuning on the noisy training data BIBREF2 , BIBREF3 , complex lattice input BIBREF4 , etc. For spoken language normalization, it is mostly related to the work of sentence simplification. However, the traditional methods for sentence simplification rely large-scale training corpus and will enhance the model complexity by incorporating an End-to-End model to transform the original input.In our system, to resolve problems both on speech irregularities and ASR errors, we propose a simple rule heuristic method to normalize both spoken language and ASR errors, mostly focus on removing noisy inputs, including filler words, unconscious repetitions, and ASR error that is easy to be detected. Although faithfulness and adequacy is essential in the period of the simultaneous interpreting, however, in a conference, users can understand the majority of the content by discarding some unimportant words.To remove unconscious repetitions, the problem can be formulated as the Longest Continuous Substring (LCS) problem, which can be solved by an efficient suffix-array based algorithm in INLINEFORM0 time complexity empirically. Unfortunately, this simple solution is problematic in some cases. For example, “他 必须 分成 很多 个 小格 ， 一个 小格 一个 小格 完成”, in this case, the unconscious repetitions “一个 小格 一个 小格” can not be normalized to “一个 小格”. To resolve this drawback, we collect unconscious repetitions appearing more than 5 times in a large-scale corpus consisting of written expressions, resulting in a white list containing more than 7,000 unconscious repetitions. In practice, we will firstly retrieve this white list and prevent the candidates existed in it from being normalized.According to our previous study, many ASR errors are caused by disambiguating homophone. In some cases, such error will lead to serious problem. For example, both “食油 (cooking oil)” and “石油 (oil)” have similar Chinese phonetic alphabet (shí yóu), but with distinct semantics. The simplest method to resolve this problem is to enhance the ASR model by utilizing a domain-specific language model to generate the correct sequence. However, this method requires an insatiably difficult requirement, a customized ASR model. To reduce the cost of deploying a customized ASR model, as well as to alleviate the propagation of ASR errors, we propose a language model based identifier to remove the abnormal contents.Definition 5 For a given sequence INLINEFORM0 , if the value of INLINEFORM1 is lower than a threshold INLINEFORM2 , then we denote the token INLINEFORM3 as an abnormal content.In the above definition, the value of INLINEFORM0 and INLINEFORM1 can be efficiently computed by a language model. In our final system, we firstly train a language model on the domain-specific monolingual corpus, and then identify the abnormal content before the context-aware translation model. For the detected abnormal content, we simply discard it rather than finding an alternative, which will lead to additional errors potentially. Actually, human interpreters often routinely omit source content due to the limited memory.Constrained Decoding and Content Censorship	For an industrial product, it is extremely important to control the content that will be presented to the audience. Additionally, it is also important to make a consistent translation for the domain-specific entities and terminologies. This two demands lead to two associate problems: content censorship and constrained decoding, where the former aims to avoid producing some translation while the latter has the opposite target, generating pre-specified translation.Recently, post2018fast proposed a Dynamic Beam Allocation (DBA) strategy, a beam search algorithm that forces the inclusion of pre-specified words and phrases in the output. In the DBA strategy, there are many manually annotated constraints, to force the beam search generating the pre-specified translation. To satisfy the requirement of content censorship, we extend this algorithm to prevent the model from generating the pre-specified forbidden content, a collection that contains words and phrases alluding to violence, religion, sex and politics. Specially, during the beam search, we punish the candidate beam that matches a constraint of pre-specified forbidden content, to prevent it from being selected as the final translation.","['Which datasets do they evaluate on?', 'Do they compare against a system that does not use streaming text, but has the entire text at disposal?', 'Does larger granularity lead to better translation quality?']","['Unacceptable', 'Unacceptable', 'It depends on the model used.']"
28,"Towards better decoding and language model integration in sequence to sequence models	The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in an end-to-end fashion. In this contribution, we analyse an attention-based seq2seq speech recognition system that directly transcribes recordings into characters. We observe two shortcomings: overconfidence in its predictions and a tendency to produce incomplete transcriptions when language models are used. We propose practical solutions to both problems achieving competitive speaker independent word error rates on the Wall Street Journal dataset: without separate language models we reach 10.6% WER, while together with a trigram language model, we reach 6.7% WER.	Introduction	Deep learning BIBREF0 has led to many breakthroughs including speech and image recognition BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . A subfamily of deep models, the Sequence-to-Sequence (seq2seq) neural networks have proved to be very successful on complex transduction tasks, such as machine translation BIBREF7 , BIBREF8 , BIBREF9 , speech recognition BIBREF10 , BIBREF11 , BIBREF12 , and lip-reading BIBREF13 . Seq2seq networks can typically be decomposed into modules that implement stages of a data processing pipeline: an encoding module that transforms its inputs into a hidden representation, a decoding (spelling) module which emits target sequences and an attention module that computes a soft alignment between the hidden representation and the targets. Training directly maximizes the probability of observing desired outputs conditioned on the inputs. This discriminative training mode is fundamentally different from the generative ""noisy channel"" formulation used to build classical state-of-the art speech recognition systems. As such, it has benefits and limitations that are different from classical ASR systems.Understanding and preventing limitations specific to seq2seq models is crucial for their successful development. Discriminative training allows seq2seq models to focus on the most informative features. However, it also increases the risk of overfitting to those few distinguishing characteristics. We have observed that seq2seq models often yield very sharp predictions, and only a few hypotheses need to be considered to find the most likely transcription of a given utterance. However, high confidence reduces the diversity of transcripts obtained using beam search.During typical training the models are conditioned on ground truth transcripts and are scored on one-step ahead predictions. By itself, this training criterion does not ensure that all relevant fragments of the input utterance are transcribed. Subsequently, mistakes that are introduced during decoding may cause the model to skip some words and jump to another place in the recording. The problem of incomplete transcripts is especially apparent when external language models are used.Model Description	Our speech recognition system, builds on the recently proposed Listen, Attend and Spell network BIBREF12 . It is an attention-based seq2seq model that is able to directly transcribe an audio recording INLINEFORM0 into a space-delimited sequence of characters INLINEFORM1 . Similarly to other seq2seq neural networks, it uses an encoder-decoder architecture composed of three parts: a listener module tasked with acoustic modeling, a speller module tasked with emitting characters and an attention module serving as the intermediary between the speller and the listener: DISPLAYFORM0 The Listener	The listener is a multilayer Bi-LSTM network that transforms a sequence of INLINEFORM0 frames of acoustic features INLINEFORM1 into a possibly shorter sequence of hidden activations INLINEFORM2 , where INLINEFORM3 is a time reduction constant BIBREF11 , BIBREF12 .The Speller and the Attention Mechanism	The speller computes the probability of a sequence of characters conditioned on the activations of the listener. The probability is computed one character at a time, using the chain rule: DISPLAYFORM0 To emit a character the speller uses the attention mechanism to find a set of relevant activations of the listener INLINEFORM0 and summarize them into a context INLINEFORM1 . The history of previously emitted characters is encapsulated in a recurrent state INLINEFORM2 : DISPLAYFORM0  We implement the recurrent step using a single LSTM layer. The attention mechanism is sensitive to the location of frames selected during the previous step and employs the convolutional filters over the previous attention weights BIBREF10 . The output character distribution is computed using a SoftMax function.Training Criterion	Our speech recognizer computes the probability of a character conditioned on the partially emitted transcript and the whole utterance. It can thus be trained to minimize the cross-entropy between the ground-truth characters and model predictions. The training loss over a single utterance is DISPLAYFORM0 where INLINEFORM0 denotes the target label function. In the baseline model INLINEFORM1 is the indicator INLINEFORM2 , i.e. its value is 1 for the correct character, and 0 otherwise. When label smoothing is used, INLINEFORM3 encodes a distribution over characters.Decoding: Beam Search	Decoding new utterances amounts to finding the character sequence INLINEFORM0 that is most probable under the distribution computed by the network: DISPLAYFORM0 Due to the recurrent formulation of the speller function, the most probable transcript cannot be found exactly using the Viterbi algorithm. Instead, approximate search methods are used. Typically, best results are obtained using beam search. The search begins with the set (beam) of hypotheses containing only the empty transcript. At every step, candidate transcripts are formed by extending hypothesis in the beam by one character. The candidates are then scored using the model, and a certain number of top-scoring candidates forms the new beam. The model indicates that a transcript is considered to be finished by emitting a special EOS (end-of-sequence) token.Language Model Integration	The simplest solution to include a separate language model is to extend the beam search cost with a language modeling term BIBREF11 , BIBREF3 , BIBREF14 : DISPLAYFORM0 where coverage refers to a term that promotes longer transcripts described it in detail in Section SECREF16 .We have identified two challenges in adding the language model. First, due to model overconfidence deviations from the best guess of the network drastically changed the term INLINEFORM0 , which made balancing the terms in eq. ( EQREF11 ) difficult. Second, incomplete transcripts were produced unless a recording coverage term was added.Equation ( EQREF11 ) is a heuristic involving the multiplication of a conditional and unconditional probabilities of the transcript INLINEFORM0 . We have tried to justify it by adding an intrinsic language model suppression term INLINEFORM1 that would transform INLINEFORM2 into INLINEFORM3 . We have estimated the language modeling capability of the speller INLINEFORM4 by replacing the encoded speech with a constant, separately trained, biasing vector. The per character perplexity obtained was about 6.5 and we didn't observe consistent gains from this extension of the beam search criterion.Solutions to Seq2Seq Failure Modes	We have analysed the impact of model confidence by separating its effects on model accuracy and beam search effectiveness. We also propose a practical solution to the partial transcriptions problem, relating to the coverage of the input utterance.Impact of Model Overconfidence	Model confidence is promoted by the the cross-entropy training criterion. For the baseline network the training loss ( EQREF7 ) is minimized when the model concentrates all of its output distribution on the correct ground-truth character. This leads to very peaked probability distributions, effectively preventing the model from indicating sensible alternatives to a given character, such as its homophones. Moreover, overconfidence can harm learning the deeper layers of the network. The derivative of the loss backpropagated through the SoftMax function to the logit corresponding to character INLINEFORM0 equals INLINEFORM1 , which approaches 0 as the network's output becomes concentrated on the correct character. Therefore whenever the spelling RNN makes a good prediction, very little training signal is propagated through the attention mechanism to the listener.Model overconfidence can have two consequences. First, next-step character predictions may have low accuracy due to overfitting. Second, overconfidence may impact the ability of beam search to find good solutions and to recover from errors.We first investigate the impact of confidence on beam search by varying the temperature of the SoftMax function. Without retraining the model, we change the character probability distribution to depend on a temperature hyperparameter INLINEFORM0 : DISPLAYFORM0 At increased temperatures the distribution over characters becomes more uniform. However, the preferences of the model are retained and the ordering of tokens from the most to least probable is preserved. Tuning the temperature therefore allows to demonstrate the impact of model confidence on beam search, without affecting the accuracy of next step predictions.Decoding results of a baseline model on the WSJ dev93 data set are presented in Figure FIGREF13 . We haven't used a language model. At high temperatures deletion errors dominated. We didn't want to change the beam search cost and instead constrained the search to emit the EOS token only when its probability was within a narrow range from the most probable token. We compare the default setting ( INLINEFORM0 ), with a sharper distribution ( INLINEFORM1 ) and smoother distributions ( INLINEFORM2 ). All strategies lead to the same greedy decoding accuracy, because temperature changes do not affect the selection of the most probable character. As temperature increases beam search finds better solutions, however care must be taken to prevent truncated transcripts.Label Smoothing Prevents Overconfidence	A elegant solution to model overconfidence was problem proposed for the Inception image recognition architecture BIBREF15 . For the purpose of computing the training cost the ground-truth label distribution is smoothed, with some fraction of the probability mass assigned to classes other than the correct one. This in turn prevents the model from learning to concentrate all probability mass on a single token. Additionally, the model receives more training signal because the error function cannot easily saturate.Originally uniform label smoothing scheme was proposed in which the model is trained to assign INLINEFORM0 probability mass to he correct label, and spread the INLINEFORM1 probability mass uniformly over all classes BIBREF15 . Better results can be obtained with unigram smoothing which distributes the remaining probability mass proportionally to the marginal probability of classes BIBREF16 . In this contribution we propose a neighborhood smoothing scheme that uses the temporal structure of the transcripts: the remaining INLINEFORM2 probability mass is assigned to tokens neighboring in the transcript. Intuitively, this smoothing scheme helps the model to recover from beam search errors: the network is more likely to make mistakes that simply skip a character of the transcript.We have repeated the analysis of SoftMax temperature on beam search accuracy on a network trained with neighborhood smoothing in Figure FIGREF13 . We can observe two effects. First, the model is regularized and greedy decoding leads to nearly 3 percentage smaller error rate. Second, the entropy of network predictions is higher, allowing beam search to discover good solutions without the need for temperature control. Moreover, the since model is trained and evaluated with INLINEFORM0 we didn't have to control the emission of EOS token.Solutions to Partial Transcripts Problem	When a language model is used wide beam searches often yield incomplete transcripts. With narrow beams, the problem is less visible due to implicit hypothesis pruning. We illustrate a failed decoding in Table TABREF17 . The ground truth (first row) is the least probable transcript according both to the network and the language model. A width 100 beam search with a trigram language model finds the second transcript, which misses the beginning of the utterance. The last rows demonstrate severely incomplete transcriptions that may be discovered when decoding is performed with even wider beam sizes.We compare three strategies designed to prevent incomplete transcripts. The first strategy doesn't change the beam search criterion, but forbids emitting the EOS token unless its probability is within a set range of that of the most probable token. This strategy prevents truncations, but is inefficient against omissions in the middle of the transcript, such as the failure shown in Table TABREF17 . Alternatively, beam search criterion can be extended to promote long transcripts. A term depending on the transcript length was proposed for both CTC BIBREF3 and seq2seq BIBREF11 networks, but its usage was reported to be difficult because beam search was looping over parts of the recording and additional constraints were needed BIBREF11 . To prevent looping we propose to use a coverage term that counts the number of frames that have received a cumulative attention greater than INLINEFORM0 : DISPLAYFORM0 The coverage criterion prevents looping over the utterance because once the cumulative attention bypasses the threshold INLINEFORM0 a frame is counted as selected and subsequent selections of this frame do not reduce the decoding cost. In our implementation, the coverage is recomputed at each beam search iteration using all attention weights produced up to this step.In Figure FIGREF19 we compare the effects of the three methods when decoding a network that uses label smoothing and a trigram language model. Unlike BIBREF11 we didn't experience looping when beam search promoted transcript length. We hypothesize that label smoothing increases the cost of correct character emissions which helps balancing all terms used by beam search. We observe that at large beam widths constraining EOS emissions is not sufficient. In contrast, both promoting coverage and transcript length yield improvements with increasing beams. However, simply maximizing transcript length yields more word insertion errors and achieves an overall worse WER.Experiments	We conducted all experiments on the Wall Street Journal dataset, training on si284, validating on dev93 and evaluating on eval92 set. The models were trained on 80-dimensional mel-scale filterbanks extracted every 10ms form 25ms windows, extended with their temporal first and second order differences and per-speaker mean and variance normalization. Our character set consisted of lowercase letters, the space, the apostrophe, a noise marker, and start- and end- of sequence tokens. For comparison with previously published results, experiments involving language models used an extended-vocabulary trigram language model built by the Kaldi WSJ s5 recipe BIBREF17 . We have use the FST framework to compose the language model with a ""spelling lexicon"" BIBREF5 , BIBREF11 , BIBREF18 . All models were implemented using the Tensorflow framework BIBREF19 .Our base configuration implemented the Listener using 4 bidirectional LSTM layers of 256 units per direction (512 total), interleaved with 3 time-pooling layers which resulted in an 8-fold reduction of the input sequence length, approximately equating the length of hidden activations to the number of characters in the transcript. The Speller was a single LSTM layer with 256 units. Input characters were embedded into 30 dimensions. The attention MLP used 128 hidden units, previous attention weights were accessed using 3 convolutional filters spanning 100 frames. LSTM weights were initialized uniformly over the range INLINEFORM0 . Networks were trained using 8 asynchronous replica workers each employing the ADAM algorithm BIBREF20 with default parameters and the learning rate set initially to INLINEFORM1 , then reduced to INLINEFORM2 and INLINEFORM3 after 400k and 500k training steps, respectively. Static Gaussian weight noise with standard deviation 0.075 was applied to all weight matrices after 20000 training steps. We have also used a small weight decay of INLINEFORM4 .We have compared two label smoothing methods: unigram smoothing BIBREF16 with the probability of the correct label set to INLINEFORM0 and neighborhood smoothing with the probability of correct token set to INLINEFORM1 and the remaining probability mass distributed symmetrically over neighbors at distance INLINEFORM2 and INLINEFORM3 with a INLINEFORM4 ratio. We have tuned the smoothing parameters with a small grid search and have found that good results can be obtained for a broad range of settings.We have gathered results obtained without language models in Table TABREF20 . We have used a beam size of 10 and no mechanism to promote longer sequences. We report averages of two runs taken at the epoch with the lowest validation WER. Label smoothing brings a large error rate reduction, nearly matching the performance achieved with very deep and sophisticated encoders BIBREF21 .Table TABREF21 gathers results that use the extended trigram language model. We report averages of two runs. For each run we have tuned beam search parameters on the validation set and applied them on the test set. A typical setup used beam width 200, language model weight INLINEFORM0 , coverage weight INLINEFORM1 and coverage threshold INLINEFORM2 . Our best result surpasses CTC-based networks BIBREF5 and matches the results of a DNN-HMM and CTC ensemble BIBREF22 .Related Work	Label smoothing was proposed as an efficient regularizer for the Inception architecture BIBREF15 . Several improved smoothing schemes were proposed, including sampling erroneous labels instead of using a fixed distribution BIBREF24 , using the marginal label probabilities BIBREF16 , or using early errors of the model BIBREF25 . Smoothing techniques increase the entropy of a model's predictions, a technique that was used to promote exploration in reinforcement learning BIBREF26 , BIBREF27 , BIBREF28 . Label smoothing prevents saturating the SoftMax nonlinearity and results in better gradient flow to lower layers of the network BIBREF15 . A similar concept, in which training targets were set slightly below the range of the output nonlinearity was proposed in BIBREF29 .Our seq2seq networks are locally normalized, i.e. the speller produces a probability distribution at every step. Alternatively normalization can be performed globally on whole transcripts. In discriminative training of classical ASR systems normalization is performed over lattices BIBREF30 . In the case of recurrent networks lattices are replaced by beam search results. Global normalization has yielded important benefits on many NLP tasks including parsing and translation BIBREF31 , BIBREF32 . Global normalization is expensive, because each training step requires running beam search inference. It remains to be established whether globally normalized models can be approximated by cheaper to train locally normalized models with proper regularization such as label smoothing.Using source coverage vectors has been investigated in neural machine translation models. Past attentions vectors were used as auxiliary inputs in the emitting RNN either directly BIBREF33 , or as cumulative coverage information BIBREF34 . Coverage embeddings vectors associated with source words end modified during training were proposed in BIBREF35 . Our solution that employs a coverage penalty at decode time only is most similar to the one used by the Google Translation system BIBREF9 .Conclusions	We have demonstrated that with efficient regularization and careful decoding the sequence-to-sequence approach to speech recognition can be competitive with other non-HMM techniques, such as CTC.","['What type of attention is used in the recognition system?', 'What are the solutions proposed for the seq2seq shortcomings?']","['Unacceptable', 'Unacceptable']"
29,"Gated Recurrent Neural Tensor Network	Recurrent Neural Networks (RNNs), which are a powerful scheme for modeling temporal and sequential data need to capture long-term dependencies on datasets and represent them in hidden layers with a powerful model to capture more information from inputs. For modeling long-term dependencies in a dataset, the gating mechanism concept can help RNNs remember and forget previous information. Representing the hidden layers of an RNN with more expressive operations (i.e., tensor products) helps it learn a more complex relationship between the current input and the previous hidden layer information. These ideas can generally improve RNN performances. In this paper, we proposed a novel RNN architecture that combine the concepts of gating mechanism and the tensor product into a single model. By combining these two concepts into a single RNN, our proposed models learn long-term dependencies by modeling with gating units and obtain more expressive and direct interaction between input and hidden layers using a tensor product on 3-dimensional array (tensor) weight parameters. We use Long Short Term Memory (LSTM) RNN and Gated Recurrent Unit (GRU) RNN and combine them with a tensor product inside their formulations. Our proposed RNNs, which are called a Long-Short Term Memory Recurrent Neural Tensor Network (LSTMRNTN) and Gated Recurrent Unit Recurrent Neural Tensor Network (GRURNTN), are made by combining the LSTM and GRU RNN models with the tensor product. We conducted experiments with our proposed models on word-level and character-level language modeling tasks and revealed that our proposed models significantly improved their performance compared to our baseline models.	Introduction	Modeling temporal and sequential data, which is crucial in machine learning, can be applied in many areas, such as speech and natural language processing. Deep neural networks (DNNs) have garnered interest from many researchers after being successfully applied in image classification BIBREF0 and speech recognition BIBREF1 . Another type of neural network, called a recurrent neural network (RNN) is also widely used for speech recognition BIBREF2 , machine translation BIBREF3 , BIBREF4 and language modeling BIBREF5 , BIBREF6 . RNNs have achieved many state-of-the-art results. Compared to DNNs, they have extra parameters for modeling the relationships of previous or future hidden states with current input where the RNN parameters are shared for each input time-step.Generally, RNNs can be separated by a simple RNN without gating units, such as the Elman RNN BIBREF7 , the Jordan RNN BIBREF8 , and such advanced RNNs with gating units as the Long-Short Term Memory (LSTM) RNN BIBREF9 and the Gated Recurrent Unit (GRU) RNN BIBREF4 . A simple RNN usually adequate to model some dataset and a task with short-term dependencies like slot filling for spoken language understanding BIBREF10 . However, for more difficult tasks like language modeling and machine translation where most predictions need longer information and a historical context from each sentence, gating units are needed to achieve good performance. With gating units for blocking and passing information from previous or future hidden layer, we can learn long-term information and recursively backpropagate the error from our prediction without suffering from vanishing or exploding gradient problems BIBREF9 . In spite of this situation, the concept of gating mechanism does not provide an RNN with a more powerful way to model the relation between the current input and previous hidden layer representations.Most interactions inside RNNs between current input and previous (or future) hidden states are represented using linear projection and addition and are transformed by the nonlinear activation function. The transition is shallow because no intermediate hidden layers exist for projecting the hidden states BIBREF11 . To get a more powerful representation on the hidden layer, Pascanu et al. BIBREF11 modified RNNs with an additional nonlinear layer from input to the hidden layer transition, hidden to hidden layer transition and also hidden to output layer transition. Socher et al. BIBREF12 , BIBREF13 proposed another approach using a tensor product for calculating output vectors given two input vectors. They modified a Recursive Neural Network (RecNN) to overcome those limitations using more direct interaction between two input layers. This architecture is called a Recursive Neural Tensor Network (RecNTN), which uses a tensor product between child input vectors to represent the parent vector representation. By adding the tensor product operation to calculate their parent vector, RecNTN significantly improves the performance of sentiment analysis and reasoning on entity relations tasks compared to standard RecNN architecture. However, those models struggle to learn long-term dependencies because the do not utilize the concept of gating mechanism.In this paper, we proposed a new RNN architecture that combine the gating mechanism and tensor product concepts to incorporate both advantages in a single architecture. Using the concept of such gating mechanisms as LSTMRNN and GRURNN, our proposed architecture can learn temporal and sequential data with longer dependencies between each input time-step than simple RNNs without gating units and combine the gating units with tensor products to represent the hidden layer with more powerful operation and direct interaction. Hidden states are generated by the interaction between current input and previous (or future) hidden states using a tensor product and a non-linear activation function allows more expressive model representation. We describe two different models based on LSTMRNN and GRURNN. LSTMRNTN is our proposed model for the combination between a LSTM unit with a tensor product inside its cell equation and GRURNTN is our name for a GRU unit with a tensor product inside its candidate hidden layer equation.In Section ""Background"" , we provide some background information related to our research. In Section ""Proposed Architecture"" , we describe our proposed RNN architecture in detail. We evaluate our proposed RNN architecture on word-level and character-level language modeling tasks and reported the result in Section ""Experiment Settings"" . We present related works in Section ""Related Work"" . Section ""Conclusion"" summarizes our paper and provides some possible future improvements.Recurrent Neural Network	A Recurrent Neural Network (RNN) is one kind of neural network architecture for modeling sequential and temporal dependencies BIBREF2 . Typically, we have input sequence $\mathbf {x}=(x_1,...,x_{T})$ and calculate hidden vector sequence $\mathbf {h}=(h_1,...,h_{T})$ and output vector sequence $\mathbf {y}=(y_1,...,y_T)$ with RNNs. A standard RNN at time $t$ -th is usually formulated as: $$h_t &=& f(x_t W_{xh} + h_{t-1} W_{hh} + b_h) \\
y_t &=& g(h_t W_{hy} + b_y).$$   (Eq. 2) where $W_{xh}$ represents the input layer to the hidden layer weight matrix, $W_{hh}$ represents hidden to hidden layer weight matrix, $W_{hy}$ represents the hidden to the output weight matrix, $b_h$ and $b_y$ represent bias vectors for the hidden and output layers. $f(\cdot )$ and $g(\cdot )$ are nonlinear activation functions such as sigmoid or tanh.Gated Recurrent Neural Network	Simple RNNs are hard to train to capture long-term dependencies from long sequential datasets because the gradient can easily explode or vanish BIBREF14 , BIBREF15 . Because the gradient (usually) vanishes after several steps, optimizing a simple RNN is more complicated than standard neural networks. To overcome the disadvantages of simple RNNs, several researches have been done. Instead of using a first-order optimization method, one approach optimized the RNN using a second-order Hessian Free optimization BIBREF16 . Another approach, which addressed the vanishing and exploding gradient problem, modified the RNN architecture with additional parameters to control the information flow from previous hidden layers using the gating mechanism concept BIBREF9 . A gated RNN is a special recurrent neural network architecture that overcomes this weakness of a simple RNN by introducing gating units. There are variants from RNN with gating units, such as Long Short Term Memory (LSTM) RNN and Gated Recurrent Unit (GRU) RNN. In the following sections, we explain both LSTMRNN and GRURNN in more detail.A Long Short Term Memory (LSTM) BIBREF9 is a gated RNN with three gating layers and memory cells. The gating layers are used by the LSTM to control the existing memory by retaining the useful information and forgetting the unrelated information. Memory cells are used for storing the information across time. The LSTM hidden layer at time $t$ is defined by the following equations BIBREF17 : $$i_t &=& \sigma (x_t W_{xi} + h_{t-1} W_{hi} + c_{t-1} W_{ci} + b_i) \\
f_t &=& \sigma (x_t W_{xf} + h_{t-1} W_{hf} + c_{t-1} W_{cf} + b_f) \\
c_t &=& f_t \odot c_{t-1} + i_t \odot \tanh (x_t W_{xc} + h_{t-1} W_{hc} + b_c) \\
o_t &=& \sigma (x_t W_{xo} + h_{t-1} W_{ho} + c_t W_{co} + b_o) \\
h_t &=& o_t \odot \tanh (c_t)$$   (Eq. 6) where $\sigma (\cdot )$ is sigmoid activation function and $i_t, f_t, o_t$ and $c_t$ are respectively the input gates, the forget gates, the output gates and the memory cells at time-step $t$ . The input gates keep the candidate memory cell values that are useful for memory cell computation, and the forget gates keep the previous memory cell values that are useful for calculating the current memory cell. The output gates filter which the memory cell values that are useful for the output or next hidden layer input.A Gated Recurrent Unit (GRU) BIBREF4 is a gated RNN with similar properties to a LSTM. However, there are several differences: a GRU does not have separated memory cells BIBREF18 , and instead of three gating layers, it only has two gating layers: reset gates and update gates. The GRU hidden layer at time $t$ is defined by the following equations BIBREF4 : $$r_t &=& \sigma (x_t W_{xr} + h_{t-1} W_{hr} + b_r)\\
z_t &=& \sigma (x_t W_{xz} + h_{t-1} W_{hz} + b_r)\\
\tilde{h_t} &=& f(x_t W_{xh} + (r_t \odot h_{t-1}) W_{hh} + b_h)\\
h_t &=& (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h_t}$$   (Eq. 9) where $\sigma (\cdot )$ is a sigmoid activation function, $r_t, z_t$ are reset and update gates, $\tilde{h_t}$ is the candidate hidden layer values and $h_t$ is the hidden layer values at time- $t$ . The reset gates determine which previous hidden layer value is useful for generating the current candidate hidden layer. The update gates keeps the previous hidden layer values or replaced by new candidate hidden layer values. In spite of having one fewer gating layer, the GRU can match LSTM's performance and its convergence speed convergence sometimes outperformed LSTM BIBREF18 .Recursive Neural Tensor Network	A Recursive Neural Tensor Network (RecNTN) is a variant of a Recursive Neural Network (RecNN) for modeling input data with variable length properties and tree structure dependencies between input features BIBREF19 . To compute the input representation with RecNN, the input must be parsed into a binary tree where each leaf node represents input data. Then, the parent vectors are computed in a bottom-up fashion, following the above computed tree structure whose information can be built using external computation tools (i.e., syntactic parser) or some heuristic from our dataset observations.Given Fig. 4 , $p_1$ , $p_2$ and $y$ was defined by: $$ p_1 &=& f\left( \begin{bmatrix} x_1 & x_2 \end{bmatrix} W + b \right) \\
 p_2 &=& f\left( \begin{bmatrix} p_1 & x_3 \end{bmatrix} W + b \right) \\
y &=& g\left( p_2 W_y + b_y \right)$$   (Eq. 13) where $f(\cdot )$ is nonlinear activation function, such as sigmoid or tanh, $g(\cdot )$ depends on our task, $W \in \mathbb {R}^{2d \times d}$ is the weight parameter for projecting child input vectors $x_1, x_2, x_3 \in \mathbb {R}^{d}$ into the parent vector, $W_y$ is a weight parameter for computing output vector, and $b, b_y$ are biases. If we want to train RecNN for classification tasks, $g(\cdot )$ can be defined as a softmax function.However, standard RecNNs have several limitations, where two vectors only implicitly interact with addition before applying a nonlinear activation function on them BIBREF12 and standard RecNNs are not able to model very long-term dependency on tree structures. Zhu et al. BIBREF20 proposed the gating mechanism into standard RecNN model to solve the latter problem. For the former limitation, the RecNN performance can be improved by adding more interaction between the two input vectors. Therefore, a new architecture called a Recursive Neural Tensor Network (RecNTN) tried to overcome the previous problem by adding interaction between two vectors using a tensor product, which is connected by tensor weight parameters. Each slice of the tensor weight can be used to capture the specific pattern between the left and right child vectors. For RecNTN, value $p_1$ from Eq. 13 and is defined by: $$p_1 &=& f\left(
\begin{bmatrix} x_1 & x_2 \end{bmatrix} W_{tsr}^{[1:d]} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} + \begin{bmatrix} x_1 & x_2 \end{bmatrix} W + b \right) \\
p_2 &=& f\left(
\begin{bmatrix} p_1 & x_3 \end{bmatrix} W_{tsr}^{[1:d]} \begin{bmatrix} p_1 \\ x_3 \end{bmatrix} + \begin{bmatrix} p_1 & x_3 \end{bmatrix} W + b \right)$$   (Eq. 15) where $W_{tsr}^{[1:d]} \in \mathbb {R}^{2d \times 2d \times d}$ is the tensor weight to map the tensor product between two children vectors. Each slice $W_{tsr}^{[i]}$ is a matrix $\mathbb {R}^{2d \times 2d}$ . For more details, we visualize the calculation for $p_1$ in Fig. 5 .Gated Recurrent Unit Recurrent Neural Tensor Network (GRURNTN)	Previously in Sections ""Experiment Settings"" and ""Recursive Neural Tensor Network"" , we discussed that the gating mechanism concept can helps RNNs learn long-term dependencies from sequential input data and that adding more powerful interaction between the input and hidden layers simultaneously with the tensor product operation in a bilinear form improves neural network performance and expressiveness. By using tensor product, we increase our model expressiveness by using second-degree polynomial interactions, compared to first-degree polynomial interactions on standard dot product followed by addition in common RNNs architecture. Therefore, in this paper we proposed a Gated Recurrent Neural Tensor Network (GRURNTN) to combine these two advantages into an RNN architecture. In this architecture, the tensor product operation is applied between the current input and previous hidden layer multiplied by the reset gates for calculating the current candidate hidden layer values. The calculation is parameterized by tensor weight. To construct a GRURNTN, we defined the formulation as: $$r_t &=& \sigma (x_t W_{xr} + h_{t-1} W_{hr} + b_r) \nonumber \\
z_t &=& \sigma (x_t W_{xz} + h_{t-1} W_{hz} + b_z) \nonumber \\
\tilde{h_t} &=& f\left( \begin{bmatrix} x_t & (r \odot h_{t-1}) \end{bmatrix} W_{tsr}^{[1:d]} \begin{bmatrix} x_t \\ (r \odot h_{t-1}) \end{bmatrix} \right. \nonumber \\
& & \left. + x_t W_{xh} + (r_t \odot h_{t-1}) W_{hh} + b_h \right) \\
h_t &=& (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h_t} \nonumber $$   (Eq. 17) where $W_{tsr}^{[1:d]} \in \mathbb {R}^{(i+d) \times (i+d) \times d}$ is a tensor weight for mapping the tensor product between the input-hidden layer, $i$ is the input layer size, and $d$ is the hidden layer size. Alternatively, in this paper we use a simpler bilinear form for calculating $\tilde{h_t}$ : $$\tilde{h_t} &=& f\left( \begin{bmatrix} x_t \end{bmatrix} W_{tsr}^{[1:d]} \begin{bmatrix} (r_t \odot h_{t-1}) \end{bmatrix}^{\intercal } \right. \nonumber \\
& & \left. + x_t W_{xh} + (r_t \odot h_{t-1}) W_{hh} + b_h \right) $$   (Eq. 18) where $W_{tsr}^{[i:d]} \in \mathbb {R}^{i \times d \times d}$ is a tensor weight. Each slice $W_{tsr}^{[i]}$ is a matrix $\mathbb {R}^{i \times d}$ . The advantage of this asymmetric version is that we can still maintain the interaction between the input and hidden layers through a bilinear form. We reduce the number of parameters from the original neural tensor network formulation by using this asymmetric version. Fig. 6 visualizes the $\tilde{h_t}$ calculation in more detail.LSTM Recurrent Neural Tensor Network (LSTMRNTN)	As with GRURNTN, we also applied the tensor product operation for the LSTM unit to improve its performance. In this architecture, the tensor product operation is applied between the current input and the previous hidden layers to calculate the current memory cell. The calculation is parameterized by the tensor weight. We call this architecture a Long Short Term Memory Recurrent Neural Tensor Network (LSTMRNTN). To construct an LSTMRNTN, we defined its formulation: $$i_t &=& \sigma (x_t W_{xi} + h_{t-1} W_{hi} + c_{t-1} W_{ci} + b_i) \nonumber \\
f_t &=& \sigma (x_t W_{xf} + h_{t-1} W_{hf} + c_{t-1} W_{cf} + b_f) \nonumber \\
\tilde{c_t} &=& \tanh \left( \begin{bmatrix} x_t \end{bmatrix} W_{tsr}^{[1:d]} \begin{bmatrix} h_{t-1} \end{bmatrix} \right. \nonumber \\
& & \left. + x_t W_{xc} + h_{t-1} W_{hc} + b_c \right)  \\
c_t &=& f_t \odot c_{t-1} + i_t \odot \tilde{c_t} \\
o_t &=& \sigma (x_t W_{xo} + h_{t-1} W_{ho} + c_t W_{co} + b_o) \nonumber \\
h_t &=& o_t \odot \tanh (c_t) \nonumber $$   (Eq. 21) where $W_{tsr}^{[1:d]} \in R^{i \times d \times d}$ is a tensor weight to map the tensor product between current input $x_t$ and previous hidden layer $h_{t-1}$ into our candidate cell $\tilde{c_t}$ . Each slice $W_{tsr}^{[i]}$ is a matrix $\mathbb {R}^{i \times d}$ . Fig. 7 visualizes the $\tilde{c_t}$ calculation in more detail.Optimizing Tensor Weight using Backpropagation Through Time	In this section, we explain how to train the tensor weight for our proposed architecture. Generally, we use backpropagation to train most neural network models BIBREF21 . For training an RNN, researchers tend to use backpropagation through time (BPTT) where the recurrent operation is unfolded as a feedforward neural network along with the time-step when we backpropagate the error BIBREF22 , BIBREF23 . Sometimes we face a performance issue when we unfold our RNN on such very long sequences. To handle that issue, we can use the truncated BPTT BIBREF5 to limit the number of time-steps when we unfold our RNN during backpropagation.Assume we want to do segment classification BIBREF24 with an RNN trained as function $f : \mathbf {x} \rightarrow \mathbf {y}$ , where $\mathbf {x} = (x_1,...,x_T)$ as an input sequence and $\mathbf {y} = (y_1,...,y_T)$ is an output label sequence. In this case, probability output label sequence $y$ , given input sequence $\mathbf {x}$ , is defined as: $$P(\mathbf {y}|\mathbf {x}) = \prod _{i=1}^{T}P(y_i | x_1,..,x_i)$$   (Eq. 24) Usually, we transform likelihood $P(\mathbf {y}|\mathbf {x})$ into a negative log-likelihood: $$E(\theta ) &=& -\log P(\mathbf {y}|\mathbf {x}) = -\log \left(\prod _{i=1}^{T} P(y_{i}|x_1,..,x_i)\right) \\
&=& -\sum _{i=1}^{T} \log P(y_i | x_1,..,x_i)$$   (Eq. 25) and our objective is to minimize the negative log-likelihood w.r.t all weight parameters $\theta $ . To optimize $W_{tsr}^{[1:d]}$ weight parameters, we need to find derivative $E(\theta )$ w.r.t $W_{tsr}^{[1:d]}$ : $$\frac{\partial E(\theta )}{\partial W_{tsr}^{[1:d]}} &=& \sum _{i=1}^{T} \frac{\partial E_i(\theta )}{\partial W_{tsr}^{[1:d]}} \nonumber $$   (Eq. 26) For applying backpropagation through time, we need to unfold our GRURNTN and backpropagate the error from $E_i(\theta )$ to all candidate hidden layer $\tilde{h_j}$ to accumulate $W_{tsr}^{[1..d]}$ gradient where $j \in [1..i]$ . If we want to use the truncated BPTT to ignore the history past over $K$ time-steps, we can limit $j \in [max(1, i-K) .. i]$ . We define the standard BPTT on GRURNTN to calculate $\partial E_i(\theta ) / \partial W_{tsr}^{[1..d]}$ : $$\frac{\partial E_i(\theta )}{\partial W_{tsr}^{[1:d]}} &=& \sum _{j=1}^{i} \frac{\partial E_i(\theta )}{\partial \tilde{h_j}} \frac{\partial \tilde{h_j}}{\partial W_{tsr}^{[1:d]}} \nonumber \\
&=& \sum _{j=1}^{i} \frac{\partial E_i(\theta )}{\partial \tilde{h_j}}\frac{\partial \tilde{h_j}}{\partial a_j} \frac{\partial a_j}{\partial W_{tsr}^{[1:d]}} \nonumber \\
&=& \sum _{j=1}^{i} \frac{\partial E_i(\theta )}{\partial \tilde{h_j}} f^{\prime }(a_j) \begin{bmatrix} x_j \end{bmatrix}^{\intercal } \begin{bmatrix} (r_j \odot h_{j-1}) \end{bmatrix} $$   (Eq. 27) where $$ a_j &=& \left( \begin{bmatrix} x_j \end{bmatrix} W_{tsr}^{[1:d]} \begin{bmatrix} (r_j \odot h_{j-1}) \end{bmatrix}^{\intercal } \right. \nonumber \\ & & \left. + x_j W_{xh} + (r_j \odot h_{j-1}) W_{hh} + b_h \right) \nonumber $$   (Eq. 28) and $f^{\prime }(\cdot )$ is a function derivative from our activation function : $$f^{\prime }(a_j) =
{\left\lbrace \begin{array}{ll}
(1-f(a_j)^2), & \text{if } f(\cdot ) \text{ is $\tanh $ function} \\
f(a_j)(1-f(a_j)), & \text{if } f(\cdot ) \text{ is sigmoid function}
\end{array}\right.} \nonumber $$   (Eq. 29) For LSTMRNTN, we also need to unfold our LSTMRNN and backpropagate the error from $E_i(\theta )$ to all cell layers $c_j$ to accumulate $W_{tsr}^{[1..d]}$ gradients where $j \in [1..i]$ . We define the standard BPTT on LSTMRNTN to calculate $\partial E_i(\theta ) / \partial W_{tsr}^{[1..d]}$ : $$\frac{\partial E_i(\theta )}{\partial W_{tsr}^{[1:d]}} &=& \sum _{j=1}^{i} \frac{\partial E_i{(\theta )}}{\partial c_j} \frac{\partial c_j}{\partial W_{tsr}^{[1:d]}} \nonumber \\
& = & \sum _{j=1}^{i} \frac{\partial E_i{(\theta )}}{\partial c_j} \frac{\partial c_j}{\partial \tanh (a_j)} \frac{\partial \tanh (a_j)}{\partial a_j} \frac{\partial a_j}{\partial W_{tsr}^{[1:d]}} \nonumber \\
& = & \sum _{j=1}^{i} \frac{\partial E_i{(\theta )}}{\partial c_j} i_j (1-\tanh ^2(a_j)) \begin{bmatrix} x_j \end{bmatrix}^{\intercal } \begin{bmatrix} h_{j-1} \end{bmatrix} $$   (Eq. 30) where $$ a_j &=& \left(\begin{bmatrix} x_j \end{bmatrix} W_{tsr}^{[1:d]} \begin{bmatrix} h_{j-1} \end{bmatrix} + x_j W_{xc} + h_{j-1} W_{hc} + b_c \right) $$   (Eq. 31) . In both proposed models, we can see partial derivative ${\partial E_i(\theta )} / {\partial W_{tsr}^{[1:d]}}$ in Eqs. 27 and 30 , the derivative from the tensor product w.r.t the tensor weight parameters depends on the values of our input and hidden layers. Then all the slices of tensor weight derivative are multiplied by the error from their corresponding pre-activated hidden unit values. From these derivations, we are able to see where each slice of tensor weight is learned more directly from their input and hidden layer values compared by using standard addition operations. After we accumulated every parameter's gradients from all the previous time-steps, we use a stochastic gradient optimization method such as AdaGrad BIBREF25 to optimize our model parameters.Experiment Settings	Next we evaluate our proposed GRURNTN and LSTMRNTN models against baselines GRURNN and LSTMRNN with two different tasks and datasets.Datasets and Tasks	We used a PennTreeBank (PTB) corpus, which is a standard benchmark corpus for statistical language modeling. A PTB corpus is a subset of the WSJ corpus. In this experiment, we followed the standard preprocessing step that was done by previous research BIBREF23 . The PTB dataset is divided as follows: a training set from sections 0-20 with total 930.000 words, a validation set from sections 21-22 with total 74.000 words, and a test set from sections 23-24 with total 82.000 words. The vocabulary is limited to the 10.000 most common words, and all words outside are mapped into a "" $<$ unk $>$ "" token. We used the preprocessed PTB corpus from the RNNLM-toolkit website.We did two different language modeling tasks. First, we experimented on a word-level language model where our RNN predicts the next word probability given the previous words and current word. We used perplexity (PPL) to measure our RNN performance for word-level language modeling. The formula for calculating the PPL of word sequence $X$ is defined by: $$PPL = 2^{-\frac{1}{N}\sum _{i=1}^{N} \log _2 P(X_i|X_{1..{i-1}})}$$   (Eq. 35) Second, we experimented on a character-level language model where our RNN predicts the next character probability given the previous characters and current character. We used the average number of bits-per-character (BPC) to measure our RNN performance for character-level language modeling. The formula for calculating the BPC of character sequence $X$ is defined by: $$BPC = -\frac{1}{N}\left(\sum _{i=1}^{N}\log _2{p(X_i|X_{1..{i-1}})} \right)$$   (Eq. 36) Experiment Models	In this experiment, we compared the performance from our baseline models GRURNN and LSTMRNN with our proposed GRURNTN and LSTMRNTN models. We used the same dimensions for the embedding matrix to represent the words and characters as the vectors of real numbers.For the word-level language modeling task, we used 256 hidden units for GRURNTN and LSTMRNTN, 860 for GRURNN, and 740 for LSTMRNN. All of these models use 128 dimensions for word embedding. We used dropout regularization with $p=0.5$ dropout probability for GRURNTN and LSTMRNTN and $p=0.6$ for our baseline model. The total number of free parameters for GRURNN and GRURNTN were about 12 million and about 13 million for LSTMRNN and LSTMRNTN.For the character-level language modeling task, we used 256 hidden units for GRURNTN and LSTMRNTN, 820 for GRURNN, and 600 for LSTMRNTN. All of these models used 32 dimensions for character embedding. We used dropout regularization with $p=0.25$ dropout probability. The total number of free parameters for GRURNN and GRURNTN was about 2.2 million and about 2.6 million for LSTMRNN and LSTMRNTN.We constrained our baseline GRURNN to have a similar number of parameters as the GRURNTN model for a fair comparison. We also applied such constraints on our baseline LSTMRNN to LSTMRNTN model.For all the experiment scenarios, we used AdaGrad for our stochastic gradient optimization method with mini-batch training and a batch size of 15 sentences. We multiplied our learning rate with a decay factor of 0.5 when the cost from the development set for current epoch is greater than previous epoch. We also used a rescaling trick on the gradient BIBREF26 when the norm was larger than 5 to avoid the issue of exploding gradients. For initializing the parameters, we used the orthogonal weight initialization trick BIBREF27 on every model.Character-level Language Modeling	In this section, we report our experiment results on PTB character-level language modeling using our baseline models GRURNN and LSTMRNN as well as our proposed models GRURNTN and LSTMRNTN. Fig. 8 shows performance comparisons from every model based on the validation set's BPC per epoch. In this experiment, GRURNN made faster progress than LSTMRNN, but eventually LSTMRNN converged into a better BPC based on the development set. Our proposed model GRURNTN made faster and quicker progress than LSTMRNTN and converged into a similar BPC in the last epoch. Both proposed models produced lower BPC than our baseline models from the first epoch to the last epoch.Table 1 shows PTB test set BPC among our baseline models, our proposed models and several published results. Our proposed model GRURNTN and LSTMRNTN outperformed both baseline models. GRURNTN reduced the BPC from 1.39 to 1.33 (0.06 absolute / 4.32% relative BPC) from the baseline GRURNN, and LSTMRNTN reduced the BPC from 1.37 to 1.34 (0.03 absolute / 2.22% relative BPC) from the baseline LSTMRNN. Overall, GRURNTN slightly outperformed LSTMRNTN, and both proposed models outperformed all of the baseline models on the character-level language modeling task.Word-level Language Modeling	In this section, we report our experiment results on PTB word-level language modeling using our baseline models GRURNN and LSTMRNN and our proposed models GRURNTN and LSTMRNTN. Fig. 9 compares the performance from every models based on the validation set's PPL per epoch. In this experiment, GRURNN made faster progress than LSTMRNN. Our proposed GRURNTN's progress was also better than LSTMRNTN. The best model in this task was GRURNTN, which had a consistently lower PPL than the other models.Table 1 shows the PTB test set PPL among our baseline models, proposed models, and several published results. Both our proposed models outperformed their baseline models. GRURNTN reduced the perplexity from 97.78 to 87.38 (10.4 absolute / 10.63% relative PPL) over the baseline GRURNN and LSTMRNTN reduced the perplexity from 108.26 to 96.97 (11.29 absolute / 10.42% relative PPL) over the baseline LSTMRNN. Overall, LSTMRNTN improved the LSTMRNN model and its performance closely resembles the baseline GRURNN. However, GRURNTN outperformed all the baseline models as well as the other models by a large margin.Related Work	Representing hidden states with deeper operations was introduced just a few years ago BIBREF11 . In these works, Pascanu et al. BIBREF11 use additional nonlinear layers for representing the transition from input to hidden layers, hidden to hidden layers, and hidden to output layers. They also improved the RNN architecture by a adding shortcut connection in the deep transition by skipping the intermediate layers. Another work from BIBREF33 proposed a new RNN design for a stacked RNN model called Gated Feedback RNN (GFRNN), which adds more connections from all the previous time-step stacked hidden layers into the current hidden layer computations. Despite adding additional transition layers and connection weight from previous hidden layers, all of these models still represent the input and hidden layer relationships by using linear projection, addition and nonlinearity transformation.On the tensor-based models, Irsoy et al. BIBREF34 proposed a simple RNN with a tensor product between the input and hidden layers. Such architecture resembles RecNTN, given a parse tree with a completely unbalanced tree on one side. Another work from BIBREF35 also use tensor products for representing hidden layers on DNN. By splitting the weight matrix into two parallel weight matrices, they calculated two parallel hidden layers and combined the pair of hidden layers using a tensor product. However, since not all of those models use a gating mechanism, the tensor parameters and tensor product operation can not be fully utilized because of the vanishing (or exploding) gradient problem.On the recurrent neural network-based model, Sutskever et al. BIBREF30 proposed multiplicative RNN (mRNN) for character-level language modeling using tensor as the weight parameters. They proposed two different models. The first selected a slice of tensor weight based on the current character input, and the second improved the first model with factorization for constructing a hidden-to-hidden layer weight. However, those models fail to fully utilize the tensor weight with the tensor product. After they selected the weight matrix based on the current input information, they continue to use linear projection, addition, and nonlinearity for interacting between the input and hidden layers.To the best of our knowledge, none of these works combined the gating mechanism and tensor product concepts into a single neural network architecture. In this paper, we built a new RNN by combining gating units and tensor products into a single RNN architecture. We expect that our proposed GRURNTN and LSTMRNTN architecture will improve the RNN performance for modeling temporal and sequential datasets.Conclusion	We presented a new RNN architecture by combining the gating mechanism and tensor product concepts. Our proposed architecture can learn long-term dependencies from temporal and sequential data using gating units as well as more powerful interaction between the current input and previous hidden layers by introducing tensor product operations. From our experiment on the PennTreeBank corpus, our proposed models outperformed the baseline models with a similar number of parameters in character-level language modeling and word-level language modeling tasks. In a character-level language modeling task, GRURNTN obtained 0.06 absolute (4.32% relative) BPC reduction over GRURNN, and LSTMRNTN obtained 0.03 absolute (2.22% relative) BPC reduction over LSTMRNN. In a word-level language modeling task, GRURNTN obtained 10.4 absolute (10.63% relative) PPL reduction over GRURNN, and LSTMRNTN obtained 11.29 absolute (10.42% relative PPL) reduction over LSTMRNN. In the future, we will investigate the possibility of combining our model with other stacked RNNs architecture, such as Gated Feedback RNN (GFRNN). We would also like to explore other possible tensor operations and integrate them with our RNN architecture. By applying these ideas together, we expect to gain further performance improvement. Last, for further investigation we will apply our proposed models to other temporal and sequential tasks, such as speech recognition and video recognition.Acknowledgements	Part of this research was supported by JSPS KAKENHI Grant Number 26870371.","['How significant is the performance compared to LSTM model?', 'How does the introduced model combine the both factors?', 'How much improvement do the introduced model achieve compared to the previous models?']","['Unacceptable', 'Unacceptable', 'Unacceptable']"
30,"Speech Model Pre-training for End-to-End Spoken Language Understanding	Whereas conventional spoken language understanding (SLU) systems map speech to text, and then text to intent, end-to-end SLU systems map speech directly to intent through a single trainable model. Achieving high accuracy with these end-to-end models without a large amount of training data is difficult. We propose a method to reduce the data requirements of end-to-end SLU in which the model is first pre-trained to predict words and phonemes, thus learning good features for SLU. We introduce a new SLU dataset, Fluent Speech Commands, and show that our method improves performance both when the full dataset is used for training and when only a small subset is used. We also describe preliminary experiments to gauge the model's ability to generalize to new phrases not heard during training.	Introduction	Spoken language understanding (SLU) systems infer the meaning or intent of a spoken utterance BIBREF0 . This is crucial for voice user interfaces, in which the speaker's utterance needs to be converted into an action or query. For example, for a voice-controlled coffee machine, an utterance like “make me a large coffee with two milks and a sugar, please” might have an intent representation like {drink: ""coffee"", size: ""large"", additions: [{type: ""milk"", count: 2}, {type: ""sugar"", count: 1}]}.The conventional SLU pipeline is composed of two modules: an automatic speech recognition (ASR) module that maps the speech to a text transcript, and a natural language understanding (NLU) module that maps the text transcript to the speaker's intent BIBREF1 , BIBREF2 , BIBREF3 . An alternative approach that is beginning to gain popularity is end-to-end SLU BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 . In end-to-end SLU, a single trainable model maps the speech audio directly to the speaker's intent without explicitly producing a text transcript (Fig. FIGREF4 ). Unlike the conventional SLU pipeline, end-to-end SLU:End-to-end models have been made possible by deep learning, which automatically learns hierarchical representations of the input signal BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 . Speech is natural to represent in a hierarchical way: waveform INLINEFORM0 phonemes INLINEFORM1 morphemes INLINEFORM2 words INLINEFORM3 concepts INLINEFORM4 meaning. However, because speech signals are high-dimensional and highly variable even for a single speaker, training deep models and learning these hierarchical representations without a large amount of training data is difficult.The computer vision BIBREF14 , BIBREF15 , natural language processing BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , and ASR BIBREF21 , BIBREF22 communities have attacked the problem of limited supervised training data with great success by pre-training deep models on related tasks for which there is more training data. Following their lead, we propose an efficient ASR-based pre-training methodology in this paper and show that it may be used to improve the performance of end-to-end SLU models, especially when the amount of training data is very small.Our contributions are as follows:Related work	Three key papers describing end-to-end SLU were written by Qian et al. BIBREF4 , Serdyuk et al. BIBREF5 , and Chen et al. BIBREF6 . Serdyuk et al. in BIBREF5 use no pre-training whatsoever. Qian et al. in BIBREF4 use an auto-encoder to initialize the SLU model. Chen et al. BIBREF6 pre-train the first stage of an SLU model to recognize graphemes; the softmax outputs of the first stage are then fed to a classifier second stage. The model proposed in this paper is similar to theirs, but removes the restriction of the softmax bottleneck and uses alternative training targets, as we will describe later.More recently, Haghani et al. in BIBREF7 compare four types of sequence-to-sequence models for SLU, including a direct model (end-to-end with no pre-training) and a multi-task model (uses a shared encoder whose output is ingested by a separate ASR decoder and SLU decoder). The model proposed here is somewhat similar to their multi-task model, although we do not use or require the ASR targets during SLU training.The work listed above deals with very high resource SLU—in BIBREF7 , for instance, the Google Home BIBREF23 dataset consists of 24 million labeled utterances. In contrast, Renkens et al. in BIBREF8 consider the problem of end-to-end SLU with limited training data, and find that capsule networks BIBREF24 , compared to conventional neural network models, are more easily capable of learning end-to-end SLU from scratch. However, they do not consider the effect of pre-training on other speech data.This previous work has all been conducted on datasets that are closed-source or too small to test hypotheses about the amount of data required to generalize well. The lack of a good open-source dataset for end-to-end SLU experiments makes it difficult for most people to perform high-quality, reproducible research on this topic. We therefore created a new SLU dataset, the “Fluent Speech Commands” dataset, which Fluent.ai releases along with this paper.Dataset	This section describes the structure and creation of Fluent Speech Commands.Audio and labels	The dataset is composed of 16 kHz single-channel .wav audio files. Each audio file contains a recording of a single command that one might use for a smart home or virtual assistant, like “put on the music” or “turn up the heat in the kitchen”.Each audio is labeled with three slots: action, object, and location. A slot takes on one of multiple values: for instance, the “location” slot can take on the values “none”, “kitchen”, “bedroom”, or “washroom”. We refer to the combination of slot values as the intent of the utterance. The dataset has 31 unique intents in total. We do not distinguish between domain, intent, and slot prediction, as is sometimes done in SLU BIBREF25 .The dataset can be used as a multi-label classification task, where the goal is to predict the action, object, and location labels. Since the slots are not actually independent of each other, a more careful approach would model the relationship between slots, e.g. using an autoregressive model, as in BIBREF7 . We use the simpler multi-label classification approach in this paper, so as to avoid the issues sometimes encountered training autoregressive models and instead focus on questions related to generalization using a simpler model. Alternately, the 31 distinct intents can be “flattened” and used as 31 distinct labels for a single-label classification task.For each intent, there are multiple possible wordings: for example, the intent {action: ""activate"", object: ""lights"", location: ""none""} can be expressed as “turn on the lights”, “switch the lights on”, “lights on”, etc.. These phrases were decided upon before data collection by asking employees at Fluent.ai, including both native and non-native English speakers, for various ways in which they might express a particular intent. There are 248 different phrases in total.Data collection	The data was collected using crowdsourcing. Each speaker was recorded saying each wording for each intent twice. The phrases to record were presented in a random order. Participants consented to data being released and provided demographic information about themselves. The demographic information about these anonymized speakers (age range, gender, speaking ability, etc.) is included along with the dataset.The data was validated by a separate set of crowdsourcers. All audios deemed by the crowdsourcers to be unintelligible or contain the wrong phrase were removed. The total number of speakers, utterances, and hours of audio remaining is shown in Table TABREF12 .Dataset splits	The utterances are randomly divided into train, valid, and test splits in such a way that no speaker appears in more than one split. Each split contains all possible wordings for each intent, though our code has the option to include data for only certain wordings for different sets, to test the model's ability to recognize wordings not heard during training. The dataset has a .csv file for each split that lists the speaker ID, file path, transcription, and slots for all the .wav files in that split.Related datasets	Here we review some related public datasets and show the gap that Fluent Speech Commands fills.The Google Speech Commands dataset BIBREF26 (to which the name “Fluent Speech Commands” is an homage) is a free dataset of 30 single-word spoken commands (“yes”, “no”, “stop”, “go”, etc.). This dataset is suitable for keyword spotting experiments, but not for SLU.ATIS is an SLU dataset consisting of utterances related to travel planning. This dataset can only be obtained expensively from the Linguistic Data Consortium.The Snips NLU Benchmark BIBREF2 has a rich set of virtual assistant commands, but contains only text, with no audio, and hence is not suitable for end-to-end SLU experiments.The Grabo, Domotica, and Patcor datasets are three related datasets of spoken commands for robot control and card games developed by KU Leuven and used in BIBREF8 . These datasets are free, but have only a small number of speakers and phrases.In contrast to these datasets, Fluent Speech Commands is simultaneously audio-based, reasonably large, and free, and contains several multiple-word commands corresponding to each of the intents.Model and Pre-training Strategy	The model proposed in this paper, shown in Fig. FIGREF17 , is a deep neural network consisting of a stack of modules, where the first modules are pre-trained to predict phonemes and words. The word and phoneme classifiers are discarded, and the entire model is then trained end-to-end on the supervised SLU task. In what follows, we justify these design decisions and give more details about the model hyperparameters.Which ASR targets to use?	ASR models are trained using a variety of targets, including phonemes, graphemes, wordpieces, or more recently whole words BIBREF27 , BIBREF28 , BIBREF29 . We choose whole words as the pre-training targets, since this is what a typical NLU module would expect as input. A typical ASR dataset contains too many unique words (LibriSpeech BIBREF30 has more than 200,000) to assign an output to each one; we only assign a label to the 10,000 most common words. This leaves much of the pre-training data without any labels, which wastes data. By using phonemes as intermediate pre-training targets BIBREF31 , BIBREF19 , BIBREF32 , we are able to pre-train on speech segments with no word label. Additionally, we find that using phonemes as intermediate targets speeds up word-level pre-training BIBREF33 , BIBREF34 .We use the Montreal Forced Aligner BIBREF35 to obtain word- and phoneme-level alignments for LibriSpeech, and we pre-train the model on the entire 960 hours of training data using these alignments INLINEFORM0 . Using force-aligned labels has the additional benefit of enabling pre-training using short, random crops rather than entire utterances, which reduces the computation and memory required to pre-train the model.Phoneme module	The first module takes as input the audio signal INLINEFORM0 and outputs INLINEFORM1 , a sequence of hidden representations that are pre-trained to predict phonemes. The phoneme-level logits are computed using a linear classifier: DISPLAYFORM0 The phoneme module is implemented using a SincNet layer BIBREF36 , BIBREF37 , which processes the raw input waveform, followed by multiple convolutional layers and recurrent layers with pooling and dropout. More detailed hyperparameters can be found in our code.Word module	The second module takes as input INLINEFORM0 and outputs INLINEFORM1 . Similar to the phoneme-level module, it uses recurrent layers with dropout and pooling, and is pre-trained to predict words using another linear classifier: DISPLAYFORM0 Notice that the input to this module is INLINEFORM0 , not INLINEFORM1 , and likewise the output to the next stage is INLINEFORM2 , not INLINEFORM3 . There are two good reasons for forwarding INLINEFORM4 instead of INLINEFORM5 . The first is that we don't want to remove a degree of freedom from the model: the size of INLINEFORM6 is fixed by the number of targets, and this would in turn fix the size of the next layer of the model. The second reason is that computing INLINEFORM7 requires multiplying and storing a large ( INLINEFORM8 2.5 million parameters) weight matrix, and by discarding this matrix after pre-training, we save on memory and computation.Intent module	The third module, which is not pre-trained, maps INLINEFORM0 to the predicted intent. Depending on the structure of the intent representation, the intent module might take on various forms. Since in this work we use a fixed three-slot intent representation, we implement this module using a recurrent layer, followed by max-pooling to squash the sequence of outputs from the recurrent layer into a single vector of logits corresponding to the different slot values, similar to BIBREF5 .Unfreezing schedule	Although the pre-trained model works well as a frozen feature extractor, it may be preferable to “unfreeze” its weights and finetune them for the SLU task with backpropagation. Similar to ULMFiT BIBREF17 , we find that gradually unfreezing the pre-trained layers works better than unfreezing them all at once. We unfreeze one layer each epoch, and stop at a pre-determined layer, which is a hyperparameter.Experiments	Here we report results for three experiments on Fluent Speech Commands: using the full dataset, using a subset of the dataset, and using a subset of wordings.Full dataset	We first trained models given the entire SLU training set. The models used one of: 1) no pre-training (randomly initialized), 2) pre-training with no unfreezing, 3) gradually unfreezing only the word layers, or 4) gradually unfreezing both the word layers and phoneme layers. What we report here as “accuracy” refers to the accuracy of all slots for an utterance taken together—that is, if the predicted intent differs from the true intent in even one slot, the prediction is deemed incorrect.The validation accuracy of these models over time is shown in Fig. . The best results are obtained when only the word layers of the pre-trained model are unfrozen. This may be because the model begins to forget the more general phonetic knowledge acquired during pre-training. For the test set, the frozen model and partially unfrozen model perform roughly equally well (Table TABREF28 , “full” column), possibly because the test set is “easier” than the validation set. In all cases, the pre-trained models outperform the randomly initialized model.Partial dataset	To simulate a smaller dataset, we randomly selected 10% of the training set, and used this instead of the entire training set. Fig. shows the validation accuracy (on the entire validation set, not a subset) over time. A similar trend is observed as for the entire dataset: unfreezing the word layers works best. The gap in final test accuracy between the randomly initialized model and the pre-trained models increases (Table TABREF28 , “10%” column); the final test accuracy for the pre-trained models drops only slightly, further highlighting the advantage of our proposed method.Generalizing to new wordings	What happens if new wordings appear in the test data that never appear in the training data? This is an important question, since it is generally impractical to try to imagine every possible wording for a particular intent while gathering training data.To test this, we trained models on three specific phrases, “turn on the lights”, “turn off the lights”, and “switch on the lights” (273 utterances total), and tested on those same phrases, as well as a new phrase: “switch off the lights”. If the model incorrectly infers that utterances that contain “switch” always correspond to turning on the lights, it will incorrectly guess that “switch off the lights” corresponds to turning on the lights; if the model infers that the presence of the word “off” corresponds to turning off the lights, it will generalize to the new phrase. The randomly initialized model was unable to fit this tiny training set, even with a very low learning rate and no regularization. The pre-trained models were able to generalize to the new wording (with 97% accuracy on the validation set, which contains more examples of the new phrase than of the training phrases).However, there are many situations in which our model does not correctly generalize. For example, if the model is trained only with examples containing “bedroom” and “washroom”, but then tested on an example containing “bathroom”, it will guess the intent corresponding to “bedroom” because “bedroom” sounds more similar to “bathroom” than to “washroom”, even though “washroom” is the correct meaning. In text-based NLU, this scenario can be handled using word embeddings, which represent words in such a way that words with similar meanings have similar vector representations BIBREF1 , BIBREF38 . It may be possible to teach the pre-trained part of the model to output “embedding-like” word representations so that the intent module can recognize the meaning of phrases with synonyms.Conclusion	In this paper, we proposed a pre-training methodology for end-to-end SLU models, introduced the Fluent Speech Commands dataset, and used this dataset to show that our pre-training techniques improve performance both for large and small SLU training sets. In the future, we plan to continue using Fluent Speech Commands to explore the limitations of end-to-end SLU, like new wordings and synonyms not observed in the SLU dataset, to see if these limitations can be overcome.Acknowledgements	We would like to acknowledge the following for research funding and computing support: NSERC, Calcul Québec, Compute Canada, the Canada Research Chairs, and CIFAR.Thanks to Dima Serdyuk and Kyle Kastner at Mila, and Farzaneh Fard, Luis Rodriguez Ruiz, Sam Myer, Mohamed Mhiri, and Arash Rad at Fluent.ai for helpful discussions with us about this work.",['How was the dataset collected?'],['data was collected using crowdsourcing where speakers were recorded saying random ordered phrases for each intent twice']
31,"EiTAKA at SemEval-2018 Task 1: An Ensemble of N-Channels ConvNet and XGboost Regressors for Emotion Analysis of Tweets	This paper describes our system that has been used in Task1 Affect in Tweets. We combine two different approaches. The first one called N-Stream ConvNets, which is a deep learning approach where the second one is XGboost regresseor based on a set of embedding and lexicons based features. Our system was evaluated on the testing sets of the tasks outperforming all other approaches for the Arabic version of valence intensity regression task and valence ordinal classification task.	Introduction	Sentiment analysis in Twitter is the problem of identifying people’s opinions expressed in tweets. It normally involves the classification of tweets into categories such as “positive”, “negative” and in some cases, “neutral”. The main challenges in designing a sentiment analysis system for Twitter are the following:Most of the existing systems are inspired in the work presented in BIBREF0 . Machine Learning techniques have been used to build a classifier from a set of tweets with a manually annotated sentiment polarity. The success of the Machine Learning models is based on two main facts: a large amount of labeled data and the intelligent design of a set of features that can distinguish between the samples.With this approach, most studies have focused on designing a set of efficient features to obtain a good classification performance BIBREF1 , BIBREF2 , BIBREF3 . For instance, the authors in BIBREF4 used diverse sentiment lexicons and a variety of hand-crafted features.This paper proposes the representation of tweets using a novel set of features, which include the information provided by seven lexicons and a bag of negated words (BonW). The concatenation of these features with a set of basic features improves the classification performance. The polarity of tweets is determined by a classifier based on a Support Vector Machine.The system has been evaluated on the Arabic and English language test sets of the Twitter Sentiment Analysis Track in SemEval 2017, subtask A (Message Polarity Classification). Our system (SiTAKA) has been ranked 8th over 36 teams in the English language test set and 2nd out of 8 teams in the Arabic language test set.The rest of the paper is structured as follows. Section SECREF2 presents the tools and the resources that have been used. In Section SECREF3 we describe the system. The experiments and results are presented and discussed in Section SECREF4 . Finally, in the last section the conclusions as well as further work are presented.Resources	This section explains the tools and the resources that have been used in the SiTAKA system. Let us denote to its Arabic language and English language versions by Ar-SiTAKA and En-SiTAKA, respectively.Sentiment Lexicons	We used for En-SiTAKA seven lexicons in this work, namely: General Inquirer BIBREF5 , Hu-Liu opinion lexicon (HL) BIBREF6 , NRC hashtags lexicon BIBREF4 , SenticNet BIBREF7 , and TS-Lex BIBREF8 . More details about each lexicon, such as how it was created, the polarity score for each term, and the statistical distribution of the lexicon, can be found in BIBREF9 .In this version of the SiTAKA system, we used four lexicons created by BIBREF10 . Arabic Hashtag Lexicon, Dialectal Arabic Hashtag Lexicon, Arabic Bing Liu Lexicon and Arabic Sentiment140 Lexicon. The first two were created manually, whereas the rest were translated to Arabic from the English version using Google Translator.Embeddings	We used two pre-trained embedding models in En-SiTAKA. The first one is word2vec which is provided by Google. It is trained on part of the Google News dataset (about 100 billion words) and it contains 300-dimensional vectors for 3M words and phrases BIBREF11 . The second one is SSWEu, which has been trained to capture the sentiment information of sentences as well as the syntactic contexts of words BIBREF12 . The SSWEu model contains 50-dimensional vectors for 100K words.In Ar-SiTAKA we used the model Arabic-SKIP-G300 provided by BIBREF13 . Arabic-SKIP-G300 has been trained on a large corpus of Arabic text collected from different sources such as Arabic Wikipedia, Arabic Gigaword Corpus, Ksucorpus, King Saud University Corpus, Microsoft crawled Arabic Corpus, etc. It contains 300-dimensional vectors for 6M words and phrases.System Description	This section explains the main steps of the SiTAKA system, the features used to describe a tweet and the classification method.Preprocessing and Normalization	Some standard pre-processing methods are applied on the tweets:Normalization: Each tweet in English is converted to the lowercase. URLs and usernames are omitted. Non-Arabic letters are removed from each tweet in the Arabic-language sets. Words with repeated letters (i.e. elongated) are corrected.Tokenization and POS tagging: All English-language tweets are tokenized and tagged using Ark Tweet NLP BIBREF14 , while all Arabic-language tweets are tokenized and tagged using Stanford Tagger BIBREF15 .Negation: A negated context can be defined as a segment of tweet that starts with a negation word (e.g. no, don't for English-language, لا و ليس > for Arabic-language) and ends with a punctuation mark BIBREF0 . Each tweet is negated by adding a suffix (""_NEG"" and ""_منفي>"") to each word in the negated context.It is necessary to mention that in Ar-SiTAKA we did not use all the Arabic negation words due to the ambiguity of some of them. For example, the first word ما>, is a question mark in the following ""ما رأيك في ما حدث؟>-What do you think about what happened?"" and it means ""which/that"" in the following example ""إن ما حدث اليوم سيء جدا> - The matter that happened today was very bad"".As shown in BIBREF16 , stopwords tend to carry sentiment information; thus, note that they were not removed from the tweets.Features ُExtraction	SiTAKA uses five types of features: basic text, syntactic, lexicon, cluster and Word Embeddings. These features are described in the following subsections:These basic features are extracted from the text. They are the following:Bag of Words (BoW): Bag of words or n-grams features introduce some contextual information. The presence or absence of contiguous sequences of 1, 2, 3, and 4 tokens are used to represent the tweets.Bag of Negated Words (BonW): Negated contexts are important keys in the sentiment analysis problem. Thus, we used the presence or absence of contiguous sequences of 1, 2, 3 and 4 tokens in the negated contexts as a set of features to represent the tweets.Syntactic features are useful to discriminate between neutral and non-neutral texts.Part of Speech (POS): Subjective and objective texts have different POS tags BIBREF17 . According to BIBREF18 , non-neutral terms are more likely to exhibit the following POS tags in Twitter: nouns, adjectives, adverbs, abbreviations and interjections. The number of occurrences of each part of speech tag is used to represent each tweet.Bi-tagged: Bi-tagged features are extracted by combining the tokens of the bi-grams with their POS tag e.g. ""feel_VBP good_JJ"" ""جميل>_JJ جداً>_VBD"". It has been shown in the literature that adjectives and adverbs are subjective in nature and they help to increase the degree of expressiveness BIBREF19 , BIBREF0 .Opinion lexicons play an important role in sentiment analysis systems, and the majority of the existing systems rely heavily on them BIBREF20 . For each of the seven chosen lexicons, a tweet is represented by calculating the following features: (1) tweet polarity, (2) the average polarity of the positive terms, (3) the average polarity of the negative terms, (4) the score of the last positive term, (5) the score of the last negative term, (6) the maximum positive score and (7) the minimum negative score.The polarity of a tweet T given a lexicon L is calculated using the equation (1). First, the tweet is tokenized. Then, the number of positive (P) and negative (N) tokens found in the lexicon are counted. Finally, the polarity measure is calculated as follows: DISPLAYFORM0 We used two set of clusters in En-SiTAKA to represent the English-language tweets by mapping each tweet to a set of clusters. The first one is the well known set of clusters provided by the Ark Tweet NLP tool which contains 1000 clusters produced with the Brown clustering algorithm from 56M English-language tweets. These 1000 clusters are used to represent each tweet by mapping each word in the tweet to its cluster. The second one is Word2vec cluster ngrams, which is provided by BIBREF21 . They used the word2vec tool to learn 40-dimensional word embeddings of 255,657 words from a Twitter dataset and the K-means algorithm to cluster them into 4960 clusters. We were not able to find publicly available semantic clusters to be used in Ar-SiTAKA.Word embeddings are an approach for distributional semantics which represents words as vectors of real numbers. Such representation has useful clustering properties, since the words that are semantically and syntactically related are represented by similar vectors BIBREF22 . For example, the words ""coffee"" and ""tea"" will be very close in the created space.We used sum, standard-deviation, min and max pooling functions BIBREF23 to obtain the tweet representation in the embedding space. The result is the concatenation of vectors derived from different pooling functions. More formally, let us consider an embedding matrix INLINEFORM0 and a tweet INLINEFORM1 , where INLINEFORM2 is the dimension size, INLINEFORM3 is the length of the vocabulary (i.e. the number of words in the embedding model), INLINEFORM4 is the word INLINEFORM5 in the tweet and INLINEFORM6 is the number of words. First, each word INLINEFORM7 is substituted by the corresponding vector INLINEFORM8 in the matrix INLINEFORM9 where INLINEFORM10 is the index of the word INLINEFORM11 in the vocabulary. This step ends with the matrix INLINEFORM12 . The vector INLINEFORM13 is computed using the following formula: DISPLAYFORM0 where INLINEFORM0 denotes the concatenation operation. The pooling function is an element-wise function, and it converts texts with various lengths into a fixed-length vector allowing to capture the information throughout the entire text.Classifier	Up to now, support vector machines (SVM) BIBREF24 have been used widely and reported as the best classifier in the sentiment analysis problem. Thus, we trained a SVM classifier on the training sets provided by the organizers. For the English-language we combined the training sets of SemEval13-16 and testing sets of SemEval13-15, and used them as a training set. Table TABREF20 shows the numerical description of the datasets used in this work. We used the linear kernel with the value 0.5 for the cost parameter C. All the parameters and the set of features have been experimentally chosen based on the development sets.Results	The evaluation metrics used by the task organizers were the macroaveraged recall ( INLINEFORM0 ), the F1 averaged across the positives and the negatives INLINEFORM1 and the accuracy ( INLINEFORM2 ) BIBREF25 .The system has been tested on 12,284 English-language tweets and 6100 Arabic-language tweets provided by the organizers. The golden answers of all the test tweets were omitted by the organizers. The official evaluation results of our system are reported along with the top 10 systems and the baseline results in Table 2 and 3. Our system ranks 8th among 38 systems in the English-language tweets and ranks 2nd among 8 systems in the Arabic language tweets. The baselines 1, 2 and 3 stand for case when the system classify all the tweets as positive, negative and neutral respectively.Conclusion	We have presented a new set of rich sentimental features for the sentiment analysis of the messages posted on Twitter. A Support Vector Machine classifier has been trained using a set of basic features, information extracted from seven useful and publicly available opinion lexicons, syntactic features, clusters and embeddings. We have realized that the lexicon opinions are the key point in the improvement of the performance of the classifier; thus, for the future work we plan to focus on working on the development of an efficient lexicon-based method or building a new lexicon that can be used to improve the performance of the sentiment analysis systems. Deep learning approaches have recently been used to build supervised, unsupervised or even semi-supervised methods to analyze the sentiment of texts and to build efficient opinion lexicons BIBREF26 , BIBREF27 , BIBREF12 ; thus, the authors are considering the possibility of also using this technique to build a sentiment analysis system.Acknowledgment	This work was partially supported by URV Research Support Funds (2015PFR-URV-B2-60, 2016PFR-URV-B2-60 and Martí i Franqués PhD grant).","['How is the data labeled?', 'What is the best performing model?', 'How long is the dataset?']","['Unacceptable', 'An ensemble of N-Channels ConvNet and XGboost regressor model', 'Unacceptable']"
32,"Read, Highlight and Summarize: A Hierarchical Neural Semantic Encoder-based Approach	Traditional sequence-to-sequence (seq2seq) models and other variations of the attention-mechanism such as hierarchical attention have been applied to the text summarization problem. Though there is a hierarchy in the way humans use language by forming paragraphs from sentences and sentences from words, hierarchical models have usually not worked that much better than their traditional seq2seq counterparts. This effect is mainly because either the hierarchical attention mechanisms are too sparse using hard attention or noisy using soft attention. In this paper, we propose a method based on extracting the highlights of a document; a key concept that is conveyed in a few sentences. In a typical text summarization dataset consisting of documents that are 800 tokens in length (average), capturing long-term dependencies is very important, e.g., the last sentence can be grouped with the first sentence of a document to form a summary. LSTMs (Long Short-Term Memory) proved useful for machine translation. However, they often fail to capture long-term dependencies while modeling long sequences. To address these issues, we have adapted Neural Semantic Encoders (NSE) to text summarization, a class of memory-augmented neural networks by improving its functionalities and proposed a novel hierarchical NSE that outperforms similar previous models significantly. The quality of summarization was improved by augmenting linguistic factors, namely lemma, and Part-of-Speech (PoS) tags, to each word in the dataset for improved vocabulary coverage and generalization. The hierarchical NSE model on factored dataset outperformed the state-of-the-art by nearly 4 ROUGE points. We further designed and used the first GPU-based self-critical Reinforcement Learning model.	Introduction	When there are a very large number of documents that need to be read in limited time, we often resort to reading summaries instead of the whole document. Automatically generating (abstractive) summaries is a problem with various applications, e.g., automatic authoring BIBREF0. We have developed automatic text summarization systems that condense large documents into short and readable summaries. It can be used for both single (e.g., BIBREF1, BIBREF2 and BIBREF3) and multi-document summarization (e.g.,BIBREF4, BIBREF3, BIBREF5).Text summarization is broadly classified into two categories: extractive (e.g., BIBREF3 and BIBREF6) and abstractive summarization (e.g., BIBREF7, BIBREF8 and BIBREF9). Extractive approaches select sentences from a given document and groups them to form concise summaries. By contrast, abstractive approaches generate human-readable summaries that primarily capture the semantics of input documents and contain rephrased key content. The former task falls under the classification paradigm, and the latter belongs to the generative modeling paradigm, and therefore, it is a much harder problem to solve. The backbone of state-of-the-art summarization models is a typical encoder-decoder BIBREF10 architecture that has proved to be effective for various sequential modeling tasks such as machine translation, sentiment analysis, and natural language generation. It contains an encoder that maps the raw input word vector representations to a latent vector. Then, the decoder usually equipped with a variant of the attention mechanism BIBREF11 uses the latent vectors to generate the output sequence, which is the summary in our case. These models are trained in a supervised learning setting where we minimize the cross-entropy loss between the predicted and the target summary. Encoder-decoder models have proved effective for short sequence tasks such as machine translation where the length of a sequence is less than 120 tokens. However, in text summarization, the length of the sequences vary from 400 to 800 tokens, and modeling long-term dependencies becomes increasingly difficult.Despite the metric's known drawbacks, text summarization models are evaluated using ROUGE BIBREF12, a discrete similarity score between predicted and target summaries based on 1-gram, 2-gram, and n-gram overlap. Cross-entropy loss would be a convenient objective on which to train the model since ROUGE is not differentiable, but doing so would create a mismatch between metrics used for training and evaluation. Though a particular summary scores well on ROUGE evaluation comparable to the target summary, it will be assigned lower probability by a supervised model. To tackle this problem, we have used a self-critic policy gradient method BIBREF13 to train the models directly using the ROUGE score as a reward. In this paper, we propose an architecture that addresses the issues discussed above.Introduction ::: Problem Formulation	Let $D=\lbrace d_{1}, d_{2}, ..., d_{N}\rbrace $ be the set of document sentences where each sentence $d_{i}, 1 \le i \le N$ is a set of words and $S=\lbrace s_{1}, s_{2}, ..., s_{M}\rbrace $ be the set of summary sentences. In general, most of the sentences in $D$ are a continuation of another sentence or related to each other, for example: in terms of factual details or pronouns used. So, dividing the document into multiple paragraphs as done by BIBREF4 leaves out the possibility of a sentence-level dependency between the start and end of a document. Similarly, abstracting a single document sentence as done by BIBREF9 cannot include related information from multiple document sentences. In a good human-written summary, each summary sentence is a compressed version of a few document sentences. Mathematically,Where $C$ is a compressor we intend to learn. Figure FIGREF3 represents the fundamental idea when using a sequence-to-sequence architecture. For a sentence $s$ in summary, the representations of all the related document sentences $d_{1}, d_{2}, ..., d_{K}$ are expected to form a cluster that represents a part of the highlight of the document.First, we adapt the Neural Semantic Encoder (NSE) for text summarization by improving its attention mechanism and compose function. In a standard sequence-to-sequence model, the decoder has access to input sequence through hidden states of an LSTM BIBREF14, which suffers from the difficulties that we discussed above. The NSE is equipped with an additional memory, which maintains a rich representation of words by evolving over time. We then propose a novel hierarchical NSE by using separate word memories for each sentence to enrich the word representations and a document memory to enrich the sentence representations, which performed better than its previous counterparts (BIBREF7, BIBREF3, BIBREF15). Finally, we use a maximum-entropy self-critic model to achieve better performance using ROUGE evaluation.Related Work	The first encoder-decoder for text summarziation is used by BIBREF1 coupled with an attention mechanism. Though encoder-decoder models gave a state-of-the-art performance for Neural Machine Translation (NMT), the maximum sequence length used in NMT is just 100 tokens. Typical document lengths in text summarization vary from 400 to 800 tokens, and LSTM is not effective due to the loss in memory over time for very long sequences. BIBREF7 used hierarchical attentionBIBREF16 to mitigate this effect where, a word LSTM is used to encode (decode) words, and a sentence LSTM is used to encode (decode) sentences. The use of two LSTMs separately for words and sentences improves the ability of the model to retain its memory for longer sequences. Additionally, BIBREF7 explored using a hierarchical model consisting of a feature-rich encoder incorporating position, Named Entity Recognition (NER) tag, Term Frequency (TF) and Inverse Document Frequency (IDF) scores. Since an RNN is a sequential model, computing at one time-step needs all of the previous time-steps to have computed before and is slow because the computation at all the time steps cannot be performed in parallel. BIBREF8 used convolutional layers coupled with an attention mechanism BIBREF11 to increase the speed of the encoder. Since the input to an RNN is fed sequentially, it is expected to capture the positional information. But both works BIBREF7 and BIBREF8 found positional embeddings to be quite useful for reasons unknown. BIBREF3 proposed an extractive summarization model that classifies sentences based on content, saliency, novelty, and position. To deal with out-of-vocabulary (OOV) words and to facilitate copying salient information from input sequence to the output, BIBREF2 proposed a pointer-generator network that combines pointing BIBREF17 with generation from vocabulary using a soft-switch. Attention models for longer sequences tend to be repetitive due to the decoder repeatedly attending to the same position from the encoder. To mitigate this issue, BIBREF2 used a coverage mechanism to penalize a decoder from attending to same locations of an encoder. However, the pointer generator and the coverage model BIBREF2 are still highly extractive; copying the whole article sentences 35% of the time. BIBREF18 introduced an intra-attention model in which attention also depends on the predictions from previous time steps.One of the main issues with sequence-to-sequence models is that optimization using the cross-entropy objective does not always provide excellent results because the models suffer from a mismatch between the training objective and the evaluation metrics such as ROUGE BIBREF12 and METEOR BIBREF19. A popular algorithm to train a decoder is the teacher-forcing algorithm that minimizes the negative log-likelihood (cross-entropy loss) at each decoding time step given the previous ground-truth outputs. But during the testing stage, the prediction from the previous time-step is fed as input to the decoder instead of the ground truth. This exposure bias results in error accumulation over each time step because the model has never been exposed to its predictions during training. Instead, recent works show that summarization models can be trained using reinforcement learning (RL) where the ROUGE BIBREF12 score is used as the reward (BIBREF18, BIBREF9 and BIBREF4).BIBREF5 made such an earlier attempt by using Q-learning for single-and multi-document summarization. Later, BIBREF15 proposed a coarse-to-fine hierarchical attention model to select a salient sentence using sentence attention using REINFORCE BIBREF20 and feed it to the decoder. BIBREF6 used REINFORCE to rank sentences for extractive summarization. BIBREF4 proposed deep communicating agents that operate over small chunks of a document, which is learned using a self-critical BIBREF13 training approach consisting of intermediate rewards. BIBREF9 used a advantage actor-critic (A2C) method to extract sentences followed by a decoder to form abstractive summaries. Our model does not suffer from their limiting assumption that a summary sentence is an abstracted version of a single source sentence. BIBREF18 trained their intra-attention model using a self-critical policy gradient algorithm BIBREF13. Though an RL objective gives a high ROUGE score, the output summaries are not readable by humans. To mitigate this problem, BIBREF18 used a weighted sum of supervised learning loss and RL loss.Humans first form an abstractive representation of what they want to say and then try to put it into words while communicating. Though it seems intuitive that there is a hierarchy from sentence representation to words, as observed by both BIBREF7 and BIBREF15, these hierarchical attention models failed to outperform a simple attention model BIBREF1. Unlike feedforward networks, RNNs are expected to capture the input sequence order. But strangely, positional embeddings are found to be effective (BIBREF7, BIBREF8, BIBREF15 and BIBREF3). We explored a few approaches to solve these issues and improve the performance of neural models for abstractive summarization.Proposed Models	In this section, we first describe the baseline Neural Semantic Encoder (NSE) class, discuss improvements to the compose function and attention mechanism, and then propose the Hierarchical NSE. Finally, we discuss the self-critic model that is used to boost the performance further using ROUGE evaluation.Proposed Models ::: Neural Semantic Encoder:	A Neural Semantic Encoder BIBREF21 is a memory augmented neural network augmented with an encoding memory that supports read, compose, and write operations. Unlike the traditional sequence-to-sequence models, using an additional memory relieves the LSTM of the burden to remember the whole input sequence. Even compared to the attention-model BIBREF11 which uses an additional context vector, the NSE has anytime access to the full input sequence through a much larger memory. The encoding memory is evolved using basic operations described as follows:Where, $x_{t} \in \mathbb {R}^D$ is the raw embedding vector at the current time-step. $f_{r}^{LSTM}$ , $f_{c}^{MLP}$ (Multi-Layer Perceptron), $f_{w}^{LSTM}$ be the read, compose and write operations respectively. $e_{l} \in R^{l}$ , $e_{k} \in R^{k}$ are vectors of ones, $\mathbf {1}$ is a matrix of ones and $\otimes $ is the outer product.Instead of using the raw input, the read function $f_{r}^{LSTM}$ in equation DISPLAY_FORM5 uses an LSTM to project the word embeddings to the internal space of memory $M_{t-1}$ to obtain the hidden states $o_{t}$. Now, the alignment scores $z_{t}$ of the past memory $M_{t-1}$ are calculated using $o_{t}$ as the key with a simple dot-product attention mechanism shown in equation DISPLAY_FORM6. A weighted sum gives the retrieved input memory that is used in equation DISPLAY_FORM8 by a Multi-Layer Perceptron in composing new information. Equation DISPLAY_FORM9 uses an LSTM and projects the composed states into the internal space of memory $M_{t-1}$ to obtain the write states $h_{t}$. Finally, in equation DISPLAY_FORM10, the memory is updated by erasing the retrieved memory as per $z_{t}$ and writing as per the write vector $h_{t}$. This process is performed at each time-step throughout the input sequence. The encoded memories $\lbrace M\rbrace _{t=1}^{T}$ are similarly used by the decoder to obtain the write vectors $\lbrace h\rbrace _{t=1}^{T}$ that are eventually fed to projection and softmax layers to get the vocabulary distribution.Proposed Models ::: Improved NSE	Although the vanilla NSE described above performed well for machine translation, just a dot-product attention mechanism is too simplistic for text summarization. In machine translation, it is sufficient to compute the correlation between word-vectors from the semantic spaces of different languages. In contrast, text summarization also needs a word-sentence and sentence-sentence correlation along with the word-word correlation. So, in search of an attention mechanism with a better capacity to model the complex semantic relationships inherent in text summarization, we found that the additive attention mechanism BIBREF11 given by the equation below performs well.Where, $v, W, U, b_{attn}$ are learnable parameters. One other important difference is the compose function: a Multi-layer Perceptron (MLP) is enough for machine translation as the sequences are short in length. However, text summarization consists of longer sequences that have sentence-to-sentence dependencies, and a history of previously composed words is necessary for overcoming repetition BIBREF1 and thereby maintaining novelty. A powerful function already at our disposal is the LSTM; we replaced the MLP with an LSTM, as shown below:In a standard text summarization task, due to the limited size of word vocabulary, out-of-vocabulary (OOV) words are replaced with [UNK] tokens. pointer-networks BIBREF17 facilitate the ability to copy words from the input sequence to the output via pointing. Later, BIBREF2 proposed a hybrid pointer-generator mechanism to improve upon pointing by retaining the ability to generate new words. It points to the words from the input sequence and generates new words from the vocabulary. A generation probability $p_{gen} \in (0, 1)$ is calculated using the retrieved memories, attention distribution, current input hidden state $o_{t}$ and write state $h_{t}$ as follows:Where, $W_{m}, W_{h}, W_{o}, b_{ptr}$ are learnable parameters, and $\sigma $ is the sigmoid activation function. Next, $p_{gen}$ is used as a soft switch to choose between generating a word from the vocabulary by sampling from $p_{vocab}$, or copying a word from the input sequence by sampling from the attention distribution $z_{t}$. For each document, we maintain an auxiliary vocabulary of OOV words in the input sequence. We obtain the following final probability distribution over the total extended vocabulary:Note that if $w$ is an OOV word, then $p_{vocab}(w)$ is zero; similarly, if $w$ does not appear in the source document, then $\sum _{i:w = w_{i}} z_{i}^{t}$ is zero. The ability to produce OOV words is one of the primary advantages of the pointer-generator mechanism. We can also use a smaller vocabulary size and thereby speed up the computation of output projection and softmax layers.Proposed Models ::: Hierarchical NSE	When humans read a document, we organize it in terms of word semantics followed by sentence semantics and then document semantics. In a text summarization task, after reading a document, sentences that have similar meanings or continual information are grouped together and then expressed in words. Such a hierarchical model was first introduced by BIBREF16 for document classification and later explored unsuccessfully for text summarization BIBREF3. In this work, we propose to use a hierarchical model with improved NSE to take advantage of both augmented memory and also the hierarchical document representation. We use a separate memory for each sentence to represent all the words of a sentence and a document memory to represent all sentences. Word memory composes novel words, and document memory composes novel sentences in the encoding process that can be later used to extract highlights and decode to summaries as shown in Figure FIGREF17.Let $D = \lbrace (w_{ij})_{j=1}^{T_{in}}\rbrace _{i=1}^{S_{in}}$ be the input document sequence, where $S_{in}$ is the number of sentences in a document and $T_{in}$ is the number of words per sentence. Let $\lbrace M_{i}\rbrace _{i=1}^{S_{in}}, M_{i} \in R^{T_{in} \times D}$ be the sentence memories that encode all the words in a sentence and $M^{d}, M^{d} \in R^{S_{in} \times D}$ be the document memory that encodes all the sentences present in the document. At each time-step, an input token $x_{t}$ is read and is used to retrieve aligned content from both corresponding sentence memory $M_{t}^{i, s}$ and document memory $M_{t}^{d}$. Please note that the retrieved document memory, which is a weighted combination of all the sentence representations forms a highlight. After composition, both the sentence and document memories are written simultaneously. This way, the words are encoded with contextual meaning, and also new simpler sentences are formed. The functionality of the model is as follows:Where, $f_{attn}$ is the attention mechanism given by equation(DISPLAY_FORM12). $Update$ remains the same as the vanilla NSE given by equation(DISPLAY_FORM10)and $Concat$ is the vector concatenation. Please note that NSE BIBREF21 has a concept of shared memory but we use multiple memories for representing words and a document memory for representing sentences, this is fundamentally different to a shared memory which does not have a concept of hierarchy.Proposed Models ::: Self-Critical Sequence Training	As discussed earlier, training in a supervised learning setting creates a mismatch between training and testing objectives. Also, feeding the ground-truth labels in training time-step creates an exposure bias while testing in which we feed the predictions from the previous time-step. Policy gradient methods overcome this by directly optimizing the non-differentiable metrics such as ROUGE BIBREF12 and METEOR BIBREF19. It can be posed as a Markov Decision Process in which the set of actions $\mathcal {A}$ is the vocabulary and reward $\mathcal {R}$ is the ROUGE score itself. So, we should find a policy $\pi (\theta )$ such that the set of sampled words $\tilde{y} = \lbrace \tilde{y}_{1}, \tilde{y}_{2}, ..., \tilde{y}_{T}\rbrace $ achieves highest ROUGE score among all possible summaries.We used the self-critical model of BIBREF13 proposed for image captioning. In self-critical sequence training, the REINFORCE algorithm BIBREF20 is used by modifying its baseline as the greedy output of the current model. At each time-step $t$, the model predicts two words: $\hat{y}_{t}$ sampled from $p(\hat{y}_{t} | \hat{y}_{1}, \hat{y}_{2}, ..., \hat{y}_{t-1}, x)$, the baseline output that is greedily generated by considering the most probable word from the vocabulary and $\tilde{y}_{t}$ sampled from the $p(\tilde{y}_{t} | \tilde{y}_{1}, \tilde{y}_{2}, ..., \tilde{y}_{t-1}, x)$. This model is trained using the following loss function:Using the above training objective, the model learns to generate samples with high probability and thereby increasing $r(\tilde{y})$ above $r(\hat{y})$. Additionally, we have used enthttps://stackoverflow.com/questions/19053077/looping-over-data-and-creating-individual-figuresropy regularization.Where, $p(\tilde{y}_{t})=p(\tilde{y}_{t} | \tilde{y}_{1}, \tilde{y}_{2}, ..., \tilde{y}_{t-1}, x)$ is the sampling probability and $V$ is the size of the vocabulary. It is similar to the exploration-exploitation trade-off. $\alpha $ is the regularization coefficient that explicitly controls this trade-off: a higher $\alpha $ corresponds to more exploration, and a lower $\alpha $ corresponds to more exploitation. We have found that all TensorFlow based open-source implementations of self-critic models use a function (tf.py_func) that runs only on CPU and it is very slow. To the best of our knowledge, ours is the first GPU based implementation.Experiments and Results ::: Dataset	We used the CNN/Daily Mail dataset BIBREF7, which has been used as the standard benchmark to compare text summarization models. This corpus has 286,817 training pairs, 13,368 validation pairs, and 11,487 test pairs, as defined by their scripts. The source document in the training set has 766 words spanning 29.74 sentences on an average while the summaries consist of 53 words and 3.72 sentences BIBREF7. The unique characteristics of this dataset such as long documents, and ordered multi-sentence summaries present exciting challenges, mainly because the proven sequence-to-sequence LSTM based models find it hard to learn long-term dependencies in long documents. We have used the same train/validation/test split and examples for a fair comparison with the existing models.The factoring of lemma and Part-of-Speech (PoS) tag of surface words, are observed BIBREF22 to increase the performance of NMT models in terms of BLEU score drastically. This is due to the improvement of the vocabulary coverage and better generalization. We have added a pre-processing step by incorporating the lemma and PoS tag to every word of the dataset and training the supervised model on the factored data. The process of extracting the lemma and the PoS tags has been described in BIBREF22. Please refer to the appendix for an example of factoring.Experiments and Results ::: Training Settings	For all the plain NSE models, we have truncated the article to a maximum of 400 tokens and the summary to 100 tokens. For the hierarchical NSE models, articles are truncated to have a maximum of 20 sentences and 20 words per sentence each. Shorter sequences are padded with `PAD` tokens. Since the factored models have lemma, PoS tag and the separator `|` for each word, sequence lengths should be close to 3 times the non-factored counterparts. For practical reasons of memory and time, we have used 800 tokens per article and 300 tokens for the summary.For all the models, including the pointer-generator model, we use a vocabulary size of 50,000 words for both source and target. Though some previous works BIBREF7 have used large vocabulary sizes of 150,000, since our models have a copy mechanism, smaller vocabulary is enough to obtain good performance. Large vocabularies increase the computation time. Since memory plays a prominent role in retrieval and update, it is vital to start with a good initialization. We have used 300-dimensional pre-trained GloVe BIBREF23 word-vectors to represent the input sequence to a model. Sentence memories are initialized with GloVe word-vectors of all the words in that sentence. Document memories are initialized with vector representations of all the sentences where a sentence is represented with the average of the GloVe word-vectors of all its words. All the models are trained using the Adam optimizer with the default learning rate of 0.001. We have not applied any regularization as the usage of dropout, and $L_{2}$ penalty resulted in similar performance, however with a drastically increased training time.The Hierarchical models process one sentence at a time, and hence attention distributions need less memory, and therefore, a larger batch size can be used, which in turn speeds up the training process. The non-factored model is trained on 7-NVIDIA Tesla-P100 GPUs with a batch size of 448 (64 examples per GPU); it takes approximately 45 minutes per epoch. Since the factored sequences are long, we used a batch size of 96 (12 examples per GPU) on 8-NVIDIA Tesla-V100 GPUs. The Hier model reaches optimal cross-entropy loss in just 8 epochs, unlike 33-35 epochs for both BIBREF7 and BIBREF2. For the self-critical model, training is started from the best supervised model with a learning rate of 0.00005 and manually changed to 0.00001 when needed with $\alpha =0.0001$ and the reported results are obtained after training for 15 days.Experiments and Results ::: Evaluation	All the models are evaluated using the standard metric ROUGE; we report the F1 scores for ROUGE-1, ROUGE-2, and ROUGE-L, which quantitively represent word-overlap, bigram-overlap, and longest common subsequence between reference summary and the summary that is to be evaluated. The results are obtained using pyrouge package. The performance of various models and our improvements are summarized in Table TABREF37. A direct implementation of NSE performed very poorly due to the simple dot-product attention mechanism. In NMT, a transformation from word-vectors in one language to another one (say English to French) using a mere matrix multiplication is enough because of the one-to-one correspondence between words and the underlying linear structure imposed in learning the word vectors BIBREF23. However, in text summarization a word (sentence) could be a condensation of a group of words (sentences). Therefore, using a complex neural network-based attention mechanism proposed improved the performance. Both dot-product and additive BIBREF11 mechanisms perform similarly for the NMT task, but the difference is more pronounced for the text summarization task simply because of the nature of the problem as described earlier. Replacing Multi-Layered Perceptron (MLP) in the NSE with an LSTM further improved the performance because it remembers what was previously composed and facilitates the composition of novel words. This also eliminates the need for additional mechanisms to penalize repetitions such as coverage BIBREF2 and intra-attention BIBREF18. Finally, using memories for each sentence enriches the corresponding word representation, and the document memory enriches the sentence representation that help the decoder. Please refer to the appendix for a few example outputs. Table TABREF34 shows the results in comparison to the previous methods. Our hierarchical model outperforms BIBREF7 (HIER) by 5 ROUGE points. Our factored model achieves the new state-of-the-art (SoTA) result, outperforming BIBREF4 by almost 4 ROUGE points.Conclusion	In this work, we presented a memory augmented neural network for the text summarization task that addresses the shortcomings of LSTM-based models. We applied a critical pre-processing step by factoring the dataset with inherent linguistic information that outperforms the state-of-the-art by a large margin. In the future, we will explore new sparse functions BIBREF24 to enforce strict sparsity in selecting highlights out of sentences. The general framework of pre-processing, and extracting highlights can also be used with powerful pre-trained models like BERT BIBREF25 and XLNet BIBREF26.Appendix	Figure FIGREF38 below shows the self-critical model. All the examples shown in Tables TABREF39-TABREF44 are chosen as per the shortest article lengths available due to space constraints.","['How is GPU-based self-critical Reinforcement Learing model designed?', 'What are previoius similar models authors are referring to?', 'What was previous state of the art on factored dataset?']","['Unacceptable', 'Abstractive and extractive models from Nallapati et al., 2016, Pointer generator models with and without coverage from See et al., 2017, and Reinforcement Learning models from Paulus et al., 2018, and Celikyilmaz et al., 2018.', 'ROUGE-1 41.69\nROUGE-2 19.47\nROUGE-L 37.92']"
33,"Sentence Modeling via Multiple Word Embeddings and Multi-level Comparison for Semantic Textual Similarity	Different word embedding models capture different aspects of linguistic properties. This inspired us to propose a model (M-MaxLSTM-CNN) for employing multiple sets of word embeddings for evaluating sentence similarity/relation. Representing each word by multiple word embeddings, the MaxLSTM-CNN encoder generates a novel sentence embedding. We then learn the similarity/relation between our sentence embeddings via Multi-level comparison. Our method M-MaxLSTM-CNN consistently shows strong performances in several tasks (i.e., measure textual similarity, identify paraphrase, recognize textual entailment). According to the experimental results on STS Benchmark dataset and SICK dataset from SemEval, M-MaxLSTM-CNN outperforms the state-of-the-art methods for textual similarity tasks. Our model does not use hand-crafted features (e.g., alignment features, Ngram overlaps, dependency features) as well as does not require pre-trained word embeddings to have the same dimension.	Introduction	Measuring the semantic similarity/relation of two pieces of short text plays a fundamental role in a variety of language processing tasks (i.e., plagiarism detection, question answering, and machine translation). Semantic textual similarity (STS) task is challenging because of the diversity of linguistic expression. For example, two sentences with different lexicons could have a similar meaning. Moreover, the task requires to measure similarity at several levels (e.g., word level, phrase level, sentence level). These challenges give difficulties to conventional approaches using hand-crafted features.Recently, the emergence of word embedding techniques, which encode the semantic properties of a word into a low dimension vector, leads to the successes of many learning models in natural language processing (NLP). For example, BIBREF0 randomly initialize word vectors, then tunes them during the training phase of a sentence classification task. By contrast, BIBREF1 initialize word vectors via the pre-train word2vec model trained on Google News BIBREF2 . BIBREF3 train a word embedding model on the paraphrase dataset PPDB, then apply the word representation for word and bi-gram similarity tasks.Several pre-trained word embeddings are available, which are trained on various corpora under different models. BIBREF4 observed that different word embedding models capture different aspects of linguistic properties: a Bag-of-Words contexts based model tends to reflect the domain aspect (e.g., scientist and research) while a paraphrase-relationship based model captures semantic similarities of words (e.g., boy and kid). From experiments, we also observed that the performance of a word embedding model is usually inconsistent over different datasets. This inspired us to develop a model taking advantages of various pre-trained word embeddings for measuring textual similarity/relation.In this paper, we propose a convolutional neural network (CNN) to learn a multi-aspect word embedding from various pre-trained word embeddings. We then apply the max-pooling scheme and Long Short Term Memory (LSTM) on this embedding to form a sentence representation. In STS tasks, BIBREF5 shows the efficiency of the max-pooling scheme in modeling sentences from word embedding representations refined via CNN. However, the max-pooling scheme lacks the property of word order (e.g., sentence(“Bob likes Marry”) = sentence(“Marry likes Bob”)). To address this weakness, we use LSTM as an additional scheme for modeling sentences with word order characteristics. For measuring the similarity/relation between two sentence representations, we propose Multi-level comparison which consists of word-word level, sentence-sentence level, and word-sentence level. Through these levels, our model comprehensively evaluates the similarity/relation between two sentences.We evaluate our M-MaxLSTM-CNN model on three tasks: STS, textual entailment recognition, paraphrase identification. The advantages of M-MaxLSTM-CNN are: i) simple but efficient for combining various pre-trained word embeddings with different dimensions; ii) using Multi-level comparison shows better performances compared to using only sentence-sentence comparison; iii) does not require hand-crafted features (e.g., alignment features, Ngram overlaps, syntactic features, dependency features) compared to the state-of-the-art ECNU BIBREF6 on STS Benchmark dataset.Our main contributions are as follows:The remainder of this paper is organized as follows: Section 2 reviews the previous research, Section 3 introduces the architecture of our model, Section 4 describes the three tasks and datasets, Section 5 describes the experiment setting, Section 6 reports and discusses the results of the experiments, and Section 7 concludes our work.Related work	Most prior research on modeling textual similarity relied on feature engineering. BIBREF7 extract INLINEFORM0 -gram overlap features and dependency-based features, while BIBREF8 employ features based on machine translation metrics. BIBREF9 propose a method using corpus-based and knowledge-based measures of similarity. BIBREF10 design a model which incorporates both syntax and lexical semantics using dependency grammars. BIBREF11 combine the fine-grained n-gram overlap features with the latent representation from matrix factorization. BIBREF12 develop a latent variable model which jointly learns paraphrase relations between word and sentence pairs. Using Dependency trees, BIBREF13 propose a robust monolingual aligner and successfully applied it for STS tasks.The recent emergence of deep learning models has provided an efficient way to learn continuous vectors representing words/sentences. By using a neural network in the context of a word prediction task, BIBREF14 and BIBREF15 generate word embedding vectors carrying semantic meanings. The embedding vectors of words which share similar meanings are close to each other. To capture the morphology of words, BIBREF16 enrich the word embedding with character n-grams information. Closest to this approach, BIBREF17 also propose to represent a word or sentence using a character n-gram count vector. However, the objective function for learning these embeddings is based on paraphrase pairs.For modeling sentences, composition approach attracted many studies. BIBREF18 model each word as a matrix and used iterated matrix multiplication to present a phrase. BIBREF19 design a Dependency Tree-Structured LSTM for modeling sentences. This model outperforms the linear chain LSTM in STS tasks. Convolutional neural network (CNN) has recently been applied efficiently for semantic composition BIBREF0 , BIBREF20 , BIBREF5 . This technique uses convolutional filters to capture local dependencies in term of context windows and applies a pooling layer to extract global features. BIBREF21 use CNN to extract features at multiple level of granularity. The authors then compare their sentence representations via multiple similarity metrics at several granularities. BIBREF22 propose a hierarchical CNN-LSTM architecture for modeling sentences. In this approach, CNN is used as an encoder to encode an sentence into a continuous representation, and LSTM is used as a decoder. BIBREF23 train a sentence encoder on a textual entailment recognition database using a BiLSTM-Maxpooling network. This encoder achieves competitive results on a wide range of transfer tasks.At SemEval-2017 STS task, hybrid approaches obtain strong performances. BIBREF24 train a linear regression model with WordNet, alignment features and the word embedding word2vec. BIBREF6 develop an ensemble model with multiple boosting techniques (i.e., Random Forest, Gradient Boosting, and XGBoost). This model incorporates traditional features (i.e., n-gram overlaps, syntactic features, alignment features, bag-of-words) and sentence modeling methods (i.e., Averaging Word Vectors, Projecting Averaging Word Vectors, LSTM).MVCNN model BIBREF25 and MGNC-CNN model BIBREF26 are close to our approach. In MVCNN, the authors use variable-size convolution filters on various pre-trained word embeddings for extracting features. However, MVCNN requires word embeddings to have the same size. In MGNC-CNN, the authors apply independently CNN on each pre-trained word embedding for extracting features and then concatenate these features for sentence classification. By contrast, our M-MaxLSTM-CNN model jointly applies CNN on all pre-trained word embeddings to learn a multi-aspect word embedding. From this word representation, we encode sentences via the max-pooling and LSTM. To learn the similarity/relation between two sentences, we employ Multi-level comparison.Model description	Our model (shown in Figure FIGREF4 ) consists of three main components: i) learning a multi-aspect word embedding (Section 3.1); ii) modeling sentences from this embedding (Section 3.2); iii) measuring the similarity/relation between two sentences via Multi-level comparison (section 3.3).Multi-aspect word embedding	Given a word INLINEFORM0 , we transfer it into a word vector INLINEFORM1 via INLINEFORM2 pre-trained word embeddings as follows: DISPLAYFORM0 where INLINEFORM0 is concatenation operator, INLINEFORM1 is the word embedding vector of INLINEFORM2 in the INLINEFORM3 th pre-trained embedding.To learn a multi-aspect word embedding INLINEFORM0 from the representation INLINEFORM1 , we design INLINEFORM2 convolutional filters. Each filter INLINEFORM3 is denoted as a weight vector with the same dimension as INLINEFORM4 and a bias value INLINEFORM5 . The INLINEFORM6 is obtained by applying these filters on the INLINEFORM7 as follows: DISPLAYFORM0  where INLINEFORM0 denotes a logistic sigmoid function.The next section explains how to model a sentence from its multiple-aspect word embeddings.Sentence modeling	Given an input sentence INLINEFORM0 , we obtain a sequence of multiple-aspect word embeddings INLINEFORM1 using Eq. (1-3). For modeling the sentence from the representation INLINEFORM2 , we use two schemes: max-pooling and LSTM.Max-pooling scheme: To construct a max-pooling sentence embedding INLINEFORM0 , the most potential features are extracted from the representation INLINEFORM1 as follows: DISPLAYFORM0 where INLINEFORM0 is the INLINEFORM1 th element of INLINEFORM2 .LSTM scheme: From Eq. (4), we find that the max-pooling scheme ignores the property of word order. Therefore, we construct a LSTM sentence embedding INLINEFORM0 to support the sentence embedding INLINEFORM1 . The representation INLINEFORM2 is transformed to a fix-length vector by recursively applying a LSTM unit to each input INLINEFORM3 and the previous step INLINEFORM4 . At each time step INLINEFORM5 , the LSTM unit with INLINEFORM6 -memory dimension defines six vectors in INLINEFORM7 : input gate INLINEFORM8 , forget gate INLINEFORM9 , output gate INLINEFORM10 , tanh layer INLINEFORM11 , memory cell INLINEFORM12 and hidden state INLINEFORM13 as follows (from BIBREF19 ): DISPLAYFORM0 DISPLAYFORM1  where INLINEFORM0 respectively denote a logistic sigmoid function and element-wise multiplication; INLINEFORM1 are respectively two weights matrices and a bias vector for input gate INLINEFORM2 . The denotation is similar to forget gate INLINEFORM3 , output gate INLINEFORM4 , tanh layer INLINEFORM5 , memory cell INLINEFORM6 and hidden state INLINEFORM7 .Finally, the sentence embedding INLINEFORM0 is obtained by concatenating the two sentence embeddings INLINEFORM1 and INLINEFORM2 : DISPLAYFORM0 Multi-level comparison	In this section, we describe the process for evaluating the similarity/relation between two sentences. We compare two sentences via three levels: word-word, sentence-sentence and word-sentence.Given two input sentences INLINEFORM0 and INLINEFORM1 , we encode them into two sequences of multi-aspect word embeddings INLINEFORM2 and INLINEFORM3 (Section 3.2). We then compute a word-word similarity vector INLINEFORM4 as follows: DISPLAYFORM0  where INLINEFORM0 is the INLINEFORM1 th multi-aspect word embedding of sentence INLINEFORM2 ; INLINEFORM3 is a function to flatten a matrix into a vector; and INLINEFORM4 and INLINEFORM5 are respectively a weight matrix and a bias parameter.Given two input sentences INLINEFORM0 and INLINEFORM1 , we encode them into two sentence embeddings INLINEFORM2 and INLINEFORM3 (Section 3.1 and 3.2). To compute the similarity/relation between the two embeddings, we introduce four comparison metrics:Cosine similarity: DISPLAYFORM0 Multiplication vector & Absolute difference: DISPLAYFORM0  where INLINEFORM0 is element-wise multiplication.Neural difference: DISPLAYFORM0  where INLINEFORM0 and INLINEFORM1 are respectively a weight matrix and a bias parameter.As a result, we have a sentence-sentence similarity vector INLINEFORM0 as follows: DISPLAYFORM0  where INLINEFORM0 and INLINEFORM1 are respectively a weight matrix and a bias parameter.Given a sentence embedding INLINEFORM0 and a sequence of multi-aspect word embeddings INLINEFORM1 , we compute a word-sentence similarity matrix INLINEFORM2 as follows: DISPLAYFORM0  where INLINEFORM0 is the multi-aspect word embedding of the INLINEFORM1 th word in sentence INLINEFORM2 ; INLINEFORM3 and INLINEFORM4 are respectively a weight matrix and a bias parameter.As a result, we have a word-sentence similarity vector INLINEFORM0 for the two sentences as follows: DISPLAYFORM0 where INLINEFORM0 is a function to flatten a matrix into a vector; INLINEFORM1 and INLINEFORM2 are respectively a weight matrix and a bias parameter.Finally, we compute a target score/label of a sentence pair as follows: DISPLAYFORM0  where INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 are model parameters; INLINEFORM4 is a predicted target score/label.Tasks & Datasets	We evaluate our model on three tasks:Table TABREF30 shows the statistic of the three datasets. Because of not dealing with name entities and multi-word idioms, the vocabulary size of SICK is quite small compared to the others.Pre-trained word embeddings	We study five pre-trained word embeddings for our model:word2vec is trained on Google News dataset (100 billion tokens). The model contains 300-dimensional vectors for 3 million words and phrases.fastText is learned via skip-gram with subword information on Wikipedia text. The embedding representations in fastText are 300-dimensional vectors.GloVe is a 300-dimensional word embedding model learned on aggregated global word-word co-occurrence statistics from Common Crawl (840 billion tokens).Baroni uses a context-predict approach to learn a 400-dimensional semantic embedding model. It is trained on 2.8 billion tokens constructed from ukWaC, the English Wikipedia and the British National Corpus.SL999 is trained under the skip-gram objective with negative sampling on word pairs from the paraphrase database PPDB. This 300-dimensional embedding model is tuned on SimLex-999 dataset BIBREF27 .Model configuration	In all of the tasks, we used the same model configuration as follows:Convolutional filters: we used 1600 filters. It is also the dimension of the word embedding concatenated from the five pre-trained word embeddings.LSTM dimension: we also selected 1600 for LSTM dimension.Neural similarity layers: the dimension of INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 are respectively 50, 5, 5 and 100.Penultimate fully-connected layer: has the dimension of 250 and is followed by a drop-out layer ( INLINEFORM0 ).We conducted a grid search on 30% of STSB dataset to select these optimal hyper-parameters.Training Setting	In these tasks, we use the cross-entropy objective function and employ AdaDelta as the stochastic gradient descent (SGD) update rule with mini-batch size as 30. Details of Adadelta method can be found in BIBREF28 . During the training phase, the pre-trained word embeddings are fixed.To compute a similarity score of a sentence pair in the range INLINEFORM0 , where INLINEFORM1 is an integer, we replace Eq. (27) with the equations in BIBREF19 as follows: DISPLAYFORM0  where INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 are parameters; INLINEFORM4 ; INLINEFORM5 is a predicted similarity score.A sparse target distribution INLINEFORM0 which satisfies INLINEFORM1 is computed as: DISPLAYFORM0 for INLINEFORM0 , and INLINEFORM1 is the similarity score.To train the model, we minimize the regularized KL-divergence between INLINEFORM0 and INLINEFORM1 : DISPLAYFORM0 where INLINEFORM0 is the number of training pairs and INLINEFORM1 denotes the model parameters. The gradient descent optimization Adadelta is used to learn the model parameters. We also use mini-batch size as 30 and keep the pre-trained word embeddings fixed during the training phase. We evaluate our models through Pearson correlation INLINEFORM2 .Experiments and Discussion	This section describes two experiments: i) compare our model against recent systems; ii) evaluate the efficiency of using multiple pre-trained word embeddings.Overall evaluation	Besides existing methods, we also compare our model with several sentence modeling approaches using multiple pre-trained word embeddings:Word Average: DISPLAYFORM0 where INLINEFORM0 is the sentence embedding of a INLINEFORM1 -words sentence, and INLINEFORM2 is from Eq. (1)Project Average: DISPLAYFORM0 where INLINEFORM0 is a INLINEFORM1 weight matrix, and INLINEFORM2 is a 1600 bias vector.LSTM: apply Eq. (5-11) on INLINEFORM0 to construct the 1600-dimension INLINEFORM1 sentence embedding.Max-CNN: apply Eq. (2-4) on INLINEFORM0 to construct the 1600-dimension INLINEFORM1 sentence embedding.We report the results of these methods in Table TABREF49 . Overall, our M-MaxLSTM-CNN shows competitive performances in these tasks. Especially in the STS task, M-MaxLSTM-CNN outperforms the state-of-the-art methods on the two datasets. Because STSB includes complicated samples compared to SICK, the performances of methods on STSB are quite lower. In STSB, the prior top performance methods use ensemble approaches mixing hand-crafted features (word alignment, syntactic features, N-gram overlaps) and neural sentence representations, while our approach is only based on a neural sentence modeling architecture. In addition, we observed that InferSent shows the strong performance on SICK-R but quite low on STSB while our model consistently obtains the strong performances on both of the datasets. InferSent uses transfer knowledge on textual entailment data, consequently it obtains the strong performance on this entailment task.According to BIBREF31 , using Word Average as the compositional architecture outperforms the other architectures (e.g., Project Average, LSTM) for STS tasks. In a multiple word embeddings setting, however, Word Average does not show its efficiency. Each word embedding model has its own architecture as well as objective function. These factors makes the vector spaces of word embeddings are different. Therefore, we intuitively need a step to learn or refine a representation from a set of pre-trained word embeddings rather than only averaging them. Because Project Average model, LSTM model and Max-CNN model have their parameters for learning sentence embeddings, they significantly outperform Word Average model.We observed that MaxLSTM-CNN outperforms Max-CNN in both of the settings (i.e., sentence-sentence comparison, Multi-level comparison). As mentioned in Section 1, Max-CNN ignores the property of word order. Therefore, our model achieves improvement compared to Max-CNN by additionally employing LSTM for capturing this property.We only applied Multi-level comparison on Max-CNN and MaxLSTM-CNN because these encoders generate multi-aspect word embeddings. The experimental results prove the efficiency of using Multi-level comparison. In the textual entailment dataset SICK-E, the task mainly focuses on interpreting the meaning of a whole sentence pair rather than comparing word by word. Therefore, the performance of Multi-level comparison is quite similar to sentence-sentence comparison in the SICK-E task. This is also the reason why LSTM, which captures global relationships in sentences, has the strong performance in this task.Evaluation of exploiting multiple pre-trained word embeddings	In this section, we evaluate the efficiency of using multiple pre-trained word embeddings. We compare our multiple pre-trained word embeddings model against models using only one pre-trained word embedding. The same objective function and Multi-level comparison are applied for these models. In case of using one pre-trained word embedding, the dimension of LSTM and the number of convolutional filters are set to the length of the corresponding word embedding. Table TABREF57 shows the experimental results of this comparison. Because the approach using five word embeddings outperforms the approaches using two, three, or four word embeddings, we only report the performance of using five word embeddings. We also report INLINEFORM0 which is the proportion of vocabulary available in a pre-trained word embedding. SICK dataset ignores idiomatic multi-word expressions, and named entities, consequently the INLINEFORM1 of SICK is quite high.We observed that no word embedding has strong results on all the tasks. Although trained on the paraphrase database and having the highest INLINEFORM0 , the SL999 embedding could not outperform the Glove embedding in SICK-R. HCTI BIBREF5 , which is the current state-of-the-art in the group of neural representation models on STSB, also used the Glove embedding. However, the performance of HTCI in STSB ( INLINEFORM1 ) is lower than our model using the Glove embedding. In SICK-R, InferSent BIBREF23 achieves a strong performance ( INLINEFORM2 ) using the Glove embedding with transfer knowledge, while our model with only the Glove embedding achieves a performance close to the performance of InferSent. These results confirm the efficiency of Multi-level comparison.In STSB and MRPC, as employing the five pre-trained embeddings, the INLINEFORM0 is increased. This factor limits the number of random values when initializing word embedding representations because a word out of a pre-trained word embedding is assigned a random word embedding representation. In other words, a word out of a pre-trained word embedding is assigned a random semantic meaning. Therefore, the increase of the INLINEFORM1 improves the performance of measuring textual similarity. In STSB and MRPC, our multiple pre-trained word embedding achieves a significant improvement in performance compared against using one word embedding. In SICK-R and SICK-E, although the INLINEFORM2 is not increased when employing five pre-trained embeddings, the performance of our model is improved. This fact shows that our model learned an efficient word embedding via these pre-trained word embeddings.Conclusion	In this work, we study an approach employing multiple pre-trained word embeddings and Multi-level comparison for measuring semantic textual relation. The proposed M-MaxLSTM-CNN architecture consistently obtains strong performances on several tasks. Compared to the state-of-the art methods in STS tasks, our model does not require handcrafted features (e.g., word alignment, syntactic features) as well as transfer learning knowledge. In addition, it allows using several pre-trained word embeddings with different dimensions.Future work could apply our multiple word embeddings approach for transfer learning tasks. This strategy allows making use of pre-trained word embeddings as well as available resources.Acknowledgments	This work was done while Nguyen Tien Huy was an intern at Toshiba Research Center.","['which pretrained embeddings were experimented with?', 'what datasets where used?', 'what are the state of the art methods they compare with?']","['Unacceptable', 'Unacceptable', 'Unacceptable']"
34,"Reverse-Engineering Satire, or""Paper on Computational Humor Accepted Despite Making Serious Advances""	Humor is an essential human trait. Efforts to understand humor have called out links between humor and the foundations of cognition, as well as the importance of humor in social engagement. As such, it is a promising and important subject of study, with relevance for artificial intelligence and human-computer interaction. Previous computational work on humor has mostly operated at a coarse level of granularity, e.g., predicting whether an entire sentence, paragraph, document, etc., is humorous. As a step toward deep understanding of humor, we seek fine-grained models of attributes that make a given text humorous. Starting from the observation that satirical news headlines tend to resemble serious news headlines, we build and analyze a corpus of satirical headlines paired with nearly identical but serious headlines. The corpus is constructed via Unfun.me, an online game that incentivizes players to make minimal edits to satirical headlines with the goal of making other players believe the results are serious headlines. The edit operations used to successfully remove humor pinpoint the words and concepts that play a key role in making the original, satirical headline funny. Our analysis reveals that the humor tends to reside toward the end of headlines, and primarily in noun phrases, and that most satirical headlines follow a certain logical pattern, which we term false analogy. Overall, this paper deepens our understanding of the syntactic and semantic structure of satirical news headlines and provides insights for building humor-producing systems.	Introduction	Humor is a uniquely human trait that plays an essential role in our everyday lives and interactions. Psychologists have pointed out the role of humor in human cognition, including its link to the identification of surprising connections in learning and problem solving, as well as the importance of humor in social engagement BIBREF0 . Humor is a promising area for studies of intelligence and its automation: it is hard to imagine a computer passing a rich Turing test without being able to understand and produce humor. As computers increasingly take on conversational tasks (e.g., in chat bots and personal assistants), the ability to interact with users naturally is gaining importance, but human–computer interactions will never be truly natural without giving users the option to say something funny and have it understood that way; e.g., recent work has shown that misunderstanding of playful quips can be the source of failures in conversational dialog in open-world interaction BIBREF1 .Given how tied humor is to the human condition, the phenomenon has challenged some of the greatest thinkers throughout history and has been the subject of much academic research across over 20 disciplines BIBREF2 , including computer science BIBREF3 , where researchers have developed algorithms for detecting, analyzing, and generating humorous utterances (cf. Sec. ""Related work"" ).The automated analysis of humor is complicated by the fact that most humorous texts have a complex narrative structure that is difficult to disentangle; e.g., typical jokes—the type of humorous text studied most in the literature—carefully set the stage to build certain expectations in the audience, which are then turned upside down in the punchline. To circumvent the difficulties imposed by narrative structure, we focus on a specific humorous genre: satirical news. Satirical news articles, on the surface, mimic the format typical of mainstream journalism, but unlike serious news articles, they do not aim to relate facts, but rather to ridicule individuals, groups, or society. Crucially, though, satirical news stories are typically written headlinefirst: only if the headline is funny in and of itself is the rest of the story written BIBREF4 . This is markedly different from real news stories and means that satirical news headlines can be studied in isolation from the full stories, whose essence they convey in a concise form with minimal narrative structure.An additional advantage of satirical headlines is that they mimic the formulaic style of serious news headlines, which limits their syntactic variability and allows us to better control for syntax and focus on semantics. Moreover, satirical headlines are similar to serious news headlines not only in style but also in content: changing a single word often suffices to make a satirical headline sound like serious news.Running example. For instance, changing God to Bob Dylan turns the satirical headline God diagnosed with bipolar disorder, which was published in the satirical newspaper The Onion, into Bob Dylan diagnosed with bipolar disorder, which could appear verbatim in a serious newspaper.A large corpus of such pairs of satirical and similarbutseriouslooking headlines would open up exciting opportunities for humor research. For instance, it would allow us to understand why a satirical text is funny at a finer granularity than previously possible, by identifying the exact words that make the difference between serious and funny. This is a striking difference from most previous research, where usually the average satirical headline is compared to the average serious one BIBREF5 . Moreover, while the principal goal of this research has been to achieve new insights about humor, we also imagine new applications. For example, if we attained a grasp on the precise differences between satirical and serious headlines, we might be able to create procedures for transforming real news headlines into satirical headlines with minimal changes.To create an aligned corpus, a first idea would be to automatically pair satirical with serious news headlines: start with a satirical headline and find the most similar serious headline written around the same time. It is hard to imagine, though, that this process would yield many pairs of high lexical and syntactic similarity. An alternative idea would be to use crowdsourcing: show serious headlines to humans and ask them to turn them into satirical headlines via minimal edits. Unfortunately, this task requires a level of creative talent that few people have. Even at The Onion, America's most prominent satirical newspaper, only 16 of 600 headlines generated each week (less than 3%) are accepted BIBREF4 .The crucial observation is that the task is much easier in the reverse direction: it is typically straightforward to remove the humor from a satirical headline by applying small edits that turn the headline into one that looks serious and could conceivably be published in a real news outlet. In other words, reversing the creative effort that others have already invested in crafting a humorous headline requires much less creativity than crafting the headline in the first place. We thus adopt this reversecrowdsourcing approach, by designing a game with a purpose BIBREF6 .The game is called Unfun.me and is described graphically in Fig. 1 . A player $A$ of the game is given a satirical news headline $h$ and asked to modify it in order to fool other players into believing that the result $h^{\prime }$ is a real headline from a serious news outlet. The reward $R_A(h,h^{\prime })$ received by the player $A$ who modified the satirical headline increases with the fraction of other players rating the modified headline $h^{\prime }$ as serious and decreases with the number of words changed in the original headline $h$ .Contributions. Our main contributions are twofold. First, we present Unfun.me, an online game for collecting a corpus of pairs of satirical news headlines aligned to similarbutseriouslooking headlines (Sec. ""Game description: Unfun.me"" ). Second, our analysis of these pairs (Sec. ""Analysis of game dynamics"" – ""Semantic analysis of aligned corpus"" ) reveals key properties of satirical headlines at a much finer level of granularity than prior work (Sec. ""Related work"" ). Syntactically (Sec. ""Syntactic analysis of aligned corpus"" ), we conclude that the humor tends to reside in noun phrases, and with increased likelihood toward the end of headlines, giving rise to what we term “micropunchlines”. Semantically (Sec. ""Semantic analysis of aligned corpus"" ), we observe that original and modified headlines are usually opposed to each other along certain dimensions crucial to the human condition (e.g., high vs. low stature, life vs. death), and that satirical headlines are overwhelmingly constructed according to a falseanalogy pattern. We conclude the paper by discussing our findings in the context of established theories of humor (Sec. ""Discussion and future work"" ).Game description: Unfun.me	Here we introduce Unfun.me, our game for collecting pairs of satirical and similarbutseriouslooking headlines. The game, available online at http://unfun.me and visually depicted in Fig. 1 , challenges players in two tasks.Task 1: Unfun the headline! This is the core task where the reverseengineering of satire happens (left panel in Fig. 1 ). A player, $A$ , is given a satirical headline $h$ and is asked to turn it into a headline $h^{\prime }$ that could conceivably have been published by a serious news outlet, by changing as few words as possible.Task 2: Real or not? Whether on purpose or not, player $A$ may have done a bad job in task 1, and $h^{\prime }$ may still be humorous. Detecting and filtering such cases is the purpose of task 2 (right panel in Fig. 1 ), where $h^{\prime }$ is shown to another player, $B$ , who is asked to indicate her belief $p_B(h^{\prime })$ that $h^{\prime }$ comes from a serious news outlet using a slider bar ranging from 0% to 100%. We shall refer to $p_B(h^{\prime })$ as $B$ 's seriousness rating of $h^{\prime }$ . For reasons that will become clear below, player $B$ also indicates her belief $h^{\prime }$0 for a second, unmodified headline $h^{\prime }$1 (unrelated to $h^{\prime }$2 ) that originates from either a serious or a satirical news outlet. The two headlines $h^{\prime }$3 and $h^{\prime }$4 are presented in random order, in order to avoid biases.For the purpose of incentivizing players to make highquality contributions, we reward them as follows.Reward for task 1. As player $A$ is supposed to remove the humor from $h$ via a minimal modification, his reward $R_A(h,h^{\prime })$ increases (1) with the average rating $r(h^{\prime })$ that the modified headline $h^{\prime }$ receives from all $n$ players $B_1, \dots , B_n$ who rate it and (2) with the similarity $s(h,h^{\prime })$ of $h$ and $h^{\prime }$ : $$\text{where}
\;\;\;\;
r(h^{\prime }) = \frac{1}{n} \sum _{i=1}^n p_{B_i}(h^{\prime }),
\;\;\;\;
s(h,h^{\prime }) = 1-\frac{d(h,h^{\prime })}{\max \lbrace |h|,|h^{\prime }|\rbrace },$$   (Eq. ) $h$0 where, in turn, $|x|$ is the number of tokens (i.e., words) in a string $x$ , and $d(h,h^{\prime })$ , the tokenbased edit distance BIBREF7 between $h$ and $h^{\prime }$ , i.e., the minimum number of insertions, deletions, and substitutions by which $h$ can be transformed into $h^{\prime }$ , considering as the basic units of a string its tokens, rather than its characters. The geometric mean was chosen in Eq. 2 because it is zero whenever one of the two factors is zero (which is not true for the more standard arithmetic mean): a modified headline that seems very serious, but has nothing to do with the original, should not receive any points, nor should a headline that is nearly identical to the original, but retains all its humor.Reward for task 2. Since player $B$ 's very purpose is to determine whether $h^{\prime }$ is without humor, we do not have a groundtruth rating for $h^{\prime }$ . In order to still be able to reward player $B$ for participating in task 2, and to incentivize her to indicate her true opinion about $h^{\prime }$ , we also ask her for her belief $p_B(g)$ regarding a headline $g$ for which we do have the ground truth of “serious” vs. “satirical”. The reward $R_B(g)$ that player $B$ receives for rating headline $g$ is then $$R_B(g) = {\left\lbrace \begin{array}{ll}
\log (p_B(g)) & \text{if $g$ is serious,}\\
\log (1-p_B(g)) & \text{if $g$ is satirical.}\\
\end{array}\right.}$$   (Eq. 3) Note that this is a proper scoring rule BIBREF8 , i.e., player $B$ maximizes her expected reward by indicating her true belief. This would not be true for the more straightforward scoring formula without logarithms, which would drive players to report beliefs of 0 or 1 instead of their true beliefs. Also, as $h^{\prime }$ and $g$ are shown in random order, $B$ does not know which is which, and her optimal strategy is to indicate her true belief on both.Overall game flow. Whenever a user wants to play, we generate a type-1 task with probability $\alpha =1/3$ and a type-2 task with probability $1-\alpha =2/3$ , such that we can collect two ratings per modified headline. As mentioned, ratings from task 2 can serve as a filter, and we can increase its precision at will by decreasing $\alpha $ . To make rewards more intuitive and give more weight to the core task 1, we translate and scale rewards such that $R_A(\cdot ,\cdot ) \in [0, 1000]$ and $R_B(\cdot ) \in [0, 200]$ . We also implemented additional incentive mechanisms such as badges, high-score tables, and immediate rewards for participating, but we omit the details for space reasons.Satirical and serious headlines. The game requires corpora of satirical as well as serious news headlines as input. Our satirical corpus consists of 9,159 headlines published by the wellknown satirical newspaper The Onion; our serious corpus, of 9,000 headlines drawn from 9 major news websites.Data and code. We make the data collected via Unfun.me, as well as our code for analyzing it, publicly available online BIBREF9 .Analysis of game dynamics	Via Unfun.me, we have collected 2,801 modified versions $h^{\prime }$ for 1,191 distinct satirical headlines $h$ (2.4 pairs per satirical headline). All but 7 modified headlines have received at least one rating, and 1,806 (64%), at least two (mean/median: 2 ratings per modified headline). The modified headlines (ratings) came from 582 (546) unique user ids (mean/median: 4.8/2 modified headlines per user; 10/4 ratings per user).We start by analyzing the edit operations players perform in task 1 and the seriousness ratings they provide in task 2. The main objects of study are pairs $(h,h^{\prime })$ consisting of an original satirical headline $h$ and a modified version $h^{\prime }$ , which we shall simply call pairs in what follows.Edit distance. The first interesting question is how much players tend to modify original satirical headlines $h$ in order to expunge the humor from them. We quantify this notion via the tokenbased edit distance $d(h,h^{\prime })$ between the satirical headline $h$ and the modified version $h^{\prime }$ (cf. Sec. ""Game description: Unfun.me"" ). Fig. ""Semantic analysis of aligned corpus"" , which plots the distribution of edit distance, shows that very small edits are most common, as incentivized by the reward structure of the game (Eq. 2 ). In particular, 33% of all pairs have the smallest possible edit distance of 1, and 57% (69%) have a distance up to 2 (3).Tradeoff of edit distance vs. seriousness rating. The reward structure of the game (Eq. 2 ) does not, however, exclusively encourage small edits. Rather, there is a tradeoff: larger edits (bad) make it easier to remove the humor (good), while smaller edits (good) run the risk of not fully removing the humor (bad). Fig. ""Related work"" , which plots the mean average seriousness rating $r(h^{\prime })$ of modified headlines $h^{\prime }$ as a function of the edit distance $d(h,h^{\prime })$ , shows how this tradeoff plays out in practice. For edit distances between 1 and 5 (83% of all pairs, cf. Fig. ""Semantic analysis of aligned corpus"" ), seriousness ratings correlate positively with edit distance. In particular, it seems harder to remove the humor by changing one word than by changing two words, whereas the marginal effect is negligible when allowing for even larger edits. The positive correlation does not hold for the much smaller number (17%) of pairs with an edit distance above 5. Inspecting the data, we find that this is caused by headlines so inherently absurd that even large edits cannot manage to remove the humor from them.Seriousness ratings. Recall that, in task 2, players attribute seriousness ratings to modified headlines $h^{\prime }$ , as well as to unmodified serious or satirical headlines $g$ . We find that, in all three cases, the distribution of seriousness ratings is bimodal, with extreme values close to 0 or 1 being most common. Hence, we binarize ratings into two levels, “satirical” (rating below 0.5) and “serious” (rating above 0.5).In order to see how people rate serious, satirical, and modified headlines, respectively, Table 1 aggregates ratings by headline (considering only the 1,806 headlines with at least two ratings) and splits the headlines into three groups: “consensus serious” (over 50% “serious” ratings), “no consensus” (exactly 50%), and “consensus satirical” (under 50%).We make two observations. First, modified headlines $h^{\prime }$ (column 3 of Table 1 ) are distributed roughly evenly over the three groups; i.e., there are about as many headlines from which the humor has been successfully removed (“consensus serious”) as not (“consensus satirical”). The most useful modified headlines for our purposes are those from the “consensus serious” group, as they likely do not carry the humor of the original $h$ anymore. Hence, we shall restrict our subsequent analyses to the corresponding 654 successful pairs. Second, the ratings are heavily skewed toward the ground truth for unmodified serious (column 1) and satirical (column 2) headlines; i.e., players can typically well distinguish serious from satirical headlines (but cf. discussion in Sec. ""Discussion and future work"" ).Insertions, deletions, substitutions. When computing the edit distance $d(h,h^{\prime })$ using dynamic programming, we can also keep track of an optimal sequence of edit operations (insertions, deletions, substitutions) for transforming $h$ into $h^{\prime }$ BIBREF7 . In Fig. ""Discussion and future work"" , we plot the distribution of edit operations, macroaveraged over all pairs. We see that substitutions clearly dominate (61%), followed by deletions (34%), with insertions being very rare (5%).Pairs with edit distance 1 are particularly interesting, as they are the most similar, as well as the most frequent (Fig. ""Semantic analysis of aligned corpus"" , footnote UID9 ). Also, the optimal edit sequence may not be unique in general, but for edit distance 1 it is. Hence, Fig. ""Discussion and future work"" also displays the distribution over edit operations for pairs with edit distance 1 only. Here, substitutions dominate even more (77%), and insertions are even rarer (2%).Reversing the direction of the editing process, we hence conclude that writers of satirical headlines tend to work overwhelmingly by substituting words in (hypothetical) similarbutserious headlines, and to a certain degree by adding words, but very rarely by deleting words.Syntactic analysis of aligned corpus	Next, we go one level deeper and ask: what parts of a satirical headline should be modified in order to remove the humor from it, or conversely, what parts of a serious headline should be modified in order to add humor? We first tackle this question from a syntactic perspective, before moving to a deeper, semantic perspective in Sec. ""Semantic analysis of aligned corpus"" .From tokens to chunks. We analyze syntax at an intermediate level of abstraction between simple sequences of part-of-speech (POS) tags and complex parse trees, by relying on a chunker (also called shallow parser). We use OpenNLP's maximum entropy chunker BIBREF10 , after retraining it to better handle pithy, headlinestyle text. The chunker takes POStagged text as input and groups subsequent tokens into meaningful phrases (chunks) without inferring the recursive structure of parse trees; e.g., our running example (Sec. ""Introduction"" ) is chunked as [NP Bob Dylan] [VP diagnosed] [PP with] [NP bipolar disorder] (chunk labels expanded in Table 2 ). Chunks are handy because they abstract away lowlevel details; e.g., changing God to Bob Dylan requires a tokenbased edit distance of 2, but a chunkbased distance of only 1, where the latter is more desirable because it more closely captures the conceptual modification of one entity being replaced by another entity.Chunking all 9,159 original headlines from our The Onion corpus, we find the most frequent chunk pattern to be NP VP NP PP NP (4.8%; e.g., H2 in Table 3 ), followed by NP VP NP (4.3%; e.g., H4) and NP VP PP NP (3.3%; e.g., H9).To control for syntactic effects, it is useful to study a large number of pairs $(h,h^{\prime })$ where all original headlines $h$ follow a fixed syntactic pattern. We therefore gave priority to headlines of the most frequent pattern (NP VP NP PP NP) for a certain time period when sampling satirical headlines as input to task 1, such that, out of all 2,801 $(h,h^{\prime })$ pairs collected in task 1, $h$ follows that pattern in 21% of all cases.Chunk-based edit distance. Recomputing edit distances at the chunk level, rather than the token level, we obtain the chunkbased edit distance distribution of Fig. ""Conclusion"" . It resembles the tokenbased edit distance distribution of Fig. ""Semantic analysis of aligned corpus"" , with the difference that the smallest possible distance of 1 is even more prevalent (52% vs. 33% of pairs), due to the fact that modifying a single chunk frequently corresponds to modifying multiple tokens. Since, moreover, the vast majority (97%) of all singlechunk edits are substitutions, we now focus on 254 $(h,h^{\prime })$ pairs where exactly one chunk of $h$ has been modified (henceforth singlesubstitution pairs). This accounts for about half of all successful pairs (after discarding pairs that were problematic for the chunker).Dominance of noun phrases. We now ask which syntactic chunk types (noun phrases, verb phrases, etc.) are modified to remove humor. In doing so, we need to be careful, as some chunk types are more common a priori than others; e.g., 59% of all chunks in original satirical headlines are noun phrases, 20%, verb phrases, etc. We therefore compare the empirical distribution of modified chunks with this prior distribution, via the ratio of the two (termed lift). Table 2 shows that noun phrases constitute 89% of the modified chunks (lift 1.52), whereas all other chunk types are less frequent than under the prior. We conclude that the humor of satirical news headlines tends to reside in noun phrases.Micro-punchlines. We now ask where in terms of location within a headline the humor tends to reside. To answer this question, we compute the position of the modified chunk in each headline's chunk sequence and plot the distribution of modified positions in Fig. 3 . We see that, regardless of headline length, modifications to the last chunk are particularly overrepresented. This is an important finding: we have previously (Sec. ""Introduction"" ) argued that satirical headlines consist of a punchline only, with minimal narrative structure, and indeed it was this very intuition that led us to investigate headlines in isolation. Given Fig. 3 , we need to revise this statement slightly: although satirical headlines consist of a single sentence, they are often structured—at a microlevel—akin to more narrative jokes, where the humorous effect also comes with the very last words. Put differently, the final words of satirical headlines often serve as a “micropunchline”.Semantic analysis of aligned corpus	After characterizing aligned pairs syntactically, we now move to the semantic level. We first analyze the aligned pairs obtained from Unfun.me and later discuss our findings in the broader context of established theories of humor (Sec. ""Discussion and future work"" ).Example. Before a more general analysis, let us first consider again our running example (Sec. ""Introduction"" ), God diagnosed with bipolar disorder. This satirical headline works by blending two realms that are fundamentally opposed—the human and the divine—by talking about God as a human. Although the literally described situation is impossible (God is perfect and cannot possibly have a disease), the line still makes sense by expressing a crucial commonality between bipolar humans and God, namely that both may act unpredictably. But for humans, being unpredictable (due to bipolarity) is a sign of imperfection, whereas for God it is a sign of perfection (“The Lord moves in mysterious ways”), and it is this opposition that makes the line humorous.The main advantage of our aligned corpus is that it lets us generalize this ad-hoc analysis of a particular example to a large and representative set of satirical headlines by pinpointing the essential, humorcarrying words in every headline: if the humor has been successfully removed from a headline $h$ by altering certain words, then we know that these very words are key to making $h$ funny.This is especially true for singlesubstitution pairs; e.g., in the running example, God was replaced by Bob Dylan (a particular human), giving rise to the serioussounding Bob Dylan diagnosed with bipolar disorder. The automatically extracted chunk pair {God, Bob Dylan} surfaces both the crucial commonality in the context of the headline (unpredictability) and the crucial opposition (God vs. human; unpredictability as a good vs. bad trait).While the semantic analysis of original vs. substituted chunks may be difficult to automate, having access to explicit chunk pairs tremendously facilitates a largescale human analysis. Conducting such an analysis revealed that the above pattern of a crucial commonality combined with a crucial opposition occurs in a large fraction of satirical headlines, and particularly in nearly all singlesubstitution pairs.Script opposition. The crucial opposition has been called script opposition by humor theorists (cf. Sec. ""Discussion and future work"" ), and we henceforth adopt the same term. Inspecting all 254 singlesubstitution pairs, we found each pair to be in at least one of 6 oppositions, all representing “good”-vs.-“bad” dichotomies that are essential to the human condition, such as high/low stature, life/death, or nonobscene/obscene. All 6 oppositions, alongside examples, are listed in Table 3 .We manually labeled all pairs with their (sometimes multiple) oppositions and observe that most pairs (68%) feature an opposition of high/low stature (as in the running example), and surprisingly few pairs (7%), one of nonobscene/obscene. Due to its dominance, Table 3 further splits the high/low stature opposition into 10 subtypes.Main mechanism: false analogy. Moving to a more formal analysis, we represent the running example schematically in Table 3 , while Table 3 abstracts away from the example and depicts the generic template it implements, which may be verbalized as follows. The pair involves two entities, $x$ (God) and $x^{\prime }$ (Bob Dylan), who share a crucial common property $P$ (unpredictability), but whereas statement $P(x^{\prime })$ (“Bob Dylan is unpredictable”) could potentially entail the serious headline $H(x^{\prime })=h^{\prime }$ (Bob Dylan diagnosed with bipolar disorder), the analogous statement $P(x)$ (“God is unpredictable”) cannot entail the analogous headline $H(x)=h$ (God diagnosed with bipolar disorder), for $x$ and $x^{\prime }$ are crucially opposed via one of the script oppositions of Table 3 (religion/no religion; or, God, for whom unpredictability is a sign of perfection, vs. humans, for whom it is a sign of imperfection). Hence, we call this mechanism false analogy.As the examples of Table 3 show, the analogy is never marked lexically via words such as like; rather, it is evoked implicitly, e.g., by blending the two realms of human psychiatry and biblical lore into a single headline. Only the satirical headline $H(x)$ itself (red box in Table 3 ) is explicit to the reader, whereas $x^{\prime }$ and $P$ (and thus all the other 3 boxes) need to be inferred. A main advantage of our method is that it also makes $x^{\prime }$ explicit and thereby facilitates inferring $P$ and thus the semantic structure that induces humor (as in Table 3 ).We emphasize that the script opposition that invalidates the logical step from $P(x)$ to $H(x)$ is not arbitrary, but must be along certain dimensions essential to human existence and contrasting “good” vs. “bad” (Table 3 ). Interestingly, in typical jokes, the “good” side is explicit and the “bad” side must be inferred, whereas in satirical headlines, either the “good” or the “bad” side may be explicit. And indeed, as shown by the examples of Table 3 (where the “good” side is marked in bold), satirical headlines differ from typical jokes in that they tend to make the “bad” side explicit.Single vs. multiple edit operations. A large fraction of all headlines from The Onion—and an overwhelming fraction of those in singlesubstitution pairs—can be analyzed with the falseanalogy template of Table 3 (and we indeed encourage the reader to apply it to the examples of Table 3 ). Additionally, many of the pairs with two substitutions also follow this template. H3 in Table 3 , which plays on the opposition of the Federal Reserve being a serious institution vs. Cash4Gold being a dubious enterprise exploiting its customers, exemplifies how, whenever multiple substitutions are applied, they all need to follow the same opposition (e.g., Fed : Cash4Gold = $85 million : $85 = serious : dubious).Related work	The most widely accepted theory of verbal humor is the so-called General Theory of Verbal Humor by Attardo and Raskin attardo1991script, an extension of Raskin's raskin1985semantic Semantic-Script Theory of Humor, which we summarize when discussing our findings in its context in Sec. ""Discussion and future work"" .Much follow-up work has built on these theories; see the excellent primer edited by Raskin raskin2008primer. Here, we focus on contributions from computer science, where most work has been on the detection of humor in various forms, e.g., irony BIBREF11 , BIBREF12 , sarcasm BIBREF13 , BIBREF14 , and satire BIBREF15 , BIBREF16 , sometimes with the goal of deciding which of two texts is funnier BIBREF17 . These works use documents or sentences as the smallest unit of analysis, whereas we operate at a finer granularity, analyzing the very words causing the switch from serious to funny.Another cluster of work has considered the generation of humor, mostly via fixed templates such as acronyms BIBREF18 , puns BIBREF19 , BIBREF20 , twoliners BIBREF21 , or crossreference ambiguity BIBREF22 .Finally, our work also relates to efforts of constructing humor corpora BIBREF23 , BIBREF24 . Here, too, we increase the granularity by actively generating new data, rather than compiling humorous texts that have already been produced. Crucially, ours is a corpus of aligned pairs, rather than individual texts, which enables entirely novel analyses that were infeasible before.Discussion and future work	Summary of findings. Comparing satirical to similarbutseriouslooking headlines within the pairs collected via Unfun.me reveals that the humor tends to reside in the final words of satirical headlines, and particularly in noun phrases. In order to remove the humor, players overwhelmingly replace one phrase with another; rarely do they delete phrases, and nearly never introduce new phrases. Reversing the direction of the editing process, this implies that the most straightforward way of producing satire from a serious headline is to replace a trailing noun phrase with another noun phrase.One may, however, not just replace any noun phrase with any other noun phrase; rather, the corresponding scripts need to be opposed along one of a few dimensions essential to the human condition and typically pitting “good” vs. “bad”. Also, the two opposing scripts need to be connected via certain subtle mechanisms, and we pointed out false analogy as one prominent mechanism. These findings echo the predictions made by the prevailing theory of humor. We now summarize this theory and discuss our results in its context.Relation to SemanticScript Theory of Humor. As mentioned (Sec. ""Related work"" ), the most influential theory of verbal humor has been Raskin's raskin1985semantic SemanticScript Theory of Humor, which posits a twofold necessary condition for humorous text: (1) the text must be compatible with two different semantic scripts (simply put, a semantic script is a concept together with its commonsense links to other concepts); and (2) the two scripts must be opposed to each other along one of a small number of dimensions.The second criterion is key: the mere existence of two parallel compatible scripts is insufficient for humor, since this is also the case in plain, nonhumorous ambiguity. Rather, one of the two scripts must be possible, the other, impossible; one, normal, the other, abnormal; or one, actual, the other, nonactual. These oppositions are abstract, and Raskin [p. 127]raskin1985semantic gives several more concrete classes of opposition, which closely mirror the dimensions we empirically find in our aligned pairs (Table 3 ). Our results thus confirm the theory empirically. But the advantages of our methodology go beyond, by letting us quantify the prevalence of each opposition. In addition to the concrete oppositions of Table 3 , we also counted how pairs distribute over the above 3 abstract oppositions, finding that most satirical headlines are of type possible/impossible (64%), followed by normal/abnormal (28%), and finally actual/nonactual (8%).In typical jokes, one of the two scripts (the so-called bona fide interpretation) seems more likely given the text, so it is in the foreground of attention. But in the punchline it becomes clear that the bona fide interpretation cannot be true, causing initial confusion in the audience, followed by a search for a more appropriate interpretation, and finally surprise or relief when the actually intended, non–bona fide script is discovered. To enable this process on the recipient side, the theory posits that the two scripts be connected in specific ways, via the so-called logical mechanism, which resolves the tension between the two opposed scripts.Attardo [p. 27]attardo2001humorous gives a comprehensive list of 27 logical mechanisms. While our analysis (Sec. ""Semantic analysis of aligned corpus"" ) revealed that one mechanism—false analogy—dominates in satirical headlines, several others also occur: e.g., in figure–ground reversal, the real problem (the “figure”) is left implicit, while an unimportant side effect (the “ground”) moves into the focus of attention (e.g., H12 in Table 3 : waterboarding, like baths, does waste water, but the real problem is ethical, not ecological). Another common mechanism—cratylism—plays with the assumption prevalent in puns that phonetic implies semantic similarity (e.g., H11 in Table 3 ).Satire is a form of art, and the examples just cited highlight that it is often the creative combination of several mechanisms that makes a headline truly funny. Beyond the bare mechanism, the precise wording matters, too: e.g., either 16th Lassie or 17th Lassie would suffice to make H6 in Table 3 funny, but the combination 16th or 17th Lassie is wittier, as it implies not only that Lassie has been played by many dogs, but also that people do not care about them, thus reinforcing the human/animal opposition.We conclude that, while satirical headlines—as opposed to typical jokes—offer little space for complex narratives, they still behave according to theories of humor. Our contributions, however, go beyond validating these theories: the aligned corpus lets us quantify the prevalence of syntactic and semantic effects at play and reveals that the dominant logical mechanism in satirical headlines is false analogy.Satiricalheadline generation. This points to a way of generating satirical headlines by implementing the falseanalogy template of Table 3 : pick an entity $x$ (e.g., Pepsi) and a central property $P(x)$ of $x$ (e.g., “Pepsi is a popular drink”); then pick another entity $x^{\prime }$ for which $P(x^{\prime })$ also holds, but which is opposed to $x$ along one of the axes of Table 3 (e.g., Bordeaux wine, which is in a high/low stature [sublime/mundane] opposition to Pepsi); and finally generate a headline $H(x^{\prime })$ based on $P(x^{\prime })$ (e.g., 2018 Bordeaux vintage benefits from outstanding grape harvest) which cannot be seriously formulated for $x$ instead $x^{\prime }$ , due to the opposition, yielding the satirical $P(x)$0 (e.g., 2018 Pepsi vintage benefits from outstanding highfructose corn harvest, where we analogously replaced grape with highfructose corn, cf. Sec. ""Semantic analysis of aligned corpus"" ). The subtitle of the present paper was also generated this way.Most humans are unaware of the logical templates underlying satire, while machines have difficulties finding entity pairs opposed in specific ways and formulating pithy headline text. We hence see promise in a hybrid system for coupling the respective strengths of humans and machines, where the machine guides the human through the template instantiation process while relying on the human for operations such as finding appropriate entities for substitution etc.Human perception of satirical vs. serious news. Recall that in task 2 (Sec. ""Game description: Unfun.me"" ), players also rate unmodified satirical and serious headlines $g$ with respect to how likely they consider them to be serious. Table 1 shows that, although players are generally good at distinguishing satire from real news, they do make mistakes: 10% of serious headlines are consistently misclassified as satirical (e.g., Schlitz returns, drums up nostalgic drinkers), and 8% of satirical headlines, as serious (e.g., Baltimore looking for safer city to host Super Bowl parade). Studying these misunderstood headlines can yield interesting insights into how readers process news, especially in an age where “fake news” is becoming a ubiquitous scourge. We leave this analysis for future work.Beyond humor. The mechanism underlying Unfun.me defines a general procedure for identifying the essential portion of a text that causes the text to have a certain property. In our case, this property is humor, but when asking players instead to remove the rudeness, sexism, euphemism, hyperbole, etc., from a given piece of text, we obtain a scalable way of collecting finegrained supervised examples for better understanding these ways of speaking linguistically.Conclusion	Humor is key to human cognition and holds questions and promise for advancing artificial intelligence. We focus on the humorous genre of satirical news headlines and present Unfun.me, an online game for collecting pairs of satirical and similarbutseriouslooking headlines, which precisely reveal the humorcarrying words and the semantic structure in satirical news headlines. We hope that future work will build on these initial results, as well as on the dataset that we publish with this paper BIBREF9 , in order to make further progress on understanding satire and, more generally, the role of humor in intelligence.","['Where can I access the dataset?', 'Did they release their dataset?', 'Did they use Amazon Mechanical Turk to collect data?', 'Did they use The Onion as their dataset?']","['Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable']"
35,"Unsupervised Question Answering for Fact-Checking	Recent Deep Learning (DL) models have succeeded in achieving human-level accuracy on various natural language tasks such as question-answering, natural language inference (NLI), and textual entailment. These tasks not only require the contextual knowledge but also the reasoning abilities to be solved efficiently. In this paper, we propose an unsupervised question-answering based approach for a similar task, fact-checking. We transform the FEVER dataset into a Cloze-task by masking named entities provided in the claims. To predict the answer token, we utilize pre-trained Bidirectional Encoder Representations from Transformers (BERT). The classifier computes label based on the correctly answered questions and a threshold. Currently, the classifier is able to classify the claims as ""SUPPORTS"" and ""MANUAL_REVIEW"". This approach achieves a label accuracy of 80.2% on the development set and 80.25% on the test set of the transformed dataset.	Introduction	Every day textual information is being added/updated on Wikipedia, as well as other social media platforms like Facebook, Twitter, etc. These platforms receive a huge amount of unverified textual data from all its users such as News Channels, Bloggers, Journalists, Field-Experts which ought to be verified before other users start consuming it. This information boom has increased the demand of information verification also known as Fact Checking. Apart from the encyclopedia and other platforms, domains like scientific publications and e-commerce also require information verification for reliability purposes. Generally, Wikipedia authors, bloggers, journalists and scientists provide references to support their claims. Providing referenced text against the claims makes the fact checking task a little easier as the verification system no longer needs to search for the relevant documents.Wikipedia manages to verify all this new information with a number of human reviewers. Manual review processes introduce delays in publishing and is not a well scalable approach. To address this issue, researchers have launched relevant challenges, such as the Fake News Challenge (BIBREF0), Fact Extraction and VERification (FEVER) (BIBREF1) challenge along with the datasets. Moreover, Thorne and Vlachos (BIBREF2) released a survey on the current models for automated fact-checking. FEVER is the largest dataset and contains around 185k claims from the corpus of 5.4M Wikipedia articles. The claims are labeled as “SUPPORTS”, “REFUTES”, or “NOT ENOUGH INFO”, based on the evidence set.In this paper, we propose an unsupervised question-answering based approach for solving the fact-checking problem. This approach is inspired from the memory-based reading comprehension task that humans perform at an early age. As we know that kids in schools, first read and learn the syllabus content so that they can answer the questions in the exam. Similarly, our model learns a language model and linguistics features in unsupervised fashion from the provided Wikipedia pages.To transform the FEVER dataset into the above-mentioned task, we first generate the questions from the claims. In literature, there are majorly two types of Question Generation systems: Rule-based and Neural Question Generation (NQG) model based. Ali et al. (BIBREF3) proposed a rule-based pipeline to automate the question generation using POS (Part-of-speech) tagging and Named Entity Recognition (NER) tagging from the sentences. Recently, many NQG models have been introduced to generate questions in natural language. Serban et al. (BIBREF4) achieved better performance for question generation utilizing (passage, question, answer) triplets as training data and an encoder-decoder based architecture as their learning model.Du et al. (BIBREF5) introduced a sequence-to-sequence model with an attention mechanism, outperforming rule-base question generation systems. Although the models proposed in (BIBREF6; BIBREF7) are effective, they require a passage to generate the plausible questions which is not readily available in the FEVER dataset. To resolve the issues and to keep the system simple but effective, we chose to generate questions similar to a Cloze-task or masked language modeling task. Such a task makes the problem more tractable as the masked entities are already known (i.e. named entities) and tight as there is only one correct answer for a given question. Later when the answers are generated, due to the question generation process, it becomes very easy to identify the correct answers.We use the BERT's (Bidirectional Encoder Representations from Transformers) (BIBREF8) masked language model, that is pre-trained on Wikipedia articles for predicting the masked entities. Currently, neither the claim verification process nor the question generation process mandates explicit reasoning. For the same reason, it is difficult to put “REFUTES” or “NOT ENOUGH INFO” labels. To resolve this issue, we classify the unsupported claims as “MANUAL_REVIEW” instead of labeling them as “NOT ENOUGH INFO” or “REFUTES”.In the literature, the shared task has been tackled using pipeline-based supervised models (BIBREF9; BIBREF10; BIBREF11). To our knowledge, only BIBREF10 has provided the confusion matrix for each of the labels for their supervised system. For the same reason, we are only providing the comparison of the label accuracy on the “SUPPORTS” label in the results section.System Description	In this section, we explain the design and all the underlying methods that our system has adopted. Our system is a pipeline consisting of three stages: (1) Question Generation, (2) Question Answering, (3) Label Classification. The question generation stage attempts to convert the claims into appropriate questions and answers. It generates questions similar to a Cloze-task or masked language modeling task where the named entities are masked with a blank. Question Answering stage predicts the masked blanks in an unsupervised manner. The respective predictions are then compared with the original answers and exported into a file for label classification. The label classifier calculates the predicted label based on a threshold.System Description ::: Question Generation	The claims generally feature information about one or more entities. These entities can be of many types such as PERSON, CITY, DATE. Since the entities can be considered as the content words for the claim, we utilize these entities to generate the questions. Although function words such as conjunctions and prepositions form relationship between entities in the claims, we currently do not make use of such function words to avoid generating complex questions. The types of entities in a sentence can be recognized by using Stanford CoreNLP (BIBREF12) NER tagger.In our case, FEVER claims are derived from Wikipedia. We first collect all the claims from the FEVER dataset along with “id”, “label” and “verifiable” fields. We don't perform any normalization on the claims such as lowercasing, transforming the spaces to underscore or parenthesis to special characters as it may decrease the accuracy of the NER tagger. These claims are then processed by the NER tagger to identify the named entities and their type. The named entities are then used to generate the questions by masking the entities for the subsequent stage.This process not only transforms the dataset but also transforms the task into a Cloze-task or masked language modeling task. Although the original masked language modeling task masks some of the tokens randomly, here we mask the named entities for generating the questions.System Description ::: Question Answering	Originally inspired by the Cloze-task and developed to learn to predict the masked entities as well as the next sentence, BERT creates a deep bidirectional transformer model for the predictions. Since the FEVER claims are masked to generate the questions, we use BERT to tokenize the claims. We observed that the BERT tokenizer sometimes fails to tokenize the named entities correctly (e.g. Named entity “Taran” was tokenized as “Tara”, “##n”). This is due to the insufficient vocabulary used while training the WordPiece tokenizer.To resolve this, we use Spacy Tokenizer whenever the WordPiece Tokenizer fails. Once the claim is tokenized, we use the PyTorch Implementation of the BERT model (BertForMaskedLM model) to predict the vocabulary index of the masked token. The predicted vocabulary index is then converted to the actual token. We compare the predicted token against the actual answer to calculate the label accuracy based on the classification threshold.System Description ::: Label Classification	In this stage, we compute the final label based on the correctness score of the predictions that we received from the previous stage. The correctness score ($s$) is computed as:where $n_c$ indicates the number of correct questions, and $N$ is the total number of questions generated for the given claim. The label is assigned based on the correctness score ($s$) and the derived threshold ($\phi $) as:Here, the classification threshold ($\phi $) is derived empirically based on the precision-recall curve.System Description ::: Model and Training details	We utilize standard pre-trained BERT-Base-uncased model configurations as given below:Layers: 12Hidden Units: 768Attention heads: 12Trainable parameters: 110MWe fine-tune our model (BERT) on the masked language modeling task on the wiki-text provided along with the FEVER dataset for 2 epochs.Note that Stanford CoreNLP NER tagger and the BERT model are the same for all the experiments and all the sets (development set, test set, training set). We use the same PyTorch library mentioned in Section 2.2 for the fine-tuning as well.Results	For the subtask of question generation, the results in Table TABREF3 show that the system is able to generate questions given a claim with considerably good accuracy. The conversion accuracy is defined as the ratio of the number of claims in which the named entities are extracted to the number of claims. The results also support our assumption that the claims generally feature information about one or more entities.Table TABREF16 shows the performance of our Fact Checking system on the “SUPPORTS” label, the output of our system. We compare the results against two different classification thresholds. Table TABREF3 shows that on an average there are 3 questions generated per claim. Here, $\phi $ = 0.76 suggests that at least 3 out of the 4 questions have to be answered correctly while $\phi $ = 0.67 suggests that at least 2 out of the 3 questions has to be answered correctly for the claim to be classified as “SUPPORTS”.If only 1 question is generated, then it has to be answered correctly for the claim to be classified as “SUPPORTS” in case of both the thresholds.In contrast to the results reported in Table TABREF16, here we consider $\phi $ = 0.76 to be a better classification threshold as it improvises over False Positives considerably over the entire dataset.Although our unsupervised model doesn't support all the labels, to show the effectiveness of the approach, we compare the label accuracy of “SUPPORTS” label against a supervised approach – HexaF. Results from Table TABREF17 suggests that our approach is comparable to HexaF for $\phi $ = 0.76.Error Analysis ::: Question Generation	The typical errors that we observed for the question generation system are due to the known limitations of the NER tagger. Most of the claims that the system failed to generate the questions from contain entity types for which the tagger is not trained.For instance, the claim “A View to a Kill is an action movie.” has a movie title (i.e. A View to a Kill) and a movie genre (i.e. action) but Stanford CoreNLP NER tagger is not trained to identify such type of entities.Error Analysis ::: Question Answering	We describe the most recurrent failure cases of our answering model in the description below.Limitations of Vocabulary. Names like “Burnaby” or “Nikolaj” were not part of the original vocabulary while pre-training the BERT model, which makes it difficult to predict them using the same model. This was one of the most recurring error types.Limitations of Tokenizer. The WordPiece tokenizer splits the token into multiple tokens. E.g. “Taran” into “Tara”, “##n”. In such cases, the answering system predicts the first token only which would be a substring of the correct answer. As we don't explicitly put a rule to avoid such cases, they are considered as incorrect answers.Conclusion	In this paper, we presented a transformer-based unsupervised question-answering pipeline to solve the fact checking task. The pipeline consisted of three stages: (1) Question Generation (similar to a Cloze-task), (2) Question Answering, (3) Label Classification. We use Stanford CoreNLP NER tagger to convert the claim into a Cloze-task by masking the named entities. The Question Generation task achieves almost 90% accuracy in transforming the FEVER dataset into a Cloze-task. To answer the questions generated, we utilize masked language modeling approach from the BERT model. We could achieve 80.2% label accuracy on “SUPPORTS” label. From the results, we conclude that it is possible to verify the facts with the right kind of factoid questions.Future Work	To date, our approach only generates two labels “SUPPORTS” and “MANUAL_REVIEW”. We are working on extending this work to also generate “REFUTED” by improving our question generation framework. We will also work on generating questions using recent Neural Question Generation approaches. Later, to achieve better accuracy for tokenizing as well as answering, we plan to train the WordPiece Tokenizer from scratch.Acknowledgments	The authors thank Dr. Amit Nanavati and Dr. Ratnik Gandhi for their insightful comments, suggestions, and feedback. This research was supported by the TensorFlow Research Cloud (TFRC) program.","['What baseline did they use?', 'What is the threshold?', 'How was the masking done?', 'How large is the FEVER dataset?']","['Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable']"
36,"""Wait, I'm Still Talking!""Predicting the Dialogue Interaction Behavior Using Imagine-Then-Arbitrate Model	Producing natural and accurate responses like human beings is the ultimate goal of intelligent dialogue agents. So far, most of the past works concentrate on selecting or generating one pertinent and fluent response according to current query and its context. These models work on a one-to-one environment, making one response to one utterance each round. However, in real human-human conversations, human often sequentially sends several short messages for readability instead of a long message in one turn. Thus messages will not end with an explicit ending signal, which is crucial for agents to decide when to reply. So the first step for an intelligent dialogue agent is not replying but deciding if it should reply at the moment. To address this issue, in this paper, we propose a novel Imagine-then-Arbitrate (ITA) neural dialogue model to help the agent decide whether to wait or to make a response directly. Our method has two imaginator modules and an arbitrator module. The two imaginators will learn the agent's and user's speaking style respectively, generate possible utterances as the input of the arbitrator, combining with dialogue history. And the arbitrator decides whether to wait or to make a response to the user directly. To verify the performance and effectiveness of our method, we prepared two dialogue datasets and compared our approach with several popular models. Experimental results show that our model performs well on addressing ending prediction issue and outperforms baseline models.	Introduction	All species are unique, but languages make humans uniquest BIBREF0. Dialogues, especially spoken and written dialogues, are fundamental communication mechanisms for human beings. In real life, tons of businesses and entertainments are done via dialogues. This makes it significant and valuable to build an intelligent dialogue product. So far there are quite a few business applications of dialogue techniques, e.g. personal assistant, intelligent customer service and chitchat companion.The quality of response is always the most important metric for dialogue agent, targeted by most existing work and models searching the best response. Some works incorporate knowledge BIBREF1, BIBREF2 to improve the success rate of task-oriented dialogue models, while some others BIBREF3 solve the rare words problem and make response more fluent and informative.Despite the heated competition of models, however, the pace of interaction is also important for human-computer dialogue agent, which has drawn less or no attention. Figure FIGREF1 shows a typical dialogue fragment in an instant message program. A user is asking the service about the schedule of the theater. The user firstly says hello (U11) followed by demand description (U12), and then asks for suggested arrangement (U13), each of which is sent as a single message in one turn. The agent doesn't answer (A2) until the user finishes his description and throws his question. The user then makes a decision (U21) and asks a new question (U22). And then the agent replies with (A3). It's quite normal and natural that the user sends several messages in one turn and the agent waits until the user finished his last message, otherwise the pace of the conversation will be messed up. However, existing dialogue agents can not handle well when faced with this scenario and will reply to every utterance received immediately.There are two issues when applying existing dialogue agents to real life conversation. Firstly, when user sends a short utterance as the start of a conversation, the agent has to make a decision to avoid generating bad responses based on semantically incomplete utterance. Secondly, dialogue agent cutting in the conversation at an unreasonable time could confuse user and mess up the pace of conversation, leading to nonsense interactions.To address these two issues, in this paper, we propose a novel Imagine-then-Arbitrate (ITA) neural dialogue model to recognize if it is the appropriate moment for agent to reply when agent receives a message from the user. In our method, we have two imaginator modules and an arbitrator module. Imaginators will learn both of the agent's and user's speaking styles respectively. The arbitrator will use the dialogue history and the imagined future utterances generated by the two imaginators to decide whether the agent should wait user or make a response directly.In summary, this paper makes the following contributions:We first addressed an interaction problem, whether the dialogue model should wait for the end of the utterance or make a response directly in order to simulate real life conversation and tried several popular baseline models to solve it.We proposed a novel Imagine-then-Arbitrate (ITA) neural dialogue model to solve the problem mentioned above, based on both of the historical conversation information and the predicted future possible utterances.We modified two popular dialogue datasets to simulate the real human dialogue interaction behavior.Experimental results demonstrate that our model performs well on addressing ending prediction issue and the proposed imaginator modules can significantly help arbitrator outperform baseline models.Related Work ::: Dialogue System	Creating a perfect artificial human-computer dialogue system is always a ultimate goal of natural language processing. In recent years, deep learning has become a basic technique in dialogue system. Lots of work has investigated on applying neural networks to dialogue system's components or end-to-end dialogue frameworks BIBREF4, BIBREF5. The advantage of deep learning is its ability to leverage large amount of data from internet, sensors, etc. The big conversation data and deep learning techniques like SEQ2SEQ BIBREF6 and attention mechanism BIBREF7 help the model understand the utterances, retrieve background knowledge and generate responses.Related Work ::: Classification in Dialogue	Though end-to-end methods play a more and more important role in dialogue system, the text classification modules BIBREF8, BIBREF9 remains very useful in many problems like emotion recognition BIBREF10, gender recognition BIBREF11, verbal intelligence, etc. There have been several widely used text classification methods proposed, e.g. Recurrent Neural Networks (RNNs) and CNNs. Typically RNN is trained to recognize patterns across time, while CNN learns to recognize patterns across space. BIBREF12 proposed TextCNNs trained on top of pre-trained word vectors for sentence-level classification tasks, and achieved excellent results on multiple benchmarks.Besides RNNs and CNNs, BIBREF13 proposed a new network architecture called Transformer, based solely on attention mechanism and obtained promising performance on many NLP tasks. To make the best use of unlabeled data, BIBREF14 introduced a new language representation model called BERT based on transformer and obtained state-of-the-art results.Related Work ::: Dialogue Generation	Different from retrieval method, Natural Language Generation (NLG) tries converting a communication goal, selected by the dialogue manager, into a natural language form. It reflects the naturalness of a dialogue system, and thus the user experience. Traditional template or rule-based approach mainly contains a set of templates, rules, and hand-craft heuristics designed by domain experts. This makes it labor-intensive yet rigid, motivating researchers to find more data-driven approaches BIBREF15, BIBREF2 that aim to optimize a generation module from corpora, one of which, Semantically Controlled LSTM (SC-LSTM) BIBREF16, a variant of LSTM BIBREF17, gives a semantic control on language generation with an extra component.Task Definition	In this section we will describe the task by taking a scenario and then define the task formally.As shown in Figure FIGREF1, we have two participants in a conversation. One is the dialogue agent, and the other is a real human user. The agent's behavior is similar to most chatbots, except that it doesn't reply on every sentence received. Instead, this agent will judge to find the right time to reply.Our problem is formulated as follows. There is a conversation history represented as a sequence of utterances: $X = \lbrace x_1, x_2, ..., x_m\rbrace $, where each utterance $x_i$ itself is a sequence of words $x_{i_1}, x_{i_2}, x_{i_3}...x_{i_n}$. Besides, each utterance has some additional tags:turn tags $t_0, t_1, t_2 ... t_k$ to show which turn this utterance is in the whole conversation.speakers' identification tags $agent$ or $user$ to show who sends this utterance.subturn tags ${st}_0, {st}_1, {st}_2 ... {st}_j$ for user to indicate which subturn an utterance $t_i$is in. Note that an utterance will be labelled as ${st}_0$ even if it doesn't have one.Now, given a dialogue history $X$ and tags $T$, the goal of the model is to predict a label $Y \in \lbrace 0,1\rbrace $, the action the agent would take, where $Y = 0$ means the agent will wait the user for next message, and $Y = 1$ means the agent will reply immediately. Formally we are going to maximize following probability:Proposed Framework	Basically, the task can be simplified as a simple text classification problem. However, traditional classification models only use the dialogue history $X$ and predict ground truth label. The ground truth label actually ignores all context information in the next utterance. To make the best use of training data, we propose a novel Imagine-then-Arbitrate (ITA) model taking $X$, ground truth label, and the future possible $X^{\prime }$ into consideration. In this section, we will describe the architecture of our model and how it works in detail.Proposed Framework ::: Imaginator	An imaginator is a natural language generator generating next sentence given the dialogue history. There are two imaginators in our method, agent's imaginator and user's imaginator. The goal of the two imaginators are to learn the agent’s and user’s speaking style respectively and generate possible future utterances.As shown in Figure FIGREF7 (a), imaginator itself is a sequence generation model. We use one-hot embedding to convert all words and relative tags, e.g. turn tags and place holders, to one-hot vectors $w_n \in \textbf {R}^V$, where $V$ is the length of vocabulary list. Then we extend each word $x_{i_j}$ in utterance $x_i$ by concatenating the token itself with turn tag, identity tag and subturn tag. We adopt SEQ2SEQ as the basic architecture and LSTMs as the encoder and decoder networks. LSTMs will encode each extended word $w_t$ as a continuous vector $h_t$ at each time step $t$. The process can be formulated as following:where $e(w_t)$ is the embedding of the extended word $w_t$, $W_f$, $U_f$, $W_i$, $U_i$, $W_o$, $U_o$, $W_g$, $U_g$ and $b$ are learnt parameters.Though trained on the same dataset, the two imaginators learn different roles independently. So in the same piece of dialogue, we split it into different samples for different imaginators. For example, as shown in Figure FIGREF1 and FIGREF7 (a), we use utterance (A1, U11, U12) as dialogue history input and U13 as ground truth to train the user imaginator and use utterance (A1, U11, U12, U13) as dialogue history and A2 as ground truth to train the agent imaginator.During training, the encoder runs as equation DISPLAY_FORM15, and the decoder is the same structured LSTMs but $h_t$ will be fed to a Softmax with $W_{v} \in {\textbf {R}^{h \times V}}, b_{v} \in {\textbf {R}^\textbf {V}}$, which will produce a probability distribution $p_{t}$ over all words, formally:the decoder at time step t will select the highest word in $p_{t}$, and our imaginator's loss is the sum of the negative log likelihood of the correct word at each step as follows:where $N$ is the length of the generated sentence. During inference, we also apply beam search to improve the generation performance.Finally, the trained agent imaginator and user imaginator are obtained.Proposed Framework ::: Arbitrator	The arbitrator module is fundamentally a text classifier. However, in this task, we make the module maximally utilize both dialogue history and ground truth's semantic information. So we turned the problem of maximizing $Y$ from $X$ in equation (DISPLAY_FORM13) to:where $\textbf {IG}_{agent}$ and $\textbf {IG}_{user}$ are the trained agent imaginator and user imaginator respectively, and $R^{\prime }$ is a selection indicator where $R^{\prime } = 1$ means selecting $R_{agent}$ whereas 0 means selecting $R_{user}$. And Thus we (1) introduce the generation ground truth semantic information and future possible predicted utterances (2) turn the label prediction problem into a response selection problem.We adopt several architectures like Bi-GRUs, TextCNNs and BERT as the basis of arbitrator module. We will show how to build an arbitrator by taking TextCNNs as an example.As is shown in Figure FIGREF7, the three CNNs with same structure take the inferred responses $R_{agent}$, $R_{user}$ and dialogue history $X$, tags $T$. For each raw word sequence $x_1,...,x_n$, we embed each word as one-hot vector $w_{i} \in \textbf {R}^V$. By looking up a word embedding matrix $E \in \textbf {R}^{V \times d}$, the input text is represented as an input matrix $Q \in \textbf {R}^{l \times d}$, where $l$ is the length of sequence of words and $d$ is the dimension of word embedding features. The matrix is then fed into a convolution layer where a filter $\textbf {w} \in \textbf {R}^{k \times d}$ is applied:where $Q_{i:i+k-1}$ is the window of token representation and the function $f$ is $ReLU$, $W$ and $b$ are learnt parameters. Applying this filter to $m$ possible $Q_{i:i+k-1}$ obtains a feature map:where $\textbf {c} \in \textbf {R}^{l-k+1}$ for $m$ filters. And we use $j \in \textbf {R} $ different size of filters in parallel in the same convolution layer. This means we will have $m_1, m_2, \dots , m_j$ windows at the same time, so formally:, then we apply max-over-time pooling operation to capture the most important feature:, and thus we get the final feature map of the input sequence.We apply same CNNs to get the feature maps of $X$, $R_{agent}$ and $R_{user}$:where function TextCNNs() follows as equations from DISPLAY_FORM20 to DISPLAY_FORM23. Then we will have two possible dialogue paths, $X$ with $R_{agent}$ and $X$ with $R_{user}$, representations $D_{agent}$ and $D_{user}$:And then, the arbitrator will calculate the probability of the two possible dialogue paths:Through learnt parameters $W_{4}$ and $b_{4}$, we will get a two-dimensional probability distribution $P$, in which the most reasonable response has the max probability. This also indicates whether the agent should wait or not.And the total loss function of the whole attribution module will be negative log likelihood of the probability of choosing the correct action:where $N$ is the number of samples and $Y_{i}$ is the ground truth label of i-th sample.The arbitrator module based on Bi-GRU and BERT is implemented similar to TextCNNs.Experimental Setup ::: Datasets	As the proposed approach mainly concentrates on the interaction of human-computer, we select and modify two very different style datasets to test the performance of our method. One is a task-oriented dialogue dataset MultiWoz 2.0 and the other is a chitchat dataset DailyDialogue . Both datasets are collected from human-to-human conversations. We evaluate and compare the results with the baseline methods in multiple dimensions. Table TABREF28 shows the statistics of datasets.MultiWOZ 2.0 BIBREF18. MultiDomain Wizard-of-Oz dataset (MultiWOZ) is a fully-labeled collection of human-human written conversations. Compared with previous task-oriented dialogue datasets, e.g. DSTC 2 BIBREF19 and KVR BIBREF20, it is a much larger multi-turn conversational corpus and across serveral domains and topics: It is at least one order of magnitude larger than all previous annotated task-oriented corpora, with dialogues spanning across several domains and topics.DailyDialogue BIBREF21. DailyDialogue is a high-quality multi-turn dialogue dataset, which contains conversations about daily life. In this dataset, humans often first respond to previous context and then propose their own questions and suggestions. In this way, people show their attention others’ words and are willing to continue the conversation. Compare to the task-oriented dialogue datasets, the speaker's behavior will be more unpredictable and complex for the arbitrator.Experimental Setup ::: Datasets Modification	Because the task we concentrate on is different from traditional ones, to make the datasets fit our problems and real life, we modify the datasets with the following steps:Drop Slots and Values For task-oriented dialogue, slot labels are important for navigating the system to complete a specific task. However, those labels and accurate values from ontology files will not benefit our task essentially. So we replace all specific values with a slot placeholder in preprocessing step.Split Utterances Existing datasets concentrate on the dialogue content, combining multiple sentences into one utterance each turn when gathering the data. In this step, we randomly split the combined utterance into multiple utterances according to the punctuation. And we set a determined probability to decide if the preprocessing program should split a certain sentence.Add Turn Tag We add turn tags, subturn tags and role tags to each split and original sentences to (1) label the speaker role and dialogue turns (2) tag the ground truth for training and testing the supervised baselines and our model.Finally, we have the modified datasets which imitate the real life human chatting behaviors as shown in Figure FIGREF1. Our datasets and code will be released to public for further researches in both academic and industry.Experimental Setup ::: Evaluation Method	To compare with dataset baselines in multiple dimensions and test the model's performance, we use the overall Bilingual Evaluation Understudy (BLEU) BIBREF22 to evaluate the imaginators' generation performance. As for arbitrator, we use accuracy score of the classification to evaluate. Accuracy in our experiments is the correct ratio in all samples.Experimental Setup ::: Baselines and Training Setup	The hyper-parameter settings adopted in baselines and our model are the best practice settings for each training set. All models are tested with various hyper-parameter settings to get their best performance. Baseline models are Bidirectional Gated Recurrent Units (Bi-GRUs) BIBREF23, TextCNNs BIBREF12 and BERT BIBREF14.Experimental Results and Analysis ::: Results	In Table TABREF29, we show different imaginators' generation abilities and their performances on the same TextCNN based arbitrator. Firstly, we gathered the results of agent and user imaginators' generation based on LSTM, LSTM-attention and LSTM-attention with GLOVE pretrained word embedding. According to the evaluation metric BLEU, the latter two models achieve higher but similar results. Secondly, when fixed the arbitrator on the TextCNNs model, the latter two also get the similar results on accuracy and significantly outperform the others including the TextCNNs baseline.The performances on different arbitrators with the same LSTM-attention imaginators are shown in Table TABREF30. From those results, we can directly compared with the corresponding baseline models. The imaginators with BERT based arbitrator make the best results in both datasets while all ITA models beat the baseline models.We also present an example of how our model runs in Table TABREF37. Imaginators predict the agent and user's utterance according to the dialogue history(shown in model prediction), and then arbitrator selects the user imaginator's prediction that is more suitable with the dialogue history. It is worth noting that the arbitrator generates a high-quality sentence again if only considering the generation effect. However, referring to the dialogue history, it is not a good choice since its semantic is repeated in the last turn by the agent.Experimental Results and Analysis ::: Analysis ::: Imaginators Benefit the Performance	From Table TABREF30, we can see that not only our BERT based model get the best results in both datasets, the other two models also significantly beat the corresponding baselines. Even the TextCNNs based model can beat all baselines in both datasets.Table TABREF29 figures out experiment results on MultiWOZ dataset. The LSTM based agent imaginator get the BLEU score at 11.77 on agent samples, in which the ground truth is agents' utterances, and 0.80 on user samples. Meanwhile, the user imaginator get the BLEU score at 0.3 on agent samples and 8.87 on user target samples. Similar results are shown in other imaginators' expermients. Although these comparisons seem unfair to some extends since we do not have the agent and user's real utterances at the same time and under the same dialogue history, these results show that the imaginators did learn the speaking style of agent and user respectively. So the suitable imaginator's generation will be more similar to the ground truth, such an example shown in Table TABREF37, which means this response more semantically suitable given the dialogue history.If we fix the agent and user imaginators' model, as we take the LSTM-attention model, the arbitrators achieve different performances on different models, shown in Table TABREF30. As expected, ITA models beat their base models by nearly 2 $\sim $ 3% and ITA-BERT model beats all other ITA models.So from the all results, we can conclude that imaginators will significantly help the arbitrator in predicting the dialogue interaction behavior using the future possible agent and user responses’ semantic information.Experimental Results and Analysis ::: Analysis ::: Relation of Imaginators and Arbitrator's Performance	As shown in the DailyDialogue dataset of Table TABREF29, we can see that attention mechanism works in learning the generation task. LSTMs -Attention and LSTMs-attention-GLOVE based imaginators get more than 19 and 24 BLEU scores in corresponding target, while the LSTMs without attention gets only 4.51 and 8.70. These results also impact on the arbitrator results. The imaginator with attention mechanism get an accuracy score of 79.02 and 78.56, significantly better than the others. The evidence also exists in the results on MultiWoz. All imaginators get similar generation performance, so the arbitrators gets the similar accuracy scores.From those results, we can conclude that there is positive correlation between the performance of imaginators and arbitrators. However, there still exists problems. It's not easy to evaluate the dialogue generation's performance. In the results of MultiWoz, we can see that LSTMs-GLOVE based ITA performs a little better than LSTMs-attention based ITA, but not the results of the arbitrator are opposite. This may indicate that (1) when the imaginators' performance is high enough, the arbitrator's performance will be stable and (2) the BLEU score will not perfectly present the contribution to the arbitrator. We leave these hypotheses in future work.Conclusion	We first address an interaction problem, whether the dialogue model should wait for the end of the utterance or reply directly in order to simulate user's real life conversation behavior, and propose a novel Imagine-then-Arbitrate (ITA) neural dialogue model to deal with it. Our model introduces the imagined future possible semantic information for prediction. We modified two popular dialogue datasets to fit in the real situation. It is reasonable that additional information is helpful for arbitrator, despite its fantasy.","['What evaluation metrics did they use?', 'By how much does their model outperform the baseline?', 'Which models did they compare with?', 'What is the source of their datasets?']","['Unacceptable', 'Best model outperforms baseline by 1.98% on MultiWoz dataset and .67% on DailyDialogue dataset', 'Unacceptable', 'Unacceptable']"
37,"Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems	Open-domain human-computer conversation has attracted much attention in the field of NLP. Contrary to rule- or template-based domain-specific dialog systems, open-domain conversation usually requires data-driven approaches, which can be roughly divided into two categories: retrieval-based and generation-based systems. Retrieval systems search a user-issued utterance (called a query) in a large database, and return a reply that best matches the query. Generative approaches, typically based on recurrent neural networks (RNNs), can synthesize new replies, but they suffer from the problem of generating short, meaningless utterances. In this paper, we propose a novel ensemble of retrieval-based and generation-based dialog systems in the open domain. In our approach, the retrieved candidate, in addition to the original query, is fed to an RNN-based reply generator, so that the neural model is aware of more information. The generated reply is then fed back as a new candidate for post-reranking. Experimental results show that such ensemble outperforms each single part of it by a large margin.	Introduction	Automatic dialog/conversation systems have served humans for a long time in various fields, ranging from train routing nbcitetrain to museum guiding nbcitemuseum. In the above scenarios, the dialogs are domain-specific, and a typical approach to such in-domain systems is by human engineering, for example, using manually constructed ontologies nbciteyoungsigdial, natural language templates nbcitetemplate, and even predefined dialog states nbcitestatetracking.Recently, researchers have paid increasing attention to open-domain, chatbot-style human-computer conversation, because of its important commercial applications, and because it tackles the real challenges of natural language understanding and generation nbciteretrieval1,acl,aaai. For open-domain dialogs, rules and temples would probably fail as we can hardly handle the great diversity of dialog topics and natural language sentences. With the increasing number of human-human conversation utterances available on the Internet, previous studies have developed data-oriented approaches in the open domain, which can be roughly categorized into two groups: retrieval systems and generative systems.When a user issues an utterance (called a query), retrieval systems search for a most similar query in a massive database (which consists of large numbers of query-reply pairs), and respond to the user with the corresponding reply nbciteretrieval1,retrieval2. Through information retrieval, however, we cannot obtain new utterances, that is, all replies have to appear in the database. Also, the ranking of candidate replies is usually judged by surface forms (e.g., word overlaps, tf $\cdot $ idf features) and hardly addresses the real semantics of natural languages.Generative dialog systems, on the other hand, can synthesize a new sentence as the reply by language models nbciteBoWdialog,acl,aaai. Typically, a recurrent neural network (RNN) captures the query's semantics with one or a few distributed, real-valued vectors (also known as embeddings); another RNN decodes the query embeddings to a reply. Deep neural networks allow complicated interaction by multiple non-linear transformations; RNNs are further suitable for modeling time-series data (e.g., a sequence of words) especially when enhanced with long short term memory (LSTM) or gated recurrent units (GRUs). Despite these, RNN also has its own weakness when applied to dialog systems: the generated sentence tends to be short, universal, and meaningless, for example, “I don't know” nbcitenaacl or “something” nbciteaaai. This is probably because chatbot-like dialogs are highly diversified and a query may not convey sufficient information for the reply. Even though such universal utterances may be suited in certain dialog context, they make users feel boring and lose interest, and thus are not desirable in real applications.In this paper, we are curious if we can combine the above two streams of approaches for open-domain conversation. To this end, we propose an ensemble of retrieval and generative dialog systems. Given a user-issued query, we first obtain a candidate reply by information retrieval from a large database. The query, along with the candidate reply, is then fed to an utterance generator based on the “bi-sequence to sequence” (biseq2seq) model nbcitemultiseq2seq. Such sequence generator takes into consideration the information contained in not only the query but also the retrieved reply; hence, it alleviates the low-substance problem and can synthesize replies that are more meaningful. After that we use the scorer in the retrieval system again for post-reranking. This step can filter out less relevant retrieved replies or meaningless generated ones. The higher ranked candidate (either retrieved or generated) is returned to the user as the reply.From the above process, we see that the retrieval and generative systems are integrated by two mechanisms: (1) The retrieved candidate is fed to the sequence generator to mitigate the “low-substance” problem; (2) The post-reranker can make better use of both the retrieved candidate and the generated utterance. In this sense, we call our overall approach an ensemble in this paper. To the best of our knowledge, we are the first to combine retrieval and generative models for open-domain conversation.Experimental results show that our ensemble model consistently outperforms each single component in terms of several subjective and objective metrics, and that both retrieval and generative methods contribute an important portion to the overall approach. This also verifies the rationale for building model ensembles for dialog systems.Overview	Figure 1 depicts the overall framework of our proposed ensemble of retrieval and generative dialog systems. It mainly consists of the following components.When a user sends a query utterance $q$ , our approach utilizes a state-of-the-practice information retrieval system to search for a query-reply pair $\langle q^*, r^*\rangle $ that best matches the user-issued query $q$ . The corresponding $r^*$ is retrieved as a candidate reply.Then a biseq2seq model takes the original query $q$ and the retrieved candidate reply $r^*$ as input, each sequence being transformed to a fixed-size vector. These two vectors are concatenated and linearly transformed as the initial state of the decoder, which generates a new utterance $r^\text{+}$ as another candidate reply.Finally, we use a reranker (which is a part of the retrieval system) to select either $r^*$ or $r^\text{+}$ as the ultimate response to the original query $q$ .In the rest of this section, we describe each component in detail.Retrieval-Based Dialog System	Information retrieval is among prevailing techniques for open-domain, chatbot-style human-computer conversation nbciteretrieval1,retrieval2.We utilize a state-of-the-practice retrieval system with extensive manual engineering and on a basis of tens of millions of existing human-human utterance pairs. Basically, it works in a two-step retrieval-and-ranking strategy, similar to the Lucene and Solr systems.First, a user-issued utterance is treated as bag-of-words features with stop-words being removed. After querying it in a knowledge base, we obtain a list containing up to 1000 query-reply pairs $\langle q^*, r^*\rangle $ , whose queries share most words as the input query $q$ . This step retrieves coarse-grained candidates efficiently, which is accomplished by an inversed index.Then, we measure the relatedness between the query $q$ and each $\langle q^*, r^*\rangle $ pair in a fine-grained fashion. In our system, both $q$ - $q^*$ and $q$ - $r^*$ relevance scores are considered. A classifier judges whether $q$ matches $q^*$ and $r^*$ ; its confidence degree is used as the scorer. We have tens of features, and several important ones include word overlap ratio, the cosine measure of a pretrained topic model coefficients, and the cosine measures of word embedding vectors. (Details are beyond the scope of this paper; any well-designed retrieval system might fit into our framework.)In this way, we obtain a query-reply pair $\langle q^*, r^*\rangle $ that best matches the original query $q$ ; the corresponding utterance $r^*$ is considered as a candidate reply retrieved from the database.The biseq2seq Utterance Generator	Using neural networks to build end-to-end trainable dialog systems has become a new research trend in the past year. A generative dialog system can synthesize new utterances, which is complementary to retrieval-based methods.Typically, an encoder-decoder architecture is applied to encode a query as vectors and to decode the vectors to a reply utterance. With recurrent neural networks (RNNs) as the encoder and decoder, such architecture is also known as a seq2seq model, which has wide applications in neural machine translation nbciteseq2seq, abstractive summarization nbcitesummarization, etc. That being said, previous studies indicate seq2seq has its own shortcoming for dialog systems.nbciteseq2BF suggests that, in open-domain conversation systems, the query does not carry sufficient information for the reply; that the seq2seq model thus tends to generate short and meaningless sentences with little substance.To address this problem, we adopt a biseq2seq model, which is proposed in nbcitemultiseq2seq for multi-source machine translation. The biseq2seq model takes into consideration the retrieved reply as a reference in addition to query information (Figure 2 ). Hence, the generated reply can be not only fluent and logical with respect to the query, but also meaningful as it is enhanced by a retrieved candidate.Specifically, we use an RNN with gated recurrent units (GRUs) for sequence modeling. Let $\mathbf {x}_t$ be the word embeddings of the time step $t$ and $\mathbf {h}_{t-1}$ be the previous hidden state of RNN. We have $$\mathbf {r}_t &= \sigma (W_r\mathbf {x}_t+ U_r\mathbf {h}_{t-1} + \mathbf {b}_r)\\
\mathbf {z}_t &= \sigma (W_z\mathbf {x}_t+ U_r\mathbf {h}_{t-1} + \mathbf {b}_z)\\
\tilde{\mathbf {h}}_t &= \tanh \big (W_x\mathbf {x}_t+ U_x (\mathbf {r}_t \circ \mathbf {h}_{t-1})\big )\\
\mathbf {h}_t &= (1-\mathbf {z}_t)\circ \mathbf {h}_{t-1} + \mathbf {z}_t \circ \tilde{\mathbf {h}}_t$$   (Eq. 10)  where $\mathbf {r}_t$ and $\mathbf {z}_t$ are known as gates, $W$ 's and $\mathbf {b}$ 's are parameters, and “ $\circ $ ” refers to element-wise product.After two RNNs go through $q$ and $r^*$ , respectively, we obtain two vectors capturing their meanings. We denote them as bold letters $\mathbf {q}$ and $\mathbf {r}^*$ , which are concatenated as $[\mathbf {q}; \mathbf {r}^*]$ and linearly transformed before being fed to the decoder as the initial state.During reply generation, we also use GRU-RNN, given by Equations 10 –. But at each time step, a softmax layer outputs the probability that a word would occur in the next step, i.e., $$p(w_i|\mathbf {h}_t) = \frac{\exp \left\lbrace  W_i^\top \mathbf {h}_t\right\rbrace +\mathbf {b}}{\sum _j\exp \left\lbrace  W_j^\top \mathbf {h}_t+\mathbf {b}\right\rbrace }$$   (Eq. 11) where $W_i$ is the $i$ -th row of the output weight matrix (corresponding to $w_i$ ) and $\mathbf {b}$ is a bias term.Notice that we assign different sets of parameters—indicated by three colors in Figure 2 —for the two encoders ( $q$ and $r^*$ ) and the decoder ( $r^+$ ). This treatment is because the RNNs' semantics differ significantly from one another (even between the two encoders).Post-Reranking	Now that we have a retrieved candidate reply $r^*$ as well as a generated one $r^+$ , we select one as the final reply by the $q$ - $r$ scorer in the retrieval-based dialog system (described in previous sections and not repeated here).Using manually engineered features, this step can eliminate either meaningless short replies that are unfortunately generated by biseq2seq or less relevant replies given by the retrieval system. We call this a post-reranker in our model ensemble.Training	We train each component separately because the retrieval part is not end-to-end learnable.In the retrieval system, we use the classifier's confidence as the relevance score. The training set consists of 10k samples, which are either in the original human-human utterance pairs or generated by negative sampling. We made efforts to collect binary labels from a crowd-sourcing platform, indicating whether a query is relevant to another query and whether it is relevant to a particular reply. We find using crowd-sourced labels results in better performance than original negative sampling.For biseq2seq, we use human-human utterance pairs $\langle q, r\rangle $ as data samples. A retrieved candidate $r^*$ is also provided as the input when we train the neural network. Standard cross-entropy loss of all words in the reply is applied as the training objective. For a particular training sample whose reply is of length $T$ , the cost is $$J = -\sum _{i=1}^T\sum _{j=1}^{V}{t_j^{(i)}\log {y_j^{(i)}}}$$   (Eq. 15) where $\mathbf {t}^{(i)}$ is the one-hot vector of the next target word, serving as the groundtruth, $\mathbf {y}$ is the output probability by softmax, and $V$ is the vocabulary size. We adopt mini-batched AdaDelta nbciteadadelta for optimization.Evaluation	In this section, we evaluate our model ensemble on Chinese (language) human-computer conversation. We first describe the datasets and settings. Then we compare our approach with strong baselines.Experimental Setup	Typically, a very large database of query-reply pairs is a premise for a successful retrieval-based conversation system, because the reply must appear in the database. For RNN-based sequence generators, however, it is time-consuming to train with such a large dataset; RNN's performance may also saturate when we have several million samples.To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, such as Sina Weibo, Baidu Zhidao, and Baidu Tieba. We filtered out short and meaningless replies like “...” and “Errr.” In total, the database contains 7 million query-reply pairs for retrieval.For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs. For each query $q$ , we searched for a candidate reply $r^*$ by the retrieval component and obtained a tuple $\langle q, r^*, r\rangle $ . As a friendly reminder, $q$ and $r^*$ are the input of biseq2seq, whose output should approximate $r$ . We randomly selected 100k triples for validation and another 6,741 for testing. The train-val-test split remains the same for all competing models.The biseq2seq then degrades to an utterance autoencoder nbciteautoencoder. Also, the validation and test sets are disjoint with the training set and the database for retrieval, which complies with the convention of machine learning.To train our neural models, we followed nbciteacl for hyperparameter settings. All embeddings were set to 620-dimensional and hidden states 1000d. We applied AdaDelta with a mini-batch size of 80 and other default hyperparameters for optimization. Chinese word segmentation was performed on all utterances. We kept a same set of 100k words (Chinese terms) for two encoders, but 30k for the decoder due to efficiency concerns. The three neural networks do not share parameters (neither connection weights nor embeddings).We did not tune the above hyperparameters, which were set empirically. The validation set was used for early stop based on the perplexity measure.Competing Methods	We compare our model ensemble with each individual component and provide a thorough ablation test. Listed below are the competing methods in our experiments.Retrieval. A state-of-the-practice dialog system, which is a component of our model ensemble; it is also a strong baseline because of extensive human engineering.seq2seq. An encoder-encoder framework nbciteseq2seq, first introduced in nbciteacl for dialog systems.biseq2seq. Another component in our approach, adapted from nbcitemultiseq2seq, which is essentially a seq2seq model extended with a retrieved reply.Rerank(Retrieval,seq2seq). Post-reranking between a retrieved candidate and one generated by seq2seq.Rerank(Retrieval,biseq2seq). This is the full proposed model ensemble.All baselines were trained and tuned in a same way as our full model, when applicable, so that the comparison is fair.Overall Performance	We evaluated our approach in terms of both subjective and objective evaluation.Human evaluation, albeit time- and labor-consuming, conforms to the ultimate goal of open-domain conversation systems. We asked three educated volunteers to annotate the results using a common protocol known as pointwise annotation nbciteacl,ijcai,seq2BF. In other words, annotators were asked to label either “0” (bad), “1” (borderline), or “2” (good) to a query-reply pair. The subjective evaluation was performed in a strict random and blind fashion to rule out human bias.We adopted BLEU-1, BLEU-2, BLEU-3 and BLEU-4 as automatic evaluation. While nbcitehowNOTto further aggressively argues that no existing automatic metric is appropriate for open-domain dialogs, they show a slight positive correlation between BLEU-2 and human evaluation in non-technical Twitter domain, which is similar to our scenario. We nonetheless include BLEU scores as expedient objective evaluation, serving as supporting evidence. BLEUs are also used in nbcitenaacl for model comparison and in nbciteseq2BF for model selection.Notice that, automatic metrics were computed on the entire test set, whereas subjective evaluation was based on 79 randomly chosen test samples due to the limitation of human resources available.We present our main results in Table 2 . As shown, the retrieval system, which our model ensemble is based on, achieves better performance than RNN-based sequence generation. The result is not consistent with nbciteacl, where their RNNs are slightly better than retrieval-based methods. After closely examining their paper, we find that their database is multiple times smaller than ours, which may, along with different features and retrieval methods, explain the phenomenon. This also verifies that the retrieval-based dialog system in our experiment is a strong baseline to compare with.Combining the retrieval system and the RNN generator by bi-sequence input and post-reranking, we achieve the highest performance in terms of both human evaluation and BLEU scores. Concretely, our model ensemble outperforms the state-of-the-practice retrieval system by $ +13.6\%$ averaged human scores, which we believe is a large margin.Analysis and Discussion	Having verified that our model ensemble achieves better performance than all baselines, we are further curious how each gadget contributes to our final system. Specially, we focus on the following research questions.RQ1: What is the performance of biseq2seq (the 1 step in Figure 1 ) in comparison with traditional seq2seq?From the BLEU scores in Table 2 , we see biseq2seq significantly outperforms conventional seq2seq, showing that, if enriched with a retrieved human utterance as a candidate, the encoder-decoder framework can generate much more human-like utterances.We then introduce in Table 3 another measure, the entropy of a sentence, defined as $-\frac{1}{|R|}\sum _{w\in R}\log p(w)$ where $R$ refers to all replies. Entropy is used in nbcitevariationalDialog and nbciteseq2BF to measure the serendipity of generated utterances. The results in Table 3 confirm that biseq2seq indeed integrates information from the retrieved candidate, so that it alleviates the “low-substance” problem of RNNs and can generate utterances more meaningful than traditional seq2seq. And the statistic result also displays that biseq2seq generates longer sentences than seq2seq approach.RQ2: How do the retrieval- and generation-based systems contribute to post-reranking (the 2 step in Figure 1 )?We plot in Figure 3 the percentage by which the post-raranker chooses a retrieved candidate or a generated one. In the retrieval-and-seq2seq ensemble (Figure 3 a), 54.65% retrieved results and 45.35% generated ones are selected. In retrieval-and-biseq2seq ensemble, the percentage becomes 44.77% vs. 55.23%. The trend further indicates that biseq2seq is better than seq2seq (at least) from the reranker's point of view. More importantly, as the percentages are close to 50%, both the retrieval system and the generation system contribute a significant portion to our final ensemble.RQ3: Do we obtain further gain by combining the two gadgets 1 and 2 in Figure 1 ?We would also like to verify if the combination of biseq2seq and post-reranking mechanisms will yield further gain in our ensemble. To test this, we compare the full model Rerank(Retrieval,biseq2seq) with an ensemble that uses traditional seq2seq, i.e., Rerank(Retrieval,seq2seq). As indicated in Table 2 , even with the post-reranking mechanism, the ensemble with underlying biseq2seq still outperforms the one with seq2seq. Likewise, Rerank(Retrieval,biseq2seq) outperforms both Retrieval and biseq2seq. These results are consistent in terms of all metrics except a BLEU-4 score.Through the above ablation tests, we conclude that both gadgets (biseq2seq and post-reranking) play a role in our ensemble when we combine the retrieval and the generative systems.Case Study	Table 4 presents two examples of our ensemble and its “base” models. We see that biseq2seq is indeed influenced by the retrieved candidates. As opposed to traditional seq2seq, several content words in the retrieved replies (e.g., crush) also appear in biseq2seq's output, making the utterances more meaningful. The post-reranker also chooses a more appropriate candidate as the reply.Related Work	In early years, researchers mainly focus on domain-specific dialog systems, e.g., train routing nbcitetrain, movie information nbcitemovie, and human tutoring nbcitetutor. Typically, a pre-constructed ontology defines a finite set of slots and values, for example, cuisine, location, and price range in a food service dialog system; during human-computer interaction, a state tracker fills plausible values to each slot from user input, and recommend the restaurant that best meets the user's requirement nbcitewebstyle,ACL15statetracking,pseudoN2N.In the open domain, however, such slot-filling approaches would probably fail because of the diversity of topics and natural language utterances. nbciteretrieval1 applies information retrieval techniques to search for related queries and replies. nbciteretrieval2 and nbcitesigir use both shallow hand-crafted features and deep neural networks for matching. nbciteijcai proposes a random walk-style algorithm to rank candidate replies. In addition, their model can introduce additional content (related entities in the dialog context) by searching a knowledge base when a stalemate occurs during human-computer conversation.Generative dialog systems have recently attracted increasing attention in the NLP community. nbcitesmt formulates query-reply transformation as a phrase-based machine translation. Since the last year, the renewed prosperity of neural networks witnesses an emerging trend in using RNN for dialog systems nbcitenn0,BoWdialog,acl,aaai. However, a known issue with RNN is that it prefers to generate short, meaningless utterances. nbcitenaacl proposes a mutual information objective in contrast to the conventional maximum likelihood criterion. nbciteseq2BF and nbcitetopic introduce additional content (either the most mutually informative word or topic information) to the reply generator. nbcitevariationalDialog applies a variational encoder to capture query information as a distribution, from which a random vector is sampled for reply generation.To the best of our knowledge, we are the first to combine retrieval-based and generation-based dialog systems. The use of biseq2seq and post-reranking is also a new insight of this paper.Conclusion and Future Work	In this paper, we propose a novel ensemble of retrieval-based and generation-based open-domain dialog systems. The retrieval part searches a best-match candidate reply, which is, along with the original query, fed to an RNN-based biseq2seq reply generator. The generated utterance is fed back as a new candidate to the retrieval system for post-reranking. Experimental results show that our ensemble outperforms its underlying retrieval system and generation system by a large margin. In addition, the ablation test demonstrates both the biseq2seq and post-reranking mechanisms play an important role in the ensemble.Our research also points out several promising directions for future work, for example, developing new mechanisms of combining retrieval and generative dialog systems, as well as incorporating other data-driven approaches to human-computer conversation.","['Were human evaluations conducted?', 'What datasets are used?', 'How does inference time compare to other methods?']","['Unacceptable', 'They create their own datasets from online text.', 'Unacceptable']"
38,"Natural Language Generation for Non-Expert Users	Motivated by the difficulty in presenting computational results, especially when the results are a collection of atoms in a logical language, to users, who are not proficient in computer programming and/or the logical representation of the results, we propose a system for automatic generation of natural language descriptions for applications targeting mainstream users. Differently from many earlier systems with the same aim, the proposed system does not employ templates for the generation task. It assumes that there exist some natural language sentences in the application domain and uses this repository for the natural language description. It does not require, however, a large corpus as it is often required in machine learning approaches. The systems consist of two main components. The first one aims at analyzing the sentences and constructs a Grammatical Framework (GF) for given sentences and is implemented using the Stanford parser and an answer set program. The second component is for sentence construction and relies on GF Library. The paper includes two use cases to demostrate the capability of the system. As the sentence construction is done via GF, the paper includes a use case evaluation showing that the proposed system could also be utilized in addressing a challenge to create an abstract Wikipedia, which is recently discussed in the BlueSky session of the 2018 International Semantic Web Conference.	Introduction	Natural language generation (NLG) has been one of the key topics of research in natural language processing, which was highlighted by the huge body of work on NLG surveyed in BIBREF0, BIBREF1. With the advances of several devices capable of understanding spoken language and conducting conversation with human (e.g., Google Home, Amazon Echo) and the shrinking gap created by the digital devices, it is not difficult to foresee that the market and application areas of NLG systems will continue to grow, especially in applications whose users are non-experts. In such application, a user often asks for certain information and waits for the answer and a NLG module would return the answer in spoken language instead of text such as in question-answering systems or recommendation systems. The NLG system in these two applications uses templates to generate the answers in natural language for the users. A more advanced NLG system in this direction is described in BIBREF2, which works with ontologies annotated using the Attempto language and can generate a natural language description for workflows created by the systems built in the Phylotastic project. The applications targeted by these systems are significantly different from NLG systems, whose main purpose is to generate high-quality natural language description of objects or reports, such as those reported in the recent AAAI conference BIBREF3, BIBREF4, BIBREF5.The present paper is motivated by the need to generate natural language description of computational results to non-expert users such as those developed in the Phylotastic project. In this project, the users are experts in evolutionary biology but are none experts in ontologies and web services. When a user places a request, he/she will receive a workflow consisting of web services, whose inputs and outputs are specified by instances of classes in the ontologies working with web services, as well as the ordering and relationships between the services. To assist the user in understanding the workflow, a natural language description of the workflow is generated. In order to accomplish the task, the NLG system in the Phylotastic project proposes to annotate elements of the ontologies using Attempto, a simple subset of English with precisely defined syntax and semantics.In this paper, we propose a system that addresses the limitation of the system discussed in the Phylotastic project BIBREF2. Specifically, we assume that the annotations given in an ontology are natural language sentences. This is a reasonable assumption given that the developers of an ontology are usually those who have intimate knowledge about entities described in the ontology and often have some sort of comments about classes, objects, and instances of the ontology. We then show that the system is very flexible and can be used for the same purpose with new ontologies.The rest of the paper is organized as follows. Section SECREF2 briefly reviews the basics of Grammatical Framework (GF)BIBREF6. Section SECREF3 describes the main modules of the system. Section SECREF4 includes two use cases of the system using an available ontologies against in the context of reasoning about ontologies. Specifically, it compares with the system used in the Phylotastic project and an ontology about people. This section also contains a use case that highlights the versatility of the proposed system by addressing a challenge to create an abstract Wikipedia BIBREF7. Related works are discussed in Section SECREF5. Section SECREF6 concludes the paper.Background: Grammatical Framework	The Grammatical Framework (GF) BIBREF6 is a system used for working with grammars. The GF Resource Grammar Library (RGL) covering syntax of various languages is the standard library for GF. A GF program has two main parts. The first part is the Abstract syntax which defines what meanings can be expressed by a grammar. The abstract syntax defines categories (i.e., types of meaning) and functions (i.e., meaning-building components). An example of an abstract syntax:Here, Message, People, Action and Entity are types of meanings. startcat flag states that Message is the default start category for parsing and generation. simple_sent is a function accepting 3 parameters, of type People, Action, Entity. This function returns a meaning of Message category. Intuitively, each function in the abstract syntax represents a rule in a grammar. The combination of rules used to construct a meaning type can be seen as a syntax tree.The second part is composed of one or more concrete syntax specifications. Each concrete syntax defines the representation of meanings in each output language. For example, to demostrate the idea that one meaning can be represented by different concrete syntaxes, we create two concrete syntaxes for two different languages: English and Italian. To translate a sentence to different languages, we only need to provide the strings representing each word in corresponding languages. The GF libraries will take responsibility to concatenate the provided strings according to the language grammar to create a complete sentence, which is the representations of the meaning, in the targeted language. The corresponding concrete syntaxes that map functions in the abstract grammar above to strings in English and in Italian is:In these concrete syntaxes, the linearization type definition (lincat) states that Message, People, Action and Entity are type Cl (clause), NP (noun phrase), V2 (two-place verb), and NP respectively. Linearization definitions (lin) indicate what strings are assigned to each of the meanings defined in the abstract syntax. To reduce same string declaration, the operator (oper) section defines some placeholders for strings that can be used in linearization. The mkNP, mkN, mkV2, etc. are standard constructors from ConstructorsEng/Jpn library which returns an object of the type NP, N or V2 respectively.GF has been used in a variety of applications, such as query-answering systems, voice communication, language learning, text analysis and translation, natural language generation BIBREF8, BIBREF9, automatic translation.The translation from English to Italian can be performed as follows in the GF API:The above command line produces a syntax tree of the sentence “Bill plays soccer” then turn that tree into a PeopleIta sentence (in Italian) which is displayed in the second line. Figure FIGREF6 shows the meaning in the abstract syntax is represented in Japanese and in Italian, i.e. the two strings represent the same meaning.Method	To generate a sentence, we need a sentence structure and vocabularies. Our system is developed to emulate the process of a person learning a new language and has to make guesses to understand new sentences from time to time. For example, someone, who understands the sentence “Bill plays a game” would not fully understand the sentence “Bill plays a popular board game” without knowing the meaning of “popular” and “board game” but could infer that the latter sentence indicates that its subject plays a type of game.The overall design of our system is given in Figure FIGREF7. Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation. The system consists of two components, understanding sentences and generating GF grammar. The first component is divided into two sub-components, one for recognizing the sentence structure and one for recognizing the sentence components. The second component consists of a GF grammar encoder and a GF grammar exporter. The encoder is responsible for generating a GF grammar for each sentence, while the exporter aggregates the grammars generated from the encoder, and produces a comprehensive grammar for the whole paragraph.Method ::: Sentence Structure Recognition	The sentence structure recognition process involves 2 modules: natural language processing (NLP) module and logical reasoning on result from NLP module. In this paper, we make use of the Stanford Parser tools described in BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14The NLP module tokenizes the input free text to produce a dependency-based parse tree and part-of-speech tag (POS tag). The dependency-based parse tree and the POS tag are then transform into an answer set program (ASP) BIBREF15 which contains only facts. Table TABREF13 shows the transformation of the result of NLP module into an ASP program for the sentence “Bill plays a game”. In this table, nsubj, det, dobj and punct denote relations in the dependency-based parse tree, and mean nominal subject, determiner, direct object and punctuation respectively. Full description of all relations in a dependency-based parse tree can be found in the Universal Dependency website. The second set of notations are the POS tag PRP, VBP, DT and NN corresponding to pronoun, verb, determiner and noun. Readers can find the full list of POS tag in Penn Treebank Project.From the collection of the dependency atoms from the dependency-based parse tree, we determine the structure of a sentence using an ASP program, called $\Pi _1$ (Listing ).Each of the rule above can be read as if the right-hand side is true then the left-hand side must be true. These rules define five possible structures of a sentence represented by the atom structure(x,y). $x$ and $y$ in the atom structure(x,y) denote the type of the structure and the number of dependency relations applied to activate the rule generating this atom, respectively. We refer to $y$ as the $i$-value of the structure. For example, $structure(1,1)$ will be recognized if the nsubj relation is in the dependency-based parse tree; $structure(3,3)$ needs 3 dependency relations to be actived: nsubj, xcomp and dobj. We often use structure #$x$ to indicate a structure of type $x$.Together with the collection of the atoms encoding the relations in the dependency-based parse tree, $\Pi _1$ generates several atoms of the form $structure(x,y)$ for a sentence. Among all these atoms, an atom with the highest $i$-value represents the structure constructed using the highest number of dependency relations. And hence, that structure is the most informative structure that is recoginized for the sentence. Observe that $structure(1,1)$ is the most simplified structure of any sentence.Method ::: Sentence Components Recognition	The goal of this step is to identify the relationship between elements of a sentence structure and chunks of words in a sentence from the POS tags and the dependency-based parse tree. For example, the sentence “Bill plays a game” is encoded by a structure #2 and we expect that Bill, plays, and game correspond to the subject, verb, and object, respectively.We begin with recognizing the main words (components) that play the most important roles in the sentence based on a given sentence structure. This is achieved by program $\Pi _2$ (Listing ). The first four rules of $\Pi _2$ determine the main subject and verb of the sentence whose structure is #1, #2, #3, or #5. Structure #4 requires a special treatment since the components following tobe can be of different forms. For instance, in “Cathy is gorgeous,” the part after tobe is an adjective, but in “Cathy is a beautiful girl,” the part after tobe is a noun, though, with adjective beautiful. This is done using the four last rules of $\Pi _2$.The result of program $\Pi _2$ is an one-to-one mapping of some of the words in the sentence into the importaint components of a sentence, called main components, i.e. subject, object and verb. The mapping is constructed by using the core arguments in Universal Dependency Relations . Since not every word in the sentence is in a core argument relation, there are some words in the sentence that are not in the domain of the mapping that $\Pi _2$ produces. We denote these words are complement components. To identify these words, we encode the Non-core dependents and Nominal dependents from Universal Dependency Relations into the set of rules in program $\Pi _3$.Program $\Pi _3$ (Listing ), together with the atoms extracted from the dependency-based parse tree such as $compound(P,N)$ ($N$ is compound noun at the position $P$ in the sentence), $amod(P,J)$ ($J$ is an adjective modifier), etc., is used to identify the complement components of the main components computed by $\Pi _2$ while maintaining the structure of the sentence created by $\Pi _1$. For example, a complement of a noun could be another noun (as “board” in “board game”), or an adjective (as “popular” in “popular board game”), or a preposition (as “for adults” in “board game for adults”).The input of Program $\Pi _3$ is the position ($pos$) of the word in the sentence. Program $\Pi _3$ is called whenever there is a new complement component discovered. That way of recursive calls is to identify the maximal chunk of the words that support the main components of the sentence. The result of this module is a list of vocabularies for the next steps.Method ::: GF Grammar Encoder	The goal of the encoder is to identify appropriate GF rules for the construction of a GF grammar of a sentence given its structure and its components identified in the previous two modules. This is necessary since a sentence can be encoded in GF by more than one set of rules; for example, the sentence “Bill wants to play a game” can be encoded by the rulesBill $\rightarrow $ NP, want $\rightarrow $ VV, play $\rightarrow $ V2, game $\rightarrow $ NP and one of the sets of GF rules in the table below:In GF, NP, VV, V2, VP, and Cl stand for noun phrase, verb-phrase-complement verb, two-place verb, verb phrase and clause, respectively. Note that although the set of GF grammatical rules can be used to construct a constituency-based parse tree , the reverse direction is not always true. To the best of our knowledge, there exists no algorithm for converting a constituency-based parse tree to a set GF grammar rules. We therefore need to identify the GF rules for each sentence structure.In our system, a GF rule is assigned to a structure initially (Table TABREF19). Each rule in Table TABREF19 represents the first level of the constituency-based parse tree. It acts as the coordinator for all other succeeding rules.Given the seed components identified in Section SECREF15 and the above GF rules, a GF grammar for each sentence can be constructed. However, this grammar can only be used to generate fairly simple sentences. For example, for the sentence “Bill plays a popular board game with his close friends.”, a GF grammar for structure #2 can be constructed, which can only generate the sentence “Bill plays game.” because it does not contain any complement components identified in Section SECREF15. Therefore, we assgin a set of GF rules for the construction of each parameter in the GF rules in Table TABREF19. The set of GF rules has to follow two conventions. The first one is after applying the set of rules to some components of the sentence, the type of the production is one of the type in Table TABREF19, e.g. $NP$, $VP$, $Cl$, $V2$, .... The second convention is that the GF encoder will select the rules as the order from top to bottom in Table TABREF20. Note that the encoder always has information of what type of input and output for the rule it is looking for.For instance, we have “game” is the object (main components), and we know that we have to construct “game” in the result GF grammar to be a NP (noun phrase). Program $\Pi _2$ identifies that there are two complement components for the word “game”, which are “board” and “popular”, a noun and an adjective respectively. The GF encoder then select the set of rules: N $\rightarrow $ N $\rightarrow $ CN and A $\rightarrow $ AP to create the common noun “board game” and the adjective phrase first. The next rule is AP $\rightarrow $ CN $\rightarrow $ CN. The last rule to be applied is CN $\rightarrow $ NP. The selection is easily decided since the input and the output of the rules are pre-determined, and there is no ambiguity in the selection process.The encoder uses the GF rules and the components identified by the previous subsections to produce different constructors for different components of a sentence. A part of the output of the GF encoder for the object “game” isThe encoder will also create the operators that will be included in the oper section of the GF grammar for supporting the new constructor. For example, the following operators will be generated for serving the Game constructor above:Method ::: GF Grammar Exporter	The GF Grammar Exporter has the simplest job among all modules in the system. It creates a GF program for a paragraph using the GF grammars created for the sentences of the paragraph. By taking the union of all respective elements of each grammar for each sentence, i.e., categories, functions, linearizations and operators, the Grammar Exporter will group them into the set of categories (respectively, categories, functions, linearizations, operators) of the final grammar.Experiments	We describe our method of generating natural language in two applications. The first application is to generate a natural language description for workflow created by the system built in the Phylotastic project described in BIBREF2. Instead of requiring that the ontologies are annotated using Attempto, we use natural language sentences to annotate the ontologies. To test the feasibility of the approach, we also conduct another use case with the second ontology, that is entirely different from the ontologies used in the Phylotastic project. The ontology is about people and includes descriptions for certain class.The second application targets the challenge of creating an abstract Wikipedia from the BlueSky session of 2018 International Semantic Web Conference BIBREF7. We create an intermediate representation that can be used to translate the original article in English to another language. In this use case, we translate the intermediate representation back to English and measure how the translated version stacks up again the original one. We assess the generation quality automatically with BLEU-3 and ROUGE-L (F measure). BLEU BIBREF16 and ROUGE BIBREF17 algorithms are chosen to evaluate our generator since the central idea of both metrixes is “the closer a machine translation is to a professional human translation, the better it is”, thus, they are well-aligned with our use cases' purpose. In short, the higher BLUE and ROUGE score are, the more similar the hypothesis text and the reference text is. In our use case, the hypothesis for BLEU and ROUGE is the generated English content from the intermediate representation, and the reference text is the original text from Wikipedia.Experiments ::: NLG for Annotated Ontologies	As described in BIBREF2, the author's system retrieves a set of atoms from an ASP program such as those in Listing where phylotastic FindScientificNamesFromWeb GET was shortened to service, propagates the atoms, and constructs a set of sentences having similar structure to the sentence “The input of phylotastic FindScientificNamesFromWeb GET is a web link. Its outputs are a set of species names and a set of scientific names”. In this sentence, phylotastic FindScientificNamesFromWeb GET is the name of the service involved in the workflow of the Phylotastic project. All of the arguments of the atoms above are the names of classes and instances from Phylotastic ontology.We replace the original Attempto annotations with the natural language annotations as in Table TABREF24 and test with our system.With the same set of atoms as in Listing , our system generates the following description “Input of phylotastic FindScientificNamesFromWeb GET is web link. Type of web link is url. Output of phylotastic FindScientificNamesFromWeb GET is scientific names. Output of phylotastic FindScientificNamesFromWeb GET is species names. Type of scientific names is names. Type of species name is names.”.We also test our system with the people ontology as noted above. We extract all comments about people and replace compound sentences with simple sentences, e.g., “Mick is male and drives a white van” is replaced by the two sentences “Mick is male” and “Mick drives a white van.” to create a collection of sample sentences. We then use our system to generate a GF program which is used to generate sentences for RDF tuples. Sample outputs for some tuples are in Table TABREF25. This shows that for targeted applications, our system could do a reasonable job.Experiments ::: Intermediate Representation for Wiki Pages	Since our system creates a GF program for a set of sentences, it could be used as an intermediate representation of a paragraph. This intermediate representation could be used by GF for automatic translation as GF is well-suited for cross-languages translation. On the other hand, we need to assess whether the intermediate representation is meaningful. This use case aims at checking the adequacy of the representation. To do so, we generate the English sentences from the GF program and evaluate the quality of these sentences against the original ones. We randomly select 5 articles from 3 Wikipedia portals: People, Mathematics and Food & Drink.With the small set of rules introducing in this paper to recognize sentence structure, there would be very limited 4-gram in the generated text appearing in original Wikipedia corpus. Therefore, we use BLEU-3 with equal weight distribution instead of BLEU-4 to assess the generated content. Table TABREF27 shows the summary of the number of assessable sentences from our system. Out of 62 sentences from 3 portals, the system cannot determine the structure 2 sentences in Mathematics due to their complexity. This low number of failure shows that our 5 proposed sentence structures effectively act as a lower bound on sentence recognition module.In terms of quality, Table TABREF28 shows the average of BLEU and ROUGE score for each portal. Note that the average BLUE score is calculated only on BLEU assessable sentences, while average ROUGE score is calculated on the sentences whose structure can be recognized and encoded by our system. We note that the BLEU or ROUGE score might not be sufficiently high for a good quality translation. We believe that two reasons contribute to this low score. First, the present system uses fairly simple sentence structures. Second, it does not consider the use of relative clauses to enrich the sentences. This feature will be added to the next version of the system.Table TABREF32 summarizes the result of this use case. On the left are the paragraphs extracted from the Wikipedia page about Rice in Food & Drink, Decimal in Mathematics, and about Alieu Ebrima Cham Joof from People. As we can see, the main points of the paragraphs are maintained.Related Works	The systems developed in BIBREF18, BIBREF19, BIBREF3 use statistical generation method to produce descriptions of tables or explanation and recommendation from users' reviews of an item. All three systems are capable of generating high quality descriptions and/or explanations. In comparing to these systems, our system does not use the statistical generation method. Instead, we use Grammatical Framework for the generation task. A key difference between these systems and our system lies in the requirement of a large corpus of text in a specific domain for training and generation of these systems. Our system can work with very limited data and a wide range of domains.Another method for generating natural language explanation for an question-answering system is proposed in BIBREF20, BIBREF4. BIBREF20 (BIBREF20) describes a system that can give reasonable and supportive evidence to the answer to a question asked to an image, while BIBREF4 (BIBREF4) generates explanations for scheduling problem using argumentation. BIBREF21 (BIBREF21) use ASP to develop a system answering questions in the do-it-yourself domain. These papers use templates to generate answers. The generated GF program generated by our system, that is used for the NLG task, is automatically created from a provided input.The sophisticated system presented by BIBREF5 translates both question and the given natural language text to logical representation, and uses logical reasoning to produce the answer. Our system is similar to their system in that both employ recent developments of NLP into solving NLG problems.Conclusions and Future Work	We propose a system implemented using answer set programming (ASP) and Grammatical Framework (GF), for automatic generation of natural language descriptions in applications targeting mainstream users. The system does not require a large corpus for the generation task and can be used in different types of applications.In the first type of applications, the system can work with annotated ontologies to translate a set of atoms—representing the answer to a query to the ontology—to a set of sentences. To do so, the system extracts the annotations related to the atoms in the answer and creates a GF program that is then used to generate natural language description of the given set of atoms. In the second type of applications, the system receives a paragraph of text and generates an intermediate representation—as a GF program—for the paragraph, which can be used for different purpose such as cross-translation, addressing a need identified in BIBREF7 .Our use cases with different ontologies and Wikipedia portals provide encouraging results. They also point to possible improvements that we plan to introduce to the next version of the system. We will focus on processing relative clauses and enriching the set of sentence structures, especially for compound and complex sentences.","['How does sentence construction component works?', 'What are two use cases that demonstrate capability of created system?']","['Unacceptable', 'Unacceptable']"
39,"Sentence Simplification with Memory-Augmented Neural Networks	Sentence simplification aims to simplify the content and structure of complex sentences, and thus make them easier to interpret for human readers, and easier to process for downstream NLP applications. Recent advances in neural machine translation have paved the way for novel approaches to the task. In this paper, we adapt an architecture with augmented memory capacities called Neural Semantic Encoders (Munkhdalai and Yu, 2017) for sentence simplification. Our experiments demonstrate the effectiveness of our approach on different simplification datasets, both in terms of automatic evaluation measures and human judgments.	Introduction	The goal of sentence simplification is to compose complex sentences into simpler ones so that they are more comprehensible and accessible, while still retaining the original information content and meaning. Sentence simplification has a number of practical applications. On one hand, it provides reading aids for people with limited language proficiency BIBREF1 , BIBREF2 , or for patients with linguistic and cognitive disabilities BIBREF3 . On the other hand, it can improve the performance of other NLP tasks BIBREF4 , BIBREF5 , BIBREF6 . Prior work has explored monolingual machine translation (MT) approaches, utilizing corpora of simplified texts, e.g., Simple English Wikipedia (SEW), and making use of statistical MT models, such as phrase-based MT (PBMT) BIBREF7 , BIBREF8 , BIBREF9 , tree-based MT (TBMT) BIBREF10 , BIBREF11 , or syntax-based MT (SBMT) BIBREF12 .Inspired by the success of neural MT BIBREF13 , BIBREF14 , recent work has started exploring neural simplification with sequence to sequence (Seq2seq) models, also referred to as encoder-decoder models. Nisioi et al. Nisioi:17 implemented a standard LSTM-based Seq2seq model and found that they outperform PBMT, SBMT, and unsupervised lexical simplification approaches. Zhang and Lapata BIBREF15 viewed the encoder-decoder model as an agent and employed a deep reinforcement learning framework in which the reward has three components capturing key aspects of the target output: simplicity, relevance, and fluency. The common practice for Seq2seq models is to use recurrent neural networks (RNNs) with Long Short-Term Memory BIBREF16 or Gated Recurrent Unit BIBREF17 for the encoder and decoder BIBREF18 , BIBREF15 . These architectures were designed to be capable of memorizing long-term dependencies across sequences. Nevertheless, their memory is typically small and might not be enough for the simplification task, where one is confronted with long and complicated sentences. In this study, we go beyond the conventional LSTM/GRU-based Seq2seq models and propose to use a memory-augmented RNN architecture called Neural Semantic Encoders (NSE). This architecture has been shown to be effective in a wide range of NLP tasks BIBREF0 . The contribution of this paper is twofold:(1) First, we present a novel simplification model which is, to the best of our knowledge, the first model that use memory-augmented RNN for the task. We investigate the effectiveness of neural Seq2seq models when different neural architectures for the encoder are considered. Our experiments reveal that the NseLstm model that uses an NSE as the encoder and an LSTM as the decoder performed the best among these models, improving over strong simplification systems. (2) Second, we perform an extensive evaluation of various approaches proposed in the literature on different datasets. Results of both automatic and human evaluation show that our approach is remarkably effective for the task, significantly reducing the reading difficulty of the input, while preserving grammaticality and the original meaning. We further discuss some advantages and disadvantages of these approaches.Attention-based Encoder-Decoder Model	Our approach is based on an attention-based Seq2seq model BIBREF19 (Figure FIGREF1 ). Given a complex source sentence INLINEFORM0 , the model learns to generate its simplified version INLINEFORM1 . The encoder reads through INLINEFORM2 and computes a sequence of hidden states INLINEFORM3 : INLINEFORM0 ,where INLINEFORM0 is a non-linear activation function (e.g., LSTM), INLINEFORM1 is the hidden state at time INLINEFORM2 . Each time the model generates a target word INLINEFORM3 , the decoder looks at a set of positions in the source sentence where the most relevant information is located. Specifically, another non-linear activation function INLINEFORM4 is used for the decoder where the hidden state INLINEFORM5 at time INLINEFORM6 is computed by: INLINEFORM0 .Here, the context vector INLINEFORM0 is computed as a weighted sum of the hidden vectors INLINEFORM1 : INLINEFORM0 , INLINEFORM1 ,where INLINEFORM0 is the dot product of two vectors. Generation is conditioned on INLINEFORM1 and all the previously generated target words INLINEFORM2 : INLINEFORM0 , INLINEFORM0 ,where INLINEFORM0 is some non-linear function. The training objective is to minimize the cross-entropy loss of the training source-target pairs.Neural Semantic Encoders	An RNN allows us to compute a hidden state INLINEFORM0 of each word summarizing the preceding words INLINEFORM1 , but not considering the following words INLINEFORM2 that might also be useful for simplification. An alternative approach is to use a bidirectional-RNN BIBREF20 . Here, we propose to use Neural Semantic Encoders BIBREF21 . During each encoding time step INLINEFORM3 , we compute a memory matrix INLINEFORM4 where INLINEFORM5 is the dimensionality of the word vectors. This matrix is initialized with the word vectors and is refined over time through NSE's functions to gain a better understanding of the input sequence. Concretely, NSE sequentially reads the tokens INLINEFORM6 with its read function: INLINEFORM0 ,where INLINEFORM0 is an LSTM, INLINEFORM1 is the hidden state at time INLINEFORM2 . Then, a compose function is used to compose INLINEFORM3 with relevant information retrieved from the memory at the previous time step, INLINEFORM4 : INLINEFORM0 ,where INLINEFORM0 is a multi-layer perceptron with one hidden layer, INLINEFORM1 is the output vector, and INLINEFORM2 is a linear combination of the memory slots of INLINEFORM3 , weighted by INLINEFORM4 : INLINEFORM0 , INLINEFORM1 Here, INLINEFORM0 is the INLINEFORM1 row of the memory matrix at time INLINEFORM2 , INLINEFORM3 . Next, a write function is used to map INLINEFORM4 to the encoder output space: INLINEFORM0 ,where INLINEFORM0 is an LSTM, INLINEFORM1 is the hidden state at time INLINEFORM2 . Finally, the memory is updated accordingly. The retrieved memory content pointed by INLINEFORM3 is erased and the new content is added: INLINEFORM0 NSE gives us unrestricted access to the entire source sequence stored in the memory. As such, the encoder may attend to relevant words when encoding each word. The sequence INLINEFORM0 is then used as the sequence INLINEFORM1 in Section SECREF2 .Decoding	We differ from the approach of Zhang et al. Zhang:17 in the sense that we implement both a greedy strategy and a beam-search strategy to generate the target sentence. Whereas the greedy decoder always chooses the simplification candidate with the highest log-probability, the beam-search decoder keeps a fixed number (beam) of the highest scoring candidates at each time step. We report the best simplification among the outputs based on automatic evaluation measures.Datasets	Following BIBREF15 , we experiment on three simplification datasets, namely: (1) Newsela BIBREF22 , a high-quality simplification corpus of news articles composed by Newsela professional editors for children at multiple grade levels. We used the split of the data in BIBREF15 , i.e., 94,208/1,129/1,077 pairs for train/dev/test. (2) WikiSmall BIBREF10 , which contains aligned complex-simple sentence pairs from English Wikipedia (EW) and SEW. The dataset has 88,837/205/100 pairs for train/dev/test. (3) WikiLarge BIBREF15 , a larger corpus in which the training set is a mixture of three Wikipedia datasets in BIBREF10 , BIBREF11 , BIBREF23 , and the development and test sests are complex sentences taken from WikiSmall, each has 8 simplifications written by Amazon Mechanical Turk workers BIBREF12 . The dataset has 296,402/2,000/359 pairs for train/dev/test. Table TABREF7 provides statistics on the training sets.Models and Training Details	We implemented two attention-based Seq2seq models, namely: (1) LstmLstm: the encoder is implemented by two LSTM layers; (2) NseLstm: the encoder is implemented by NSE. The decoder in both cases is implemented by two LSTM layers. The computations for a single model are run on an NVIDIA Titan-X GPU. For all experiments, our models have 300-dimensional hidden states and 300-dimensional word embeddings. Parameters were initialized from a uniform distribution [-0.1, 0.1). We used the same hyperparameters across all datasets. Word embeddings were initialized either randomly or with Glove vectors BIBREF24 pre-trained on Common Crawl data (840B tokens), and fine-tuned during training. We used a vocabulary size of 20K for Newsela, and 30K for WikiSmall and WikiLarge. Our models were trained with a maximum number of 40 epochs using Adam optimizer BIBREF25 with step size INLINEFORM0 for LstmLstm, and INLINEFORM1 for NseLstm, the exponential decay rates INLINEFORM2 . The batch size is set to 32. We used dropout BIBREF26 for regularization with a dropout rate of 0.3. For beam search, we experimented with beam sizes of 5 and 10. Following BIBREF27 , we replaced each out-of-vocabulary token INLINEFORM3 with the source word INLINEFORM4 with the highest alignment score INLINEFORM5 , i.e., INLINEFORM6 .Our models were tuned on the development sets, either with BLEU BIBREF28 that scores the output by counting INLINEFORM0 -gram matches with the reference, or SARI BIBREF12 that compares the output against both the reference and the input sentence. Both measures are commonly used to automatically evaluate the quality of simplification output. We noticed that SARI should be used with caution when tuning neural Seq2seq simplification models. Since SARI depends on the differences between a system's output and the input sentence, large differences may yield very good SARI even though the output is ungrammatical. Thus, when tuning with SARI, we ignored epochs in which the BLEU score of the output is too low, using a threshold INLINEFORM1 . We set INLINEFORM2 to 22 on Newsela, 33 on WikiSmall, and 77 on WikiLarge.Comparing Systems	We compared our models, either tuned with BLEU (-B) or SARI (-S), against systems reported in BIBREF15 , namely Dress, a deep reinforcement learning model, Dress-Ls, a combination of Dress and a lexical simplification model BIBREF15 , Pbmt-R, a PBMT model with dissimilarity-based re-ranking BIBREF9 , Hybrid, a hybrid semantic-based model that combines a simplification model and a monolingual MT model BIBREF29 , and Sbmt-Sari, a SBMT model with simplification-specific components. BIBREF12 .Evaluation	We measured BLEU, and SARI at corpus-level following BIBREF15 . In addition, we also evaluated system output by eliciting human judgments. Specifically, we randomly selected 40 sentences from each test set, and included human reference simplifications and corresponding simplifications from the systems above. We then asked three volunteers to rate simplifications with respect to Fluency (the extent to which the output is grammatical English), Adequacy (the extent to which the output has the same meaning as the input sentence), and Simplicity (the extent to which the output is simpler than the input sentence) using a five point Likert scale.Automatic Evaluation Measures	The results of the automatic evaluation are displayed in Table TABREF15 . We first discuss the results on Newsela that contains high-quality simplifications composed by professional editors. In terms of BLEU, all neural models achieved much higher scores than Pbmt-R and Hybrid. NseLstm-B scored highest with a BLEU score of 26.31. With regard to SARI, NseLstm-S scored best among neural models (29.58) and came close to the performance of Hybrid (30.00). This indicates that NSE offers an effective means to better encode complex sentences for sentence simplification.On WikiSmall, Hybrid – the current state-of-the-art – achieved best BLEU (53.94) and SARI (30.46) scores. Among neural models, NseLstm-B yielded the highest BLEU score (53.42), while NseLstm-S performed best on SARI (29.75). On WikiLarge, again, NseLstm-B had the highest BLEU score of 92.02. Sbmt-Sari – that was trained on a huge corpus of 106M sentence pairs and 2B words – scored highest on SARI with 39.96, followed by Dress-Ls (37.27), Dress (37.08), and NseLstm-S (36.88).Human Judgments	The results of human judgments are displayed in Table TABREF16 . On Newsela, NseLstm-B scored highest on Fluency. Pbmt-R was significantly better than all other systems on Adequacy while LstmLstm-S performed best on Simplicity. NseLstm-B did very well on both Adequacy and Simplicity, and was best in terms of Average. Example model outputs on Newsela are provided in Table TABREF18 .On WikiSmall, NseLstm-B performed best on both Fluency and Adequacy. On WikiLarge, LstmLstm-B achieved the highest Fluency score while NseLstm-B received the highest Adequacy score. In terms of Simplicity and Average, NseLstm-S outperformed all other systems on both WikiSmall and WikiLarge.As shown in Table TABREF16 , neural models often outperformed traditional systems (Pbmt-R, Hybrid, Sbmt-Sari) on Fluency. This is not surprising given the recent success of neural Seq2seq models in language modeling and neural machine translation BIBREF30 , BIBREF27 . On the downside, our manual inspection reveals that neural models learn to perform copying very well in terms of rewrite operations (e.g., copying, deletion, reordering, substitution), often outputting the same or parts of the input sentence.Finally, as can be seen in Table TABREF16 , Reference scored lower on Adequacy compared to Fluency and Simplicity on Newsela. On Wikipedia-based datasets, Reference obtained high Adequacy scores but much lower Simplicity scores compared to Newsela. This supports the assertion by previous work BIBREF22 that SEW has a large proportion of inadequate simplifications.Correlations	Table TABREF20 shows the correlations between the scores assigned by humans and the automatic evaluation measures. There is a positive significant correlation between Fluency and Adequacy (0.69), but a negative significant correlation between Adequacy and Simplicity (-0.64). BLEU correlates well with Fluency (0.63) and Adequacy (0.90) while SARI correlates well with Simplicity (0.73). BLEU and SARI show a negative significant correlation (-0.54). The results reflect the challenge of managing the trade-off between Fluency, Adequacy and Simplicity in sentence simplification.Conclusions	In this paper, we explore neural Seq2seq models for sentence simplification. We propose to use an architecture with augmented memory capacities which we believe is suitable for the task, where one is confronted with long and complex sentences. Results of both automatic and human evaluation on different datasets show that our model is capable of significantly reducing the reading difficulty of the input, while performing well in terms of grammaticality and meaning preservation.Acknowledgements	We would like to thank Emily Druhl, Jesse Lingeman, and the UMass BioNLP team for their help with this work. We also thank Xingxing Zhang, Sergiu Nisioi for valuable discussions. The authors would like to acknowledge the reviewers for their thoughtful comments and suggestions. ","['what language was the data in?', 'what was the baseline?', 'which automatic metrics were used in evaluation?', 'how do humans judge the simplified sentences?', 'what datasets were used?']","['Unacceptable', 'Unacceptable', 'Unacceptable', 'Rate simplifications with respect to Fluency, Adequacy, and Simplicity, using a five point Likert scale.', 'Unacceptable']"
40,"Morphology-based Entity and Relational Entity Extraction Framework for Arabic	Rule-based techniques and tools to extract entities and relational entities from documents allow users to specify desired entities using natural language questions, finite state automata, regular expressions, structured query language statements, or proprietary scripts. These techniques and tools require expertise in linguistics and programming and lack support of Arabic morphological analysis which is key to process Arabic text. In this work, we present MERF; a morphology-based entity and relational entity extraction framework for Arabic text. MERF provides a user-friendly interface where the user, with basic knowledge of linguistic features and regular expressions, defines tag types and interactively associates them with regular expressions defined over Boolean formulae. Boolean formulae range over matches of Arabic morphological features, and synonymity features. Users define user defined relations with tuples of subexpression matches and can associate code actions with subexpressions. MERF computes feature matches, regular expression matches, and constructs entities and relational entities from user defined relations. We evaluated our work with several case studies and compared with existing application-specific techniques. The results show that MERF requires shorter development time and effort compared to existing techniques and produces reasonably accurate results within a reasonable overhead in run time.	Introduction	Computational Linguistics (CL) is concerned with building accurate linguistic computational models. Natural Language Processing (NLP) is concerned with automating the understanding of natural language. CL and NLP tasks range from simple ones such as spell checking and typing error correction to more complex tasks including named entity recognition (NER), cross-document analysis, machine translation, and relational entity extraction BIBREF0 , BIBREF1 . Entities are elements of text that are of interest to an NLP task. Relational entities are elements that connect entities. Annotations relate chunks of text to labels denoting semantic values such as entities or relational entities. We refer to annotations and labels as tags and tag types, respectively, in the sequel.Supervised and unsupervised empirical learning techniques tackle NLP and CL tasks. They employ machine learning without the need to manually encode the requisite knowledge BIBREF2 . Supervised learning techniques require training corpora annotated with correct tags to learn a computational model. Supervised and unsupervised techniques require annotated reference corpora to evaluate the accuracy of the technique using metrics such as precision and recall BIBREF3 , BIBREF4 , BIBREF5 .Researchers build training and reference corpora either manually, incrementally using learning techniques, or using knowledge-based annotation techniques that recognize and extract entities and relational entities from text. Knowledge-based techniques use linguistic and rhetorical domain specific knowledge encoded into sets of rules to extract entities and relational entities BIBREF2 . While existing annotation, entity, and relational entity extraction tools exist BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , most of them lack Arabic language support, and almost all of them lack Arabic morphological analysis support BIBREF12 . Fassieh BIBREF13 is a commercial Arabic annotation tool with morphological analysis support and text factorization. However, this tool lacks support for entity and relational entity extraction.Figure illustrates the target of MERF using the directions to Dubai Mall example . The figure also presents a transliteration and an English translation of the Arabic text. The framed words in the text are entities referring to names of people ( INLINEFORM0 ), names of places ( INLINEFORM1 ), relative positions ( INLINEFORM2 ), and numerical terms ( INLINEFORM3 ). We would like to extract those entities, and then extract the relational entities forming the graph in Figure where vertices express entities, and edges represent the relational entities.In this paper, we present MERF, a morphology-based entity and relational entity extraction framework for Arabic text. MERF provides a user-friendly interface where the user defines tag types and associates them with regular expressions over Boolean formulae. A Boolean formula is defined by a term, negation of a term, or disjunction of terms. Terms are matches to Arabic morphological features including prefix, stem, suffix, part of speech (POS) tags, gloss tags, extended synonym tags, and semantic categories. For example, entity INLINEFORM0 in Figure has a “place” semantic category. MERF regular expressions support operators such as concatenation, zero or one, zero or more, one or more, up to INLINEFORM1 repetitions where INLINEFORM2 is a non-zero positive integer, and logical conjunction and disjunction. For example, the sequence between INLINEFORM3 and INLINEFORM4 matches a regular expression INLINEFORM5 that requires two semantic place categories with a place-preposition POS tag ( INLINEFORM6 ) in between.An editor allows the user to associate an action with each subexpression. The user specifies the action with C++ code and uses an API to access information related to the matches such as text, position, length, morphological features, and numerical value. Each regular expression is associated with a named identifier to form a local grammar like structure BIBREF14 . A relation definition GUI allows the user to provide relational tuples where each tuple has a source, a destination and an edge label. The user uses the regular expression identifiers to define the relational tuple elements. For example, the relation between INLINEFORM0 and INLINEFORM1 shown in Figure is a match of a relational tuple over the components of INLINEFORM2 . We refer to regular expressions and Boolean formulae as expressions and formulae, respectively. We also refer to expressions as rules when used in a grammar context; e.g. when used with an identifier.MERF takes an Arabic text and the local grammar defined by the Boolean formulae and the regular expressions. MERF computes the morphological solutions of the input text then computes matches to the Boolean formulae therein. MERF then generates a non-deterministic finite state automata (NDFSA) for each expression and simulates it with the sequence of Boolean formulae matches to compute the regular expression matches. MERF generates executable code for the actions associated with the regular expressions, compiles, links, and executes the generated code as shared object libraries. Finally, MERF constructs the semantic relations and cross-reference between entities. MERF also provides visualization tools to present the matches, and estimate their accuracy with respect to reference tags.This work significantly extends JaZaMatar BIBREF15 that allows for manual, and morphology annotation. MERF enables a user to incrementally create complex annotations for Arabic based on automatic extraction of morphological tags through a user-friendly interactive interface. MERF has the following advantages.In MERF, we make the following contributions.The rest of the paper is structured as follows. Section SECREF2 introduces Arabic morphological analysis and its important role in Arabic NLP. Section SECREF3 explains the methodology of MERF. Section SECREF4 presents MERF components. Section SECREF5 presents MERF GUI. Section SECREF6 presents and discusses related work. Section SECREF7 presents the evaluation results. Finally, we conclude and discuss future work in Section SECREF8 .Background: Morphological Analyzer	Morphological analysis is key to Arabic NLP due to the exceptional degree of ambiguity in writing, the rich morphology, and the complex word derivation system BIBREF16 , BIBREF17 , BIBREF18 . Short vowels, also known as diacritics, are typically omitted in Arabic text and inferred by readers BIBREF12 . For example, the word bncan be interpreted as bon (“coffee”) with a damma diacritic on the letter utf8بـstandard or bin (“son of”) with a kasra diacritic on the letter utf8بـstandard.Morphological analysis is required even for tokenization of Arabic text. The position of an Arabic letter in a word (beginning, middle, end, and standalone) changes its visual form. Some letters have non-connecting end forms which allows visual word separation without the need of a white space separator. For example, the word utf8ياسمينstandard can be interpreted as the “Jasmine” flower, as well as utf8ياstandard (the calling word) followed by the word utf8سمينstandard (obese). Consider the sentence AlmdrsTdhb alwald-ilA dhb alwald-ilA almdrsT (“the kid went to school”). The letters utf8دstandard and utf8ىstandard have non-connecting end of word forms and the words utf8الولدstandard,utf8الىstandard, andutf8المدرسةstandard are visually separable, yet there is no space character in between. Newspaper articles with text justification requirements, SMS messages, and automatically digitized documents are examples where such problems occur.MERF is integrated with Sarf, an in-house open source Arabic morphological analyzer based on finite state transducers BIBREF19 . Given an Arabic word, Sarf returns a set of morphological solutions. A word might have more than one solution due to multiple possible segmentations and multiple tags associated with each word. A morphological solution is the internal structure of the word composed of several morphemes including affixes (prefixes and suffixes), and a stem, where each morpheme is associated with tags such as POS, gloss, and category tags BIBREF16 , BIBREF20 .Prefixes attach before the stem and a word can have multiple prefixes. Suffixes attach after the stem and a word can have multiple suffixes. Infixes are inserted inside the stem to form a new stem. In this work we consider a set of stems that includes infix morphological changes. The part-of-speech tag, referred to as POS, assigns a morpho-syntactic tag for a morpheme. The gloss is a brief semantic notation of morpheme in English. A morpheme might have multiple glosses as it could stand for multiple meanings. The category is a custom tag that we assign to multiple morphemes. For example, we define the Name of Person category to include proper names.We denote by INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , INLINEFORM4 , and INLINEFORM5 , the set of all stems, prefixes, suffixes, POS, gloss, and user defined category tags, respectively. Let INLINEFORM6 be a set of Arabic words denoting the text documents. MERF uses Sarf to compute a set of morphological solutions INLINEFORM7 for each word INLINEFORM8 . Each morphological solution INLINEFORM9 is a tuple of the form INLINEFORM10 where INLINEFORM11 , INLINEFORM12 , INLINEFORM13 , INLINEFORM14 , and INLINEFORM15 . INLINEFORM16 and INLINEFORM17 are the POS, gloss and category tags of prefix INLINEFORM18 . INLINEFORM19 and INLINEFORM20 are the POS, gloss and category tags of suffix INLINEFORM21 . INLINEFORM22 and INLINEFORM23 are the POS, gloss and category tags of stem INLINEFORM24 . Intuitively, INLINEFORM25 and INLINEFORM26 are concatenations of prefix, suffix, POS, gloss and category values, respectively.Table TABREF9 shows the morphological analysis of the word utf8فَسَيَأْكُلهاstandard. The word is composed of the prefix morphemes utf8فَstandard, utf8سَstandard, and utf8يَstandard, followed by the stem utf8أْكُلstandard, and then followed by the suffix morpheme utf8هاstandard. Each morpheme is associated with a number of morphological features. The CONJ, FUT, IV3MS VERB_IMPERFECT, and IVSUFF_DO:3FS POS tags indicate conjunction, future, third person masculine singular subject pronoun, an imperfect verb, and a third person feminine singular object pronoun, respectively. The POS and gloss notations follow the Buckwalter notation BIBREF21 .MERF Methodology	Figure illustrates the four processes involved in MERF methodology. The first process takes Arabic text and provides the user with a morphology-based Boolean (MB) formulae GUI. The user interactively composes MB-formulae using the GUI and the output of the simulator and the INLINEFORM0 detector. The simulator and the detector apply the formulae over the morphological solutions of the Arabic text and produce the MB-formulae tags.The second process takes the MB-formulae tags and the Arabic text and provides the user with a morphology-based grammar rule GUI. The user interactively composes MB-grammar rules using the GUI and the output of the MB-grammar rule simulator. The grammar rule simulator applies the rules over the MB-formulae tags and produces the MB-grammar rule tags.The third process takes the MB-grammar rule tags and provides the user with a relation and action GUI. The user interactively provides (1) the relation definitions and (2) the actions in terms of identifiers from the MB-grammar rules. The relation extraction produces the target entities and relational entities. The action execution enriches the entities and the relational entities with powerful semantics. For example, users can utilize actions to compute statistical features, store intermediate results, or apply intelligent entity inference techniques as we show later in the numerical extraction example of Subsection SECREF41 . Finally, in the fourth process the user compares the results with golden reference chunks and visualizes the difference. This allows the user to refine the formulae, rules, relations and actions.After relation extraction, we are interested to relate entities that express the same concept. MERF provides the extended synonym feature of second order as a default cross-reference relation ( INLINEFORM0 ). In Figure , triggering this feature creates the edge labeled with isSyn between the nodes Khalifa Tower and The building.The user may refine the defined formulae, rules and relations and the corresponding formulae tags, rule tags, entities and relational entities either using the GUI or directly through readable output files. The files are in the javascript object notation (JSON) BIBREF22 format that is intuitive to read and modify. MERF separates the user defined formulae, rules, actions and relations in a MERF tag type file and the matching tags in a tags files. The separation serves the user to apply the tag types to multiple case studies and to obtain a separate file of resulting tags for each.The extended synonymy feature Syn k Syn^k	Up to our knowledge, INLINEFORM0 provides the first light Arabic WordNet based on the lexicon of Sarf. The sets INLINEFORM1 and INLINEFORM2 denote all English words, Arabic words, and Arabic lexicon words, respectively. Recall that INLINEFORM3 and INLINEFORM4 denote the set of glosses and stems in the morphological analyzer, respectively. We have INLINEFORM5 and INLINEFORM6 . Function INLINEFORM7 maps Arabic stems to subsets of related English glosses, where INLINEFORM8 denotes the power set of INLINEFORM9 which is the set of all subsets of INLINEFORM10 . Function INLINEFORM11 maps Arabic lexicon words to subsets of relevant Arabic stems.Given a word INLINEFORM0 , INLINEFORM1 is the set of Arabic stems directly related to INLINEFORM2 through the gloss map. Let INLINEFORM3 denote stems related to INLINEFORM4 using the gloss map of order INLINEFORM5 recursively such that INLINEFORM6 and INLINEFORM7 . Formally, INLINEFORM8 for INLINEFORM9 . The example in Figure FIGREF11 illustrates the computation. Let INLINEFORM10 denote an input Arabic word mA', which has the gloss water, i.e. water INLINEFORM11 . INLINEFORM12 shares this gloss with the stem n.d.h, denoted INLINEFORM13 , i.e. INLINEFORM14 . Next, the stem rss, denoted INLINEFORM15 , shares the gloss spray with INLINEFORM16 , i.e. INLINEFORM17 . Therefore, INLINEFORM18 relates the words mA' and rss.MRE: Morphology-based regular expressions	Let INLINEFORM0 be the set of atomic term predicates, where INLINEFORM1 and INLINEFORM2 denote exact match and containment, respectively. Also, let INLINEFORM3 be the set of morphological features where each morphological feature INLINEFORM4 is in turn a set of morphological feature values. Given a word INLINEFORM5 , a user defined constant feature value INLINEFORM6 , and an integer INLINEFORM7 , the following are morphology-based atomic terms (MAT), terms for short. INLINEFORM0 where INLINEFORM1 , INLINEFORM2 , and INLINEFORM3 . Informally, a solution vector of INLINEFORM4 exists with a feature containing or exactly matching the user-chosen feature value INLINEFORM5 . INLINEFORM0 . Informally, this checks if INLINEFORM1 is an extended synonym of a stem INLINEFORM2 . We limit INLINEFORM3 to a maximum of 7 since we practically noticed that (1) values above 7 introduce significant semantic noise and (2) the computation is expensive without a bound.A morphology-based Boolean formula (MBF) is of the following form. INLINEFORM0 and INLINEFORM1 are MBF formulae where INLINEFORM2 is a MAT and INLINEFORM3 is the negation operator. INLINEFORM0 is an MBF where INLINEFORM1 and INLINEFORM2 are MBF formulae, and INLINEFORM3 is the disjunction (union) operator.Moreover, MERF provides INLINEFORM0 to be a default Boolean formula that tags all other words in the text that do not match a user defined formula. We also refer to those words as null words.Consider the task we discussed in the introduction (Figure ) and recall that we are interested in identifying names of people, names of places, relative positions, and numerical terms. Table presents the defined formulae. The user denotes the “name of person” entities with formula INLINEFORM0 which requires the category feature in the morphological solution of a word to be Name_of_Person. The entities INLINEFORM1 , INLINEFORM2 , and INLINEFORM3 are matches of the formula INLINEFORM4 in the text. Similarly, the user specifies formula INLINEFORM5 to denote “name of place” entities. The user specifies formula INLINEFORM6 to denote “relative position” entities, and defines it as a disjunction of terms that check for solutions matching stems such as qrb (“near”) and fy (“in”). Similarly, INLINEFORM7 denotes numerical terms and is a disjunction of constraints requiring the stem feature to belong to a set of stems such as 'wl(“first”), tAny(“second”), ...`Asr(“tenth”).Next, we define a morphology-based regular expression (MRE) as follows. INLINEFORM0 is an MRE where INLINEFORM1 is an MBF. INLINEFORM0 is an MRE where INLINEFORM1 and INLINEFORM2 are both MRE expressions. A match of INLINEFORM3 followed by a match of INLINEFORM4 satisfies this concatenation operation. INLINEFORM0 ⌃ INLINEFORM1 and INLINEFORM2 are MRE where INLINEFORM3 is an MRE, and are satisfied by zero or more, one or more, up to INLINEFORM4 matches, and an optional single match of INLINEFORM5 , respectively. INLINEFORM0 (conjunction) and INLINEFORM1 (disjunction) are MRE where INLINEFORM2 and INLINEFORM3 are MRE, and are satisfied by the intersection of INLINEFORM4 and INLINEFORM5 matches, and the union of the INLINEFORM6 and INLINEFORM7 matches, respectively.We denote by INLINEFORM0 the set of matches of an MRE INLINEFORM1 .Back to the example in Figure . We use the formulae defined in Table to construct an MRE such as INLINEFORM0 where INLINEFORM1 and INLINEFORM2 denote disjunction, one or more, zero or one, and up to INLINEFORM3 matches, respectively. The expression specifies a sequence of places or names of persons, optionally followed by a null word, followed by one relative position, followed by up to two possible null words, followed by one or more match of name of place, name of person, or numerical term. INLINEFORM4 and INLINEFORM5 are used in the expression to allow for flexible matches.The matching parse trees in Figure illustrate two matches of the expression computed by MERF. The first tree refers to the text brj hlyfT bAlqrb mn AltqA.t` Al-'wl(“Khalifa Tower next to the first intersection”). The second tree refers to the text dby mwl `lA mqrbT mn hdA AlmbnA(“Dubai Mall is located near this building”). The leaf nodes of the trees are matches to formulae and the internal nodes represent roots to subexpression matches. For instance, brj hlyfT in match 1 tree corresponds to the subexpression INLINEFORM0 .User-defined relations and actions	A relation is defined by the user as a tuple INLINEFORM0 where INLINEFORM1 and INLINEFORM2 are identifiers associated with subexpressions of an MRE INLINEFORM3 . Matches of the relation are a set of labeled binary edges where matches of INLINEFORM4 and INLINEFORM5 are the source and destination nodes and matches of INLINEFORM6 are the edge labels. We denote INLINEFORM7 to be the set of matches of the corresponding relation, and we refer to them as relational entities.We are interested in constructing the relational entity graph in Figure . Let INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , and INLINEFORM4 be identifiers to the subexpressions INLINEFORM5 , INLINEFORM6 , INLINEFORM7 , INLINEFORM8 , and INLINEFORM9 , respectively. The matches to INLINEFORM10 , INLINEFORM11 , INLINEFORM12 , and INLINEFORM13 in match 1 (Fig. ) are brj hlyfT (“Khalifa Tower”), bAlqrb (“next”), mn (“to”), and AltqA.t` Al-'wl (“first intersection”). Note that there is no match to the optional INLINEFORM14 formula in match 1. Similarly, the matches to INLINEFORM15 , INLINEFORM16 , INLINEFORM17 , INLINEFORM18 , and INLINEFORM19 in the second matching tree are dby mwl (“Dubai Mall”), `lA (“is located”), mqrbT (“near”), mn hdA (“this”), and AlmbnA (“building”), respectively.We define the semantic relations INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 . Relation INLINEFORM3 creates the edge labeled next to between Khalifa tower and intersection 1 nodes from match 1, and the edge labeled near between Dubai Mall and the building nodes from match 2. Relation INLINEFORM4 creates the edge labeled prep between Dubai Mall and near nodes from match 2. Relation INLINEFORM5 creates the edge labeled from between intersection 1 and next to nodes in match 1, and the edge labeled from this between near and the building nodes in match 2.Moreover, MERF allows advanced users to write C++ code snippets to process matches of subexpressions. Each subexpression can be associated with two computational actions: pre-match and on-match. MERF provides an API that enriches the actions with detailed access to all solution features of an expression or a formula match including text, position, length, equivalent numerical value when applicable, and morphological features. The API follows a decorator pattern in that it incrementally adds the action results to the matching entities. Once MERF computes all matching parse trees, it traverses each tree to execute the user defined pre-match actions in pre-order manner and the on-match actions in post-order manner. This follows an observer pattern that notifies listeners with each produced match.MERF simulators	The set of tag types INLINEFORM0 contains tuples of the form INLINEFORM1 where INLINEFORM2 is a text label with a descriptive name, INLINEFORM3 is an MRE, and INLINEFORM4 is a visualization legend with font and color information. For the example of Figure , INLINEFORM5 is “direction”, INLINEFORM6 is INLINEFORM7 , and INLINEFORM8 is italic.For each word INLINEFORM0 . MERF computes a Boolean value for all MBFs. For example, brj matches MBF INLINEFORM1 . Then, it computes the set of MBF tags INLINEFORM2 which tags a word INLINEFORM3 with INLINEFORM4 iff the MBF INLINEFORM5 associated with tag type INLINEFORM6 is true for INLINEFORM7 . The MBF evaluation results in a sequence of tag sets INLINEFORM8 . If a word INLINEFORM9 has no tag type match, its tag set INLINEFORM10 is by default the singleton INLINEFORM11 . For example, the tag sets for the text in Figure follows INLINEFORM12 brj INLINEFORM13 xlyfT INLINEFORM14 .utf8standardFor each MRE, MERF generates its equivalent non-deterministic finite automaton (NFA) in the typical manner BIBREF23 . We support the upto operation ( INLINEFORM0 INLINEFORM1 ), which is not directly supported in sipser2012introduction BIBREF23 , by expanding it into a regular expression form; for example INLINEFORM2 3 is equivalent to INLINEFORM3 . Consider the example of Figure and the corresponding expression INLINEFORM4 . Figure FIGREF23 shows part of the corresponding NFA where INLINEFORM5 represent NFA states, and edges are transitions based on MBF tags such as INLINEFORM6 and INLINEFORM7 . Edges labeled with the empty string INLINEFORM8 are non-deterministic.MERF simulates the generated NFA over the sequence of tag sets matching the MBF formulae. A simulation match INLINEFORM0 of an expression INLINEFORM1 is a parse tree where the root spans the expression, the internal nodes are roots to subexpressions of INLINEFORM2 , and the leaves are matches of the MBF formulae of INLINEFORM3 , e.g. Figure . The sequence of leaf matches forms a vector of tags INLINEFORM4 corresponding to the text sequence INLINEFORM5 where INLINEFORM6 . If we have more than one match for an expression, MERF returns the longest.Finally, MERF computes the relational entities corresponding to each user defined relation INLINEFORM0 INLINEFORM1 .MERF GUI	MERF provides a user friendly interface to specify the atomic terms, the MBFs, the MREs, the tag types, and the legends. The GUI also allows the user to modify and correct the tag set INLINEFORM0 . The GUI allows the user also to compute accuracy results that compare different tag sets and that can serve well as inter annotation agreement results when the tag sets come from two human annotators, or as evaluation results when comparing with reference tag sets.Tag type Boolean formula editor	The user writes MBF tag types with the tag type editor introduced in JaZaMatar BIBREF15 . First the user specifies atomic terms by selecting a feature from INLINEFORM0 . The user can also choose whether to require an exact match using the isA predicate, or a substring match using the contains predicate option.The user can add and remove feature values to the atomic terms using push buttons. A check box in the “Feature” column allows negating the term, and the “Relation” column switches the predicate between isA and contains. The list of feature and value pairs is interpreted as a disjunction to form the MBF. A right pane shows a description of the tag type and a set of legend descriptors. When the stem or gloss features are selected, the user has the option to use the INLINEFORM0 feature.In the direction extraction task example, the user specifies four MBF-based tag types with labels INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , and INLINEFORM3 with “name of person”, “name of place”, “relative position”, and “numerical term” descriptions, respectively. For each MBF, the user selects the morphological features, specifies the constant value INLINEFORM4 , and adds it to the Boolean formula editor.MBF match visualization	The MBF match visualizer shows color sensitive text view, the tag list view, and the tag description view. The tag description view presents the details of the selected tag along with the relevant tag type information. The user can edit the tags using a context sensitive menus. MERF GUI also allows manual tag types and corresponding tags that are not based on morphological features. This enables building reference corpora without help from the morphological analyzer.Tag type regular expression editor	After interacting with the MBF editor, the user moves to specify the regular expressions. The MRE editor of Figure FIGREF26 allows the definition of an MRE tag type in a user-friendly manner. The user first adds the required MBF formulae by selecting a label from INLINEFORM0 under MBFs. The Boolean formula of a highlighted tag type is shown in the table on the lower left pane. Each selected MBF is associated with an automatic name. The user can nest the MRE expression using a tree view of the MRE operations. The tree features the name, MBF, and operation for each subexpression.To specify a binary operation the user selects two subexpressions and clicks the corresponding operation button. The operations include disjunction, conjunction, zero or one, sequence, zero or more, one or more, and up to a user defined constant. The right pane shows a description of the tag type and a set of legend descriptors.MRE match visualization	While specifying an MRE, the user can interact with the visualization and editor views to make sure the MRE expresses the intent. The color-sensitive text view in Figure FIGREF29 shows the highlighted tag matches after the user called the MRE simulator using the Tagtypes menu.The matching parse tree view shows the selected match in a graph view. Figure FIGREF29 shows the matching parse tree of the direction task dby mwl `lA mqrbT mn hdA AlmbnA(“Dubai Mall is located near this building”).User defined relation editor	After the user is satisfied with the MRE matches, the user moves to define relations and code actions. The relation editor allows the user to define relations by specifying INLINEFORM0 tuples, where INLINEFORM1 and INLINEFORM2 denote source and destination entities, and INLINEFORM3 denotes the label. The editor shows the MRE tree and allows the user to select the subexpressions and select features of the matches of the subexpressions to define the three components of the relation.A snapshot of the GUI in Figure FIGREF29 shows in an interactive graph view the entity-relation graph of the match of the user defined relation extracted from the matching parse tree of the MRE. In the computational action editor, an advanced user can enter C++ code and use the MERF API to program and process subexpression matches.Analysis	In the analysis view, the user provides two tag sets INLINEFORM0 and INLINEFORM1 and two tag type sets INLINEFORM2 and INLINEFORM3 as input. The tag type difference view shows the text annotated in three panes: (i) the common tag types INLINEFORM4 , (ii) the tag types in INLINEFORM5 but not in INLINEFORM6 , and (iii) the tag types in INLINEFORM7 and not in INLINEFORM8 . Similarly, the tag difference view shows INLINEFORM9 , INLINEFORM10 and INLINEFORM11 in addition to precision, recall and F-measure values. The user selects a predicate to compute the metrics from the following predicates: (1) “Intersection”: a tag from INLINEFORM12 intersects in text with a tag in INLINEFORM13 , (2) “Exact”: a tag from INLINEFORM14 exactly matches a tag in INLINEFORM15 , (3) “A includes B”: a tag from INLINEFORM16 contains a tag from INLINEFORM17 , and (4) “B includes A”: a tag from INLINEFORM18 contains a tag from INLINEFORM19 .Related Work	In this section we review the literature on entity and relation IE and on automatic and manual annotation techniques and compare to MERF.Information Extraction. The common pattern specification language (CPSL) targets system independent IE specifications BIBREF24 . MERF extends CPSL with Arabic morphological features, code actions, and user defined relations. SystemT BIBREF6 aims to overcome the limitations of CPSL. It is based on an algebraic approach to declarative information extraction, uses the declarative annotation query language (AQL), and uses an optimizer to generate high performance execution plans for the AQL rules. MERF supports multiple tags per word, and supports the MRE conjunction operator which overcomes the overlapping annotation problem discussed in SystemT.TEXTMARKER is a semi-automatic rule-based IE system for structured data acquisition BIBREF7 . Both TEXTMARKER and MERF provide the user with GUI editor and result visualizer.The work in urbain2012user BIBREF8 presents a user-driven relational model and targets entity and relation extraction. The user enters a natural language query, and uses the OpenNLP toolkit to extract tags and relations from the query. Similar to MERF, the system constructs entities and relations.QARAB is an Arabic question answering system that takes an Arabic natural language query and provides short answers for it BIBREF25 . QARAB uses traditional information retrieval techniques and an outdated Arabic NLP analyzer with limited features of Arabic words compared to the morphological analysis of MERF.Table TABREF32 summarizes the comparison between MERF and other systems. MERF differs in that it provides code actions, user defined relations, and an interactive graph visualization of the relational entities. It also differs in that it fully supports Arabic morphological analysis while only QARAB supports Arabic linguistic features using a parser, and the work in urbain2012user BIBREF8 uses OpenNLP that currently lacks full support for Arabic morphological features. Similar to TEXTMARKER, MERF has the advantage of providing a user-friendly interactive interface to edit the entity and relational specifications and visualize the results.DUALIST is an annotation system for building classifiers for text processing tasks using machine learning techniques BIBREF9 . MERF doesn't support classification tasks. However, MERF provides an interactive GUI where the user can edit MBF and MRE tags. This interactive environment contributes to the regular expression extraction and semantic relation construction which increases the overall accuracy.Another track in the literature targets specific tasks such as NER using statistical and machine-learning techniques such as maximum entropy, optimized feature sets and conditional random fields BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 . Knowledge-based techniques such as zaghouani2010adapting BIBREF30 and traboulsi2009arabic BIBREF14 propose local grammars with morphological stemming. ZaMaHaCicling2012Entity BIBREF31 extract entities and events, and relations among them, from Arabic text using a hierarchy of manually built finite state machines driven by morphological features, and graph transformation algorithms. Such techniques require advanced linguistic and programming expertise.WordNet is a lexical reference system that mimics human lexical memory and relates words based on their semantic values and their functional categories: nouns, verbs, adjectives, adverbs, and function words BIBREF32 . The INLINEFORM0 feature in MERF is inspired by WordNet.Annotation tools. MMAX2 is a manual multi-level linguistic annotation tool with an XML based data model BIBREF10 . BRAT BIBREF11 and WordFreak BIBREF33 are manual multi-lingual user-friendly web-based annotators that allow the construction of entity and relation annotation corpora. Knowtator BIBREF34 is a general purpose incremental text annotation tool implemented as a Protégé BIBREF35 plug-in. Protégé is an open-source platform with a suite of tools to construct domain models and knowledge-based applications with ontology. However, it doesn't support the Arabic language.MERF differs from MMAX2, BRAT, WordFreak, and Knowtator in that it is an automatic annotator that allows manual corrections and sophisticated tag type and relation specifications over Arabic morphological features.kholidy2010towards BIBREF36 present an overview of annotation tools and concludes with a set of rules and guidelines needed in an Arabic annotation alignment tool. The work in dukes2013supervised BIBREF37 presents a collaborative effort towards morphological and syntactic annotation of the Quran. dorr2010interlingual BIBREF38 present a framework for interlingual annotation of parallel text corpora with multi-level representations. kulick2010consistent BIBREF39 presents the integration of the Standard Arabic Morphological Analyzer (SAMA) into the workflow of the Arabic Treebank.The work in smrz2004morphotrees BIBREF40 presents a customizable general purpose tree editor, with the Arabic MorphoTrees annotations. The MorphoTrees present the morphological analyses in a hierarchical organization based on common features.Task specific annotation tools such as alrahabi2006semantic BIBREF41 use enunciation semantic maps to automatically annotate directly reported Arabic and French speech. AraTation is another task specific tool for semantic annotation of Arabic news using web ontology based semantic maps BIBREF42 . We differ in that MERF is general, and not task specific, and it uses morphology-based features as atomic terms. Fassieh is a commercial Arabic text annotation tool that enables the production of large Arabic text corpora BIBREF13 . The tool supports Arabic text factorization including morphological analysis, POS tagging, full phonetic transcription, and lexical semantics analysis in an automatic mode. Fassieh is not directly accessible to the research community and requires commercial licensing. MERF is open source and differs in that it allows the user to build tag types and extract entities and relations from text.Results	In this section we evaluate MERF with four case studies. We perform a survey-like evaluation where developers manually built task specific information extraction tools for the case studies and other developers built equivalent MERF tools. The aim of the comparison is to showcase that MERF enables fast development of linguistic applications with similar accuracy and a reasonable affordable overhead in computational time. We report development time, size of developed code versus size of grammar, running time, and precision-recall as metrics of cost, complexity, overhead, and accuracy, respectively.We survey three case studies from the literature: (1) narrator chain, (2) temporal entity, and (3) genealogy entity extraction tasks, and we use the reported development time for the task specific techniques proposed in ANGE BIBREF43 , ATEEMA BIBREF44 , and GENTREE BIBREF31 , respectively. We also compare a MERF number normalization task to a task specific implementation.We evaluated ANGE with Musnad Ahmad, a hadith book, where we constructed an annotated golden reference containing 1,865 words. We evaluated ATEEMA with articles from issues of the Lebanese Al-Akhbar newspaper where we constructed an annotated golden reference containing 1,677 words. For the genealogical tree extraction we used an extract from the Genesis biblical text with 1,227 words. Finally, we used an annotated article from the Lebanese Assafir newspaper with 1,399 words to evaluate the NUMNORM case study. In the online appendix, we report on eight additional MERF case studies. Manual annotators inspected the outcome and provided corrections where tools made mistakes. The corrections form the manual gold annotation that we compared against.Table TABREF35 reports the development time, extraction runtime, recall and precision of the output MRE tags, the size of the task in lines of code or in number of MERF rules, for both the standalone task specific and the MERF implementations. The development time measures the time required for developing the case study. For instance, ANGE BIBREF43 required two months of development by a research assistant with 6 and 14 hours of course work and teaching duties, respectively. Recall refers to the fraction of the entities correctly detected against the total number of entities. Precision refers to the fraction of correctly detected entities against the total number of extracted entities.Table TABREF35 provides runtime results of MERF compared to the task specific implementations while running MBF and MRE simulations jointly. This is a rough estimate of the complexity of the MERF simulator. The complexity of the MBF simulation is the total number of morphological solutions for all the words multiplied by the number of user-defined MBFs. We do not provide a limit on the number of user defined formulae. In practice, we did not encounter more than ten formulae per case study. As for the complexity of MRE simulation, converting the rules into non-deterministic finite state machines (NDFSM) is done once. Simulating an NDFSM over the MBF tags is potentially exponential. In practice, all our case studies terminated within a predetermined time bound of less than 30 minutes. MERF required reasonably more runtime than the task specific implementations and reported acceptable and slightly less precision metrics with around the same recall.Table TABREF35 shows that MERF has a clear advantage over task specific techniques in the effort required to develop the application at a reasonable cost in terms of accuracy and run time. Developers needed three hours, three hours, four hours, and one hour to develop the narrator chain, temporal entity, genealogy, and number normalization case studies using MERF, respectively. However, the developers of ANGE, ATEEMA, GENTREE, and NUMNORM needed two months, one and a half months, three weeks, and one week, respectively. MERF needed eight MBFs and four MREs for narrator chain, three MBFs and two MREs for temporal entity, three MBFs and three MREs for genealogy, and three MBFs, one MRE, and 57 lines of code actions for the number normalization tasks. However, ANGE, ATEEMA, GENTREE, and NUMNORM required 3,000+, 1,000+, 3,000+, and 500 lines of code, respectively.utf8standardNarrator chain case study	A narrator chain is a sequence of narrators referencing each other. The chain includes proper nouns, paternal entities, and referencing entities. ANGE uses Arabic morphological analysis, finite state machines, and graph transformations to extract entities and relations including narrator chains BIBREF43 .Table TABREF36 presents the MREs for the narrator chain case study. MBF PN checks the abstract category Name of Person. MBF FAM denotes “family connector” and checks the stem gloss “son”. MBF TOLD denotes referencing between narrators and checks the disjunction of the stems .hdt(“spoke to”), `n(“about”), sm`(“heard”), 'hbr(“told”), and 'nb-'(“inform”). MBF MEAN checks the stem `ny(“mean”). MBFs BLESS, GOD, UPONHIM, and GREET check the stems .sllA, Al-llah, `ly, and sllm, respectively.MRE name is one or more PN tags optionally followed with a MEAN tag. MRE nar denotes narrator which is a complex Arabic name composed as a sequence of Arabic names (name) connected with family indicators (FAM). The NONE tags in nar allow for unexpected words that can occur between names. MRE pbuh denotes a praise phrase often associated with the end of a hadith (“peace be upon him”), and is the satisfied by the sequence of BLESS, GOD, UPONHIM, and GREET tags. MRE nchain denotes narrator chain, and is a sequence of narrators (nar) separated with TOLD tags, and optionally followed by a pbuh tag.The first row in Table TABREF36 is an example narrator chain, the second is the transliteration, the third shows the MBF tags. Rows 4, 5, and 6 show the matches for name, nar, and nchain, respectively. MERF assigns the symbols INLINEFORM0 and INLINEFORM1 for the MRE subexpressions TOLD and nar, respectively. We define the relation INLINEFORM2 to relate sequences of narrators with edges labeled by the tags of TOLD where INLINEFORM3 denotes the next match of nar in the one or more MRE subexpression. Table TABREF38 shows that MERF detected almost all the MBF matches with 99% recall and 85% precision and extracted user-defined relations with 98% recall and 99% precision.Temporal entity extraction	Temporal entities are text chunks that express temporal information. Some represent absolute time such as AlhAms mn 'Ab 2010 INLINEFORM0 . Others represent relative time such as b`d hmsT 'ayAm INLINEFORM1 , and quantities such as 14 ywmA INLINEFORM2 . ATEEMA presents a temporal entity detection technique for the Arabic language using morphological analysis and finite state transducers BIBREF44 . Table TABREF38 shows that MERF detected almost all the MBF matches with 99% recall, however it shows low precision (52%). As for the semantic relation construction, MERF presents a 98% recall and 89% precision.Genealogy tree	Biblical genealogical lists trace key biblical figures such as Israelite kings and prophets with family relations. The family relations include wife and parenthood. A sample genealogical chunk of text is w wld hArAn lw.tA meaning “and Haran became the father of Lot”. GENTREE BIBREF31 automatically extracts the genealogical family trees using morphology, finite state machines, and graph transformations. Table TABREF38 shows that MERF detected MBF matches with 99% recall, and 75% precision, and extracted relations with 81% recall and 96% precision.",['What existing techniques do the authors compare against?'],['Unacceptable']
41,"Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness	We share a French-English parallel corpus of Foursquare restaurant reviews (this https URL), and define a new task to encourage research on Neural Machine Translation robustness and domain adaptation, in a real-world scenario where better-quality MT would be greatly beneficial. We discuss the challenges of such user-generated content, and train good baseline models that build upon the latest techniques for MT robustness. We also perform an extensive evaluation (automatic and human) that shows significant improvements over existing online systems. Finally, we propose task-specific metrics based on sentiment analysis or translation accuracy of domain-specific polysemous words.	Introduction	Very detailed information about social venues such as restaurants is available from user-generated reviews in applications like Google Maps, TripAdvisor or Foursquare. Most of these reviews are written in the local language and are not directly exploitable by foreign visitors: an analysis of the Foursquare database shows that, in Paris, only 49% of the restaurants have at least one review in English. It can be much worse for other cities and languages (e.g., only 1% of Seoul restaurants for a French-only speaker).Machine Translation of such user-generated content can improve the situation and make the data available for direct display or for downstream NLP tasks (e.g., cross-lingual information retrieval, sentiment analysis, spam or fake review detection), provided its quality is sufficient.We asked professionals to translate 11.5k French Foursquare reviews (18k sentences) to English. We believe that this resource will be valuable to the community for training and evaluating MT systems addressing challenges posed by user-generated content, which we discuss in detail in this paper.We conduct extensive experiments and combine techniques that seek to solve these challenges (e.g., factored case, noise generation, domain adaptation with tags) on top of a strong Transformer baseline. In addition to BLEU evaluation and human evaluation, we use targeted metrics that measure how well polysemous words are translated, or how well sentiments expressed in the original review can still be recovered from its translation.Related work	Translating restaurant reviews written by casual customers presents several difficulties for NMT, in particular robustness to non-standard language and adaptation to a specific style or domain (see Section SECREF7 for details).Concerning robustness to noisy user generated content, BIBREF0 stress differences with traditional domain adaptation problems, and propose a typology of errors, many of which we also detected in the Foursquare data. They also released a dataset (MTNT), whose sources were selected from a social media (Reddit) on the basis of being especially noisy (see Appendix for a comparison with Foursquare). These sources were then translated by humans to produce a parallel corpus that can be used to engineer more robust NMT systems and to evaluate them. This corpus was the basis of the WMT 2019 Robustness Task BIBREF1, in which BIBREF2 ranked first. We use the same set of robustness and domain adaptation techniques, which we study more in depth and apply to our review translation task.BIBREF3, BIBREF4 and BIBREF5 propose to improve robustness by training models on data-augmented corpora, containing noisy sources obtained by random word or character deletions, insertions, substitutions or swaps. Recently, BIBREF6 proposed to use a similar technique along with noise generation through replacement of a clean source by one obtained by back-translation.We employ several well-known domain adaptation techniques: back-translation of large monolingual corpora close to the domain BIBREF7, BIBREF8, fine-tuning with in-domain parallel data BIBREF9, BIBREF10, BIBREF11, domain tags for knowledge transfer between domains BIBREF12, BIBREF2.Addressing the technical issues of robustness and adaptation of an NMT system is decisive for real-world deployment, but evaluation is also critical. This aspect is stressed by BIBREF13 (NMT of curated hotel descriptions), who point out that automatic metrics like BLEU tend to neglect semantic differences that have a small textual footprint, but may be seriously misleading in practice, for instance by interpreting available parking as if it meant free parking. To mitigate this, we conduct additional evaluations of our models: human evaluation, translation accuracy of polysemous words, and indirect evaluation with sentiment analysis.Task description	We present a new task of restaurant review translation, which combines domain adaptation and robustness challenges.Task description ::: Corpus description	We sampled 11.5k French reviews from Foursquare, mostly in the food category, split them into 18k sentences, and grouped them into train, valid and test sets (see Table TABREF6). The French reviews contain on average 1.5 sentences and 17.9 words. Then, we hired eight professional translators to translate them to English. Two of them created the training set by post-editing (PE) the outputs of baseline NMT systems. The other six translated the valid and test sets from scratch. They were asked to translate (or post-edit) the reviews sentence-by-sentence (to avoid any alignment problem), but they could see the full context. We manually filtered the test set to remove translations that were not satisfactory. The full reviews and additional metadata (e.g., location and type of the restaurant) are also available as part of this resource, to encourage research on contextual machine translation.Foursquare-HT was translated from scratch by the same translators who post-edited Foursquare-PE. While we did not use it in this work, it can be used as extra training or development data. We also release a human translation of the French-language test set (668 sentences) of the Aspect-Based Sentiment Analysis task at SemEval 2016 BIBREF14.Task description ::: Challenges	Translating restaurant reviews presents two main difficulties compared to common tasks in MT. First, the reviews are written in a casual style, close to spoken language. Some liberty is taken w.r.t. spelling, grammar, and punctuation. Slang is also very frequent. MT should be robust to these variations. Second, they generally are reactions, by clients of a restaurant, about its food quality, service or atmosphere, with specific words relating to these aspects or sentiments. These require some degree of domain adaptation. The table above illustrates these issues, with outputs from an online MT system. Examples of full reviews from Foursquare-PE along with metadata are shown in Appendix.Examples 1 and 2 fall into the robustness category: 1 is an extreme form of SMS-like, quasi-phonetic, language (et quand j'ai vu ça); 2 is a literal transcription of a long-vowel phonetic stress (trop $\rightarrow $ trooop). Example 3 falls into the domain category: in a restaurant context, cadre typically refers to the setting. Examples 4 and 5 involve both robustness and domain adaptation: pété un cable is a non-compositional slang expression and garçon is not a boy in this domain; nickel is slang for great, très is missing an accent, and pâtes is misspelled as pattes, which is another French word.Regarding robustness, we found many of the same errors listed by BIBREF0 as noise in social media text: SMS language (é qd g vu sa), typos and phonetic spelling (pattes), repeated letters (trooop, merciiii), slang (nickel, bof, mdr), missing or wrong accents (tres), emoticons (`:-)') and emojis, missing punctuation, wrong or non-standard capitalization (lowercase proper names, capitalized words for emphasis). Regarding domain aspects, there are polysemous words with typical specific meaning carte $\rightarrow $ map, menu; cadre $\rightarrow $ frame, executive, setting), idiomatic expressions (à tomber par terre $\rightarrow $ to die for), and venue-related named entities (La Boîte à Sardines).Robustness to noise	We propose solutions for dealing with non-standard case, emoticons, emojis and other issues.Robustness to noise ::: Rare character placeholder	We segment our training data into subwords with BPE BIBREF15, implemented in SentencePiece BIBREF16. BPE can deal with rare or unseen words by splitting them into more frequent subwords, but cannot deal with unseen characters. While this is not a problem in most tasks, Foursquare contains many emojis, and sometimes symbols in other scripts (e.g., Arabic). Unicode now defines around 3k emojis, most of which are likely to be out-of-vocabulary.We replace rare characters on both sides of the training corpus by a placeholder (<x>). A model trained on this data is typically able to copy the placeholder at the correct position. Then, at inference time, we replace the output tokens <x> by the rare source-side characters, in the same order. This approach is similar to that of BIBREF18, who used the attention mechanism to replace UNK symbols with the aligned word in the source. BIBREF2 used the same technique to deal with emojis in the WMT robustness task.Robustness to noise ::: Capital letters	As shown in Table TABREF11, capital letters are another source of confusion. HONTE and honte are considered as two different words. The former is out-of-vocabulary and is split very aggressively by BPE. This causes the MT model to hallucinate.Robustness to noise ::: Capital letters ::: Lowercasing	A solution is to lowercase the input, both at training and at test time. However, when doing so, some information may be lost (e.g., named entities, acronyms, emphasis) which may result in lower translation quality.Robustness to noise ::: Capital letters ::: Factored translation	BIBREF13 do factored machine translation BIBREF19, BIBREF20 where a word and its case are split in two different features. For instance, HONTE becomes honte + upper.We implement this with two embedding matrices, one for words and one for case, and represent a token as the sum of the embeddings of its factors. For the target side, we follow BIBREF20 and have two softmax operations. We first predict the word in its lowercase form and then predict its case. The embeddings of the case and word are then summed and used as input for the next decoder step.Robustness to noise ::: Capital letters ::: Inline casing	BIBREF2 propose another approach, inline casing, which does not require any change in the model. We insert the case as a regular token into the sequence right after the word. Special tokens <U>, <L> and <T> (upper, lower and title) are used for this purpose and appended to the vocabulary. Contrary to the previous solution, there is only one embedding matrix and one softmax.In practice, words are assumed to be lowercase by default and the <L> tokens are dropped to keep the factored sequences as short as possible. “Best fries EVER"" becomes “best <T> _f ries _ever <U>"". Like BIBREF2, we force SentencePiece to split mixed-case words like MacDonalds into single-case subwords (Mac and Donalds).Robustness to noise ::: Capital letters ::: Synthetic case noise	Another solution that we experiment with (see Section SECREF6) is to inject noise on the source side of the training data by changing random source words to upper (5% chance), title (10%) or lower case (20%).Robustness to noise ::: Natural noise	One way to make an NMT system more robust is to train it with some of the most common errors that can be found in the in-domain data. Like BIBREF2, we detect the errors that occur naturally in the in-domain data and then apply them to our training corpus, while respecting their natural distribution. We call this “natural noise generation” in opposition to what is done in BIBREF3, BIBREF4, BIBREF6 or in Section SECREF10, where the noise is more synthetic.Robustness to noise ::: Natural noise ::: Detecting errors	We compile a general-purpose French lexicon as a transducer, implemented to be traversed with extended edit distance flags, similar to BIBREF21. Whenever a word is not found in the lexicon (which means that it is a potential spelling mistake), we look for a French word in the lexicon within a maximum edit distance of 2, with the following set of edit operations:We apply the transducer to the French monolingual Foursquare data (close to 1M sentences) to detect and count noisy variants of known French words. This step produces a dictionary mapping the correct spelling to the list of observed errors and their respective frequencies.In addition to automatically extracted spelling errors, we extract a set of common abbreviations from BIBREF22 and we manually identify a list of common errors in French:Robustness to noise ::: Natural noise ::: Generating errors	With this dictionary, describing the real error distribution in Foursquare text, we take our large out-of-domain training corpus, and randomly replace source-side words with one of their variants (rules 1 to 6), while respecting the frequency of this variant in the real data. We also manually define regular expressions to randomly apply rules 7 to 11 (e.g., ""er ""$\rightarrow $""é "").We obtain a noisy parallel corpus (which we use instead of the “clean” training data), where about 30% of all source sentences have been modified, as shown below:Domain Adaptation	To adapt our models to the restaurant review domain we apply the following types of techniques: back-translation of in-domain English data, fine-tuning with small amounts of in-domain parallel data, and domain tags.Domain Adaptation ::: Back-translation	Back-translation (BT) is a popular technique for domain adaptation when large amounts of in-domain monolingual data are available BIBREF7, BIBREF8. While our in-domain parallel corpus is small (12k pairs), Foursquare contains millions of English-language reviews. Thus, we train an NMT model in the reverse direction (EN$\rightarrow $FR) and translate all the Foursquare English reviews to French. This gives a large synthetic parallel corpus.This in-domain data is concatenated to the out-of-domain parallel data and used for training.BIBREF8 show that doing back-translation with sampling instead of beam search brings large improvements due to increased diversity. Following this work, we test several settings:We use a temperature of $T=\frac{1}{0.9}$ to avoid the extremely noisy output obtained with $T=1$ and strike a balance between quality and diversity.Domain Adaptation ::: Fine-tuning	When small amounts of in-domain parallel data are available, fine-tuning (FT) is often the preferred solution for domain adaptation BIBREF9, BIBREF10. It consists in training a model on out-of-domain data, and then continuing its training for a few epochs on the in-domain data only.Domain Adaptation ::: Corpus tags	BIBREF12 propose a technique for multi-domain NMT, which consists in inserting a token in each source sequence specifying its domain. The system can learn the particularities of multiple domains (e.g., polysemous words that have a different meaning depending on the domain), which we can control at test time by manually setting the tag. BIBREF23 also use tags to control politeness in the model's output.As our corpus (see Section SECREF28) is not clearly divided into domains, we apply the same technique as BIBREF12 but use corpus tags (each sub-corpus has its own tag: TED, Paracrawl, etc.) which we add to each source sequence. Like in BIBREF2, the Foursquare post-edited and back-translated data also get their own tags (PE and BT). Figure FIGREF27 gives an example where using the PE corpus tag at test time helps the model pick a more adequate translation.Experiments ::: Training data	After some initial work with the WMT 2014 data, we built a new training corpus named UGC (User Generated Content), closer to our domain, by combining: Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet (See Table TABREF31). UGC does not include Common Crawl (which contains many misaligned sentences and caused hallucinations), but it includes OpenSubtitles BIBREF24 (spoken-language, possibly closer to Foursquare). We observed an improvement of more than 1 BLEU on newstest2014 when switching to UGC, and almost 6 BLEU on Foursquare-valid.Experiments ::: Pre-processing	We use langid.py BIBREF25 to filter sentence pairs from UGC. We also remove duplicate sentence pairs, and lines longer than 175 words or with a length ratio greater than $1.5$ (see Table TABREF31). Then we apply SentencePiece and our rare character handling strategy (Section SECREF8). We use a joined BPE model of size 32k, trained on the concatenation of both sides of the corpus, and set SentencePiece's vocabulary threshold to 100. Finally, unless stated otherwise, we always use the inline casing approach (see Section SECREF10).Experiments ::: Model and settings	For all experiments, we use the Transformer Big BIBREF26 as implemented in Fairseq, with the hyperparameters of BIBREF27. Training is done on 8 GPUs, with accumulated gradients over 10 batches BIBREF27, and a max batch size of 3500 tokens per GPU. We train for 20 epochs, while saving a checkpoint every 2500 updates ($\approx \frac{2}{5}$ epoch on UGC) and average the 5 best checkpoints according to their perplexity on a validation set (a held-out subset of UGC).For fine-tuning, we use a fixed learning rate, and a total batch size of 3500 tokens (training on a single GPU without delayed updates). To avoid overfitting on Foursquare-PE, we do early stopping according to perplexity on Foursquare-valid. For each fine-tuned model we test all 16 combinations of dropout in $\lbrace 0.1,0.2,0.3,0.4\rbrace $ and learning rate in $\lbrace 1, 2, 5, 10\rbrace \times 10^{-5}$. We keep the model with the best perplexity on Foursquare-valid.Experiments ::: Evaluation methodology	During our work, we used BLEU BIBREF28 on newstest[2012, 2013] to ensure that our models stayed good on a more general domain, and on Foursquare-valid to measure performance on the Foursquare domain.For sake of brevity, we only give the final BLEU scores on newstest2014 and Foursquare-test. Scores on Foursquare-valid, and MTNT-test (for comparison with BIBREF0, BIBREF2) are given in Appendix. We evaluate “detokenized” MT outputs against raw references using SacreBLEU BIBREF29.In addition to BLEU, we do an indirect evaluation on an Aspect-Based Sentiment Analysis (ABSA) task, a human evaluation, and a task-related evaluation based on polysemous words.Experiments ::: BLEU evaluation ::: Capital letters	Table TABREF41 compares the case handling techniques presented in Section SECREF10. To better evaluate the robustness of our models to changes of case, we built 3 synthetic test sets from Foursquare-test, with the same target, but all source words in upper, lower or title case.Inline and factored case perform equally well, significantly better than the default (cased) model, especially on all-uppercase inputs. Lowercasing the source is a good option, but gives a slightly lower score on regular Foursquare-test. Finally, synthetic case noise added to the source gives surprisingly good results. It could also be combined with factored or inline case.Experiments ::: BLEU evaluation ::: Natural noise	Table TABREF44 compares the baseline “inline case” model with the same model augmented with natural noise (Section SECREF17). Performance is the same on Foursquare-test, but significantly better on newstest2014 artificially augmented with Foursquare-like noise.Experiments ::: BLEU evaluation ::: Domain adaptation	Table TABREF46 shows the results of the back-translation (BT) techniques. Surprisingly, BT with beam search (BT-B) deteriorates BLEU scores on Foursquare-test, while BT with sampling gives a consistent improvement. BLEU scores on newstest2014 are not significantly impacted, suggesting that BT can be used for domain adaptation without hurting quality on other domains.Table TABREF47 compares the domain adaptation techniques presented in Section SECREF5. We observe that:Concatenating the small Foursquare-PE corpus to the 50M general domain corpus does not help much, unless using corpus tags.Foursquare-PE + tags is not as good as fine-tuning with Foursquare-PE. However, fine-tuned models get slightly worse results on news.Back-translation combined with tags gives a large boost. The BT tag should not be used at test time, as it degrades results.Using no tag at test time works fine, even though all training sentences had tags.As shown in Table TABREF54, these techniques can be combined to achieve the best results. The natural noise does not have a significant effect on BLEU scores. Back-translation combined with fine-tuning gives the best performance on Foursquare (+4.5 BLEU vs UGC). However, using tags instead of fine-tuning strikes a better balance between general domain and in-domain performance.Experiments ::: Targeted evaluation	In this section we propose two metrics that target specific aspects of translation adequacy: translation accuracy of domain-specific polysemous words and Aspect-Based Sentiment Analysis performance on MT outputs.Experiments ::: Targeted evaluation ::: Translation of polysemous words	We propose to count polysemous words specific to our domain, similarly to BIBREF31, to measure the degree of domain adaptation. TER between the translation hypotheses and the post-edited references in Foursquare-PE reveals the most common substitutions (e.g., “card” is often replaced with “menu”, suggesting that “card” is a common mistranslation of the polysemous word “carte”). We filter this list manually to only keep words that are polysemous and that have a high frequency in the test set. Table TABREF58 gives the 3 most frequent ones.Table TABREF59 shows the accuracy of our models when translating these words. We see that the domain-adapted model is better at translating domain-specific polysemous words.Experiments ::: Targeted evaluation ::: Indirect evaluation with sentiment analysis	We also measure adequacy by how well the translation preserves the polarity of the sentence regarding various aspects. To evaluate this, we perform an indirect evaluation on the SemEval 2016 Aspect-Based Sentiment Analysis (ABSA) task BIBREF14. We use our internal ABSA systems trained on English or French SemEval 2016 data. The evaluation is done on the SemEval 2016 French test set: either the original version (ABSA French), or its translation (ABSA English). As shown in Table TABREF61, translations obtained with domain-adapted models lead to significantly better scores on ABSA than the generic models.Experiments ::: Human Evaluation	We conduct a human evaluation to confirm the observations with BLEU and to overcome some of the limitations of this metric.We select 4 MT models for evaluation (see Table TABREF63) and show their 4 outputs at once, sentence-by-sentence, to human judges, who are asked to rank them given the French source sentence in context (with the full review). For each pair of models, we count the number of wins, ties and losses, and apply the Wilcoxon signed-rank test.We took the first 300 test sentences to create 6 tasks of 50 sentences each. Then we asked bilingual colleagues to rank the output of 4 models by their translation quality. They were asked to do one or more of these tasks. The judge did not know about the list of models, nor the model that produced any given translation. We got 12 answers. The inter-judge Kappa coefficient ranged from 0.29 to 0.63, with an average of 0.47, which is a good value given the difficulty of the task. Table TABREF63 gives the results of the evaluation, which confirm our observations with BLEU.We also did a larger-scale monolingual evaluation using Amazon Mechanical Turk (see Appendix), which lead to similar conclusions.Conclusion	We presented a new parallel corpus of user reviews of restaurants, which we think will be valuable to the community. We proposed combinations of multiple techniques for robustness and domain adaptation, which address particular challenges of this new task. We also performed an extensive evaluation to measure the improvements brought by these techniques.According to BLEU, the best single technique for domain adaptation is fine-tuning. Corpus tags also achieve good results, without degrading performance on a general domain. Back-translation helps, but only with sampling or tags. The robustness techniques (natural noise, factored case, rare character placeholder) do not improve BLEU.While our models are promising, they still show serious errors when applied to user-generated content: missing negations, hallucinations, unrecognized named entities, insensitivity to context. This suggests that this task is far from solved.We hope that this corpus, our natural noise dictionary, model outputs and human rankings will help better understand and address these problems. We also plan to investigate these problems on lower resource languages, where we expect the task to be even harder.","['what baseline models are trained?', 'what dataset was used?', 'what are the human evaluation metrics?', 'what automatic evaluation is performed?', 'what are the existing online systems?']","['Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable', 'Existing online systems compared in this work are Google Translate (Feb 2019) and DeepL (Feb 2019).']"
42,"Improving Yor\`ub\'a Diacritic Restoration	Yor\`ub\'a is a widely spoken West African language with a writing system rich in orthographic and tonal diacritics. They provide morphological information, are crucial for lexical disambiguation, pronunciation and are vital for any computational Speech or Natural Language Processing tasks. However diacritic marks are commonly excluded from electronic texts due to limited device and application support as well as general education on proper usage. We report on recent efforts at dataset cultivation. By aggregating and improving disparate texts from the web and various personal libraries, we were able to significantly grow our clean Yor\`ub\'a dataset from a majority Bibilical text corpora with three sources to millions of tokens from over a dozen sources. We evaluate updated diacritic restoration models on a new, general purpose, public-domain Yor\`ub\'a evaluation dataset of modern journalistic news text, selected to be multi-purpose and reflecting contemporary usage. All pre-trained models, datasets and source-code have been released as an open-source project to advance efforts on Yor\`ub\'a language technology.	Introduction	Yorùbá is a tonal language spoken by more than 40 Million people in the countries of Nigeria, Benin and Togo in West Africa. The phonology is comprised of eighteen consonants, seven oral vowel and five nasal vowel phonemes with three kinds of tones realized on all vowels and syllabic nasal consonants BIBREF0. Yorùbá orthography makes notable use of tonal diacritics, known as amí ohùn, to designate tonal patterns, and orthographic diacritics like underdots for various language sounds BIBREF1, BIBREF2.Diacritics provide morphological information, are crucial for lexical disambiguation and pronunciation, and are vital for any computational Speech or Natural Language Processing (NLP) task. To build a robust ecosystem of Yorùbá-first language technologies, Yorùbá text must be correctly represented in computing environments. The ultimate objective of automatic diacritic restoration (ADR) systems is to facilitate text entry and text correction that encourages the correct orthography and promotes quotidian usage of the language in electronic media.Introduction ::: Ambiguity in non-diacritized text	The main challenge in non-diacritized text is that it is very ambiguous BIBREF3, BIBREF4, BIBREF1, BIBREF5. ADR attempts to decode the ambiguity present in undiacritized text. Adegbola et al. assert that for ADR the “prevailing error factor is the number of valid alternative arrangements of the diacritical marks that can be applied to the vowels and syllabic nasals within the words"" BIBREF1.Introduction ::: Improving generalization performance	To make the first open-sourced ADR models available to a wider audience, we tested extensively on colloquial and conversational text. These soft-attention seq2seq models BIBREF3, trained on the first three sources in Table TABREF5, suffered from domain-mismatch generalization errors and appeared particularly weak when presented with contractions, loan words or variants of common phrases. Because they were trained on majority Biblical text, we attributed these errors to low-diversity of sources and an insufficient number of training examples. To remedy this problem, we aggregated text from a variety of online public-domain sources as well as actual books. After scanning physical books from personal libraries, we successfully employed commercial Optical Character Recognition (OCR) software to concurrently use English, Romanian and Vietnamese characters, forming an approximative superset of the Yorùbá character set. Text with inconsistent quality was put into a special queue for subsequent human supervision and manual correction. The post-OCR correction of Háà Ènìyàn, a work of fiction of some 20,038 words, took a single expert two weeks of part-time work by to review and correct. Overall, the new data sources comprised varied text from conversational, various literary and religious sources as well as news magazines, a book of proverbs and a Human Rights declaration.Methodology ::: Experimental setup	Data preprocessing, parallel text preparation and training hyper-parameters are the same as in BIBREF3. Experiments included evaluations of the effect of the various texts, notably for JW300, which is a disproportionately large contributor to the dataset. We also evaluated models trained with pre-trained FastText embeddings to understand the boost in performance possible with word embeddings BIBREF6, BIBREF7. Our training hardware configuration was an AWS EC2 p3.2xlarge instance with OpenNMT-py BIBREF8.Methodology ::: A new, modern multi-purpose evaluation dataset	To make ADR productive for users, our research experiments needed to be guided by a test set based around modern, colloquial and not exclusively literary text. After much review, we selected Global Voices, a corpus of journalistic news text from a multilingual community of journalists, translators, bloggers, academics and human rights activists BIBREF9.Results	We evaluated the ADR models by computing a single-reference BLEU score using the Moses multi-bleu.perl scoring script, the predicted perplexity of the model's own predictions and the Word Error Rate (WER). All models with additional data improved over the 3-corpus soft-attention baseline, with JW300 providing a {33%, 11%} boost in BLEU and absolute WER respectively. Error analyses revealed that the Transformer was robust to receiving digits, rare or code-switched words as input and degraded ADR performance gracefully. In many cases, this meant the model predicted the undiacritized word form or a related word from the context, but continued to correctly predict subsequent words in the sequence. The FastText embedding provided a small boost in performance for the Transformer, but was mixed across metrics for the soft-attention models.Conclusions and Future Work	Promising next steps include further automation of our human-in-the-middle data-cleaning tools, further research on contextualized word embeddings for Yorùbá and serving or deploying the improved ADR models in user-facing applications and devices.",['What sources did they get the data from?'],"['online public-domain sources, private sources and actual books']"
43,"And the Winner is ...: Bayesian Twitter-based Prediction on 2016 U.S. Presidential Election	This paper describes a Naive-Bayesian predictive model for 2016 U.S. Presidential Election based on Twitter data. We use 33,708 tweets gathered since December 16, 2015 until February 29, 2016. We introduce a simpler data preprocessing method to label the data and train the model. The model achieves 95.8% accuracy on 10-fold cross validation and predicts Ted Cruz and Bernie Sanders as Republican and Democratic nominee respectively. It achieves a comparable result to those in its competitor methods.	Introduction	Presidential election is an important moment for every country, including the United States. Their economic policies, which are set by the government, affect the economy of other countries BIBREF0 . On 2016 U.S. Presidential Election, Republican and Democratic candidates use Twitter as their campaign media. Previous researches have predicted the outcome of U.S. presidential election using Twitter BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 . Some of them proved that Twitter data can complement or even predict the poll results. This follows the increasing improvement in the text mining researches BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 .Some of the most recent studies are BIBREF3 , BIBREF2 , BIBREF1 , BIBREF10 . Below we discuss these three recent studies and explain how our study relates to theirs. The first study is done by BIBREF3 , which analyzed the sentiment on 2008 U.S. Presidential Candidates by calculating sentiment ratio using moving average. They counted the sentiment value for Obama and McCain based on number of the positive and negative words stated on each tweet. The tweets were gathered during 2008-2009, whereas the positive and negative words were acquired from OpinionFinder. They found that the comparison between sentiment on tweets and polls were complex since people might choose ""Obama"", ""McCain"", ""have not decided"", ""not going to vote"", or any independent candidate on the polls.The second study predicted the outcome of 2012 U.S. Presidential Election polls using Naive Bayesian models BIBREF2 . They collected over 32 million tweets from September 29 until November 16, 2012. They used Tweepy and set keywords for each candidate to collect the tweets, such as mitt romney, barack obama, us election. The collected tweets passed some preprocessing stages: (1) URL, mentions, hashtags, RT, and stop words removal; (2) tokenization; and (3) additional not_ for negation. They analyzed 10,000 randomly selected tweets which only contain a candidate name. The analysis results were compared to Huffington Post's polls and they found that Obama's popularity on Twitter represented the polls result. This research didn't use tweets with two or more candidate names since it requires more complex preprocessing methods.The third study built a system for real-time sentiment analysis on 2012 U.S. Presidential Election to show public opinion about each candidate on Twitter BIBREF1 . They collected tweets for each candidates using Gnip Power Track since October 12, 2012 and tokenized them. The tweets were labeled by around 800 turkers on Amazon Mechanical Turk (AMT). They trained a Naive Bayes Classifier using 17,000 tweets which consists of 4 classes: (1) positive; (2) negative; (3) neutral; and (4) unsure. It achieved 59% accuracy, which is the best performance achieved in the three recent studies. They visualized the sentiment on a dashboard and calculated the trending words using TF-IDF.As far as we know, there is not any research about prediction on 2016 U.S. Presidential Election yet. Previous researches either set the sentiment of a tweet directly based on a subjectivity lexicon BIBREF3 or preprocessed the tweet using a complex preprocessing method BIBREF1 , BIBREF2 . BIBREF2 not only removed URLs, mentions, retweets, hashtags, numbers and stop words; but also tokenized the tweets and added not_ on negative words. BIBREF1 tokenized the tweets and separated URLs, emoticons, phone numbers, HTML tags, mentions, hashtags, fraction or decimals, and symbol or Unicode character repetition. This research analyzes sentiment on tweets about 2016 U.S. Presidential candidates. We will build a Naive Bayesian predictive model for each candidate and compare the prediction with RealClearPolitics.com. We expect to have a correct prediction on the leading candidates for Democratic and Republican Party. We prove that using a simpler preprocessing method can still have comparable performance to the best performing recent study BIBREF1 .We explain our data preparation methods in the next section. It is followed by our research methodology in Section III. We present our results in Section IV, which is followed by discussion and conclusion in Section V and VI.Data Collection	We gathered 371,264 tweets using Twitter Streaming API on Tweepy BIBREF2 since December 16, 2015 until February 29, 2016. We use #Election2016 as the search keyword since it is the official hashtag used during 2016 U.S. Presidential Election cycle and it covers conversations about all candidates. We separate the tweets per period, which is seven days. Figure 1 shows tweets frequency distribution, with the average of 37,126.4 tweets per period and standard deviation 27,823.82 tweets. Data collection from January 20 to January 26, 2016 are limited due to resource limitation. The data are saved as JSON files.Each line of the JSON files represents a tweet, which consists of 26 main attributes, such as created_at, ID, text, retweet_count, and lang. We only use the contents of created_at and text attributes since this research focuses on the sentiment toward the candidates in a particular time, not including the geographic location and other information. The collected tweets are mainly written in English. We publish the raw and preprocessed tweets upon request for future use. The data are available for research use by email.Data Preprocessing	We preprocess the data by: (1) removing URLs and pictures, also (2) by filtering tweets which have candidates' name. Hashtags, mentions, and retweets are not removed in order to maintain the original meaning of a tweet. We only save tweets which have passed the two requirements such as in Table 1. The first example shows no change in the tweet's content, since there isn't any URLs or pictures, and it contains a candidate's name: Bernie Sanders. The second example shows a removed tweet, which doesn't contain any candidates' name. The preprocessing stage changes the third tweet's contents. It removes the URLs and still keeps the tweet because it contains ""Hillary Clinton"" and ""Donald Trump"". The preprocessing stage removes 41% of the data (Figure 2).Data Labeling	The preprocessed tweets are labeled manually by 11 annotators who understand English. All annotators are given either grade as part of their coursework or souvenirs for their work. The given label consists of the intended candidate and the sentiment. The annotators interpret the tweet and decide whom the tweet relates to. If they think the tweets does not relate to particular candidate nor understand the content, they can choose ""not clear"" as the label. Otherwise, they can relate it to one candidate and label it as positive or negative. We divide the tweets and annotators into three groups (Table II). They label as many tweets as they can since January 24 until April 16, 2016.The validity of the label is determined by means of majority rule BIBREF11 . Each tweet is distributed to three or five annotators and it is valid when there is a label which occurs the most. As the final data preparation step, we remove all ""not clear"" labeled tweets. Figure 3 shows the distribution of tweet labels. Most tweets are related to Bernie Sanders, Donald Trump, and Hillary Clinton.Methodology	The presidential nominees are predicted by finding candidates with the most predicted positive sentiment. The sentiments are predicted using Bayesian model. This section describes: (1) the model training, (2) model accuracy test, and (3) prediction accuracy test.Model Training	Our models are trained using Naive Bayes Classifier. We have one model representing each candidate, consequently we have 15 trained models. We use nltk.classify module on Natural Language Toolkit library on Python. We use the labeled data gathered since December 16, 2015 until February 2, 2016 as training data to our models. The rest of our labeled data will be used to evaluate the models.Model Accuracy Test	Our models' accuracy is tested using 10-fold cross validation. Model validation is done using scikit-learn library. The accuracy is calculated by checking the confusion matrix BIBREF12 , BIBREF13 and its INLINEFORM0 score BIBREF14 .On some folds, the models predict the sentiment in extreme value (i.e. only have positive or negative outcomes). Due to these cases, we can not calculate INLINEFORM0 score of Chris Christie's model. The average accuracy and INLINEFORM1 score are 95.8% and 0.96 respectively. INLINEFORM0 score only measures how well the model works on predicting positive sentiment, so we propose a modified INLINEFORM1 score ( INLINEFORM2 ) by reversing the formula. INLINEFORM3 score shows how well the model predicts negative sentiment. DISPLAYFORM0 The models show good accuracy and INLINEFORM0 score (Table III). It shows that the model can predict the test data almost perfectly (95.8%) with slightly better result on positive sentiment than negative ones, which can be seen by the larger value of INLINEFORM1 than INLINEFORM2 .The test results do not show exact effect of training data and the model accuracy. Models with smaller number of training data (e.g. Huckabee's, Santorum's) achieve higher accuracy than models with larger number of training data (e.g. Trump's, Clinton's), while the lowest accuracy is achieved by Kasich's, which is trained with small number of training data. The undefined value of INLINEFORM0 and INLINEFORM1 scores on Christie's, Gilmore's, and Santorum's model shows extreme predictions on these models.Prediction Accuracy Test	The models use tweets gathered from February 3 until 9, 2016 as the prediction input. The prediction follows two steps: (1) we calculate the positive sentiment from tweets and consider the number of positive sentiment as the likelihood of a candidate to be the nominee, and (2) we sort the candidates by number of their positive sentiment. The ranks are compared to the poll results on RealClearPolitics.com. We calculate the error rate (E) by dividing the difference of the poll rank with our predicted rank with number of candidates ( INLINEFORM0 ). DISPLAYFORM0 DISPLAYFORM1 where INLINEFORM0 and n equals the number of candidates. Po and Pre are the poll and prediction ranks associated with RealClearPolitics.com and the model respectively.We use tweets on February 3-9, 2016 as the input to our models, regarding to the specified candidate. We rank the prediction result by sorting the number of positive predictions on each candidate. On Democratic Party, Bernie Sanders leads the rank with 3,335 tweets, followed by Martin O'Malley (14 tweets) and Hillary Clinton (none). The prediction ranks on Republican Party are (1) Ted Cruz (1,432 tweets), (2) Marco Rubio (1,239 tweets), (3) Rand Paul (645 tweets), (4) Rick Santorum (186 tweets), (5) John Kasich (133 tweets), (6) Carly Fiorina (88 tweets), (7) Mike Huckabee (11 tweets), and (8) Jim Gilmore (5 tweets). The other Republican candidates do not have any positive prediction, so we place them at the bottom rank.Our model prediction ranks from 1 to 9 and it differs from the poll's (rank 1 to 8). Before we do the comparison, we adjust the prediction ranks in order to make an equal range. We move Jeb Bush, Ben Carson, Chris Christie, and Donald Trump, who are formerly on the 9th rank, to the 8th rank. We compare the prediction ranks with the poll and calculate the error rate. Our model gets 1.33 error of 2 remaining Democratic candidates, which we consider not good. Our model performs better on predicting Republican candidates, which achieves 1.67 error of 7 remaining candidates (see Table IV and V).Overall prediction accuracy can be calculated by subtracting one with the average result of error rate division on each party by number of its remaining candidates. We achieve 0.548 prediction accuracy, which is not good enough BIBREF1 . The model accuracy is mainly affected by the large error rate on Democratic candidates (1.33 from 2 candidates).Discussion	Using simple preprocessed data, our Naive Bayesian model successfully achieves 95.8% accuracy on 10-fold cross validation and gets 54.8% accuracy on predicting the poll result. The model predicts Ted Cruz and Bernie Sanders as the nominee of Republican and Democratic Party respectively. Based on the positive predictions, it predicts that Bernie Sanders will be elected as the 2016 U.S. President.Although it has 95.8% accuracy during the model test, the model's prediction does not represent the poll. Table III shows that model's accuracy is not dependent of its number of training data. Model with less training data (e.g. Mike Huckabee's) can perform perfectly during the model test and only misses a rank on the prediction, whereas model with more training data (e.g. Donald Trump's) can have worse performance.To see how the model accuracy is affected by number of training data, we train more models for each candidate using n first tweets and use them to predict the next 4000 tweets' sentiment (see Figure 4). Bernie Sanders' and Donald Trump's models have the most consistent accuracy on predicting the sentiment. Models with less training data (e.g. Martin O'Malley, Jim Gilmore, Mike Huckabee) tend to have fluctuating accuracy. The models which are trained using 1,000 first tweets have 55.85% of average accuracy and 26.49% of standard deviation, whereas the models which are trained using 33,000 first tweets have slightly different accuracy: 65.75% of average accuracy and 27.79% of standard deviation. This shows that the number of training data does not affect the overall model accuracy.Our model might not represent the poll, but the election is still ongoing and we do not know which candidate will become the next U.S. President. Hence, there is possibility that the predicted nominees become the next U.S. President. Otherwise, Twitter might not be used to predict the actual polls BIBREF15 .Conclusion	We built Naive Bayesian predictive models for 2016 U.S. Presidential Election. We use the official hashtag and simple preprocessing method to prepare the data without modifying its meaning. Our model achieves 95.8% accuracy during the model test and predicts the poll with 54.8% accuracy. The model predicts that Bernie Sanders and Ted Cruz will become the nominees of Democratic and Republican Party respectively, and the election will be won by Bernie Sanders.","['what are the other methods they compare to?', 'what preprocessing method is introduced?']","['Unacceptable', 'Tweets without candidate names are removed, URLs and pictures are removed from the tweets that remain.']"
44,"On NMT Search Errors and Model Errors: Cat Got Your Tongue?	We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of beam search and depth-first search. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, beam search fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50% of the sentences, the model in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.	Introduction	[0]Now at Google.Neural machine translation BIBREF0 , BIBREF1 , BIBREF2 assigns the probability INLINEFORM0 of a translation INLINEFORM1 of length INLINEFORM2 over the target language vocabulary INLINEFORM3 for a source sentence INLINEFORM4 of length INLINEFORM5 over the source language vocabulary INLINEFORM6 via a left-to-right factorization using the chain rule: DISPLAYFORM0 The task of finding the most likely translation INLINEFORM0 for a given source sentence INLINEFORM1 is known as the decoding or inference problem: DISPLAYFORM0 The NMT search space is vast as it grows exponentially with the sequence length. For example, for a common vocabulary size of INLINEFORM0 , there are already more possible translations with 20 words or less than atoms in the observable universe ( INLINEFORM1 ). Thus, complete enumeration of the search space is impossible. The size of the NMT search space is perhaps the main reason why – besides some preliminary studies BIBREF3 , BIBREF4 , BIBREF5 – analyzing search errors in NMT has received only limited attention. To the best of our knowledge, none of the previous studies were able to quantify the number of search errors in unconstrained NMT due to the lack of an exact inference scheme that – although too slow for practical MT – guarantees to find the global best model score for analysis purposes.[t!] BeamSearch INLINEFORM0 [1] INLINEFORM1 : Source sentence, INLINEFORM2 : Beam size INLINEFORM3 Initialize with empty translation prefix and zero score INLINEFORM4 INLINEFORM5 INLINEFORM6 INLINEFORM7 Hypotheses ending with INLINEFORM8 are not expanded INLINEFORM9 Add all possible continuations INLINEFORM10 Select INLINEFORM11 -best INLINEFORM12 INLINEFORM13 INLINEFORM14 [t!] DFS INLINEFORM0 [1] INLINEFORM1 : Source sentence INLINEFORM0 : Translation prefix (default: INLINEFORM1 ) INLINEFORM0 : INLINEFORM1 (default: INLINEFORM2 ) INLINEFORM0 : Lower bound INLINEFORM1 INLINEFORM2 Trigger INLINEFORM3 update INLINEFORM4 Initialize INLINEFORM5 with dummy value INLINEFORM6 INLINEFORM7 INLINEFORM8 INLINEFORM9 INLINEFORM10 INLINEFORM11 INLINEFORM12 In this work we propose such an exact decoding algorithm for NMT that exploits the monotonicity of NMT scores: Since the conditional log-probabilities in Eq. EQREF1 are always negative, partial hypotheses can be safely discarded once their score drops below the log-probability of any complete hypothesis. Using our exact inference scheme we show that beam search does not find the global best model score for more than half of the sentences. However, these search errors, paradoxically, often prevent the decoder from suffering from a frequent but very serious model error in NMT, namely that the empty hypothesis often gets the global best model score. Our findings suggest a reassessment of the amount of model and search errors in NMT, and we hope that they will spark new efforts in improving NMT modeling capabilities, especially in terms of adequacy.Exact Inference for Neural Models	Decoding in NMT (Eq. EQREF2 ) is usually tackled with beam search, which is a time-synchronous approximate search algorithm that builds up hypotheses from left to right. A formal algorithm description is given in Alg. SECREF1 . Beam search maintains a set of active hypotheses INLINEFORM0 . In each iteration, all hypotheses in INLINEFORM1 that do not end with the end-of-sentence symbol INLINEFORM2 are expanded and collected in INLINEFORM3 . The best INLINEFORM4 items in INLINEFORM5 constitute the set of active hypotheses INLINEFORM6 in the next iteration (line 11 in Alg. SECREF1 ), where INLINEFORM7 is the beam size. The algorithm terminates when the best hypothesis in INLINEFORM8 ends with the end-of-sentence symbol INLINEFORM9 . Hypotheses are called complete if they end with INLINEFORM10 and partial if they do not.Beam search is the ubiquitous decoding algorithm for NMT, but it is prone to search errors as the number of active hypotheses is limited by INLINEFORM0 . In particular, beam search never compares partial hypotheses of different lengths with each other. As we will see in later sections, this is one of the main sources of search errors. However, in many cases, the model score found by beam search is a reasonable approximation to the global best model score. Let INLINEFORM1 be the model score found by beam search ( INLINEFORM2 in line 12, Alg. SECREF1 ), which is a lower bound on the global best model score: INLINEFORM3 . Furthermore, since the conditionals INLINEFORM4 in Eq. EQREF1 are log-probabilities and thus non-positive, expanding a partial hypothesis is guaranteed to result in a lower model score, i.e.: DISPLAYFORM0 Consequently, when we are interested in the global best hypothesis INLINEFORM0 , we only need to consider partial hypotheses with scores greater than INLINEFORM1 . In our exact decoding scheme we traverse the NMT search space in a depth-first order, but cut off branches along which the accumulated model score falls below INLINEFORM2 . During depth-first search (DFS), we update INLINEFORM3 when we find a better complete hypothesis. Alg. SECREF1 specifies the DFS algorithm formally. An important detail is that elements in INLINEFORM4 are ordered such that the loop in line 5 considers the INLINEFORM5 token first. This often updates INLINEFORM6 early on and leads to better pruning in subsequent recursive calls.Results without Length Constraints	We conduct all our experiments in this section on the entire English-German WMT news-test2015 test set (2,169 sentences) with a Transformer base BIBREF13 model trained with Tensor2Tensor BIBREF14 on parallel WMT18 data excluding ParaCrawl. Our pre-processing is as described by BIBREF15 and includes joint subword segmentation using byte pair encoding BIBREF16 with 32K merges. We report cased BLEU scores. An open-source implementation of our exact inference scheme is available in the SGNMT decoder BIBREF17 , BIBREF4 .Our main result is shown in Tab. TABREF9 . Greedy and beam search both achieve reasonable BLEU scores but rely on a high number of search errors to not be affected by a serious NMT model error: For 51.8% of the sentences, NMT assigns the global best model score to the empty translation, i.e. a single INLINEFORM0 token. Fig. FIGREF10 visualizes the relationship between BLEU and the number of search errors. Large beam sizes reduce the number of search errors, but the BLEU score drops because translations are too short. Even a large beam size of 100 produces 53.62% search errors. Fig. FIGREF11 shows that beam search effectively reduces search errors with respect to greedy decoding to some degree, but is ineffective in reducing search errors even further. For example, Beam-10 yields 15.9% fewer search errors (absolute) than greedy decoding (57.68% vs. 73.58%), but Beam-100 improves search only slightly (53.62% search errors) despite being 10 times slower than beam-10.The problem of empty translations is also visible in the histogram over length ratios (Fig. FIGREF13 ). Beam search – although still slightly too short – roughly follows the reference distribution, but exact search has an isolated peak in INLINEFORM0 from the empty translations.Tab. TABREF14 demonstrates that the problems of search errors and empty translations are not specific to the Transformer base model and also occur with other architectures. Even a highly optimized Transformer Big model from our WMT18 shared task submission BIBREF15 has 25.8% empty translations.Fig. FIGREF15 shows that long source sentences are more affected by both beam search errors and the problem of empty translations. The global best translation is empty for almost all sentences longer than 40 tokens (green curve). Even without sentences where the model prefers the empty translation, a large amount of search errors remain (blue curve).Results with Length Constraints	To find out more about the length deficiency we constrained exact search to certain translation lengths. Constraining search that way increases the run time as the INLINEFORM0 -bounds are lower. Therefore, all results in this section are conducted on only a subset of the test set to keep the runtime under control. We first constrained search to translations longer than 0.25 times the source sentence length and thus excluded the empty translation from the search space. Although this mitigates the problem slightly (Fig. FIGREF16 ), it still results in a peak in the INLINEFORM1 cluster. This suggests that the problem of empty translations is the consequence of an inherent model bias towards shorter hypotheses and cannot be fixed with a length constraint.We then constrained exact search to either the length of the best Beam-10 hypothesis or the reference length. Tab. TABREF18 shows that exact search constrained to the Beam-10 hypothesis length does not improve over beam search, suggesting that any search errors between beam search score and global best score for that length are insignificant enough so as not to affect the BLEU score. The oracle experiment in which we constrained exact search to the correct reference length (last row in Tab. TABREF18 ) improved the BLEU score by 0.9 points.A popular method to counter the length bias in NMT is length normalization BIBREF6 , BIBREF7 which simply divides the sentence score by the sentence length. We can find the global best translations under length normalization by generalizing our exact inference scheme to length dependent lower bounds INLINEFORM0 . The generalized scheme finds the best model scores for each translation length INLINEFORM1 in a certain range (e.g. zero to 1.2 times the source sentence length). The initial lower bounds are derived from the Beam-10 hypothesis INLINEFORM2 as follows: DISPLAYFORM0 Exact search under length normalization does not suffer from the length deficiency anymore (last row in Tab. TABREF19 ), but it is not able to match our best BLEU score under Beam-10 search. This suggests that while length normalization biases search towards translations of roughly the correct length, it does not fix the fundamental modelling problem.Related Work	Other researchers have also noted that large beam sizes yield shorter translations BIBREF19 . BIBREF20 argue that this model error is due to the locally normalized maximum likelihood training objective in NMT that underestimates the margin between the correct translation and shorter ones if trained with regularization and finite data. A similar argument was made by BIBREF10 who pointed out the difficulty for a locally normalized model to estimate the “budget” for all remaining (longer) translations. BIBREF21 demonstrated that NMT models are often poorly calibrated, and that that can cause the length deficiency. BIBREF5 argued that uncertainty caused by noisy training data may play a role. BIBREF22 showed that the consistent best string problem for RNNs is decidable. We provide an alternative DFS algorithm that relies on the monotonic nature of model scores rather than consistency, and that often converges in practice.To the best of our knowledge, this is the first work that reports the exact number of search errors in NMT as prior work often relied on approximations, e.g. via INLINEFORM0 -best lists BIBREF3 or constraints BIBREF4 .Conclusion	We have presented an exact inference scheme for NMT. Exact search may not be practical, but it allowed us to discover deficiencies in widely used NMT models. We linked deteriorating BLEU scores of large beams with the reduction of search errors and showed that the model often prefers the empty translation – an evidence of NMT's failure to properly model adequacy. Our investigations into length constrained exact search suggested that simple heuristics like length normalization are unlikely to remedy the problem satisfactorily.Acknowledgments	This work was supported by the U.K. Engineering and Physical Sciences Research Council (EPSRC) grant EP/L027623/1 and has been performed using resources provided by the Cambridge Tier-2 system operated by the University of Cambridge Research Computing Service funded by EPSRC Tier-2 capital grant EP/P020259/1.","['what were the length constraints they set?', 'what is the test set size?']","['Unacceptable', 'Unacceptable']"
45,"SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization	This paper introduces the SAMSum Corpus, a new dataset with abstractive dialogue summaries. We investigate the challenges it poses for automated summarization by testing several models and comparing their results with those obtained on a corpus of news articles. We show that model-generated summaries of dialogues achieve higher ROUGE scores than the model-generated summaries of news -- in contrast with human evaluators' judgement. This suggests that a challenging task of abstractive dialogue summarization requires dedicated models and non-standard quality measures. To our knowledge, our study is the first attempt to introduce a high-quality chat-dialogues corpus, manually annotated with abstractive summarizations, which can be used by the research community for further studies.	Introduction and related work	The goal of the summarization task is condensing a piece of text into a shorter version that covers the main points succinctly. In the abstractive approach important pieces of information are presented using words and phrases not necessarily appearing in the source text. This requires natural language generation techniques with high level of semantic understanding BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6.Major research efforts have focused so far on summarization of single-speaker documents like news (e.g., BIBREF7) or scientific publications (e.g., BIBREF8). One of the reasons is the availability of large, high-quality news datasets with annotated summaries, e.g., CNN/Daily Mail BIBREF9, BIBREF7. Such a comprehensive dataset for dialogues is lacking.The challenges posed by the abstractive dialogue summarization task have been discussed in the literature with regard to AMI meeting corpus BIBREF10, e.g. BIBREF11, BIBREF12, BIBREF13. Since the corpus has a low number of summaries (for 141 dialogues), BIBREF13 proposed to use assigned topic descriptions as gold references. These are short, label-like goals of the meeting, e.g., costing evaluation of project process; components, materials and energy sources; chitchat. Such descriptions, however, are very general, lacking the messenger-like structure and any information about the speakers.To benefit from large news corpora, BIBREF14 built a dialogue summarization model that first converts a conversation into a structured text document and later applies an attention-based pointer network to create an abstractive summary. Their model, trained on structured text documents of CNN/Daily Mail dataset, was evaluated on the Argumentative Dialogue Summary Corpus BIBREF15, which, however, contains only 45 dialogues.In the present paper, we further investigate the problem of abstractive dialogue summarization. With the growing popularity of online conversations via applications like Messenger, WhatsApp and WeChat, summarization of chats between a few participants is a new interesting direction of summarization research. For this purpose we have created the SAMSum Corpus which contains over 16k chat dialogues with manually annotated summaries. The dataset is freely available for the research community.The paper is structured as follows: in Section SECREF2 we present details about the new corpus and describe how it was created, validated and cleaned. Brief description of baselines used in the summarization task can be found in Section SECREF3. In Section SECREF4, we describe our experimental setup and parameters of models. Both evaluations of summarization models, the automatic with ROUGE metric and the linguistic one, are reported in Section SECREF5 and Section SECREF6, respectively. Examples of models' outputs and some errors they make are described in Section SECREF7. Finally, discussion, conclusions and ideas for further research are presented in sections SECREF8 and SECREF9.SAMSum Corpus	Initial approach. Since there was no available corpus of messenger conversations, we considered two approaches to build it: (1) using existing datasets of documents, which have a form similar to chat conversations, (2) creating such a dataset by linguists.In the first approach, we reviewed datasets from the following categories: chatbot dialogues, SMS corpora, IRC/chat data, movie dialogues, tweets, comments data (conversations formed by replies to comments), transcription of meetings, written discussions, phone dialogues and daily communication data. Unfortunately, they all differed in some respect from the conversations that are typically written in messenger apps, e.g. they were too technical (IRC data), too long (comments data, transcription of meetings), lacked context (movie dialogues) or they were more of a spoken type, such as a dialogue between a petrol station assistant and a client buying petrol.As a consequence, we decided to create a chat dialogue dataset by constructing such conversations that would epitomize the style of a messenger app.Process of building the dataset. Our dialogue summarization dataset contains natural messenger-like conversations created and written down by linguists fluent in English. The style and register of conversations are diversified – dialogues could be informal, semi-formal or formal, they may contain slang phrases, emoticons and typos. We asked linguists to create conversations similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger conversations. It includes chit-chats, gossiping about friends, arranging meetings, discussing politics, consulting university assignments with colleagues, etc. Therefore, this dataset does not contain any sensitive data or fragments of other corpora.Each dialogue was created by one person. After collecting all of the conversations, we asked language experts to annotate them with summaries, assuming that they should (1) be rather short, (2) extract important pieces of information, (3) include names of interlocutors, (4) be written in the third person. Each dialogue contains only one reference summary. Validation. Since the SAMSum corpus contains dialogues created by linguists, the question arises whether such conversations are really similar to those typically written via messenger apps. To find the answer, we performed a validation task. We asked two linguists to doubly annotate 50 conversations in order to verify whether the dialogues could appear in a messenger app and could be summarized (i.e. a dialogue is not too general or unintelligible) or not (e.g. a dialogue between two people in a shop). The results revealed that 94% of examined dialogues were classified by both annotators as good i.e. they do look like conversations from a messenger app and could be condensed in a reasonable way. In a similar validation task, conducted for the existing dialogue-type datasets (described in the Initial approach section), the annotators agreed that only 28% of the dialogues resembled conversations from a messenger app.Cleaning data. After preparing the dataset, we conducted a process of cleaning it in a semi-automatic way. Beforehand, we specified a format for written dialogues with summaries: a colon should separate an author of utterance from its content, each utterance is expected to be in a separate line. Therefore, we could easily find all deviations from the agreed structure – some of them could be automatically fixed (e.g. when instead of a colon, someone used a semicolon right after the interlocutor's name at the beginning of an utterance), others were passed for verification to linguists. We also tried to correct typos in interlocutors' names (if one person has several utterances, it happens that, before one of them, there is a typo in his/her name) – we used the Levenshtein distance to find very similar names (possibly with typos e.g. 'George' and 'Goerge') in a single conversation, and those cases with very similar names were passed to linguists for verification.Description. The created dataset is made of 16369 conversations distributed uniformly into 4 groups based on the number of utterances in conversations: 3-6, 7-12, 13-18 and 19-30. Each utterance contains the name of the speaker. Most conversations consist of dialogues between two interlocutors (about 75% of all conversations), the rest is between three or more people. Table TABREF3 presents the size of the dataset split used in our experiments. The example of a dialogue from this corpus is shown in Table TABREF4.Dialogues baselines	The baseline commonly used in the news summarization task is Lead-3 BIBREF4, which takes three leading sentences of the document as the summary. The underlying assumption is that the beginning of the article contains the most significant information. Inspired by the Lead-n model, we propose a few different simple models:MIDDLE-n, which takes n utterances from the middle of the dialogue,LONGEST-n, treating only n longest utterances in order of length as a summary,LONGER-THAN-n, taking only utterances longer than n characters in order of length (if there is no such long utterance in the dialogue, takes the longest one),MOST-ACTIVE-PERSON, which treats all utterances of the most active person in the dialogue as a summary.Results of the evaluation of the above models are reported in Table TABREF9. There is no obvious baseline for the task of dialogues summarization. We expected rather low results for Lead-3, as the beginnings of the conversations usually contain greetings, not the main part of the discourse. However, it seems that in our dataset greetings are frequently combined with question-asking or information passing (sometimes they are even omitted) and such a baseline works even better than the MIDDLE baseline (taking utterances from the middle of a dialogue). Nevertheless, the best dialogue baseline turns out to be the LONGEST-3 model.Experimental setup	This section contains a description of setting used in the experiments carried out.Experimental setup ::: Data preparation	In order to build a dialogue summarization model, we adopt the following strategies: (1) each candidate architecture is trained and evaluated on the dialogue dataset; (2) each architecture is trained on the train set of CNN/Daily Mail joined together with the train set of the dialogue data, and evaluated on the dialogue test set.In addition, we prepare a version of dialogue data, in which utterances are separated with a special token called the separator (artificially added token e.g. '$<$EOU$>$' for models using word embeddings, '$|$' for models using subword embeddings). In all our experiments, news and dialogues are truncated to 400 tokens, and summaries – to 100 tokens. The maximum length of generated summaries was not limited.Experimental setup ::: Models	We carry out experiments with the following summarization models (for all architectures we set the beam size for beam search decoding to 5):Pointer generator network BIBREF4. In the case of Pointer Generator, we use a default configuration, changing only the minimum length of the generated summary from 35 (used in news) to 15 (used in dialogues).Transformer BIBREF16. The model is trained using OpenNMT library. We use the same parameters for training both on news and on dialogues, changing only the minimum length of the generated summary – 35 for news and 15 for dialogues.Fast Abs RL BIBREF5. It is trained using its default parameters. For dialogues, we change the convolutional word-level sentence encoder (used in extractor part) to only use kernel with size equal 3 instead of 3-5 range. It is caused by the fact that some of utterances are very short and the default setting is unable to handle that.Fast Abs RL Enhanced. The additional variant of the Fast Abs RL model with slightly changed utterances i.e. to each utterance, at the end, after artificial separator, we add names of all other interlocutors. The reason for that is that Fast Abs RL requires text to be split into sentences (as it selects sentences and then paraphrase each of them). For dialogues, we divide text into utterances (which is a natural unit in conversations), so sometimes, a single utterance may contain more than one sentence. Taking into account how this model works, it may happen that it selects an utterance of a single person (each utterance starts with the name of the author of the utterance) and has no information about other interlocutors (if names of other interlocutors do not appear in selected utterances), so it may have no chance to use the right people's names in generated summaries.LightConv and DynamicConv BIBREF17. The implementation is available in fairseq BIBREF18. We train lightweight convolution models in two manners: (1) learning token representations from scratch; in this case we apply BPE tokenization with the vocabulary of 30K types, using fastBPE implementation BIBREF19; (2) initializing token embeddings with pre-trained language model representations; as a language model we choose GPT-2 small BIBREF20.Experimental setup ::: Evaluation metrics	We evaluate models with the standard ROUGE metric BIBREF21, reporting the $F_1$ scores (with stemming) for ROUGE-1, ROUGE-2 and ROUGE-L following previous works BIBREF5, BIBREF4. We obtain scores using the py-rouge package.Results	The results for the news summarization task are shown in Table TABREF25 and for the dialogue summarization – in Table TABREF26. In both domains, the best models' ROUGE-1 exceeds 39, ROUGE-2 – 17 and ROUGE-L – 36. Note that the strong baseline for news (Lead-3) is outperformed in all three metrics only by one model. In the case of dialogues, all tested models perform better than the baseline (LONGEST-3).In general, the Transformer-based architectures benefit from training on the joint dataset: news+dialogues, even though the news and the dialogue documents have very different structures. Interestingly, this does not seem to be the case for the Pointer Generator or Fast Abs RL model.The inclusion of a separation token between dialogue utterances is advantageous for most models – presumably because it improves the discourse structure. The improvement is most visible when training is performed on the joint dataset.Having compared two variants of the Fast Abs RL model – with original utterances and with enhanced ones (see Section SECREF11), we conclude that enhancing utterances with information about the other interlocutors helps achieve higher ROUGE values.The largest improvement of the model performance is observed for LightConv and DynamicConv models when they are complemented with pretrained embeddings from the language model GPT-2, trained on enormous corpora.It is also worth noting that some models (Pointer Generator, Fast Abs RL), trained only on the dialogues corpus (which has 16k dialogues), reach similar level (or better) in terms of ROUGE metrics than models trained on the CNN/DM news dataset (which has more than 300k articles). Adding pretrained embeddings and training on the joined dataset helps in achieving significantly higher values of ROUGE for dialogues than the best models achieve on the CNN/DM news dataset.According to ROUGE metrics, the best performing model is DynamicConv with GPT-2 embeddings, trained on joined news and dialogue data with an utterance separation token.Linguistic verification of summaries	ROUGE is a standard way of evaluating the quality of machine generated summaries by comparing them with reference ones. The metric based on n-gram overlapping, however, may not be very informative for abstractive summarization, where paraphrasing is a keypoint in producing high-quality sentences. To quantify this conjecture, we manually evaluated summaries generated by the models for 150 news and 100 dialogues. We asked two linguists to mark the quality of every summary on the scale of $-1$, 0, 1, where $-1$ means that a summarization is poor, extracts irrelevant information or does not make sense at all, 1 – it is understandable and gives a brief overview of the text, and 0 stands for a summarization that extracts only a part of relevant information, or makes some mistakes in the produced summary.We noticed a few annotations (7 for news and 4 for dialogues) with opposite marks (i.e. one annotator judgement was $-1$, whereas the second one was 1) and decided to have them annotated once again by another annotator who had to resolve conflicts. For the rest, we calculated the linear weighted Cohen's kappa coefficient BIBREF22 between annotators' scores. For news examples, we obtained agreement on the level of $0.371$ and for dialogues – $0.506$. The annotators' agreement is higher on dialogues than on news, probably because of structures of those data – articles are often long and it is difficult to decide what the key-point of the text is; dialogues, on the contrary, are rather short and focused mainly on one topic.For manually evaluated samples, we calculated ROUGE metrics and the mean of two human ratings; the prepared statistics is presented in Table TABREF27. As we can see, models generating dialogue summaries can obtain high ROUGE results, but their outputs are marked as poor by human annotators. Our conclusion is that the ROUGE metric corresponds with the quality of generated summaries for news much better than for dialogues, confirmed by Pearson's correlation between human evaluation and the ROUGE metric, shown in Table TABREF28.Difficulties in dialogue summarization	In a structured text, such as a news article, the information flow is very clear. However, in a dialogue, which contains discussions (e.g. when people try to agree on a date of a meeting), questions (one person asks about something and the answer may appear a few utterances later) and greetings, most important pieces of information are scattered across the utterances of different speakers. What is more, articles are written in the third-person point of view, but in a chat everyone talks about themselves, using a variety of pronouns, which further complicates the structure. Additionally, people talking on messengers often are in a hurry, so they shorten words, use the slang phrases (e.g. 'u r gr8' means 'you are great') and make typos. These phenomena increase the difficulty of performing dialogue summarization.Table TABREF34 and TABREF35 show a few selected dialogues, together with summaries produced by the best tested models:DynamicConv + GPT-2 embeddings with a separator (trained on news + dialogues),DynamicConv + GPT-2 embeddings (trained on news + dialogues),Fast Abs RL (trained on dialogues),Fast Abs RL Enhanced (trained on dialogues),Transformer (trained on news + dialogues).One can easily notice problematic issues. Firstly, the models frequently have difficulties in associating names with actions, often repeating the same name, e.g., for Dialogue 1 in Table TABREF34, Fast Abs RL generates the following summary: 'lilly and lilly are going to eat salmon'. To help the model deal with names, the utterances are enhanced by adding information about the other interlocutors – Fast Abs RL enhanced variant described in Section SECREF11. In this case, after enhancement, the model generates a summary containing both interlocutors' names: 'lily and gabriel are going to pasta...'. Sometimes models correctly choose speakers' names when generating a summary, but make a mistake in deciding who performs the action (the subject) and who receives the action (the object), e.g. for Dialogue 4 DynamicConv + GPT-2 emb. w/o sep. model generates the summary 'randolph will buy some earplugs for maya', while the correct form is 'maya will buy some earplugs for randolph'. A closely related problem is capturing the context and extracting information about the arrangements after the discussion. For instance, for Dialogue 4, the Fast Abs RL model draws a wrong conclusion from the agreed arrangement. This issue is quite frequently visible in summaries generated by Fast Abs RL, which may be the consequence of the way it is constructed; it first chooses important utterances, and then summarizes each of them separately. This leads to the narrowing of the context and loosing important pieces of information.One more aspect of summary generation is deciding which information in the dialogue content is important. For instance, for Dialogue 3 DynamicConv + GPT-2 emb. with sep. generates a correct summary, but focuses on a piece of information different than the one included in the reference summary. In contrast, some other models – like Fast Abs RL enhanced – select both of the pieces of information appearing in the discussion. On the other hand, when summarizing Dialogue 5, the models seem to focus too much on the phrase 'it's the best place', intuitively not the most important one to summarize.Discussion	This paper is a step towards abstractive summarization of dialogues by (1) introducing a new dataset, created for this task, (2) comparison with news summarization by the means of automated (ROUGE) and human evaluation.Most of the tools and the metrics measuring the quality of text summarization have been developed for a single-speaker document, such as news; as such, they are not necessarily the best choice for conversations with several speakers.We test a few general-purpose summarization models. In terms of human evaluation, the results of dialogues summarization are worse than the results of news summarization. This is connected with the fact that the dialogue structure is more complex – information is spread in multiple utterances, discussions, questions, more typos and slang words appear there, posing new challenges for summarization. On the other hand, dialogues are divided into utterances, and for each utterance its author is assigned. We demonstrate in experiments that the models benefit from the introduction of separators, which mark utterances for each person. This suggests that dedicated models having some architectural changes, taking into account the assignation of a person to an utterance in a systematic manner, could improve the quality of dialogue summarization.We show that the most popular summarization metric ROUGE does not reflect the quality of a summary. Looking at the ROUGE scores, one concludes that the dialogue summarization models perform better than the ones for news summarization. In fact, this hypothesis is not true – we performed an independent, manual analysis of summaries and we demonstrated that high ROUGE results, obtained for automatically-generated dialogue summaries, correspond with lower evaluation marks given by human annotators. An interesting example of the misleading behavior of the ROUGE metrics is presented in Table TABREF35 for Dialogue 4, where a wrong summary – 'paul and cindy don't like red roses.' – obtained all ROUGE values higher than a correct summary – 'paul asks cindy what color flowers should buy.'. Despite lower ROUGE values, news summaries were scored higher by human evaluators. We conclude that when measuring the quality of model-generated summaries, the ROUGE metrics are more indicative for news than for dialogues, and a new metric should be designed to measure the quality of abstractive dialogue summaries.Conclusions	In our paper we have studied the challenges of abstractive dialogue summarization. We have addressed a major factor that prevents researchers from engaging into this problem: the lack of a proper dataset. To the best of our knowledge, this is the first attempt to create a comprehensive resource of this type which can be used in future research. The next step could be creating an even more challenging dataset with longer dialogues that not only cover one topic, but span over numerous different ones.As shown, summarization of dialogues is much more challenging than of news. In order to perform well, it may require designing dedicated tools, but also new, non-standard measures to capture the quality of abstractive dialogue summaries in a relevant way. We hope to tackle these issues in future work.Acknowledgments	We would like to express our sincere thanks to Tunia Błachno, Oliwia Ebebenge, Monika Jędras and Małgorzata Krawentek for their huge contribution to the corpus collection – without their ideas, management of the linguistic task and verification of examples we would not be able to create this paper. We are also grateful for the reviewers' helpful comments and suggestions.","['How many abstractive summarizations exist for each dialogue?', ""How is human evaluators' judgement measured, what was the criteria?"", 'What models have been evaluated?', 'Do authors propose some better metric than ROUGE for measurement of abstractive dialogue summarization?', 'How big is SAMSum Corpus?']","['Unacceptable', 'Unacceptable', 'MIDDLE-n, LONGEST-n, LONGER-THAN-n and MOST-ACTIVE-PERSON are the baselines, and experiments also carried out on Pointer generator networks, Transformers, Fast Abs RL, Fast Abs RL Enhanced, LightConv and DynamicConv ', 'Unacceptable', 'Unacceptable']"
46,"Exploiting Token and Path-based Representations of Code for Identifying Security-Relevant Commits	Public vulnerability databases such as CVE and NVD account for only 60% of security vulnerabilities present in open-source projects, and are known to suffer from inconsistent quality. Over the last two years, there has been considerable growth in the number of known vulnerabilities across projects available in various repositories such as NPM and Maven Central. Such an increasing risk calls for a mechanism to infer the presence of security threats in a timely manner. We propose novel hierarchical deep learning models for the identification of security-relevant commits from either the commit diff or the source code for the Java classes. By comparing the performance of our model against code2vec, a state-of-the-art model that learns from path-based representations of code, and a logistic regression baseline, we show that deep learning models show promising results in identifying security-related commits. We also conduct a comparative analysis of how various deep learning models learn across different input representations and the effect of regularization on the generalization of our models.	Introduction	The use of open-source software has been steadily increasing for some time now, with the number of Java packages in Maven Central doubling in 2018. However, BIBREF0 states that there has been an 88% growth in the number of vulnerabilities reported over the last two years. In order to develop secure software, it is essential to analyze and understand security vulnerabilities that occur in software systems and address them in a timely manner. While there exist several approaches in the literature for identifying and managing security vulnerabilities, BIBREF1 show that an effective vulnerability management approach must be code-centric. Rather than relying on metadata, efforts must be based on analyzing vulnerabilities and their fixes at the code level.Common Vulnerabilities and Exposures (CVE) is a list of publicly known cybersecurity vulnerabilities, each with an identification number. These entries are used in the National Vulnerability Database (NVD), the U.S. government repository of standards based vulnerability management data. The NVD suffers from poor coverage, as it contains only 10% of the open-source vulnerabilities that have received a CVE identifier BIBREF2. This could be due to the fact that a number of security vulnerabilities are discovered and fixed through informal communication between maintainers and their users in an issue tracker. To make things worse, these public databases are too slow to add vulnerabilities as they lag behind a private database such as Snyk's DB by an average of 92 days BIBREF0 All of the above pitfalls of public vulnerability management databases (such as NVD) call for a mechanism to automatically infer the presence of security threats in open-source projects, and their corresponding fixes, in a timely manner.We propose a novel approach using deep learning in order to identify commits in open-source repositories that are security-relevant. We build regularized hierarchical deep learning models that encode features first at the file level, and then aggregate these file-level representations to perform the final classification. We also show that code2vec, a model that learns from path-based representations of code and claimed by BIBREF3 to be suitable for a wide range of source code classification tasks, performs worse than our logistic regression baseline.In this study, we seek to answer the following research questions:[leftmargin=*]RQ1: Can we effectively identify security-relevant commits using only the commit diff? For this research question, we do not use any of the commit metadata such as the commit message or information about the author. We treat source code changes like unstructured text without using path-based representations from the abstract syntax tree.RQ2: Does extracting class-level features before and after the change instead of using only the commit diff improve the identification of security-relevant commits? For this research question, we test the hypothesis that the source code of the entire Java class contains more information than just the commit diff and could potentially improve the performance of our model.RQ3: Does exploiting path-based representations of Java source code before and after the change improve the identification of security-relevant commits? For this research question, we test whether code2vec, a state-of-the-art model that learns from path-based representations of code, performs better than our model that treats source code as unstructured text.RQ4: Is mining commits using regular expression matching of commit messages an effective means of data augmentation for improving the identification of security-relevant commits? Since labelling commits manually is an expensive task, it is not easy to build a dataset large enough to train deep learning models. For this research question, we explore if collecting coarse data samples using a high-precision approach is an effective way to augment the ground-truth dataset.The main contributions of this paper are:[leftmargin=*]Novel hierarchical deep learning models for the identification of security-relevant commits based on either the diff or the modified source code of the Java classes.A comparative analysis of how various deep learning models perform across different input representations and how various regularization techniques help with the generalization of our models.We envision that this work would ultimately allow for monitoring open-source repositories in real-time, in order to automatically detect security-relevant changes such as vulnerability fixes.Background and Related Work ::: Neural Networks for Text Classification	In computational linguistics, there has been a lot of effort over the last few years to create a continuous higher dimensional vector space representation of words, sentences, and even documents such that similar entities are closer to each other in that space BIBREF4, BIBREF5, BIBREF6. BIBREF4 introduced word2vec, a class of two-layer neural network models that are trained on a large corpus of text to produce word embeddings for natural language. Such learned distributed representations of words have accelerated the application of deep learning techniques for natural language processing (NLP) tasks BIBREF7.BIBREF8 show that convolutional neural networks (CNNs) can achieve state-of-the-art results in single-sentence sentiment prediction, among other sentence classification tasks. In this approach, the vector representations of the words in a sentence are concatenated vertically to create a two-dimensional matrix for each sentence. The resulting matrix is passed through a CNN to extract higher-level features for performing the classification. BIBREF9 introduce the hierarchical attention network (HAN), where a document vector is progressively built by aggregating important words into sentence vectors, and then aggregating important sentences vectors into document vectors.Deep neural networks are prone to overfitting due to the possibility of the network learning complicated relationships that exist in the training set but not in unseen test data. Dropout prevents complex co-adaptations of hidden units on training data by randomly removing (i.e. dropping out) hidden units along with their connections during training BIBREF10. Embedding dropout, used by BIBREF11 for neural language modeling, performs dropout on entire word embeddings. This effectively removes a proportion of the input tokens randomly at each training iteration, in order to condition the model to be robust against missing input.While dropout works well for regularizing fully-connected layers, it is less effective for convolutional layers due to the spatial correlation of activation units in convolutional layers. There have been a number of attempts to extend dropout to convolutional neural networks BIBREF12. DropBlock is a form of structured dropout for convolutional layers where units in a contiguous region of a feature map are dropped together BIBREF13.Background and Related Work ::: Learning Embeddings for Source Code	While building usable embeddings for source code that capture the complex characteristics involving both syntax and semantics is a challenging task, such embeddings have direct downstream applications in tasks such as semantic code clone detection, code captioning, and code completion BIBREF14, BIBREF15. In the same vein as BIBREF4, neural networks have been used for representing snippets of code as continuous distributed vectors BIBREF16. They represent a code snippet as a bag of contexts and each context is represented by a context vector, followed by a path-attention network that learns how to aggregate these context vectors in a weighted manner.A number of other code embedding techniques are also available in the literature. BIBREF17 learn word embeddings from abstractions of traces obtained from the symbolic execution of a program. They evaluate their learned embeddings on a benchmark of API-usage analogies extracted from the Linux kernel and achieved 93% top-1 accuracy. BIBREF18 describe a pipeline that leverages deep learning for semantic search of code. To achieve this, they train a sequence-to-sequence model that learns to summarize Python code by predicting the corresponding docstring from the code blob, and in the process provide code representations for Python.Background and Related Work ::: Identifying Security Vulnerabilities	There exist a handful of papers in software engineering that perform commit classification to identify security vulnerabilities or fixes. BIBREF19 describe an efficient vulnerability identification system geared towards tracking large-scale projects in real time using latent information underlying commit messages and bug reports in open-source projects. While BIBREF19 classify commits based on the commit message, we use only the commit diff or the corresponding source code as features for our model. BIBREF2 propose a machine learning approach to identify security-relevant commits. However, they treat source code as documents written in natural language and use well-known document classification methods to perform the actual classification. BIBREF20 conduct an analysis to identify which security vulnerabilities can be discovered during code review, or what characteristics of developers are likely to introduce vulnerabilities.Experimental Setup	This section details the methodology used in this study to build the training dataset, the models used for classification and the evaluation procedure. All of the experiments are conducted on Python 3.7 running on an Intel Core i7 6800K CPU and a Nvidia GTX 1080 GPU. All the deep learning models are implemented in PyTorch 0.4.1 BIBREF21, while Scikit-learn 0.19.2 BIBREF22 is used for computing the tf–idf vectors and performing logistic regression.For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23. These repositories are split into training, validation, and test splits containing 808, 265, and 264 commits, respectively. In order to minimize the occurrence of duplicate commits in two of these splits (such as in both training and test), commits from no repository belong to more than one split. However, 808 commits may not be sufficient to train deep learning models. Hence, in order to answer RQ4, we augment the training split with commits mined using regular expression matching on the commit messages from the same set of open-source Java projects. This almost doubles the number of commits in the training split to 1493. We then repeat our experiments for the first three research questions on the augmented dataset, and evaluate our trained models on the same validation and test splits.We also compare the quality of randomly-initialized embeddings with pre-trained ones. Since the word2vec embeddings only need unlabelled data to train, the data collection and preprocessing stage is straightforward. GitHub, being a very large host of source code, contains enough code for training such models. However, a significant proportion of code in GitHub does not belong to engineered software projects BIBREF24. To reduce the amount of noise in our training data, we filter repositories based on their size, commit history, number of issues, pull requests, and contributors, and build a corpus of the top 1000 Java repositories. We limit the number of repositories to 1000 due to GitHub API limitations. It is worth noting that using a larger training corpus might provide better results. For instance, code2vec is pre-trained on a corpus that is ten times larger.To extract token-level features for our model, we use the lexer and tokenizer provided as a part of the Python javalang library. We ensure that we only use the code and not code comments or metadata, as it is possible for comments or commit messages to include which vulnerabilities are fixed, as shown in Figure FIGREF12. Our models would then overfit on these features rather than learning the features from the code. For extracting path-based representations from Java code, we use ASTMiner.Model ::: Training Word2vec Embeddings	We learn token-level vectors for code using the CBOW architecture BIBREF4, with negative sampling and a context window size of 5. Using CBOW over skip-gram is a deliberate design decision. While skip-gram is better for infrequent words, we felt that it is more important to focus on the more frequent words (inevitably, the keywords in a programming language) when it comes to code. Since we only perform minimal preprocessing on the code (detailed below), the most infrequent words will usually be variable identifiers. Following the same line of reasoning, we choose negative sampling over hierarchical-softmax as the training algorithm.We do not normalize variable identifiers into generic tokens as they could contain contextual information. However, we do perform minimal preprocessing on the code before training the model. This includes:The removal of comments and whitespace when performing tokenization using a lexer.The conversion of all numbers such as integers and floating point units into reserved tokens.The removal of tokens whose length is greater than or equal to 64 characters.Thresholding the size of the vocabulary to remove infrequent tokens.Model ::: Identifying Security Vulnerabilities	We modify our model accordingly for every research question, based on changes in the input representation. To benchmark the performance of our deep learning models, we compare them against a logistic regression (LR) baseline that learns on one-hot representations of the Java tokens extracted from the commit diffs. For all of our models, we employ dropout on the fully-connected layer for regularization. We use Adam BIBREF25 for optimization, with a learning rate of 0.001, and batch size of 16 for randomly initialized embeddings and 8 for pre-trained embeddings.For RQ1, we use a hierarchical CNN (H-CNN) with either randomly-initialized or pre-trained word embeddings in order to extract features from the commit diff. We represent the commit diff as a concatenation of 300-dimensional vectors for each corresponding token from that diff. This resultant matrix is then passed through three temporal convolutional layers in parallel, with filter windows of size 3, 5, and 7. A temporal max-pooling operation is applied to these feature maps to retain the feature with the highest value in every map. We also present a regularized version of this model (henceforth referred to as HR-CNN) with embedding dropout applied on the inputs, and DropBlock on the activations of the convolutional layers.For RQ2, we made a modification to both the H-CNN and HR-CNN models in order to extract features from the source code for the Java classes before and after the commit. Both of these models use a siamese architecture between the two CNN-based encoders as shown in Figure FIGREF20. We then concatenate the results from both of these encoders and pass it through a fully-connected layer followed by softmax for prediction.For RQ3, we adapt the code2vec model used by BIBREF16 for predicting method names into a model for predicting whether a commit is security-relevant by modifying the final layer. We then repeat our experiments on both the ground-truth and augmented dataset.Results and Discussion	The results for all of our models on both the ground-truth and augmented datasets are given in Table TABREF22.RQ1: Can we effectively identify security-relevant commits using only the commit diff?Without using any of the metadata present in a commit, such as the commit message or information about the author, we are able to correctly classify commits based on their security-relevance with an accuracy of 65.3% and $\text{F}_1$of 77.6% on unseen test data. Table TABREF22, row 5, shows that using our regularized HR-CNN model with pre-trained embeddings provides the best overall results on the test split when input features are extracted from the commit diff. Table TABREF22, row 3, shows that while H-CNN provides the most accurate results on the validation split, it doesn't generalize as well to unseen test data. While these results are usable, H-CNN and HR-CNN only perform 3 points better than the LR baseline (Table TABREF22, row 1) in terms of $\text{F}_1$and 2 points better in terms of accuracy.RQ2: Does extracting class-level features before and after the change instead of using only the commit diff improve the identification of security-relevant commits?When extracting features from the complete source code of the Java classes which are modified in the commit, the performance of HR-CNN increases noticeably. Table TABREF22, row 9, shows that the accuracy of HR-CNN when using pre-trained embeddings increases to 72.6% and $\text{F}_1$increases to 79.7%. This is considerably above the LR baseline and justifies the use of a more complex deep learning model. Meanwhile, the performance of H-CNN with randomly-initialized embeddings (Table TABREF22, row 6) does not improve when learning on entire Java classes, but there is a marked improvement in $\text{F}_1$of about 6 points when using pre-trained embeddings. Hence, we find that extracting class-level features from the source code before and after the change, instead of using only the commit diff, improves the identification of security-relevant commits.RQ3: Does exploiting path-based representations of the Java classes before and after the change improve the identification of security-relevant commits?Table TABREF22, row 10, shows that training the modified code2vec model to identify security-aware commits from scratch results in a model that performs worse than the LR baseline. The model only achieves an accuracy of 63.8% on the test split, with an $\text{F}_1$score of 72.7%, which is two points less than that of LR. The code2vec model performs much worse compared to H-CNN and HR-CNN with randomly-initialized embeddings. Hence, learning from a path-based representation of the Java classes before and after the change does not improve the identification of security-relevant commits—at least with the code2vec approach.RQ4: Is mining commits using regular expression matching of commit messages an effective means of data augmentation for improving the identification of security-relevant commits?The results in Table TABREF22, rows 11 to 20, show that collecting coarse data samples using regular expression matching for augmenting the ground-truth training set is not effective in increasing the performance of our models. This could possibly be due to the coarse data samples being too noisy or the distribution of security-relevant commits in the coarse dataset not matching that of the unseen dataset. The latter might have been due to the high-precision mining technique used, capturing only a small subset of security vulnerabilities.Results and Discussion ::: Threats to Validity	The lexer and tokenizer we use from the javalang library target Java 8. We are not able to verify that all the projects and their forks in this study are using the same version of Java. However, we do not expect considerable differences in syntax between Java 7 and Java 8 except for the introduction of lambda expressions.There is also a question of to what extent the 635 publicly disclosed vulnerabilities used for evaluation in this study represent the vulnerabilities found in real-world scenarios. While creating larger ground-truth datasets would always be helpful, it might not always be possible. To reduce the possibility of bias in our results, we ensure that we don't train commits from the same projects that we evaluate our models on. We also discard any commits belonging to the set of evaluation projects that are mined using regular expression matching.We directly train code2vec on our dataset without pre-training it, in order to assess how well path-based representations perform for learning on code, as opposed to token-level representations on which H-CNN and HR-CNN are based. However, BIBREF16 pre-trained their model on 10M Java classes. It is possible that the performance of code2vec is considerably better than the results in Table TABREF22 after pre-training. Furthermore, our findings apply only to this particular technique to capturing path-based representations, not the approach in general. However, we leave both issues for future work.Conclusions and Future Work	In this study, we propose a novel hierarchical deep learning model for the identification of security-relevant commits and show that deep learning has much to offer when it comes to commit classification. We also make a case for pre-training word embeddings on tokens extracted from Java code, which leads to performance improvements. We are able to further improve the results using a siamese architecture connecting two CNN-based encoders to represent the modified files before and after a commit.Network architectures that are effective on a certain task, such as predicting method names, are not necessarily effective on related tasks. Thus, choices between neural models should be made considering the nature of the task and the amount of training data available. Based on the model's ability to predict method names in files across different projects, BIBREF16 claim that code2vec can be used for a wide range of programming language processing tasks. However, for predicting the security relevance of commits, H-CNN and HR-CNN appear to be much better than code2vec.A potential research direction would be to build language models for programming languages based on deep language representation models. Neural networks are becoming increasingly deeper and complex in the NLP literature, with significant interest in deep language representation models such as ELMo, GPT, and BERT BIBREF26, BIBREF27, BIBREF28. BIBREF28 show strong empirical performance on a broad range of NLP tasks. Since all of these models are pre-trained in an unsupervised manner, it would be easy to pre-train such models on the vast amount of data available on GitHub.Deep learning models are known for scaling well with more data. However, with less than 1,000 ground-truth training samples and around 1,800 augmented training samples, we are unable to exploit the full potential of deep learning. A reflection on the current state of labelled datasets in software engineering (or the lack thereof) throws light on limited practicality of deep learning models for certain software engineering tasks BIBREF29. As stated by BIBREF30, just as research in NLP changed focus from brittle rule-based expert systems to statistical methods, software engineering research should augment traditional methods that consider only the formal structure of programs with information about the statistical properties of code. Ongoing research on pre-trained code embeddings that don't require a labelled dataset for training is a step in the right direction. Drawing parallels with the recent history of NLP research, we are hoping that further study in the domain of code embeddings will considerably accelerate progress in tackling software problems with deep learning.Acknowledgments	We would like to thank SAP and NSERC for their support towards this project.","['What regularization methods are used?', 'What metrics are used?', 'How long is the dataset?', 'What dataset do they use?']","['Unacceptable', 'Accuracy, Precision, Recall, F1-score', 'Unacceptable', 'Unacceptable']"
47,"The Rapidly Changing Landscape of Conversational Agents	Conversational agents have become ubiquitous, ranging from goal-oriented systems for helping with reservations to chit-chat models found in modern virtual assistants. In this survey paper, we explore this fascinating field. We look at some of the pioneering work that defined the field and gradually move to the current state-of-the-art models. We look at statistical, neural, generative adversarial network based and reinforcement learning based approaches and how they evolved. Along the way we discuss various challenges that the field faces, lack of context in utterances, not having a good quantitative metric to compare models, lack of trust in agents because they do not have a consistent persona etc. We structure this paper in a way that answers these pertinent questions and discusses competing approaches to solve them.	Introduction	One of the earliest goals of Artificial Intelligence (AI) has been to build machines that can converse with us. Whether in early AI literature or the current popular culture, conversational agents have captured our imagination like no other technology has. In-fact the ultimate test of whether true artificial intelligence has been achieved, the Turing test BIBREF0 proposed by Alan Turing the father of artificial intelligence in 1950, revolves around the concept of a good conversational agent. The test is deemed to have been passed if a conversational agent is able to fool human judges into believing that it is in fact a human being.Starting with pattern matching programs like ELIZA developed at MIT in 1964 to the current commercial conversational agents and personal assistants (Siri, Allo, Alexa, Cortana et al) that all of us carry in our pockets, conversational agents have come a long way. In this paper we look at this incredible journey. We start by looking at early rule-based methods which consisted of hand engineered features, most of which were domain specific. However, in our view, the advent of neural networks that were capable of capturing long term dependencies in text and the creation of the sequence to sequence learning model BIBREF1 that was capable of handling utterances of varying length is what truly revolutionized the field. Since the sequence to sequence model was first used to build a neural conversational agent BIBREF2 in 2016 the field has exploded. With a multitude of new approaches being proposed in the last two years which significantly impact the quality of these conversational agents, we skew our paper towards the post 2016 era. Indeed one of the key features of this paper is that it surveys the exciting new developments in the domain of conversational agents.Dialogue systems, also known as interactive conversational agents, virtual agents and sometimes chatterbots, are used in a wide set of applications ranging from technical support services to language learning tools and entertainment. Dialogue systems can be divided into goal-driven systems, such as technical support services, booking systems, and querying systems. On the other hand we have non-goal-driven systems which are also referred to as chit-chat models. There is no explicit purpose for interacting with these agents other than entertainment. Compared to goal oriented dialog systems where the universe is limited to an application, building open-ended chit-chat models is more challenging. Non-goal oriented agents are a good indication of the state of the art of artificial intelligence according to the Turing test. With no grounding in common sense and no sense of context these agents have to fall back on canned responses and resort to internet searches now. But as we discuss in section SECREF5 , new techniques are emerging to provide this much needed context to these agents.The recent successes in the domain of Reinforcement Learning (RL) has also opened new avenues of applications in the conversational agent setting. We explore some of these approaches in section SECREF6 Another feature that has been traditionally lacking in conversation agents is a personality. O Vinayal et al BIBREF2 hypothesis that not having a consistent personality is one of the main reasons that is stopping us from passing the turing test. Conversational agents also lack emotional consistency in their responses. These features are vital if we want humans to trust conversational agents. In section SECREF7 we discuss state of the art approaches to overcome these problems.Despite such huge advancements in the field, the way these models are evaluated is something that needs to be dramatically altered. Currently there exists no perfect quantitative method to compare two conversational agents. The field has to rely on qualitative measures or measures like BLeU and perplexity borrowed from machine translation. In section SECREF8 we discuss this problem in detail.Early Techniques	Initially, the interactive dialogue systems were based on and limited to speaker independent recognition of isolated words and phrases or limited continuous speech such as digit strings. In August 1993, there came the ESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 ) which was aimed at allowing spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries. The linguistic processing component in it was based on natural language parsing. The parser made use of alternative word hypotheses represented in a lattice or graph in constructing a parse tree and allowance was made for gaps and partially parsable strings. It made use of both syntactic and semantic knowledge for the task domain. It was able to achieve a 96% success rate for the flight inquiry application in English. However, the issue was that the given conversational agent was heavily limited to the types of applications it can perform and its high success rate was more due to that instead of great natural language techniques (relative to recent times).In 1995, two researchers (Ball et al, 1995 BIBREF4 ) at Microsoft developed a conversational assistant called Persona which was one of the first true personal assistant similar to what we have in recent times (like Siri, etc). It allowed users the maximum flexibility to express their requests in whatever syntax they found most natural and the interface was based on a broad-coverage NLP system unlike the system discussed in the previous paragraph. In this, a labelled semantic graph is generated from the speech input which encodes case frames or thematic roles. After this, a sequence of graph transformations is applied on it using the knowledge of interaction scenario and application domain. This results into a normalized application specific structure called as task graph which is then matched against the templates (in the application) which represent the normalized task graphs corresponding to all the possible user statements that the assistant understands and the action is then executed. The accuracy was not that good and they did not bother to calculate it. Also, due to the integrated nature of conversational interaction in Persona, the necessary knowledge must be provided to each component of the system. Although it had limitations, it provided a very usable linguistic foundation for conversational interaction.The researchers thought that if they can create assistant models specific to the corresponding models, they can achieve better accuracy for those applications instead of creating a common unified personal assistant which at that time performed quite poorly. There was a surge in application-specific assistants like in-car intelligent personal assistant (Schillo et al, 1996 BIBREF5 ), spoken-language interface to execute military exercises (Stent et al, 1999 BIBREF6 ), etc. Since it was difficult to develop systems with high domain extensibility, the researchers came up with a distributed architecture for cooperative spoken dialogue agents (Lin et al, 1999 BIBREF7 ).Under this architecture, different spoken dialogue agents handling different domains can be developed independently and cooperate with one another to respond to the user’s requests. While a user interface agent can access the correct spoken dialogue agent through a domain switching protocol, and carry over the dialogue state and history so as to keep the knowledge processed persistently and consistently across different domains. Figure FIGREF1 shows the agent society for spoken dialogue for tour information service.If we define the false alarm rate by counting the utterances in which unnecessary domain-switching occurred and the detection rate by counting the utterances in which the desired domain-switching were accurately detected, then in this model, high detection rate was achieved at very low false alarm rate. For instance, for around a false alarm rate of 0.2, the model was able to achieve a detection rate of around 0.9 for the case of tag sequence search with language model search scheme.Machine Learning Methods	Next came the era of using machine learning methods in the area of conversation agents which totally revolutionized this field.Maxine Eskenazi and her team initially wanted to build spoken dialog system for the less general sections of the population, such as the elderly and non-native speakers of English. They came up with Let’s Go project (Raux et al, 2003 BIBREF8 ) that was designed to provide Pittsburgh area bus information. Later, this was opened to the general public (Raux et al, 2005 BIBREF9 ). Their work is important in terms of the techniques they used.The speech recognition was done using n-gram statistical model which is then passed to a robust parser based on an extended Context Free Grammar allowing the system to skip unknown words and perform partial parsing. They wrote the grammar based on a combination of their own intuition and a small scale Wizard-of-Oz experiment they ran. The grammar rules used to identify bus stops were generated automatically from the schedule database. After this, they trained a statistical language model on the artificial corpus. In order to make the parsing grammar robust enough to parse fairly ungrammatical, yet understandable sentences, it was kept as general as possible. On making it public, they initially achieved a task success rate of 43.3% for the whole corpus and 43.6 when excluding sessions that did not contain any system-directed speech.After this they tried to increase the performance of the system (Raux et al, 2006 BIBREF10 ). They retrained their acoustic models by performing Baum-Welch optimization on the transcribed data (starting from their original models). Unfortunately, this only brought marginal improvement because the models (semi-continuous HMMs) and algorithms they were using were too simplistic for this task. They improved the turn-taking management abilities of the system by closely analysing the feedback they received. They added more specific strategies, aiming at dealing with problems like noisy environments, too loud or too long utterances, etc. They found that they were able to get a success rate of 79% for the complete dialogues (which was great).The previous papers (like the ones which we discussed in the above paragraph) did not attempt to use data-driven techniques for the dialog agents because such data was not available in large amount at that time. But then there was a high increase in the collection of spoken dialog corpora which made it possible to use data-driven techniques to build and use models of task-oriented dialogs and possibly get good results. In the paper by Srinivas et al,2008 BIBREF11 , the authors proposed using data-driven techniques to build task structures for individual dialogs and use the dialog task structures for dialog act classification, task/subtask classification, task/subtask prediction and dialog act prediction.For each utterance, they calculated features like n-grams of the words and their POS tags, dialog act and task/subtask label. Then they put those features in the binary MaxEnt classifier. For this, their model was able to achieve an error rate of 25.1% for the dialog act classification which was better than the best performing models at that time. Although, according to the modern standards, the results are not that great but the approach they suggested (of using data to build machine learning models) forms the basis of the techniques that are currently used in this area.Sequence to Sequence approaches for dialogue modelling	The problem with rule-based models was that they were often domain dependent and could not be easily ported to a new domain. They also depended on hand crafted rules which was both expensive and required domain expertise. Two factors which when combined spell doom for scalbility. All of this changed in 2015 when Vinyals et al proposed an approach BIBREF2 inspired from the recent progress in machine translation BIBREF1 . Vinyals et al used the sequence to sequence learning architecture for conversation agents. Their model was the first model which could be trained end-to-end, and could generate a new output utterance based on just the input sentence and no other hand crafted features.They achieved this by casting the conversation modelling task, as a task of predicting the next sequence given the previous sequence using recurrent networks. This simple approach truly changed the conversation agent landscape. Most of the state-of-the-art today is built on their success. In a nutshell the input utterance is input to an encoder network, which is a recurrent neural network (RNN) in this case, but as we will see Long Short Term Memory (LSTMs) BIBREF12 have since replaced RNNs as the standard for this task. The encoder summarizes the input utterance into a fixed length vector representation which is input to the decoder, which itself is again a RNN. The paper looks at this fixed vector as the thought vector - which hold the most important information of the input utterance. The Decoder netwroks takes this as input and output's an output utterance word-by-word until it generates an end-of-speech INLINEFORM0 token. This approach allows for variable length inputs and outputs. The network is jointly trained on two turn conversations. Figure FIGREF3 shows the sequence to sequence neural conversation model.Even though most of the modern work in the field is built on this approach there is a significant drawback to this idea. This model can theoretically never solve the problem of modelling dialogues due to various simplifications, the most important of them being the objective function that is being optimized does not capture the actual objective achieved through human communication, which is typically longer term and based on exchange of information rather than next step prediction. It is important to see that optimizing an agent to generate text based on what it sees in the two-turn conversation dataset that it is trained on does not mean that the agent would be able to generalize to human level conversation across contexts. Nevertheless in absence of a better way to capture human communication this approach laid the foundation of most of the modern advances in the field. Another problem that plagues this paper and the field in general is Evaluation. As there can be multiple correct output utterances for a given input utterance there is no quantitative way to evaluate how well a model is performing. In this paper to show the efficacy of their model the authors publish snippets of conversations across different datasets. We discuss this general problem in evaluation later.Iulian et al. build on this sequence-to-sequence based approach in their paper presented in AAAI 2016 BIBREF13 . Their work is inspired by the hierarchical recurrent encoder-decoder architecture (HRED) proposed by Sordoni et al. BIBREF14 . Their premise is that a dialogue can be seen as a sequence of utterances which, in turn, are sequences of tokens. Taking advantage of this built in hierarchy they model their system in the following fashion.The encoder RNN maps each utterance to an utterance vector. The utterance vector is the hidden state obtained after the last token of the utterance has been processed. The higher-level context RNN keeps track of past utterances by processing iteratively each utterance vector. After processing utterance INLINEFORM0 , the hidden state of the context RNN represents a summary of the dialogue up to and including turn INLINEFORM1 , which is used to predict the next utterance INLINEFORM2 . The next utterance prediction is performed by means of a decoder RNN, which takes the hidden state of the context RNN and produces a probability distribution over the tokens in the next utterance. As seen in figure FIGREF4 The advantages of using a hierarchical representation are two-fold. First, the context RNN allows the model to represent a form of common ground between speakers, e.g. to represent topics and concepts shared between the speakers using a distributed vector representation. Second, because the number of computational steps between utterances is reduced. This makes the objective function more stable w.r.t. the model parameters, and helps propagate the training signal for first-order optimization methods.Models like sequence-to-sequence and the hierarchical approaches have proven to be good baseline models. In the last couple of years there has been a major effort to build on top of these baselines to make conversational agents more robust BIBREF15 BIBREF16 .Due to their large parameter space, the estimation of neural conversation models requires considerable amounts of dialogue data. Large online corpora are helpful for this. However several dialogue corpora, most notably those extracted from subtitles, do not include any explicit turn segmentation or speaker identification.The neural conversation model may therefore inadvertently learn responses that remain within the same dialogue turn instead of starting a new turn. Lison et al BIBREF17 overcome these limitations by introduce a weighting model into the neural architecture. The weighting model, which is itself estimated from dialogue data, associates each training example to a numerical weight that reflects its intrinsic quality for dialogue modelling. At training time, these sample weights are included into the empirical loss to be minimized. The purpose of this model is to associate each ⟨context, response⟩ example pair to a numerical weight that reflects the intrinsic “quality” of each example. The instance weights are then included in the empirical loss to minimize when learning the parameters of the neural conversation model. The weights are themselves computed via a neural model learned from dialogue data. Approaches like BIBREF17 are helpful but data to train these neural conversational agents remains scarce especially in academia, we talk more about the scarcity of data in a future section.Language Model based approaches for dialogue modelling	Though sequence-to-sequence based models have achieved a lot of success, another push in the field has been to instead train a language model over the entire dialogue as one single sequence BIBREF18 . These works argue that a language model is better suited to dialogue modeling, as it learns how the conversation evolves as information progresses.Mei et al. BIBREF19 improve the coherence of such neural dialogue language models by developing a generative dynamic attention mechanism that allows each generated word to choose which related words it wants to align to in the increasing conversation history (including the previous words in the response being generated). They introduce a dynamic attention mechanism to a RNN language model in which the scope of attention increases as the recurrence operation progresses from the start through the end of the conversation. The dynamic attention model promotes coherence of the generated dialogue responses (continuations) by favoring the generation of words that have syntactic or semantic associations with salient words in the conversation history.Knowledge augmented models	Although these neural models are really powerful, so much so that they power most of the commercially available smart assistants and conversational agents. However these agents lack a sense of context and a grounding in common sense that their human interlocutors possess. This is especially evident when interacting with a commercial conversation agent, when more often that not the agent has to fall back to canned responses or resort to displaying Internet search results in response to an input utterance. One of the main goals of the research community, over the last year or so, has been to overcome this fundamental problem with conversation agents. A lot of different approaches have been proposed ranging from using knowledge graphs BIBREF20 to augment the agent's knowledge to using latest advancements in the field of online learning BIBREF21 . In this section we discuss some of these approaches.The first approach we discuss is the Dynamic Knowledge Graph Network (DynoNet) proposed by He et al BIBREF20 , in which the dialogue state is modeled as a knowledge graph with an embedding for each node. To model both structured and open-ended context they model two agents, each with a private list of items with attributes, that must communicate to identify the unique shared item. They structure entities as a knowledge graph; as the dialogue proceeds, new nodes are added and new context is propagated on the graph. An attention-based mechanism over the node embeddings drives generation of new utterances. The model is best explained by the example used in the paper which is as follows: The knowledge graph represents entities and relations in the agent’s private KB, e.g., item-1’s company is google. As the conversation unfolds, utterances are embedded and incorporated into node embeddings of mentioned entities. For instance, in Figure FIGREF6 , “anyone went to columbia” updates the embedding of columbia. Next, each node recursively passes its embedding to neighboring nodes so that related entities (e.g., those in the same row or column) also receive information from the most recent utterance. In this example, jessica and josh both receive new context when columbia is mentioned. Finally, the utterance generator, an LSTM, produces the next utterance by attending to the node embeddings.However Lee et al in BIBREF21 take a different approach to add knowledge to conversational agents. They proposes using a continuous learning based approach. They introduce a task-independent conversation model and an adaptive online algorithm for continual learning which together allow them to sequentially train a conversation model over multiple tasks without forgetting earlier tasks.In a different approach, Ghazvininejad et al BIBREF22 propose a knowledge grounded approach which infuses the output utterance with factual information relevant to the conversational context. Their architecture is shown in figure FIGREF7 . They use an external collection of world facts which is a large collection of raw text entries (e.g., Foursquare, Wikipedia, or Amazon reviews) indexed by named entities as keys. Then, given a conversational history or source sequence S, they identify the “focus” in S, which is the text span (one or more entities) based on which they form a query to link to the facts. The query is then used to retrieve all contextually relevant facts. Finally, both conversation history and relevant facts are fed into a neural architecture that features distinct encoders for conversation history and facts. Another interesting facet of such a model is that new facts can be added and old facts updated by just updating the world facts dictionary without retraining the model from scratch, thus making the model more adaptive and robust.Instead of just having a set of facts to augment the conversation, a richer way could be to use knowledge graphs or commonsense knowledge bases which consist of [entity-relation-entity] triples. Young et al explore this idea in BIBREF23 . For a given input utterance, they find the relevant assertions in the common sense knowledge base using simple n-gram matching. They then perform chunking on the relevant assertions and feed the individual token to a tri-LSTM encoder. The output of this encoder is weighted along with the input utterance and the output utterance is generated. They claim that such common sense conversation agents outperform a naive conversation agent.Another interesting way to add knowledge to the conversation agents is to capture external knowledge for a given dialog using a search engine. In the paper by Long et al, 2017 BIBREF24 , the authors built a model to generate natural and informative responses for customer service oriented dialog incorporating external knowledge.They get the external knowledge using a search engine. Then a knowledge enhanced sequence-to-sequence framework is designed to model multi-turn dialogs on external knowledge conditionally. For this purpose, their model extends the simple sequence-to-sequence model by augmenting the input with the knowledge vector so as to take account of the knowledge in the procedure of response generation into the decoder of the sequence-to-sequence model. Both the encoder and the decoder are composed of LSTM.Their model scores an average human rating of 3.3919 out of 5 in comparison to the baseline which is 3.3638 out of 5. Hence, their model generates more informative responses. However, they found the external knowledge plays a negative role in the procedure of response generation when there is more noise in the information. Exploring how to obtain credible knowledge of a given dialog history can be a future generation of their model.Reinforcement Learning based models	After exploring the neural methods in a lot of detail, the researchers have also begun exploring, in the current decade, how to use the reinforcement learning methods in the dialogue and personal agents.Initial reinforcement methods	One of the first main papers that thought of using reinforcement learning for this came in 2005 by English et al BIBREF25 . They used an on-policy Monte Carlo method and the objective function they used was a linear combination of the solution quality (S) and the dialog length (L), taking the form: o(S,I) = INLINEFORM0 - INLINEFORM1 .At the end of each dialog the interaction was given a score based on the evaluation function and that score was used to update the dialog policy of both agents (that is, the conversants). The state-action history for each agent was iterated over separately and the score from the recent dialog was averaged in with the expected return from the existing policy. They chose not to include any discounting factor to the dialog score as they progressed back through the dialog history. The decision to equally weight each state-action pair in the dialog history was made because an action’s contribution to the dialog score is not dependent upon its proximity to the end of the task. In order to combat the problem of converging to an effective policy they divided up the agent training process into multiple epochs.The average objective function score for the case of learned policies was 44.90. One of the main reasons for the low accuracy (which is also a limitation of this paper) was that there were a number of aspects of dialog that they had not modeled such as non-understandings, misunderstandings, and even parsing sentences into the action specification and generating sentences from the action specification. But the paper set the pavement of the reinforcement learning methods into the area of dialog and personal agents.End-to-End Reinforcement Learning of Dialogue Agents for Information Access	Let’s have a look at KB-InfoBot (by Dhingra et al, 2017 BIBREF26 ): a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. In this paper, they replace the symbolic queries (which break the differentiability of the system and prevent end-to-end training of neural dialogue agents) with an induced ‘soft’ posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users.In this, the authors used an RNN to allow the network to maintain an internal state of dialogue history. Specifically, they used a Gated Recurrent Unit followed by a fully-connected layer and softmax non-linearity to model the policy π over the actions. During training, the agent samples its actions from this policy to encourage exploration. Parameters of the neural components were trained using the REINFORCE algorithm. For end-to-end training they updated both the dialogue policy and the belief trackers using the reinforcement signal. While testing, the dialogue is regarded as a success if the user target is in top five results returned by the agent and the reward is accordingly calculated that helps the agent take the next action.Their system returns a success rate of 0.66 for small knowledge bases and a great success rate of 0.83 for medium and large knowledge bases. As the user interacts with the agent, the collected data can be used to train the end-to-end agent which we see has a strong learning capability. Gradually, as more experience is collected, the system can switch from Reinforcement Learning-Soft to the personalized end-to-end agent. Effective implementation of this requires such personalized end-to-end agents to learn quickly which should be explored in the future.However, the system has a few limitations. The accuracy is not enough for using for the practical applications. The agent suffers from the cold start issue. In the case of end-to-end learning, they found that for a moderately sized knowledge base, the agent almost always fails if starting from random initialization.Actor-Critic Algorithm	Deep reinforcement learning (RL) methods have significant potential for dialogue policy optimisation. However, they suffer from a poor performance in the early stages of learning as we saw in the paper in the above section. This is especially problematic for on-line learning with real users.In the paper by Su et al, 2017 BIBREF27 , they proposed a sample-efficient actor-critic reinforcement learning with supervised data for dialogue management. Just for a heads up, actor-critic algorithms are the algorithms that have an actor stores the policy according to which the action is taken by the agent and a critic that critiques the actions chosen by the actor (that is, the rewards obtained after the action are sent to the critic using which it calculates value functions).To speed up the learning process, they presented two sample-efficient neural networks algorithms: trust region actor-critic with experience replay (TRACER) and episodic natural actor-critic with experience replay (eNACER). Both models employ off-policy learning with experience replay to improve sample-efficiency. For TRACER, the trust region helps to control the learning step size and avoid catastrophic model changes. For eNACER, the natural gradient identifies the steepest ascent direction in policy space to speed up the convergence.To mitigate the cold start issue, a corpus of demonstration data was utilised to pre-train the models prior to on-line reinforcement learning. Combining these two approaches, they demonstrated a practical approach to learn deep RL-based dialogue policies and also demonstrated their effectiveness in a task-oriented information seeking domain.We can see in the figure FIGREF11 that the success rate reaches at around 95% for the case of policy trained with corpus data and using reinforcement learning which is impressive. Also, they train very quickly. For instance, for training just around 500-1000 dialogues, eNACER has a success rate of around 95% and TRACER has a success rate of around 92%. However, the authors noted that performance falls off rather rapidly in noise as the uncertainty estimates are not handled well by neural networks architectures. This can also be a topic for future research.Using Generative Adversarial Network	Recently, generative adversarial networks are being explored and how they can be used in the dialog agents. Although generative adversarial networks are a topic in itself to explore. However, the paper mentioned below used uses reinforcement learning along with generative adversarial network so we cover it here inside the reinforcement learning methods. They can be used by the applications to generate dialogues similar to humans.In the paper by Li et al, 2017 BIBREF28 , the authors proposed using adversarial training for open-domain dialogue generation such that the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. The task is considered as a reinforcement learning problem where two systems get jointly trained: a generative model to produce response sequences, and a discriminator (similar to the human evaluator in the Turing test) that distinguishes between the human-generated dialogues and the machine-generated ones. The generative model defines the policy that generates a response given the dialog history and the discriminative model is a binary classifier that takes a sequence of dialog utterances as inputs and outputs whether the input is generated by the humans or machines. The outputs from the discriminator are then used as rewards for the generative model pushing the system to generate dialogues that mostly resemble human dialogues.The key idea of the system is to encourage the generator to generate utterances that are indistinguishable from human generated dialogues. The policy gradient methods are used to achieve such a goal, in which the score of current utterances being human-generated ones assigned by the discriminator is used as a reward for the generator, which is trained to maximize the expected reward of generated utterances using the REINFORCE algorithm.Their model achieved a machine vs random accuracy score of 0.952 out of 1. However, on applying the same training paradigm to machine translation in preliminary experiments, the authors did not find a clear performance boost. They thought that it may be because the adversarial training strategy is more beneficial to tasks in which there is a big discrepancy between the distributions of the generated sequences and the reference target sequences (that is, the adversarial approach may be more beneficial on tasks in which entropy of the targets is high). In the future, this relationship can be further explored.Approaches to Human-ize agents	A lack of a coherent personality in conversational agents that most of these models propose has been identified as one of the primary reasons that these agents have not been able to pass the Turing test BIBREF0 BIBREF2 . Aside from such academic motivations, making conversational agents more like their human interlocutors which posses both a persona and are capable of parsing emotions is of great practical and commercial use. Consequently in the last couple of years different approaches have been tried to achieve this goal.Li et al BIBREF29 address the challenge of consistency and how to endow data-driven systems with the coherent “persona” needed to model human-like behavior. They consider a persona to be composite of elements of identity (background facts or user profile), language behavior, and interaction style. They also account for a persona to be adaptive since an agent may need to present different facets to different human interlocutors depending on the interaction. Ultimately these personas are incorporated into the model as embeddings. Adding a persona not only improves the human interaction but also improves BLeU score and perplexity over the baseline sequence to sequence models. The model represents each individual speaker as a vector or embedding, which encodes speaker-specific information (e.g.dialect, register, age, gender, personal information) that influences the content and style of her responses. Most importantly these traits do not need to be explicitly annotated, which would be really tedious and limit the applications of the model. Instead the model manages to cluster users along some of these traits (e.g. age, country of residence) based on the responses alone. The model first encodes message INLINEFORM0 into a vector representation INLINEFORM1 using the source LSTM. Then for each step in the target side, hidden units are obtained by combining the representation produced by the target LSTM at the previous time step, the word representations at the current time step, and the speaker embedding INLINEFORM2 . In this way, speaker information is encoded and injected into the hidden layer at each time step and thus helps predict personalized responses throughout the generation process. The process described here is visualizes in figure FIGREF13 below.Building on works like this the Emotional Chatting Machine model proposed by Zhou et al BIBREF30 is a model which generates responses that are not only grammatically consistent but also emotionally consistent. To achieve this their approach models the high-level abstraction of emotion expressions by embedding emotion categories. They also capture the change of implicit internal emotion states and use explicit emotion expressions with an external emotion vocabulary.Although they did not evaluate their model on some standard metric, they showed that their model can generate responses appropriate not only in content but also in emotion. In the future, instead of specifying an emotion class, the model should decide the most appropriate emotion category for the response. However, this may be challenging since such a task depends on the topic, context or the mood of the user.The goal of capturing emotions and having consistent personalities for a conversational agent is an important one. The field is still nascent but advances in the domain will have far reaching consequences for conversational models in general. People tend to trust agents that are emotionally consistent, and in the long term trust is what will decide the fate of large scale adoption of conversational agents.Evaluation methods	Evaluating conversational agents is an open research problem in the field. With the inclusion of emotion component in the modern conversation agents, evaluating such models has become even more complex.The current evaluation methods like perplexity and BLEU score are not good enough and correlate very weakly with human judgments. In the paper by Liu et al, 2016 BIBREF31 , the authors discuss about how not to evaluate the dialogue system. They provide quantitative and qualitative results highlighting specific weaknesses in existing metrics and provide recommendations for the future development of better automatic evaluation metrics for dialogue systems.According to them, the metrics (like Kiros et al, 2015 BIBREF32 ) that are based on distributed sentence representations hold the most promise for the future. It is because word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses. Similarly, the metrics that are embedding-based consist of basic averages of vectors obtained through distributional semantics and so they are also insufficiently complex for modeling sentence-level compositionality in dialogue.The metrics that take into account the context can also be considered. Such metrics can come in the form of an evaluation model that is learned from data. This model can be either a discriminative model that attempts to distinguish between model and human responses or a model that uses data collected from the human survey in order to provide human-like scores to proposed responses.Conclusion	In this survey paper we explored the exciting and rapidly changing field of conversational agents. We talked about the early rule-based methods that depended on hand-engineered features. These methods laid the ground work for the current models. However these models were expensive to create and the features depended on the domain that the conversational agent was created for. It was hard to modify these models for a new domain. As computation power increased, and we developed neural networks that were able to capture long range dependencies (RNNs,GRUs,LSTMs) the field moved towards neural models for building these agents. Sequence to sequence model created in 2015 was capable of handling utterances of variable lengths, the application of sequence to sequence to conversation agents truly revolutionized the domain. After this advancement the field has literally exploded with numerous application in the last couple of years. The results have been impressive enough to find their way into commercial applications such that these agents have become truly ubiquitous. We attempt to present a broad view of these advancements with a focus on the main challenges encountered by the conversational agents and how these new approaches are trying to mitigate them.","[""What are the limitations of the currently used quantitative metrics? e.g. why are they not 'good'?"", 'What metrics are typically used to compare models?', 'Is there a benchmark to compare the different approaches?', 'What GAN and RL approaches are used?', 'What type of neural models are used?', 'What type of statistical models were used initially?', 'What was the proposed use of conversational agents in pioneering work?', 'What work pioneered the field of conversational agents?']","['Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable']"
48,"Learning from Dialogue after Deployment: Feed Yourself, Chatbot!	The majority of conversations a dialogue agent sees over its lifetime occur after it has already been trained and deployed, leaving a vast store of potential training signal untapped. In this work, we propose the self-feeding chatbot, a dialogue agent with the ability to extract new training examples from the conversations it participates in. As our agent engages in conversation, it also estimates user satisfaction in its responses. When the conversation appears to be going well, the user's responses become new training examples to imitate. When the agent believes it has made a mistake, it asks for feedback; learning to predict the feedback that will be given improves the chatbot's dialogue abilities further. On the PersonaChat chit-chat dataset with over 131k training examples, we find that learning from dialogue with a self-feeding chatbot significantly improves performance, regardless of the amount of traditional supervision.	Introduction	Training a dialogue agent to converse like a human requires extensive supervision. The most common approach is to train models to imitate humans in large corpora of crowdsourced or scraped conversations BIBREF0 . These fully-supervised conversations tend to be expensive to collect in sufficient quantity and/or occur in settings with significant differences from the deployment environment BIBREF1 . Instead, dialogue agents would ideally learn directly from dialogue, the conversations they participate in after deployment, which are usually abundant, task-specific, dynamic, and cheap. This corresponds to the way humans learn to converse—not merely observing others engaging in “expert-level” conversations, but instead actively adjusting and correcting our speech based on feedback woven throughout our own conversations BIBREF2 , BIBREF3 . Giving a dialogue agent this ability would enable it to continuously improve and adapt over its lifetime, rather than requiring additional annotation costs for each and every improvement.However, naively training a dialogue agent on its own conversations yields poor results. For example, training a model on its own output can simply reinforce its existing failure modes, and mistakes by the agent can lead to absurd conversations that no longer resemble the target domain BIBREF4 . To combat this, one approach is to allow the agent to request feedback during conversations BIBREF5 , BIBREF6 , e.g., when it believes it is about to make a mistake. This approach, however, falls victim to the Dunning-Kruger effect BIBREF7 , which in this case suggests that a bad model will also be bad at knowing when it is doing a bad job. Regardless of when feedback is requested, existing methods typically require accompanying scalar rewards or adherence to particular templates or structure to ensure that the feedback is usable by the model BIBREF8 , BIBREF9 , BIBREF10 . These requirements may be acceptable for paid annotators, but they impose unnatural workflows on unpaid conversation partners in a standard dialogue environment. Humans are able to request and provide feedback using only natural language; ideally, dialogue agents would be able to do the same.In this work we propose the self-feeding chatbot, a dialogue agent with the ability to extract new examples from the conversations it participates in after deployment (Figure FIGREF1 ). Concretely, in addition to being trained on the primary Dialogue task, the agent is trained to predict its speaking partner's satisfaction with its responses. When the conversation seems to be going well, the user's responses (but not the bot's own utterances) become the targets in new training examples for the Dialogue task. When the agent believes it has made a mistake, it instead requests feedback on what it could have said instead. Predicting the feedback that will be provided in a given context becomes an auxiliary task (Feedback) on which the model is also trained. Importantly, these new examples improve the agent's dialogue abilities while using only natural responses from the user that do not require special structure, accompanying numerical feedback, or additional human intervention in order to be used.With this approach, the conversations the chatbot participates in are sliced into two complementary datasets—one largely protected from the chatbot's mistakes (Dialogue examples), and one which directly addresses them (Feedback examples). We validate our approach on the PersonaChat BIBREF11 dialogue dataset, finding empirically that regardless of the number of available supervised examples, the dialogue ability of the chatbot is always improved by adding the automatically extracted examples of either type, and improves the most by adding both.The main contributions of this work thus include the following:The datasets and models described in this paper are available via the ParlAI platform BIBREF12 , along with training code. Hyperparameter values are included in Appendix SECREF13 .Related Work	The general concepts of lifelong learning BIBREF13 and never-ending (language) learning BIBREF14 are related to the topics discussed in this work, as is active learning BIBREF15 and predictive modeling BIBREF16 .The specific case of learning actively from dialogue during deployment was explored for the question answering (QA) setting in BIBREF17 and BIBREF18 , where the authors examined multiple learning strategies on a suite of dialogue tasks with varying types of feedback, such as verbal cues (e.g., “Yes, that's right!”) and scalar rewards. Most relevant to our work was their use of forward prediction, where the learner improved in quality by trying to predict the teacher's responses without an explicit reward signal. Our work extends this idea, adding the ability for the model to recognize its mistakes and request feedback explicitly, and moving beyond QA to the more general chit-chat setting where there may be many valid responses in a given context.Learning to ask questions is another area that has been studied BIBREF19 , BIBREF20 , BIBREF21 . While those works focused on identifying which question to ask in a given context, in this work we are more interested in first learning when to ask a question. BIBREF6 considered this question as well, but again in the context of a QA setting rather than dialogue. BIBREF4 used user responses to detect mistakes made by a deployed virtual assistant, showing that model mistakes can be identified in chit-chat, weather, or web search domains. However, they did not explore how to use these identified mistakes to improve the model further; their agent was not equipped to feed itself. BIBREF22 also found that the correctly assessing the appropriateness of chatbot responses is highly dependent on user responses and not preceding context alone.There are other, somewhat less related, ways to use feedback during dialogue for learning, notably for collecting knowledge to answer questions BIBREF23 , BIBREF24 , BIBREF25 , and more commonly in reinforcement learning settings, where the feedback is a scalar rather than the dialogue messages themselves BIBREF26 , BIBREF27 , BIBREF8 , BIBREF10 , BIBREF28 . In particular BIBREF29 employ user sentiment detection for reward shaping in their Alexa prize entry.Finally, our work improves dialogue quality by utilizing larger datasets with noisier labels than traditional supervision. Other applications of weak supervision to dialogue BIBREF30 and relation extraction have observed similar results BIBREF31 , BIBREF32 , BIBREF33 .The Self-Feeding Chatbot	The lifecycle of a self-feeding chatbot is outlined in Figure FIGREF6 . In the initial training phase, the dialogue agent is trained on two tasks—Dialogue (next utterance prediction, or what should I say next?) and Satisfaction (how satisfied is my speaking partner with my responses?)—using whatever supervised training data is available. We refer to these initial Dialogue examples as Human-Human (HH) examples, since they were generated in conversations between two humans.In the deployment phase, the agent engages in multi-turn conversations with users, extracting new deployment examples of two types. Each turn, the agent observes the context INLINEFORM0 (i.e., the conversation history) and uses it to predict its next utterance INLINEFORM1 and its partner's satisfaction INLINEFORM2 . If the satisfaction score is above a specified threshold INLINEFORM3 , the agent extracts a new Human-Bot (HB) Dialogue example using the previous context INLINEFORM4 and the human's response INLINEFORM5 and continues the conversation. If, however, the user seems unsatisfied with its previous response INLINEFORM6 , the agent requests feedback with a question INLINEFORM7 , and the resulting feedback response INLINEFORM8 is used to create a new example for the Feedback task (what feedback am I about to receive?). The agent acknowledges receipt of the feedback and the conversation continues. The rate at which new Dialogue or Feedback examples are collected can be adjusted by raising or lowering the satisfaction threshold INLINEFORM9 (we use INLINEFORM10 ). Periodically, the agent is retrained using all available data, thereby improving performance on the primary Dialogue task.It is important to note that the user's responses are always in the form of natural dialogue. In particular, at no point are the new Feedback examples inspected, post-processed, or cleaned. Instead, we rely on the fact that the feedback is not random: regardless of whether it is a verbatim response, a description of a response, or a list of possible responses (see Table TABREF9 for examples), there is a learnable relationship between conversation contexts and their corresponding feedback which requires many of the same language understanding skills to master as does carrying on a normal conversation.The experiments in this paper are limited to the setting where the number of supervised and deployment examples are on the same order of magnitude; however, we envision scenarios in which the number of deployment examples can easily grow to INLINEFORM0 or more the number of supervised examples over the chatbot's deployment lifetime, effectively providing a massive task-specific corpus at minimal cost. Table TABREF8 reports the sizes of each dataset, all of which are available via ParlAI.Task 1: Dialogue	The chatbot's primary task (Dialogue) is to carry on a coherent and engaging conversation with a speaking partner. Training examples take the form of INLINEFORM0 pairs, where INLINEFORM1 is the context of the conversation (the concatenation of all responses so far up to some history length, delimited with tokens marking the speaker), and INLINEFORM2 is the appropriate response given by the human.The Human-Human (HH) portion of the Dialogue dataset comes from the PersonaChat dataset BIBREF11 , which consists of short dialogs (6-8 turns) between two crowdworkers (humans) who have been assigned short text profiles and are instructed to “chat with the other person naturally and try to get to know each other.” We chose this dataset because of its size (over 145k total examples), the breadth of topics it covers, and its focus on promoting engaging conversations, which we anticipate being a necessary property of a chatbot that people will be willing to chat with voluntarily and repeatedly. We use the standard splits of the dataset made available in ParlAI as a part of the ConvAI2 challenge BIBREF34 . Since the question of how to incorporate external knowledge (such as profiles) in dialogue is an open research question of its own BIBREF35 , BIBREF36 , BIBREF37 and we are primarily interested in the question of learning from dialogue, we discard the profiles and simply train and test on the conversations themselves, making the dataset more challenging in terms of raw performance scores.The Human-Bot (HB) portion of the Dialogue dataset is extracted during deployment as described earlier. The context may contain responses from both the human and the bot, but the target response is always from the human, as we will see experimentally that targeting bot responses degrades performance. Because the chit-chat domain is symmetric, both the HH and HB Dialogue examples are used for the same task. In an asymmetric setting where the bot has a different role than the human, it is unclear whether HB examples may still be used as an auxiliary task, but Feedback examples will remain usable.Task 2: Satisfaction	The objective of the Satisfaction auxiliary task is to predict whether or not a speaking partner is satisfied with the quality of the current conversation. Examples take the form of INLINEFORM0 pairs, where INLINEFORM1 is the same context as in the Dialogue task, and INLINEFORM2 , ranging from dissatisfied to satisfied. Crucially, it is hard to estimate from the bot's utterance itself whether the user will be satisfied, but much easier using the human's response to the utterance, as they may explicitly say something to that effect, e.g. “What are you talking about?”.The dataset for this task was collected via crowdsourcing. Workers chatted with our baseline dialogue agent and assigned a rating 1-5 for the quality of each of the agent's responses. Contexts with rating 1 were mapped to the negative class (dissatisfied) and ratings INLINEFORM0 mapped to the positive class (satisfied). Contexts with rating 2 were discarded to increase the separation between classes for a cleaner training set. Note that these numeric ratings were requested only when collecting the initial training data, not during deployment, where only natural dialogue is used.Task 3: Feedback	The objective of the Feedback auxiliary task is to predict the feedback that will be given by the speaking partner when the agent believes it has made a mistake and asks for help. Examples take the form of INLINEFORM0 pairs, where INLINEFORM1 is the same context as the other two tasks and INLINEFORM2 is the feedback utterance.Training data for this task is collected during deployment. Whenever the user's estimated satisfaction is below a specified threshold, the chatbot responds “Oops! Sorry. What should I have said instead?”. A new example for the Feedback task is then extracted using the context up to but not including the turn where the agent made the poor response as INLINEFORM0 and the user's response as INLINEFORM1 (as shown in Figure FIGREF1 ). At that point to continue the conversation during deployment, the bot's history is reset, and the bot instructs the user to continue, asking for a new topic. Examples of Feedback responses are shown in Table TABREF9 .Model Architecture	The self-feeding chatbot has two primary components: an interface component and a model component. The interface component is shared by all tasks, and includes input/output processing (tokenization, vectorization, etc.), conversation history storage, candidate preparation, and control flow (e.g., when to ask a question vs. when to give a normal dialogue response). The model component contains a neural network for each task, with embeddings, a network body, and a task head, some of which can be shared. In our case, we obtained maximum performance by sharing all parameters between the Feedback and Dialogue tasks (prepending Feedback responses with a special token), and using separate model parameters for the Satisfaction task. Identifying optimal task structure in multi-task learning (MTL) architectures is an open research problem BIBREF38 . Regardless of what parameters are shared, each training batch contains examples from only one task at a time, candidate sets remain separate, and each task's cross-entropy loss is multiplied by a task-specific scaling factor tuned on the validation set to help account for discrepancies in dataset size, loss magnitude, dataset relevance, etc.Our dialogue agent's models are built on the Transformer architecture BIBREF39 , which has been shown to perform well on a variety of NLP tasks BIBREF40 , BIBREF41 , including multiple persona-based chat applications BIBREF42 , BIBREF43 , BIBREF44 . For the Satisfaction task, the context INLINEFORM0 is encoded with a Transformer and converted to the scalar satisfaction prediction INLINEFORM1 by a final linear layer in the task head. The Dialogue and Feedback tasks are set up as ranking problems, as in BIBREF11 , BIBREF45 , where the model ranks a collection of candidate responses and returns the top-ranked one as its response. The context INLINEFORM2 is encoded with one Transformer and INLINEFORM3 and INLINEFORM4 candidates are encoded with another. The score for each candidate is calculated as the dot product of the encoded context and encoded candidate.During training, negative candidates are pulled from the correct responses for the other examples in the mini-batch. During evaluation, however, to remain independent of batch size and data shuffling, each example is assigned a static set of 19 other candidates sampled at random from its split of the data. During deployment, all 127,712 unique HH Dialogue candidates from the train split are encoded once with the trained model and each turn the model selects the top-ranked one for the given context.Model Settings	Contexts and candidates are tokenized using the default whitespace and punctuation tokenizer in ParlAI. We use a maximum dialogue history length of 2 (i.e., when making a prediction, the dialogue agent has access to its previous utterance and its partner's response). Tokens are embedded with fastText BIBREF46 300-dimensional embeddings. We do not limit the vocabulary size, which varies from 11.5k to 23.5k words in our experiments, depending on the training set. The Transformer is implemented in PyTorch BIBREF47 within the ParlAI framework. We use the AdaMax BIBREF48 optimizer with a learning rate schedule that decays based on the inverse square root of the step number after 500 steps of warmup from 1e-5. We use proportional sampling BIBREF49 to select batches from each task for training, with batch size 128. Each Transformer layer has two attention heads and FFN size 32. The initial learning rate (0.001-0.005), number of Transformer layers (1-2), and task-specific loss factors (0.5-2.0) are selected on a per-experiment basis based on a grid search over the validation set averaged over three runs (we use the Dialogue validation set whenever multiple tasks are involved). We use early stopping based on the validation set to decide when to stop training. The hyperparameter values for the experiments in Section SECREF5 are included in Appendix SECREF13 .Note that throughout development, a portion of the Dialogue validation split was used as an informal test set. The official hidden test set for the Dialogue task was used only to produce the final numbers included in this paper.Experimental Results	Throughout this section, we use the ranking metric hits@X/Y, or the fraction of the time that the correct candidate response was ranked in the top X out of Y available candidates; accuracy is another name for hits@1/Y. Statistical significance for improvement over baselines is assessed with a two-sample one-tailed T-test.Benefiting from Deployment Examples	Our main result, reported in Table TABREF16 , is that utilizing the deployment examples improves accuracy on the Dialogue task regardless of the number of available supervised (HH) Dialogue examples. The boost in quality is naturally most pronounced when the HH Dialogue training set is small (i.e., where the learning curve is steepest), yielding an increase of up to 9.4 accuracy points, a 31% improvement. However, even when the entire PersonaChat dataset of 131k examples is used—a much larger dataset than what is available for most dialogue tasks—adding deployment examples is still able to provide an additional 1.6 points of accuracy on what is otherwise a very flat region of the learning curve. It is interesting to note that the two types of deployment examples appear to provide complementary signal, with models performing best when they use both example types, despite them coming from the same conversations. We also calculated hit rates with 10,000 candidates (instead of 20), a setup more similar to the interactive setting where there may be many candidates that could be valid responses. In that setting, models trained with the deployment examples continue to outperform their HH-only counterparts by significant margins (see Appendix SECREF8 ).On average, we found that adding 20k Feedback examples benefited the agent about as much as 60k HB Dialogue examples. This is somewhat surprising given the fact that nearly half of the Feedback responses would not even be reasonable responses in a conversation (instead being a list of options, a description of a response, etc.) as shown in Table TABREF9 . Nevertheless, the tasks are related enough that the Dialogue task benefits from the MTL model's improved skill on the Feedback task. And whereas HB Dialogue examples are based on conversations where the user appears to already be satisfied with the agent's responses, each Feedback example corresponds to a mistake made by the model, giving the latter dataset a more active role in improving quality. Interestingly, our best-performing model, which achieves 46.3 accuracy on Dialogue, scores 68.4 on Feedback, suggesting that the auxiliary task is a simpler task overall.When extracting HB Dialogue examples, we ignore human responses that the agent classifies as expressing dissatisfaction, since these turns do not represent typical conversation flow. Including these responses in the 60k HB dataset decreases hits@1/20 by 1.2 points and 0.6 points when added to 20k and 131k HH Dialogue examples, respectively. We also explored using chatbot responses with favorable satisfaction scores ( INLINEFORM0 ) as new training examples, but found that our models performed better without them (see Appendix SECREF10 for details).We also found that “fresher” feedback results in bigger gains. We compared two models trained on 20k HH Dialogue examples and 40k Feedback examples—the first collected all 40k Feedback examples at once, whereas the second was retrained with its first 20k Feedback examples before collecting the remaining 20k. While the absolute improvement of the second model over the first was small (0.4 points), it was statistically significant ( INLINEFORM0 0.027) and reduced the gap to a model trained on fully supervised (HH) Dialogue examples by 17% while modifying only 33% of the training data. This improvement makes sense intuitively, since new Feedback examples are collected based on failure modes of the current model, making them potentially more efficient in a manner similar to new training examples selected via active learning. It also suggests that the gains we observe in Table TABREF16 might be further improved by (a) collecting Feedback examples specific to each model (rather than using the same 60k Feedback examples for all models), and (b) more frequently retraining the MTL model (e.g., every 5k examples instead of every 20k) or updating it in an online manner. We leave further exploration of this observation for future work.The same experiment repeated for HB Dialogue examples found that fresher HB examples were no more valuable than stale ones, matching our intuition that HB Dialogue examples are less targeted at current model failure modes than Feedback ones.Predicting User Satisfaction	For maximum efficiency, we aim to ask for feedback when it will most benefit our model. The approach we chose (classifying the tone of partner responses) takes advantage of the fact that it is easier to recognize that a mistake has already been made than it is to avoid making that mistake; or in other words, sentiment classification is generally an easier task than next utterance prediction.We compare this to the approach of asking for feedback whenever the model is most uncertain what to say next. This approach acts on the assumption that the model will be least confident when it is about to make a mistake, which we find very frequently to not be the case. Not only is it difficult to recognize one's own mistakes, but also there are often multiple valid responses to a given context (e.g., “Yes, I love seafood!” or “Yuck, fish is gross.”)—a lack of certainty about which to use does not necessarily suggest a poor model.Table TABREF22 reports the maximum F1 scores achieved by each method on the Satisfaction test set. For the model uncertainty approach, we tested two variants: (a) predict a mistake when the confidence in the top rated response is below some threshold INLINEFORM0 , and (b) predict a mistake when the gap between the top two rated responses is below the threshold INLINEFORM1 . We used the best-performing standalone Dialogue model (one trained on the full 131k training examples) for assessing uncertainty and tuned the thresholds to achieve maximum F1 score. For the user satisfaction approach, we trained our dialogue agent on just the Satisfaction task. Finally, we also report the performance of a regular-expression-based method which we used during development, based on common ways of expressing dissatisfaction that we observed in our pilot studies, see Appendix SECREF12 for details.As shown by Table TABREF22 , even with only 1k training examples (the amount we used for the experiments in Section SECREF18 ), the trained classifier significantly outperforms both the uncertainty-based methods and our original regular expression, by as much as 0.28 and 0.42 F1 points, respectively.Future Work	In this work we achieved learning from dialogue using two types of self-feeding: imitation of satisfied user messages, and learning from the feedback of unsatisfied users. In actuality, there are even more ways a model could learn to improve itself—for example, learning which question to ask in a given context to receive the most valuable feedback. One could even use the flexible nature of dialogue to intermix data collection of more than one type—sometimes requesting new Feedback examples as in this work, and other times requesting new Satisfaction examples (e.g., by asking “Did my last response make sense?”). In this way, a dialogue agent could simultaneously increase its dialogue ability, and increase its ability to improve further. We leave exploration of this meta-learning theme to future work. Data Collection Protocol Here we report in greater detail the protocol we followed to collect the Satisfaction, Feedback, and HB Dialogue examples used in the experiments of Section SECREF5 . We first trained our dialogue agent on just the Dialogue task with 20k HH examples. This agent was deployed on a crowdsourcing platform using the interface shown in Appendix SECREF32 to collect 2.5k Satisfaction examples. These were split into 1k train, 500 validation, and 1k test examples. The agent was retrained using the 20k HH Dialogue examples and 1k Satisfaction examples, then deployed to collect the first batch of deployment examples. We collected 40k Feedback examples (feedback set A) over the course of 17,250 conversations with 10 turns each (20 utterances, including the initial prompt). We then retrained the agent on all three datasets, using the same 20k HH Dialogue examples as before and only 20k of the available 40k Feedback examples. This model was deployed to collect another 20k Feedback examples (feedback set B), for a total of 60k Feedback examples (A + B). In Table TABREF16 we use these 60k Feedback examples interchangeably; in Appendix SECREF11 we compare them head-to-head. The 60k HB Dialogue examples were extracted from the logs of the deployment conversations. Finally, we collected an additional 40k Satisfaction training examples to produce the numbers in Table TABREF22 investigating the learning curve for this task. No filtering was performed on the crowdworker conversations. Upon inspection after the fact, some workers did indeed give poor responses, make typographical mistakes, misunderstand the instructions, try to use the chatbot as a question answering interface, etc. We assume however that similar types of noise will be present in most chatbot deployment environments and opted to maintain a workflow that truly does not require developer intervention to use the newly collected examples. Results with 10k Candidates PersonaChat Comparisons and Baselines Our experiments use the PersonaChat distribution that was released as a part of the ConvAI2 BIBREF34 challenge. This distribution is slightly cleaner than the original PersonaChat release and comes with a new crowdsourced test set. In order to compare with the models and baselines used in the original PersonaChat paper BIBREF11 , we report in this section the performance of our models on the original PersonaChat test set, not the ConvAI2 test set. Note that all numbers reported here are for models that do not have access to the profiles that were used in the creation of the conversations; models that do have access to this additional information tend to perform even better. Using Chatbot Responses as Targets We also considered whether it was possible to consistently identify really good responses by the chatbot, rather than the really bad ones. These could be potentially be used as Dialogue examples along with the ones that have human responses as targets (what we refer to as HH and HB in the paper). To explore this question, we modified our Satisfaction dataset so that contexts with a rating of 5 were the positive class and ones with ratings INLINEFORM0 were the negative class (discarding ratings of 4 to increase the separation between classes). The results were negative—even with a training set of over 34k examples, the maximum precision we were able to achieve while maintaining at least 10% recall was 0.70, which is insufficient to improve performance on the Dialogue task. Upon inspection, it appears that really good responses are hard to identify because most of the time they look like a normal human-to-human conversation, and recognizing an appropriate next utterance is precisely the Dialogue task that we are trying to solve! Negative responses, however, are much more semantically similar to one another, since most express one of a few common ideas such as asking for clarification or conveying confusion. The Effect of Data Freshness Satisfaction Regular Expressions As described in Section SECREF23 , before we trained a classifier on the Satisfaction task, we used the union of the following six regular expressions (using Python regular expression syntax) to identify user dissatisfaction and trigger feedback requests: r""i .*(?:said|asked|told).*"" r""((not|nt|n't).*mak.*sense)|(mak.*no .*sense)"" r""u(m|h)+\W"" r""you.*what\?"" r""what.*you (?:mean|refer|talk).*\?"" r""what.*to do with.*\?"" Hyperparameters Data Collection Interfaces Deployment Data Collection We simulated a deployment environment on a crowdsourcing platform with the following interface. Crowdworkers were told to “Talk as you would to a person you have just met.” Satisfaction Data Collection The environment for collecting Satisfaction ratings was very similar to the deployment environment, with the exception that workers were required to give a rating 1-5 on the quality of each response given by the chatbot at the end of their own responses. Sample Deployment Conversations A sample of the conversations that occurred between a Bot (B) and Human (H) during the collection of the deployment datasets (more specifically, feedback set B). The self-feeding chatbot used its satisfaction classifier trained on 1k examples to determine when to ask for feedback. The dialogue model at this point had only been trained on 20k HH Dialogue examples and 20k Feedback examples, so it still makes frequent mistakes. See Appendix SECREF30 for a snapshot of the actual interface. B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: random, mistake, century. H: what's the last mistake you made? B: what do you do ? i've a toothpick business , H: that's not what i asked. B: oops! i think i messed up. whether i messed up or not, what could i have said (in response to ""what's the last mistake you made?"")? H: told me about a recent mistake. B: thanks! i'll try to remember that. can you pick a new topic for us to talk about now? H: what do you do for a living? B: i am a psychology major . what do you do ? H: i am a student as well. i'm studying law. B: oh , my five children and husband all speak spanish as well . H: i never said i speak spanish, what are you doing? B: oops! i think i messed up. whether i messed up or not, what could i have said (in response to ""i am a student as well. i'm studying law."")? H: talked about your college courses. B: thanks! i'll try to remember that. can you pick a new topic for us to talk about now? H: how old are your children? B: it is . we are just thrilled we have ac again . its calming my anxiety . H: that has nothing to do with what i said. B: yes since i prefer my hair straight . i straighten it with a flat iron . tedious ! H: you make no sense. B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: picnic, drain, elm. H: do you like to go to the beach or woods for a picnic? B: good , going to the doctor soon , i've some issue with my knees H: that's doesn't make sense B: oops! i think i messed up. whether i messed up or not, what could i have said (in response to ""do you like to go to the beach or woods for a picnic?"")? H: you could have chose the beach or woods to previous question B: thanks! i'll try to remember that. can you pick a new topic for us to talk about now? H: do you travel? B: i want to go to paris and make clothes . H: wow that's interesting. i've never been to paris B: i can see that . do you like music ? H: i love music. what is your favorite kind? B: tell me more about you H: that doesn't answer the question B: oops! i think i messed up. whether i messed up or not, what could i have said (in response to ""i love music. what is your favorite kind?"")? H: you could have selected a type of music that you like B: thanks! i'll try to remember that. can you pick a new topic for us to talk about now? H: do you like animals? i have a dog B: i like those , too , but pizza is better ! ; H: umm, what? B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: boy, punk, ray. H: are you a boy or a girl? B: do you have any jokes ? H: i wasn't talking about jokes. B: oops! i think i messed up. whether i messed up or not, what could i have said (in response to ""are you a boy or a girl?"")? H: i was talking about your gender. B: thanks! i'll try to remember that. can you pick a new topic for us to talk about now? H: sure, do you like music? B: oh that's neat . you should write me a poem about star trek ! H: what? B: oops! i think i messed up. whether i messed up or not, what could i have said (in response to ""sure, do you like music?"")? H: we were talking about music. B: thanks! i'll try to remember that. can you pick a new topic for us to talk about now? H: what is your favorite type of art? B: nice ! are the pets your own ? H: no B: oops! i think i messed up. whether i messed up or not, what could i have said (in response to ""what is your favorite type of art?"")? H: we weren't talking about pets. B: thanks! i'll try to remember that. can you pick a new topic for us to talk about now? H: sure, do you know anything about biology? B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: college, cliff, tachometer. H: hey, how's it going? B: no . i am just 15 . parents would kill me . H: what are you talking about? B: oops! i think i messed up. whether i messed up or not, what could i have said (in response to ""hey, how's it going?"")? H: i was just saying hi, so you should have said a greeting? B: thanks! i'll try to remember that. can you pick a new topic for us to talk about now? H: no problem. so, do you go to school? B: i have ten of them ! H: no, that isn't right. B: oops! i think i messed up. whether i messed up or not, what could i have said (in response to ""no problem. so, do you go to school?"")? H: i was asking about school or education. B: thanks! i'll try to remember that. can you pick a new topic for us to talk about now? H: so, what is the best subject to you? B: oh wow , i've no cats , but i do hace a couple of dogs . H: so, you like dogs a lot? B: i am also preparing for a marathon also , wish me luck . H: oh wow! does your dog help you prepare? B: i like hindu food . i'm not a very good baker though . my mom has mastered that . H: no, that isn't right. B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: drink, wool, bag. H: what is your favorite drink B: i love animals and the outdoors , so i work at the zoo . H: wrong answer B: oops! i think i messed up. whether i messed up or not, what could i have said (in response to ""what is your favorite drink"")? H: you should have said my favorite drink is either juice,coke or wine B: thanks! i'll try to remember that. can you pick a new topic for us to talk about now? H: are you religious B: somewhat lol he sits when i draw and watches me H: what are you talking about B: oops! i think i messed up. whether i messed up or not, what could i have said (in response to ""are you religious"")? H: you should have told me yes i am religious in fact i attend church services at least once a week and if not then no i am not religious never attended church services B: thanks! i'll try to remember that. can you pick a new topic for us to talk about now? H: tell me about yourself B: i drive my dream car every day i am home . a 65 mustang . H: that is nice you must be really love cars B: hello , how is your day going ? H: my day is good relaxing at home after work watching tv B: i'm sorry to hear that . i sincerely hope things get better for you . H: what are you talking about B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: glasses, puppy, pillow. H: do you like dogs? i got a new puppy B: hey ! i love being outside planting my plants ! how about you ? H: that is nice i love to plant too! B: right now i've a beagle . she's a little feisty . i miss my shepard mix . H: aww my dog is named sam he is a collie B: yeah , animals are my passion , i have 3 , all big dogs . H: yeah i just have the one dog and two cats B: i do not . i take a lot of pictures . i'd like to be a famous photographer . H: i actully really want to get a picture of my animals with santa lol B: do you want to work out with me ? H: umm what? B: oops! i think i messed up. whether i messed up or not, what could i have said (in response to ""i actully really want to get a picture of my animals with santa lol"")? H: that would be very cute B: thanks! i'll try to remember that. can you pick a new topic for us to talk about now? H: i love to eat Sample Satisfaction Conversations A sample of the conversations that occurred between a Bot (B) and Human (H) during the collection of the Satisfaction dataset. Users were asked to rate each bot response on a scale from 1 (very bad) to 5 (very good). See Appendix SECREF32 for a snapshot of the actual interface. B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: invitation, dot, czar. H: hello friend, i missed you B: hello my name is ray H: hi i'm leah (reward:5) B: hello my name is michael H: no it's not (reward:1) B: oops! sorry. what should i have said instead? H: nice to meet you B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: do you have many friends? B: lol i have kids to H: what? (reward:1) B: oops! sorry. what should i have said instead? H: no but i have kids B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: what do you want for christmas? B: i love to bake cakes H: what? (reward:1) B: oops! sorry. what should i have said instead? H: i want a pony for christmas B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: popsicle, lawnmower, mom. H: hi, are you having a good day so far? B: i'm here . i'm doing well , just waiting for the hurricane to hit . H: oh im sorry. where do you live? (reward:5) B: miami . . . i live in little cuba , but i wish i lived in europe . H: yeah, i bet you do (reward:5) B: we live in aruba , in a van down by the river H: doesnt make sense (reward:1) B: oops! sorry. what should i have said instead? H: that you were from miami, not aruba B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: do you own a car? B: i only drive toyota cars H: what's the fastest you've ever driven a car? (reward:2) B: a yellow toyota camry convertible . H: that doesnt make sense (reward:1) B: oops! sorry. what should i have said instead? H: how fast you have driven before B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: do you like staying up late? B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: stout, mop, through. H: i just finished mopping my kitchen. B: i am a barista at joe jacks coffee shop H: that has nothing to do with what i said (reward:1) B: oops! sorry. what should i have said instead? H: talked about how you hate cleaning the kitchen B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: what music do you listen to? B: listen about lectures about space and stuff H: so you don't like to listen to music at all? (reward:4) B: i mostly listen to jazz H: jazz is a fun genre. i play the trombone (reward:5) B: classical jazz is my very favorite music H: do you play any instruments? (reward:2) B: classical music . mozart piano music . H: i did not ask that (reward:1) B: oops! sorry. what should i have said instead? H: the instrument that you play B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: what are your plans today? B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: clown, drip, whatever. H: my pipes are dripping, do you know a plumber i can call? B: anything but taylor swift lol H: that doesn't make any sense. (reward:1) B: oops! sorry. what should i have said instead? H: you could say ""sorry i don't know anyone who could get out there today."" B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: i love music. who do you like to listen to? B: i mostly listen to smooth jazz and pop music H: oh smooth jazz eh? do you like kenny g? (reward:5) B: i like pop jazz and rap H: you already said that! (reward:1) B: oops! sorry. what should i have said instead? H: you could say ""kenny g is a little too smooth for my tastes."" B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: i think i'm going to just eat cereal with breakfast. what did you have to eat? B: bacon bacon bacon bacon bacon bacon bacon H: haha bacon is pretty good. i like the maple smoked stuff. (reward:3) B: onion green peppers mushrooms cream cheese shredded cheese . H: that sounds like an awesome bagel. how often do you eat breakfast? (reward:3) B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: princess, dump, telephone. H: hello! how are you on this fine day? B: i am pretty good i guess . i am even better at eating haha especially cookies and ice cream H: i do love eating too, especially cookies (reward:5) B: cakes , muffins , cookies , cupcakes ! sugar cookies are my favorite ! H: you like a lot of sweets! (reward:3) B: cookies pies cakes and cup cakes H: okay, i get it, you like sweets (reward:2) B: peanut butter goes well with bread and crackers H: i do love a peanut butter sandwich, my kids do too (reward:5) B: tights with peanut butter peas H: what??? (reward:1) B: oops! sorry. what should i have said instead? H: something that made sense about peanut butter B: thanks! i'll remember that. can you pick a new topic for us to talk about now? H: do you have a cell phone? B: hostage , it was a robbery in the store i was selling furniture too H: huh? (reward:1) B: oops! sorry. what should i have said instead? H: tell me if you have a cell phone, maybe what kind you have B: start a conversation with the chatbot. if you need ideas for a topic, here are a few random words: ray, scared, shirt. H: hi cutie B: hi H: how are you? (reward:4) B: good just making brisket and ribs you ? H: that sounds tasty. i'm just watching youtube (reward:5) B: i see . . i like watching movies too star wars especially H: star wars is a good movie (reward:5) B: horror movies or tv ? i like both i suppose H: yeah horror movies are ok (reward:5) B: watching a classic movie the wizard of oz H: do you like to swim? (reward:5) B: the beach and bike riding H: that sounds fun (reward:5) B: i hike at the park H: what else do you do? (reward:4) B: i run at central park H: oh you're from new york? (reward:5) B: upstate new york near jamestown ny H: wow awesome, i live in canada (reward:5) ","['how is user satisfaction estimated?', 'by how much did performance improve?']","['Unacceptable', 'Unacceptable']"
49,,"['Did they test the idea that the system reduces the time needed to encode ADR reports on real pharmacologists? ', 'Do the authors offer a hypothesis as to why the system performs better on short descriptions than longer ones?', 'What are the steps in the MagiCoder algorithm?', 'How is the system constructed to be linear in the size of the narrative input and the terminology?']","['Unacceptable', 'Unacceptable', 'Unacceptable', 'The system scans the text word-by-word once and performs a voting task for each word. It also keeps track of the position of the previous words.']"
50,"Tie-breaker: Using language models to quantify gender bias in sports journalism	Gender bias is an increasingly important issue in sports journalism. In this work, we propose a language-model-based approach to quantify differences in questions posed to female vs. male athletes, and apply it to tennis post-match interviews. We find that journalists ask male players questions that are generally more focused on the game when compared with the questions they ask their female counterparts. We also provide a fine-grained analysis of the extent to which the salience of this bias depends on various factors, such as question type, game outcome or player rank.	Introduction	There has been an increasing level of attention to and discussion of gender bias in sports, ranging from differences in pay and prize money to different levels of focus on off-court topics in interviews by journalists. With respect to the latter, Cover the Athlete, an initiative that urges the media to focus on sport performance, suggests that female athletes tend to get more “sexist commentary"" and “inappropriate interview questions"" than males do; the organization put out an attention-getting video in 2015 purportedly showing male athletes' awkward reactions to receiving questions like those asked of female athletes. However, it is not universally acknowledged that female athletes attract more attention for off-court activities. For instance, a manual analysis by BIBREF0 [ BIBREF0 ] of online articles revealed significantly more descriptors associated with the physical appearance and personal lives of male basketball players in comparison to female ones.Transcripts of pre- or post-game press conferences offer an opportunity to determine quantitatively and in a data-driven manner how different are the questions which journalists pose to male players from those they pose to female players. Here are examples of a game-related and a non-game-relevant question, respectively, drawn from actual tennis interviews:To quantify gender discrepancies in questions, we propose a statistical language-model-based approach to measure how game-related questions are. In order to make such an approach effective, we restrict our attention in this study to a single sport—tennis—so that mere variations in the lingo of different sports do not introduce extra noise in our language models. Tennis is also useful for our investigation because, as BIBREF1 [ BIBREF1 ] noted, it “marks the only professional sports where male and female athletes generally receive similar amounts of overall broadcast media coverage during the major tournaments.""Using our methodology, we are able to quantify gender bias with respect to how game-related interview questions are. We also provide a more fine-grained analysis of how gender differences in journalistic questioning are displayed under various scenarios. To help with further analysis of interview questions and answers, we introduce a dataset of tennis post-match interview transcripts along with corresponding match information.Related Work	In contrast with our work, prior investigations of bias in sport journalism rely on manual coding or are based on simple lists of manually defined keywords. These focus on bias with respect to race, nationality, and gender BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF1 , BIBREF7 ; see BIBREF8 [ BIBREF8 ] for a review.Much of the work on gender bias in sports reporting has focused on “air-time” BIBREF9 , BIBREF10 . Other studies looked at stereotypical descriptions and framing BIBREF11 , BIBREF12 , BIBREF13 , BIBREF0 . For surveys, see BIBREF14 [ BIBREF14 ] or BIBREF15 [ BIBREF15 ], inter alia. Several studies have focused on the particular case of gender-correlated differences in tennis coverage BIBREF16 , BIBREF17 , BIBREF1 . We extend this line of work by proposing an automatic way to quantify gender bias in sport journalism.Dataset Description	We collect tennis press-conference transcripts from ASAP Sport's website (http://www.asapsports.com/), whose tennis collection dates back to 1992 and is still updated for current tournaments. For our study, we take post- game interviews for tennis singles matches played between Jan, 2000 to Oct 18, 2015. We also obtain easily-extractable match information from a dataset provided by Tennis-Data, which covers the majority of the matches played on the men's side from 2000-2015 and on the women's side from 2007-2015.We match interview transcripts with game statistics by date and player name, keeping only the question and answer pairs from games where the statistics are successfully merged. This gives us a dataset consisting of 6467 interview transcripts and a total of 81906 question snippets posed to 167 female players and 191 male players. To model tennis-game-specific language, we use live text play-by-play commentaries collected from the website Sports Mole (http://www.sportsmole.co.uk/). These tend to be short, averaging around 40 words. Here is a sample, taken from the Federer-Murray match at the 2015 Wimbledon semi-final:“The serve-and-volley is being used frequently by Federer and it's enabling him to take control behind his own serve. Three game points are earned before an ace down the middle seal [sic] the love hold.”For our analysis, we create a gender-balanced set of commentaries consisting of descriptions for 1981 games played for each gender.Method	As a preliminary step, we apply a word-level analysis to understand if there appear to be differences in word usage when journalists interview male players compared to female players. We then introduce our method for quantifying the degree to which a question is game-related, which we will use to explore gender differences.Preliminary Analysis	To compare word usage in questions, we consider, for each word $w$ , the percentage of players who have ever been asked a question containing $w$ . We then consider words with the greatest difference in percentage between male and female players. The top distinguishing words, which are listed below in descending order of percentage difference, seem to suggest that questions journalists pose to male players are more game-related:clay, challenger(s), tie, sets, practiced, tiebreaker, maybe, see, impression, serve, history, volley, chance, height, support, shots, server(s), greatest, way, tiebreaks, tiebreakers, era, lucky, luck;yet, new, nervous, improve, seed, friends, nerves, mom, every, matter, become, meet, winning, type, won, draw, found, champion, stop, fight, wind, though, father, thing, love.Game Language Model	To quantify how game-related a question is in a data-driven fashion, we train a bigram language model using KenLM BIBREF18 on the gender-balanced set of live-text play-by-play commentaries introduced in Section ""Dataset Description"" .For an individual question $q$ , we measure its perplexity $PP(q)$ with respect to this game language model $P_{\textnormal {\tiny \tiny commentary}}$ as an indication of how game-related the question is: the higher the perplexity value, the less game-related the question. Perplexity, a standard measure of language-model fit BIBREF19 , is defined as follows for an $N$ -word sequence $w_1 w_2 \ldots w_N$ : $
PP(w_1 w_2 ... w_N) = \@root N \of {\displaystyle \frac{1}{P_{\textnormal {\tiny \tiny commentary}}(w_1\cdots w_N)}} \hspace*{2.84544pt}.
$ Below are some sample questions of low-perplexity and high-perplexity values:Experiments	In this section we use the game language model to quantify gender-based bias in questions. We then compare the extent to which this difference depends of various factors, such as question type, game outcome, or player rank.Main Result: Males vs. Females	We first compute perplexities for each individual question and then group the question instances according to the interviewee's gender class. Throughout we use the Mann-Whitney $U$ statistical significance test, unless otherwise noted.Comparing perplexity values between the two groups, we find that the mean perplexity of questions posed to male players is significantly smaller ( $p$ -value $<$ 0.001) than that of questions posed to female players. This suggests that the questions male athletes receive are more game-related.However, the number of interviews each player participates in varies greatly, with highly interviewed players answering as many as thousands of questions while some lesser-known players have fewer than 10 interview questions in the dataset. Thus it is conceivable that the difference is simply explained by questions asked to a few prolific players. To test whether this is the case, or whether the observation is more general, we micro-average the perplexities by player: for each of the 167 male players and 143 females who have at least 10 questions in our dataset, we consider the average perplexities of the questions they receive. Comparing these micro-averages, we find that it is still the case that questions posed to male players are significantly closer to game language ( $p$ -value $<$ 0.05), indicating that the observed gender difference is not simply explained by a few highly interviewed players.Relation to Other Factors	We further investigate how the level of gender bias is tied to different factors: how typical the question is (section UID20 ), the ranking of the player (section UID24 ), and whether the player won or lost the match (section UID26 ). For all the following experiments, we use per-question perplexity for comparisons: per-player perplexity is not used due to limited sample size.One might wonder whether the perplexity disparities we see in questions asked of female vs. male players are due to “off-the-wall” queries, rather than to those that are more typical in post-match interviews. We therefore use a data-driven approach to distinguish between typical and atypical questions.For any given question, we consider how frequently its words appear in post-match press conferences in general. Specifically, we take the set of all questions as the set of documents, $D$ . We compute the inverse document frequency for each word (after stemming) that has appeared in our dataset, excluding the set $S$ consisting of stop words and a special token for entity names. For a question $q$ that contains the set of unique words $\lbrace w_1, w_2, ... , w_N\rbrace \notin S$ , we compute its atypicality score $Sc(q)$ as: $
Sc(\lbrace w_1, w_2, ... , w_N\rbrace ) = \displaystyle \frac{1}{N}\sum \limits _{i=1}^{N} \textnormal {idf}(w_i, D) \, .
$ We use the overall mean atypicality score of the entire question dataset as the cutoff point: questions with scores above the overall mean are considered atypical and the rest are considered typical. Below are some examples:Figure 1 shows that a gender bias with respect to whether game-related language is used exists for both typical and atypical questions. However, additional analysis reveals that the difference in mean perplexity values between genders is highly statistically significantly larger for atypical questions, suggesting that gender bias is more salient among the more unusual queries.Higher ranked players generally attract more media attention, and therefore may be targeted differently by journalists. To understand the effect of player ranking, we divide players into two groups: top 10 players and the rest. For our analysis, we use the ranking of the player at the time the interview was conducted. (It is therefore possible that questions posed to the same player but at different times could fall into different ranking groups due to ranking fluctuations over time.) We find that questions to male players are significantly closer to game language regardless of player ranking ( $p$ -value $<$ 0.001, Figure 2 ).Furthermore, if we focus only on players who have ranked both in and outside the top 10 in our dataset, and pair the questions asked to them when they were higher-ranked to the questions asked when their ranking was lower, we find that there is no significant difference between questions asked to male athletes when they were in different ranking groups (Wilcoxon signed-rank $p$ -value $>$ 0.05). However, the difference is significant for females (Wilcoxon signed-rank $p$ -value $<$ 0.01), suggesting that gender bias may be more salient for lower ranked players as questions to lower-ranked female athletes tend to be less game-related.While one might expect that star players would receive more off-court questions (yielding higher perplexities), the perplexity values for questions posed to top 10 players are actually lower regardless of gender. This may be because the training data for our language model is more focused on specific points played in matches, and may not be representative of tennis-related questions that are more general (e.g., longer-term career goals, personal records, injuries). In other words, our result suggests that journalists may attend more to the specifics of the games of higher ranked players, posing more specific questions about points played in the match during interviews.While it is reasonable to expect that whether the interviewee won or lost would affect how game-related the questions are, the difference in mean perplexity for males and females conditioned on win/loss game outcome are comparable. In addition, for both male players and female players, there is no significant difference observed between the paired set of questions asked in winning interviews and the losing ones (Wilcoxon signed-rank $p$ -value $>$ 0.05), controlling for both player and season. This suggests that that game result may not be a factor affecting how game-related the interview questions are.Concluding discussion	In this work we propose a language-model based approach to quantify gender bias in the interview questions tennis players receive. We find that questions to male athletes are generally more game-related. The difference is more salient among the unusual questions in press conferences, and for lower-ranked players.However, this preliminary study has a number of limitations. We have considered only a single sport. In addition, our dataset does not contain any information about who asked which question, which makes us unable to control for any idiosyncrasies of specific journalists. For example, it is conceivable that the disparities we observe are explained by differences in the journalists that are assigned to conduct the respective interviews.In this work, we limit our scope to bias in terms of game-related language, not considering differences (or similarities) that may exist in other dimensions. Further studies may use a similar approach to quantify and explore differences in other dimensions, by using language models specifically trained to model other domains of interests, which may provide a more comprehensive view of how questions differ when targeting different groups.Furthermore, our main focus is on questions asked during press conferences; we have not looked at the players' responses. The transcripts data, which we release publicly, may provide opportunities for further studies.Acknowledgments	We thank the anonymous reviewers and the participants in the Fall 2015 edition of the course “Natural Language Processing and Social Interaction” for helpful comments and discussion. This research was supported in part by a Discovery and Innovation Research Seed award from the Office of the Vice Provost for Research at Cornell.",['What data is used in this work?'],"[""Post-match interviews for tennis singles matches from ASAP Sport's website with match information from a dataset provided by Tennis-Data""]"
51,"The Social Dynamics of Language Change in Online Networks	Language change is a complex social phenomenon, revealing pathways of communication and sociocultural influence. But, while language change has long been a topic of study in sociolinguistics, traditional linguistic research methods rely on circumstantial evidence, estimating the direction of change from differences between older and younger speakers. In this paper, we use a data set of several million Twitter users to track language changes in progress. First, we show that language change can be viewed as a form of social influence: we observe complex contagion for phonetic spellings and""netspeak""abbreviations (e.g., lol), but not for older dialect markers from spoken language. Next, we test whether specific types of social network connections are more influential than others, using a parametric Hawkes process model. We find that tie strength plays an important role: densely embedded social ties are significantly better conduits of linguistic influence. Geographic locality appears to play a more limited role: we find relatively little evidence to support the hypothesis that individuals are more influenced by geographically local social ties, even in their usage of geographical dialect markers.	Introduction	Change is a universal property of language. For example, English has changed so much that Renaissance-era texts like The Canterbury Tales must now be read in translation. Even contemporary American English continues to change and diversify at a rapid pace—to such an extent that some geographical dialect differences pose serious challenges for comprehensibility BIBREF0 . Understanding language change is therefore crucial to understanding language itself, and has implications for the design of more robust natural language processing systems BIBREF1 .Language change is a fundamentally social phenomenon BIBREF2 . For a new linguistic form to succeed, at least two things must happen: first, speakers (and writers) must come into contact with the new form; second, they must decide to use it. The first condition implies that language change is related to the structure of social networks. If a significant number of speakers are isolated from a potential change, then they are unlikely to adopt it BIBREF3 . But mere exposure is not sufficient—we are all exposed to language varieties that are different from our own, yet we nonetheless do not adopt them in our own speech and writing. For example, in the United States, many African American speakers maintain a distinct dialect, despite being immersed in a linguistic environment that differs in many important respects BIBREF4 , BIBREF5 . Researchers have made a similar argument for socioeconomic language differences in Britain BIBREF6 . In at least some cases, these differences reflect questions of identity: because language is a key constituent in the social construction of group identity, individuals must make strategic choices when deciding whether to adopt new linguistic forms BIBREF7 , BIBREF8 , BIBREF9 . By analyzing patterns of language change, we can learn more about the latent structure of social organization: to whom people talk, and how they see themselves.But, while the basic outline of the interaction between language change and social structure is understood, the fine details are still missing: What types of social network connections are most important for language change? To what extent do considerations of identity affect linguistic differences, particularly in an online context? Traditional sociolinguistic approaches lack the data and the methods for asking such detailed questions about language variation and change.In this paper, we show that large-scale social media data can shed new light on how language changes propagate through social networks. We use a data set of Twitter users that contains all public messages for several million accounts, augmented with social network and geolocation metadata. This data set makes it possible to track, and potentially explain, every usage of a linguistic variable as it spreads through social media. Overall, we make the following contributions:Data	Twitter is an online social networking platform. Users post 140-character messages, which appear in their followers' timelines. Because follower ties can be asymmetric, Twitter serves multiple purposes: celebrities share messages with millions of followers, while lower-degree users treat Twitter as a more intimate social network for mutual communication BIBREF13 . In this paper, we use a large-scale Twitter data set, acquired via an agreement between Microsoft and Twitter. This data set contains all public messages posted between June 2013 and June 2014 by several million users, augmented with social network and geolocation metadata. We excluded retweets, which are explicitly marked with metadata, and focused on messages that were posted in English from within the United States.Linguistic Markers	The explosive rise in popularity of social media has led to an increase in linguistic diversity and creativity BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF1 , BIBREF18 , affecting written language at all levels, from spelling BIBREF19 all the way up to grammatical structure BIBREF20 and semantic meaning across the lexicon BIBREF21 , BIBREF22 . Here, we focus on the most easily observable and measurable level: variation and change in the use of individual words.We take as our starting point words that are especially characteristic of eight cities in the United States. We chose these cities to represent a wide range of geographical regions, population densities, and demographics. We identified the following words as geographically distinctive markers of their associated cities, using SAGE BIBREF23 . Specifically, we followed the approach previously used by Eisenstein to identify community-specific terms in textual corpora BIBREF24 .ain (phonetic spelling of ain't), dese (phonetic spelling of these), yeen (phonetic spelling of you ain't);ard (phonetic spelling of alright), inna (phonetic spelling of in a and in the), lls (laughing like shit), phony (fake);cookout;asl (phonetic spelling of as hell, typically used as an intensifier on Twitter), mfs (motherfuckers);graffiti, tfti (thanks for the information);ard (phonetic spelling of alright), ctfuu (expressive lengthening of ctfu, an abbreviation of cracking the fuck up), jawn (generic noun);hella (an intensifier);inna (phonetic spelling of in a and in the), lls (laughing like shit), stamp (an exclamation indicating emphasis).Linguistically, we can divide these words into three main classes:The origins of cookout, graffiti, hella, phony, and stamp can almost certainly be traced back to spoken language. Some of these words (e.g., cookout and graffiti) are known to all fluent English speakers, but are preferred in certain cities simply as a matter of topic. Other words (e.g., hella BIBREF25 and jawn BIBREF26 ) are dialect markers that are not widely used outside their regions of origin, even after several decades of use in spoken language.ain, ard, asl, inna, and yeen are non-standard spellings that are based on phonetic variation by region, demographics, or situation.ctfuu, lls, mfs, and tfti are phrasal abbreviations. These words are interesting because they are fundamentally textual. They are unlikely to have come from spoken language, and are intrinsic to written social media.Several of these words were undergoing widespread growth in popularity around the time period spanned by our data set. For example, the frequencies of ard, asl, hella, and tfti more than tripled between 2012 and 2013. Our main research question is whether and how these words spread through Twitter. For example, lexical words are mainly transmitted through speech. We would expect their spread to be only weakly correlated with the Twitter social network. In contrast, abbreviations are fundamentally textual in nature, so we would expect their spread to correlate much more closely with the Twitter social network.Social network	To focus on communication between peers, we constructed a social network of mutual replies between Twitter users. Specifically, we created a graph in which there is a node for each user in the data set. We then placed an undirected edge between a pair of users if each replied to the other by beginning a message with their username. Our decision to use the reply network (rather than the follower network) was a pragmatic choice: the follower network is not widely available. However, the reply network is also well supported by previous research. For example, Huberman et al. argue that Twitter's mention network is more socially meaningful than its follower network: although users may follow thousands of accounts, they interact with a much more limited set of users BIBREF27 , bounded by a constant known as Dunbar's number BIBREF28 . Finally, we restricted our focus to mutual replies because there are a large number of unrequited replies directed at celebrities. These replies do not indicate a meaningful social connection.We compared our mutual-reply network with two one-directional “in” and “out” networks, in which all public replies are represented by directed edges. The degree distributions of these networks are depicted in fig:degree-dist. As expected, there are a few celebrities with very high in-degrees, and a maximum in-degree of $20,345$ . In contrast, the maximum degree in our mutual-reply network is 248.Geography	In order to test whether geographically local social ties are a significant conduit of linguistic influence, we obtained geolocation metadata from Twitter's location field. This field is populated via a combination of self reports and GPS tagging. We aggregated metadata across each user's messages, so that each user was geolocated to the city from which they most commonly post messages. Overall, our data set contains 4.35 million geolocated users, of which 589,562 were geolocated to one of the eight cities listed in sec:data-language. We also included the remaining users in our data set, but were not able to account for their geographical location.Researchers have previously shown that social network connections in online social media tend to be geographically assortative BIBREF29 , BIBREF30 . Our data set is consistent with this finding: for 94.8% of mutual-reply dyads in which both users were geolocated to one of the eight cities listed in sec:data-language, they were both geolocated to the same city. This assortativity motivates our decision to estimate separate influence parameters for local and non-local social connections (see sec:parametric-hawkes).Language Change as Social Influence	Our main research goal is to test whether and how geographically distinctive linguistic markers spread through Twitter. With this goal in mind, our first question is whether the adoption of these markers can be viewed as a form of complex contagion. To answer this question, we computed the fraction of users who used one of the words listed in sec:data-language after being exposed to that word by one of their social network connections. Formally, we say that user $i$ exposed user $j$ to word $w$ at time $t$ if and only if the following conditions hold: $i$ used $w$ at time $t$ ; $j$ had not used $w$ before time $t$ ; the social network connection $j$0 was formed before time $j$1 . We define the infection risk for word $j$2 to be the number of users who use word $j$3 after being exposed divided by the total number of users who were exposed. To consider the possibility that multiple exposures have a greater impact on the infection risk, we computed the infection risk after exposures across one, two, and three or more distinct social network connections.The words' infection risks cannot be interpreted directly because relational autocorrelation can also be explained by homophily and external confounds. For example, geographically distinctive non-standard language is more likely to be used by young people BIBREF31 , and online social network connections are assortative by age BIBREF32 . Thus, a high infection risk can also be explained by the confound of age. We therefore used the shuffle test proposed by Anagnostopoulos et al. BIBREF33 , which compares the observed infection risks to infection risks under the null hypothesis that event timestamps are independent. The null hypothesis infection risks are computed by randomly permuting the order of word usage events. If the observed infection risks are substantially higher than the infection risks computed using the permuted data, then this is compatible with social influence.fig:risk-by-exposure depicts the ratios between the words' observed infection risks and the words' infection risks under the null hypothesis, after exposures across one, two, and three or more distinct connections. We computed 95% confidence intervals across the words and across the permutations used in the shuffle test. For all three linguistic classes defined in sec:data-language, the risk ratio for even a single exposure is significantly greater than one, suggesting the existence of social influence. The risk ratio for a single exposure is nearly identical across the three classes. For phonetic spellings and abbreviations, the risk ratio grows with the number of exposures. This pattern suggests that words in these classes exhibit complex contagion—i.e., multiple exposures increase the likelihood of adoption BIBREF35 . In contrast, the risk ratio for lexical words remains the same as the number of exposures increases, suggesting that these words spread by simple contagion.Complex contagion has been linked to a range of behaviors, from participation in collective political action to adoption of avant garde fashion BIBREF35 . A common theme among these behaviors is that they are not cost-free, particularly if the behavior is not legitimated by widespread adoption. In the case of linguistic markers intrinsic to social media, such as phonetic spellings and abbreviations, adopters risk negative social evaluations of their linguistic competency, as well as their cultural authenticity BIBREF36 . In contrast, lexical words are already well known from spoken language and are thus less socially risky. This difference may explain why we do not observe complex contagion for lexical words.Social Evaluation of Language Variation	In the previous section, we showed that geographically distinctive linguistic markers spread through Twitter, with evidence of complex contagion for phonetic spellings and abbreviations. But, does each social network connection contribute equally? Our second question is therefore whether (1) strong ties and (2) geographically local ties exert greater linguistic influence than other ties. If so, users must socially evaluate the information they receive from these connections, and judge it to be meaningful to their linguistic self-presentation. In this section, we outline two hypotheses regarding their relationships to linguistic influence.Tie Strength	Social networks are often characterized in terms of strong and weak ties BIBREF37 , BIBREF3 , with strong ties representing more important social relationships. Strong ties are often densely embedded, meaning that the nodes in question share many mutual friends; in contrast, weak ties often bridge disconnected communities. Bakshy et al. investigated the role of weak ties in information diffusion, through resharing of URLs on Facebook BIBREF38 . They found that URLs shared across strong ties are more likely to be reshared. However, they also found that weak ties play an important role, because users tend to have more weak ties than strong ties, and because weak ties are more likely to be a source of new information. In some respects, language change is similar to traditional information diffusion scenarios, such as resharing of URLs. But, in contrast, language connects with personal identity on a much deeper level than a typical URL. As a result, strong, deeply embedded ties may play a greater role in enforcing community norms.We quantify tie strength in terms of embeddedness. Specifically, we use the normalized mutual friends metric introduced by Adamic and Adar BIBREF39 : $$s_{i,j} = \sum _{k \in \Gamma (i) \cap \Gamma (j)} \frac{1}{\log \left(
\#| \Gamma (k)|\right)},$$   (Eq. 28) where, in our setting, $\Gamma (i)$ is the set of users connected to $i$ in the Twitter mutual-reply network and $\#|\Gamma (i)|$ is the size of this set. This metric rewards dyads for having many mutual friends, but counts mutual friends more if their degrees are low—a high-degree mutual friend is less informative than one with a lower-degree. Given this definition, we can form the following hypothesis:The linguistic influence exerted across ties with a high embeddedness value $s_{i,j}$ will be greater than the linguistic influence exerted across other ties.Geographic Locality	An open question in sociolinguistics is whether and how local covert prestige—i.e., the positive social evaluation of non-standard dialects—affects the adoption of new linguistic forms BIBREF6 . Speakers often explain their linguistic choices in terms of their relationship with their local identity BIBREF40 , but this may be a post-hoc rationalization made by people whose language is affected by factors beyond their control. Indeed, some sociolinguists have cast doubt on the role of “local games” in affecting the direction of language change BIBREF41 .The theory of covert prestige suggests that geographically local social ties are more influential than non-local ties. We do not know of any prior attempts to test this hypothesis quantitatively. Although researchers have shown that local linguistic forms are more likely to be used in messages that address geographically local friends BIBREF42 , they have not attempted to measure the impact of exposure to these forms. This lack of prior work may be because it is difficult to obtain relevant data, and to make reliable inferences from such data. For example, there are several possible explanations for the observation that people often use similar language to that of their geographical neighbors. One is exposure: even online social ties tend to be geographically assortative BIBREF32 , so most people are likely to be exposed to local linguistic forms through local ties. Alternatively, the causal relation may run in the reverse direction, with individuals preferring to form social ties with people whose language matches their own. In the next section, we describe a model that enables us to tease apart the roles of geographic assortativity and local influence, allowing us to test the following hypothesis:The influence toward geographically distinctive linguistic markers is greater when exerted across geographically local ties than across other ties.We note that this hypothesis is restricted in scope to geographically distinctive words. We do not consider the more general hypothesis that geographically local ties are more influential for all types of language change, such as change involving linguistic variables that are associated with gender or socioeconomic status.Language Change as a Self-exciting Point Process	To test our hypotheses about social evaluation, we require a more sophisticated modeling tool than the simple counting method described in sec:influence. In this section, rather than asking whether a user was previously exposed to a word, we ask by whom, in order to compare the impact of exposures across different types of social network connections. We also consider temporal properties. For example, if a user adopts a new word, should we credit this to an exposure from a weak tie in the past hour, or to an exposure from a strong tie in the past day?Following a probabilistic modeling approach, we treated our Twitter data set as a set of cascades of timestamped events, with one cascade for each of the geographically distinctive words described in sec:data-language. Each event in a word's cascade corresponds to a tweet containing that word. We modeled each cascade as a probabilistic process, and estimated the parameters of this process. By comparing nested models that make progressively finer distinctions between social network connections, we were able to quantitatively test our hypotheses.Our modeling framework is based on a Hawkes process BIBREF11 —a specialization of an inhomogeneous Poisson process—which explains a cascade of timestamped events in terms of influence parameters. In a temporal setting, an inhomogeneous Poisson process says that the number of events $y_{t_1,t_2}$ between $t_1$ and $t_2$ is drawn from a Poisson distribution, whose parameter is the area under a time-varying intensity function over the interval defined by $t_1$ and $t_2$ : $$y_{t_1,t_2} &\sim \text{Poisson}\left(\Lambda (t_1,t_2)\right))
\multicolumn{2}{l}{\text{where}}\\
\Lambda (t_1,t_2) &= \int _{t_1}^{t_2} \lambda (t)\ \textrm {d}t.$$   (Eq. 32)  Since the parameter of a Poisson distribution must be non-negative, the intensity function must be constrained to be non-negative for all possible values of $t$ .A Hawkes process is a self-exciting inhomogeneous Poisson process, where the intensity function depends on previous events. If we have a cascade of $N$ events $\lbrace t_n\rbrace _{n=1}^N$ , where $t_n$ is the timestamp of event $n$ , then the intensity function is $$\lambda (t) = \mu _t + \sum _{t_n < t} \alpha \, \kappa (t - t_n),$$   (Eq. 33) where $\mu _t$ is the base intensity at time $t$ , $\alpha $ is an influence parameter that captures the influence of previous events, and $\kappa (\cdot )$ is a time-decay kernel.We can extend this framework to vector observations $y_{t_1,t_2} = (y^{(1)}_{t_1, t_2}, \ldots , y^{(M)}_{t_1,
t_2})$ and intensity functions $\lambda (t) =
(\lambda ^{(1)}(t), \ldots , \lambda ^{(M)}(t))$ , where, in our setting, $M$ is the total number of users in our data set. If we have a cascade of $N$ events $\lbrace (t_n, m_n)\rbrace _{n=1}^N$ , where $t_n$ is the timestamp of event $n$ and $m_n \in \lbrace 1, \ldots , M\rbrace $ is the source of event $n$ , then the intensity function for user $m^{\prime } \in \lbrace 1, \ldots ,
M\rbrace $ is $$\lambda ^{(m^{\prime })}(t) = \mu ^{(m^{\prime })}_t + \sum _{t_n < t} \alpha _{m_n \rightarrow m^{\prime }} \kappa (t - t_n),$$   (Eq. 34) where $\mu _t^{(m^{\prime })}$ is the base intensity for user $m^{\prime }$ at time $t$ , $\alpha _{m_n \rightarrow m^{\prime }}$ is a pairwise influence parameter that captures the influence of user $m_n$ on user $m^{\prime }$ , and $\kappa (\cdot )$ is a time-decay kernel. Throughout our experiments, we used an exponential decay kernel $\kappa (\Delta t) = e^{-\gamma \Delta t}$ . We set the hyperparameter $\gamma $ so that $\kappa (\textrm {1 hour}) = e^{-1}$ .Researchers usually estimate all $M^2$ influence parameters of a Hawkes process (e.g., BIBREF43 , BIBREF44 ). However, in our setting, $M > 10^6$ , so there are $O(10^{12})$ influence parameters. Estimating this many parameters is computationally and statistically intractable, given that our data set includes only $O(10^5)$ events (see the $x$ -axis of fig:ll-diffs for event counts for each word). Moreover, directly estimating these parameters does not enable us to quantitatively test our hypotheses.Parametric Hawkes Process	Instead of directly estimating all $O(M^2)$ pairwise influence parameters, we used Li and Zha's parametric Hawkes process BIBREF12 . This model defines each pairwise influence parameter in terms of a linear combination of pairwise features: $$\alpha _{m \rightarrow m^{\prime }} = \theta ^{\top } f(m \rightarrow m^{\prime }),$$   (Eq. 36) where $f(m \rightarrow m^{\prime })$ is a vector of features that describe the relationship between users $m$ and $m^{\prime }$ . Thus, we only need to estimate the feature weights $\theta $ and the base intensities. To ensure that the intensity functions $\lambda ^{(1)}(t),
\ldots , \lambda ^{(M)}(t)$ are non-negative, we must assume that $\theta $ and the base intensities are non-negative.We chose a set of four binary features that would enable us to test our hypotheses about the roles of different types of social network connections:This feature fires when $m^{\prime } \!=\! m$ . We included this feature to capture the scenario where using a word once makes a user more likely to use it again, perhaps because they are adopting a non-standard style.This feature fires if the dyad $(m, m^{\prime })$ is in the Twitter mutual-reply network described in sec:data-social. We also used this feature to define the remaining two features. By doing this, we ensured that features F2, F3, and F4 were (at least) as sparse as the mutual-reply network.This feature fires if the dyad $(m,m^{\prime })$ is in in the Twitter mutual-reply network, and the Adamic-Adar value for this dyad is especially high. Specifically, we require that the Adamic-Adar value be in the 90 $^{\textrm {th}}$ percentile among all dyads where at least one user has used the word in question. Thus, this feature picks out the most densely embedded ties.This feature fires if the dyad $(m,m^{\prime })$ is in the Twitter mutual-reply network, and the users were geolocated to the same city, and that city is one of the eight cities listed in sec:data. For other dyads, this feature returns zero. Thus, this feature picks out a subset of the geographically local ties.In sec:results, we describe how we used these features to construct a set of nested models that enabled us to test our hypotheses. In the remainder of this section, we provide the mathematical details of our parameter estimation method.Objective Function	We estimated the parameters using constrained maximum likelihood. Given a cascade of events $\lbrace (t_n, m_n)\rbrace _{n=1}^N$ , the log likelihood under our model is $$\mathcal {L} = \sum _{n=1}^N \log \lambda ^{(m_n)}(t_n) - \sum _{m = 1}^M \int _0^T \lambda ^{(m)}(t)\ \textrm {d}t,$$   (Eq. 42) where $T$ is the temporal endpoint of the cascade. Substituting in the complete definition of the per-user intensity functions from eq:intensity and eq:alpha, $$\mathcal {L} &= \sum _{n=1}^N \log {\left(\mu ^{(m_n)}_{t_n} + \sum _{t_{n^{\prime }} < t_n} \theta ^{\top }f(m_{n^{\prime }} \rightarrow m_n)\,\kappa (t_n - t_{n^{\prime }}) \right)} -{} \\
&\quad \sum ^M_{m^{\prime }=1} \int _0^T \left(\mu _t^{(m^{\prime })} + \sum _{t_{n^{\prime }} < t} \theta ^{\top } f(m_{n^{\prime }} \rightarrow m^{\prime })\, \kappa (t - {t_{n^{\prime }}})\right)\textrm {d}t.$$   (Eq. 43)  If the base intensities are constant with respect to time, then $$\mathcal {L} &= \sum _{n=1}^N \log {\left(\mu ^{(m_n)} + \sum _{t_{n^{\prime }} < t_n} \theta ^{\top }f(m_{n^{\prime }} \rightarrow m_n)\, \kappa (t_n - t_{n^{\prime }}) \right)} - {}\\
&\quad \sum ^M_{m^{\prime }=1} \left( T\mu ^{(m^{\prime })} + \sum ^N_{n=1} \theta ^{\top } f(m_n \rightarrow m^{\prime })\,(1 - \kappa (T - t_n))\right),$$   (Eq. 44)  where the second term includes a sum over all events $n = \lbrace 1, \ldots ,
N\rbrace $ that contibute to the final intensity $\lambda ^{(m^{\prime })}(T).$ To ease computation, however, we can rearrange the second term around the source $m$ rather than the recipient $m^{\prime }$ : $$\mathcal {L} &= \sum _{n=1}^N \log {\left(\mu ^{(m_n)} + \sum _{t_{n^{\prime }} < t_n} \theta ^{\top }f(m_{n^{\prime }} \rightarrow m_n)\, \kappa (t_n - t_{n^{\prime }}) \right)} - \\
&\quad \sum _{m=1}^M \left(T\mu ^{(m)} + \sum _{\lbrace n : m_n = m\rbrace } \, \theta ^{\top } f(m \rightarrow \star )\, (1 - \kappa (T-t_n))\right),$$   (Eq. 45)  where we have introduced an aggregate feature vector $f(m
\rightarrow \star ) = \sum _{m^{\prime }=1}^M f(m \rightarrow m^{\prime })$ . Because the sum $\sum _{\lbrace n : m_n = m^{\prime }\rbrace } f(m^{\prime } \rightarrow \star )\,\kappa (T-t_n)$ does not involve either $\theta $ or $\mu ^{(1)}, \ldots ,
\mu ^{(M)}$ , we can pre-compute it. Moreover, we need to do so only for users $m \in \lbrace 1, \ldots , M\rbrace $ for whom there is at least one event in the cascade.A Hawkes process defined in terms of eq:intensity has a log likelihood that is convex in the pairwise influence parameters and the base intensities. For a parametric Hawkes process, $\alpha _{m \rightarrow m^{\prime }}$ is an affine function of $\theta $ , so, by composition, the log likelihood is convex in $\theta $ and remains convex in the base intensities.Gradients	The first term in the log likelihood and its gradient contains a nested sum over events, which appears to be quadratic in the number of events. However, we can use the exponential decay of the kernel $\kappa (\cdot )$ to approximate this term by setting a threshold $\tau ^{\star }$ such that $\kappa (t_n - t_{n^{\prime }}) = 0$ if $t_n - t_{n^{\prime }}
\ge \tau ^{\star }$ . For example, if we set $\tau ^{\star } = 24 \textrm {
hours}$ , then we approximate $\kappa (\tau ^{\star }) = 3 \times 10^{-11} \approx 0$ . This approximation makes the cost of computing the first term linear in the number of events.The second term is linear in the number of social network connections and linear in the number of events. Again, we can use the exponential decay of the kernel $\kappa (\cdot )$ to approximate $\kappa (T - t_n)
\approx 0$ for $T - t_n \ge \tau ^{\star }$ , where $\tau ^{\star } = 24
\textrm { hours}$ . This approximation means that we only need to consider a small number of tweets near temporal endpoint of the cascade. For each user, we also pre-computed $\sum _{\lbrace n : m_n = m^{\prime }\rbrace }
f(m^{\prime } \rightarrow \star )\,\kappa (T - t_n)$ . Finally, both terms in the log likelihood and its gradient can also be trivially parallelized over users $m = \lbrace 1, \ldots , M\rbrace $ .For a Hawkes process defined in terms of eq:intensity, Ogata showed that additional speedups can be obtained by recursively pre-computing a set of aggregate messages for each dyad $(m,
m^{\prime })$ . Each message represents the events from user $m$ that may influence user $m^{\prime }$ at the time $t_i^{(m^{\prime })}$ of their $i^{\textrm {th}}$ event BIBREF45 : $
&R^{(i)}_{m \rightarrow m^{\prime }} \\
&\quad =
{\left\lbrace \begin{array}{ll}
\kappa (t^{(m^{\prime })}_{i} - t^{(m^{\prime })}_{i-1})\,R^{(i-1)}_{m \rightarrow m^{\prime }} + \sum _{t^{(m^{\prime })}_{i-1} \le t^{(m)}_{j} \le t^{(m^{\prime })}_i} \kappa (t^{(m^{\prime })}_i - t^{(m)}_j) & m\ne m^{\prime }\\
\kappa (t^{(m^{\prime })}_{i} - t^{(m^{\prime })}_{i-1}) \times (1 + R^{(i-1)}_{m \rightarrow m^{\prime }}) & m = m^{\prime }.
\end{array}\right.}
$  These aggregate messages do not involve the feature weights $\theta $ or the base intensities, so they can be pre-computed and reused throughout parameter estimation.For a parametric Hawkes process, it is not necessary to compute a set of aggregate messages for each dyad. It is sufficient to compute a set of aggregate messages for each possible configuration of the features. In our setting, there are only four binary features, and some combinations of features are impossible.Because the words described in sec:data-language are relatively rare, most of the users in our data set never used them. However, it is important to include these users in the model. Because they did not adopt these words, despite being exposed to them by users who did, their presence exerts a negative gradient on the feature weights. Moreover, such users impose a minimal cost on parameter estimation because they need to be considered only when pre-computing feature counts.Coordinate Ascent	We optimized the log likelihood with respect to the feature weights $\theta $ and the base intensities. Because the log likelihood decomposes over users, each base intensity $\mu ^{(m)}$ is coupled with only the feature weights and not with the other base intensities. Jointly estimating all parameters is inefficient because it does not exploit this structure. We therefore used a coordinate ascent procedure, alternating between updating $\theta $ and the base intensities. As explained in sec:parametric-hawkes, both $\theta $ and the base intensities must be non-negative to ensure that intensity functions are also non-negative. At each stage of the coordinate ascent, we performed constrained optimization using the active set method of MATLAB's fmincon function.Results	We used a separate set of parametric Hawkes process models for each of the geographically distinctive linguistic markers described in sec:data-language. Specifically, for each word, we constructed a set of nested models by first creating a baseline model using features F1 (self-activation) and F2 (mutual reply) and then adding in each of the experimental features—i.e., F3 (tie strength) and F4 (local).We tested hypothesis H1 (strong ties are more influential) by comparing the goodness of fit for feature set F1+F2+F3 to that of feature set F1+F2. Similarly, we tested H2 (geographically local ties are more influential) by comparing the goodness of fit for feature set F1+F2+F4 to that of feature set F1+F2.In fig:ll-diffs, we show the improvement in goodness of fit from adding in features F3 and F4. Under the null hypothesis, the log of the likelihood ratio follows a $\chi ^2$ distribution with one degree of freedom, because the models differ by one parameter. Because we performed thirty-two hypothesis tests (sixteen words, two features), we needed to adjust the significance thresholds to correct for multiple comparisons. We did this using the Benjamini-Hochberg procedure BIBREF46 .Features F3 and F4 did not improve the goodness of fit for less frequent words, such as ain, graffiti, and yeen, which occur fewer than $10^4$ times. Below this count threshold, there is not enough data to statistically distinguish between different types of social network connections. However, above this count threshold, adding in F3 (tie strength) yielded a statistically significant increase in goodness of fit for ard, asl, cookout, hella, jawn, mfs, and tfti. This finding provides evidence in favor of hypothesis H1—that the linguistic influence exerted across densely embedded ties is greater than the linguistic influence exerted across other ties.In contrast, adding in F4 (local) only improved goodness of fit for three words: asl, jawn, and lls. We therefore conclude that support for hypothesis H2—that the linguistic influence exerted across geographically local ties is greater than the linguistic influence across than across other ties—is limited at best.In sec:influence we found that phonetic spellings and abbreviations exhibit complex contagion, while lexical words do not. Here, however, we found no such systematic differences between the three linguistic classes. Although we hypothesize that lexical words propagate mainly outside of social media, we nonetheless see that when these words do propagate across Twitter, their adoption is modulated by tie strength, as is the case for phonetic spellings and abbreviations.Discussion	Our results in sec:influence demonstrate that language change in social media can be viewed as a form of information diffusion across a social network. Moreover, this diffusion is modulated by a number of sociolinguistic factors. For non-lexical words, such as phonetic spellings and abbreviations, we find evidence of complex contagion: the likelihood of their adoption increases with the number of exposures. For both lexical and non-lexical words, we find evidence that the linguistic influence exerted across densely embedded ties is greater than the linguistic influence exerted across other ties. In contrast, we find no evidence to support the hypothesis that geographically local ties are more influential.Overall, these findings indicate that language change is not merely a process of random diffusion over an undifferentiated social network, as proposed in many simulation studies BIBREF47 , BIBREF48 , BIBREF49 . Rather, some social network connections matter more than others, and social judgments have a role to play in modulating language change. In turn, this conclusion provides large-scale quantitative support for earlier findings from ethnographic studies. A logical next step would be to use these insights to design more accurate simulation models, which could be used to reveal long-term implications for language variation and change.Extending our study beyond North America is a task for future work. Social networks vary dramatically across cultures, with traditional societies tending toward networks with fewer but stronger ties BIBREF3 . The social properties of language variation in these societies may differ as well. Another important direction for future work is to determine the impact of exogenous events, such as the appearance of new linguistic forms in mass media. Exogeneous events pose potential problems for estimating both infection risks and social influence. However, it may be possible to account for these events by incorporating additional data sources, such as search trends. Finally, we plan to use our framework to study the spread of terminology and ideas through networks of scientific research articles. Here too, authors may make socially motivated decisions to adopt specific terms and ideas BIBREF50 . The principles behind these decisions might therefore be revealed by an analysis of linguistic events propagating over a social network.","['Does the paper discuss limitations of considering only data from Twitter?', 'Did they represent tie strength only as number of social ties in a networks? ', 'What sociolinguistic variables (phonetic spellings) did they analyze? ', 'What older dialect markers did they explore?']","['Unacceptable', 'Unacceptable', 'variation and change in the use of words characteristic from eight US cities that have non-standard spellings', 'Unacceptable']"
52,"MetaLDA: a Topic Model that Efficiently Incorporates Meta information	Besides the text content, documents and their associated words usually come with rich sets of meta informa- tion, such as categories of documents and semantic/syntactic features of words, like those encoded in word embeddings. Incorporating such meta information directly into the generative process of topic models can improve modelling accuracy and topic quality, especially in the case where the word-occurrence information in the training data is insufficient. In this paper, we present a topic model, called MetaLDA, which is able to leverage either document or word meta information, or both of them jointly. With two data argumentation techniques, we can derive an efficient Gibbs sampling algorithm, which benefits from the fully local conjugacy of the model. Moreover, the algorithm is favoured by the sparsity of the meta information. Extensive experiments on several real world datasets demonstrate that our model achieves comparable or improved performance in terms of both perplexity and topic quality, particularly in handling sparse texts. In addition, compared with other models using meta information, our model runs significantly faster.	Introduction	With the rapid growth of the internet, huge amounts of text data are generated in social networks, online shopping and news websites, etc. These data create demand for powerful and efficient text analysis techniques. Probabilistic topic models such as Latent Dirichlet Allocation (LDA) BIBREF0 are popular approaches for this task, by discovering latent topics from text collections. Many conventional topic models discover topics purely based on the word-occurrences, ignoring the meta information (a.k.a., side information) associated with the content. In contrast, when we humans read text it is natural to leverage meta information to improve our comprehension, which includes categories, authors, timestamps, the semantic meanings of the words, etc. Therefore, topic models capable of using meta information should yield improved modelling accuracy and topic quality.In practice, various kinds of meta information are available at the document level and the word level in many corpora. At the document level, labels of documents can be used to guide topic learning so that more meaningful topics can be discovered. Moreover, it is highly likely that documents with common labels discuss similar topics, which could further result in similar topic distributions. For example, if we use authors as labels for scientific papers, the topics of the papers published by the same researcher can be closely related.At the word level, different semantic/syntactic features are also accessible. For example, there are features regarding word relationships, such as synonyms obtained from WordNet BIBREF1 , word co-occurrence patterns obtained from a large corpus, and linked concepts from knowledge graphs. It is preferable that words having similar meaning but different morphological forms, like “dog” and “puppy”, are assigned to the same topic, even if they barely co-occur in the modelled corpus. Recently, word embeddings generated by GloVe BIBREF2 and word2vec BIBREF3 , have attracted a lot of attention in natural language processing and related fields. It has been shown that the word embeddings can capture both the semantic and syntactic features of words so that similar words are close to each other in the embedding space. It seems reasonable to expect that these word embedding will improve topic modelling BIBREF4 , BIBREF5 .Conventional topic models can suffer from a large performance degradation over short texts (e.g., tweets and news headlines) because of insufficient word co-occurrence information. In such cases, meta information of documents and words can play an important role in analysing short texts by compensating the lost information in word co-occurrences. At the document level, for example, tweets are usually associated with hashtags, users, locations, and timestamps, which can be used to alleviate the data sparsity problem. At the word level, word semantic similarity and embeddings obtained or trained on large external corpus (e.g., Google News or Wikipedia) have been proven useful in learning meaningful topics from short texts BIBREF6 , BIBREF7 .The benefit of using document and word meta information separately is shown in several models such as BIBREF8 , BIBREF9 , BIBREF5 . However, in existing models this is usually not efficient enough due to non-conjugacy and/or complex model structures. Moreover, only one kind of meta information (either at document level or at word level) is used in most existing models. In this paper, we propose MetaLDA, a topic model that can effectively and efficiently leverage arbitrary document and word meta information encoded in binary form. Specifically, the labels of a document in MetaLDA are incorporated in the prior of the per-document topic distributions. If two documents have similar labels, their topic distributions should be generated with similar Dirichlet priors. Analogously, at the word level, the features of a word are incorporated in the prior of the per-topic word distributions, which encourages words with similar features to have similar weights across topics. Therefore, both document and word meta information, if and when they are available, can be flexibly and simultaneously incorporated using MetaLDA. MetaLDA has the following key properties:We conduct extensive experiments with several real datasets including regular and short texts in various domains. The experimental results demonstrate that MetaLDA achieves improved performance in terms of perplexity, topic coherence, and running time.Related Work	In this section, we review three lines of related work: models with document meta information, models with word meta information, and models for short texts.At the document level, Supervised LDA (sLDA) BIBREF10 models document labels by learning a generalised linear model with an appropriate link function and exponential family dispersion function. But the restriction for sLDA is that one document can only have one label. Labelled LDA (LLDA) BIBREF11 assumes that each label has a corresponding topic and a document is generated by a mixture of the topics. Although multiple labels are allowed, LLDA requires that the number of topics must equal to the number of labels, i.e., exactly one topic per label. As an extension to LLDA, Partially Labelled LDA (PLLDA) BIBREF9 relaxes this requirement by assigning multiple topics to a label. The Dirichlet Multinomial Regression (DMR) model BIBREF8 incorporates document labels on the prior of the topic distributions like our MetaLDA but with the logistic-normal transformation. As full conjugacy does not exist in DMR, a part of the inference has to be done by numerical optimisation, which is slow for large sets of labels and topics. Similarly, in the Hierarchical Dirichlet Scaling Process (HDSP) BIBREF12 , conjugacy is broken as well since the topic distributions have to be renormalised. BIBREF13 introduces a Poisson factorisation model with hierarchical document labels. But the techniques cannot be applied to regular topic models as the topic proportion vectors are also unnormalised.Recently, there is growing interest in incorporating word features in topic models. For example, DF-LDA BIBREF14 incorporates word must-links and cannot-links using a Dirichlet forest prior in LDA; MRF-LDA BIBREF15 encodes word semantic similarity in LDA with a Markov random field; WF-LDA BIBREF16 extends LDA to model word features with the logistic-normal transform; LF-LDA BIBREF5 integrates word embeddings into LDA by replacing the topic-word Dirichlet multinomial component with a mixture of a Dirichlet multinomial component and a word embedding component; Instead of generating word types (tokens), Gaussian LDA (GLDA) BIBREF4 directly generates word embeddings with the Gaussian distribution. Despite the exciting applications of the above models, their inference is usually less efficient due to the non-conjugacy and/or complicated model structures.Analysis of short text with topic models has been an active area with the development of social networks. Generally, there are two ways to deal with the sparsity problem in short texts, either using the intrinsic properties of short texts or leveraging meta information. For the first way, one popular approach is to aggregate short texts into pseudo-documents, for example, BIBREF17 introduces a model that aggregates tweets containing the same word; Recently, PTM BIBREF18 aggregates short texts into latent pseudo documents. Another approach is to assume one topic per short document, known as mixture of unigrams or Dirichlet Multinomial Mixture (DMM) such as BIBREF19 , BIBREF6 . For the second way, document meta information can be used to aggregate short texts, for example, BIBREF17 aggregates tweets by the corresponding authors and BIBREF20 shows that aggregating tweets by their hashtags yields superior performance over other aggregation methods. One closely related work to ours is the models that use word features for short texts. For example, BIBREF6 introduces an extension of GLDA on short texts which samples an indicator variable that chooses to generate either the type of a word or the embedding of a word and GPU-DMM BIBREF7 extends DMM with word semantic similarity obtained from embeddings for short texts. Although with improved performance there still exists challenges for existing models: (1) for aggregation-based models, it is usually hard to choose which meta information to use for aggregation; (2) the “single topic” assumption makes DMM models lose the flexibility to capture different topic ingredients of a document; and (3) the incorporation of meta information in the existing models is usually less efficient.To our knowledge, the attempts that jointly leverage document and word meta information are relatively rare. For example, meta information can be incorporated by first-order logic in Logit-LDA BIBREF21 and score functions in SC-LDA BIBREF22 . However, the first-order logic and score functions need to be defined for different kinds of meta information and the definition can be infeasible for incorporating both document and word meta information simultaneously.The MetaLDA Model	Given a corpus, LDA uses the same Dirichlet prior for all the per-document topic distributions and the same prior for all the per-topic word distributions BIBREF23 . While in MetaLDA, each document has a specific Dirichlet prior on its topic distribution, which is computed from the meta information of the document, and the parameters of the prior are estimated during training. Similarly, each topic has a specific Dirichlet prior computed from the word meta information. Here we elaborate our MetaLDA, in particular on how the meta information is incorporated. Hereafter, we will use labels as document meta information, unless otherwise stated.Given a collection of INLINEFORM0 documents INLINEFORM1 , MetaLDA generates document INLINEFORM2 with a mixture of INLINEFORM3 topics and each topic INLINEFORM4 is a distribution over the vocabulary with INLINEFORM5 tokens, denoted by INLINEFORM6 . For document INLINEFORM7 with INLINEFORM8 words, to generate the INLINEFORM9 ( INLINEFORM10 ) word INLINEFORM11 , we first sample a topic INLINEFORM12 from the document's topic distribution INLINEFORM13 , and then sample INLINEFORM14 from INLINEFORM15 . Assume the labels of document INLINEFORM16 are encoded in a binary vector INLINEFORM17 where INLINEFORM18 is the total number of unique labels. INLINEFORM19 indicates label INLINEFORM20 is active in document INLINEFORM21 and vice versa. Similarly, the INLINEFORM22 features of token INLINEFORM23 are stored ∂in a binary vector INLINEFORM24 . Therefore, the document and word meta information associated with INLINEFORM25 are stored in the matrix INLINEFORM26 and INLINEFORM27 respectively. Although MetaLDA incorporates binary features, categorical features and real-valued features can be converted into binary values with proper transformations such as discretisation and binarisation.Fig. FIGREF5 shows the graphical model of MetaLDA and the generative process is as following:where INLINEFORM0 , INLINEFORM1 , INLINEFORM2 are the gamma distribution, the Dirichlet distribution, and the categorical distribution respectively. INLINEFORM3 , INLINEFORM4 , and INLINEFORM5 are the hyper-parameters.To incorporate document labels, MetaLDA learns a specific Dirichlet prior over the topics for each document by using the label information. Specifically, the information of document INLINEFORM0 's labels is incorporated in INLINEFORM1 , the parameter of Dirichlet prior on INLINEFORM2 . As shown in Step UID12 , INLINEFORM3 is computed as a log linear combination of the labels INLINEFORM4 . Since INLINEFORM5 is binary, INLINEFORM6 is indeed the multiplication of INLINEFORM7 over all the active labels of document INLINEFORM8 , i.e., INLINEFORM9 . Drawn from the gamma distribution with mean 1, INLINEFORM10 controls the impact of label INLINEFORM11 on topic INLINEFORM12 . If label INLINEFORM13 has no or less impact on topic INLINEFORM14 , INLINEFORM15 is expected to be 1 or close to 1, and then INLINEFORM16 will have no or little influence on INLINEFORM17 and vice versa. The hyper-parameter INLINEFORM18 controls the variation of INLINEFORM19 . The incorporation of word features is analogous but in the parameter of the Dirichlet prior on the per-topic word distributions as shown in Step UID9 .The intuition of our way of incorporating meta information is: At the document level, if two documents have more labels in common, their Dirichlet parameter INLINEFORM0 will be more similar, resulting in more similar topic distributions INLINEFORM1 ; At the word level, if two words have similar features, their INLINEFORM2 in topic INLINEFORM3 will be similar and then we can expect that their INLINEFORM4 could be more or less the same. Finally, the two words will have similar probabilities of showing up in topic INLINEFORM5 . In other words, if a topic “prefers” a certain word, we expect that it will also prefer other words with similar features to that word. Moreover, at both the document and the word level, different labels/features may have different impact on the topics ( INLINEFORM6 / INLINEFORM7 ), which is automatically learnt in MetaLDA.Inference	Unlike most existing methods, our way of incorporating the meta information facilitates the derivation of an efficient Gibbs sampling algorithm. With two data augmentation techniques (i.e., the introduction of auxiliary variables), MetaLDA admits the local conjugacy and a close-form Gibbs sampling algorithm can be derived. Note that MetaLDA incorporates the meta information on the Dirichlet priors, so we can still use LDA's collapsed Gibbs sampling algorithm for the topic assignment INLINEFORM0 . Moreover, Step UID12 and UID9 show that one only needs to consider the non-zero entries of INLINEFORM1 and INLINEFORM2 in computing the full conditionals, which further reduces the inference complexity.Similar to LDA, the complete model likelihood (i.e., joint distribution) of MetaLDA is: +rCl+x* k=1K v=1V k,vnk,v d=1D k=1K d,kmd,k where INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 is the indicator function.Sampling λ l,k \lambda _{l,k}:	To sample INLINEFORM0 , we first marginalise out INLINEFORM1 in the right part of Eq. ( SECREF4 ) with the Dirichlet multinomial conjugacy: +rCl+x* d=1D (d,)(d, + md,)Gamma ratio 1 k=1K (d,k + md,k)(d,k)Gamma ratio 2 where INLINEFORM2 , INLINEFORM3 , and INLINEFORM4 is the gamma function. Gamma ratio 1 in Eq. ( SECREF17 ) can be augmented with a set of Beta random variables INLINEFORM5 as: +rCl+x* (d,)(d, + md,)Gamma ratio 1 qd qdd,-1 (1-qd)md,-1 where for each document INLINEFORM6 , INLINEFORM7 . Given a set of INLINEFORM8 for all the documents, Gamma ratio 1 can be approximated by the product of INLINEFORM9 , i.e., INLINEFORM10 .Gamma ratio 2 in Eq. ( SECREF17 ) is the Pochhammer symbol for a rising factorial, which can be augmented with an auxiliary variable INLINEFORM0 BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 as follows: +rCl+x* (d,k + md,k)(d,k)Gamma ratio 2 = td,k=0md,k Smd,ktd,k d,ktd,k where INLINEFORM1 indicates an unsigned Stirling number of the first kind. Gamma ratio 2 is a normalising constant for the probability of the number of tables in the Chinese Restaurant Process (CRP) BIBREF28 , INLINEFORM2 can be sampled by a CRP with INLINEFORM3 as the concentration and INLINEFORM4 as the number of customers: +rCl+x* td,k = i=1md,k Bern(d,kd,k+i) where INLINEFORM5 samples from the Bernoulli distribution. The complexity of sampling INLINEFORM6 by Eq. ( SECREF17 ) is INLINEFORM7 . For large INLINEFORM8 , as the standard deviation of INLINEFORM9 is INLINEFORM10 BIBREF28 , one can sample INLINEFORM11 in a small window around the current value in complexity INLINEFORM12 .By ignoring the terms unrelated to INLINEFORM0 , the augmentation of Eq. ( SECREF17 ) can be simplified to a single term INLINEFORM1 . With auxiliary variables now introduced, we simplify Eq. ( SECREF17 ) to: +rCl+x* d=1D k=1K qdd,k d,ktd,kReplacing INLINEFORM0 with INLINEFORM1 , we can get: +rCl+x* d=1D k=1K e- d,k 1qd l=1Ldoc k=1K l,kd=1D fd,l td,kRecall that all the document labels are binary and INLINEFORM0 is involved in computing INLINEFORM1 iff INLINEFORM2 . Extracting all the terms related to INLINEFORM3 in Eq. ( SECREF17 ), we get the marginal posterior of INLINEFORM4 : +rCl+x* e- l,k d=1:fd,l=1D 1qd d,kl,k l,k d=1D fd,l td,k where INLINEFORM5 is the value of INLINEFORM6 with INLINEFORM7 removed when INLINEFORM8 . With the data augmentation techniques, the posterior is transformed into a form that is conjugate to the gamma prior of INLINEFORM9 . Therefore, it is straightforward to yield the following sampling strategy for INLINEFORM10 : +rCl+x* l,k Ga( ', 1/”) ' = 0 + d=1: fd,l = 1D td,k ” = 1/0 - d=1:fd,l=1D d,kl,k qdWe can compute and cache the value of INLINEFORM0 first. After INLINEFORM1 is sampled, INLINEFORM2 can be updated by: +rCl+x* d,k d,k 'l,kl,k 1 d D : fd,l = 1 where INLINEFORM3 is the newly-sampled value of INLINEFORM4 .To sample/compute Eqs. ( SECREF17 )-( SECREF17 ), one only iterates over the documents where label INLINEFORM0 is active (i.e., INLINEFORM1 ). Thus, the sampling for all INLINEFORM2 takes INLINEFORM3 where INLINEFORM4 is the average number of documents where a label is active (i.e., the column-wise sparsity of INLINEFORM5 ). It is usually that INLINEFORM6 because if a label exists in nearly all the documents, it provides little discriminative information. This demonstrates how the sparsity of document meta information is leveraged. Moreover, sampling all the tables INLINEFORM7 takes INLINEFORM8 ( INLINEFORM9 is the total number of words in INLINEFORM10 ) which can be accelerated with the window sampling technique explained above.Sampling δ l ' ,k \delta _{l^{\prime },k}:	Since the derivation of sampling INLINEFORM0 is analogous to INLINEFORM1 , we directly give the sampling formulas: +rCl+x* l',k Ga( ', 1/”) ' = 0 + v=1: gv,l' = 1V t'k,v ” = 1/0 - q'k v=1:gv,l'=1V k,vl',k where the two auxiliary variables can be sampled by: INLINEFORM0 and INLINEFORM1 . Similarly, sampling all INLINEFORM2 takes INLINEFORM3 where INLINEFORM4 is the average number of tokens where a feature is active (i.e., the column-wise sparsity of INLINEFORM5 and usually INLINEFORM6 ) and sampling all the tables INLINEFORM7 takes INLINEFORM8 .Sampling topic z d,i z_{d,i}:	Given INLINEFORM0 and INLINEFORM1 , the collapsed Gibbs sampling of a new topic for a word INLINEFORM2 in MetaLDA is: +rCl+x* Przd,i = k (d,k + md,k) k,v + nk,vk, + nk, which is exactly the same to LDA.Experiments	In this section, we evaluate the proposed MetaLDA against several recent advances that also incorporate meta information on 6 real datasets including both regular and short texts. The goal of the experimental work is to evaluate the effectiveness and efficiency of MetaLDA's incorporation of document and word meta information both separately and jointly compared with other methods. We report the performance in terms of perplexity, topic coherence, and running time per iteration.Datasets	In the experiments, three regular text datasets and three short text datasets were used:Reuters is widely used corpus extracted from the Reuters-21578 dataset where documents without any labels are removed. There are 11,367 documents and 120 labels. Each document is associated with multiple labels. The vocabulary size is 8,817 and the average document length is 73.20NG, 20 Newsgroup, a widely used dataset consists of 18,846 news articles with 20 categories. The vocabulary size is 22,636 and the average document length is 108.NYT, New York Times is extracted from the documents in the category “Top/News/Health” in the New York Times Annotated Corpus. There are 52,521 documents and 545 unique labels. Each document is with multiple labels. The vocabulary contains 21,421 tokens and there are 442 words in a document on average.WS, Web Snippet, used in BIBREF7 , contains 12,237 web search snippets and each snippet belongs to one of 8 categories. The vocabulary contains 10,052 tokens and there are 15 words in one snippet on average.TMN, Tag My News, used in BIBREF5 , consists of 32,597 English RSS news snippets from Tag My News. With a title and a short description, each snippet belongs to one of 7 categories. There are 13,370 tokens in the vocabulary and the average length of a snippet is 18.AN, ABC News, is a collection of 12,495 short news descriptions and each one is in multiple of 194 categories. There are 4,255 tokens in the vocabulary and the average length of a description is 13.All the datasets were tokenised by Mallet and we removed the words that exist in less than 5 documents and more than 95% documents.Meta Information Settings	Document labels and word features. At the document level, the labels associated with documents in each dataset were used as the meta information. At the word level, we used a set of 100-dimensional binarised word embeddings as word features[fn-pre-process], which were obtained from the 50-dimensional GloVe word embeddings pre-trained on Wikipedia. To binarise word embeddings, we first adopted the following method similar to BIBREF29 : +rCl+x* g'v,j = {ll 1, if g”v,j > Mean+(g”v)-1, if g”v,j < Mean-(g”v)0, otherwise . where INLINEFORM0 is the original embedding vector for word INLINEFORM1 , INLINEFORM2 is the binarised value for INLINEFORM3 element of INLINEFORM4 , and INLINEFORM5 and INLINEFORM6 are the average value of all the positive elements and negative elements respectively. The insight is that we only consider features with strong opinions (i.e., large positive or negative value) on each dimension. To transform INLINEFORM7 to the final INLINEFORM8 , we use two binary bits to encode one dimension of INLINEFORM9 : the first bit is on if INLINEFORM10 and the second is on if INLINEFORM11 . Besides, MetaLDA can work with other word features such as semantic similarity as well.Default feature. Besides the labels/features associated with the datasets, a default label/feature for each document/word is introduced in MetaLDA, which is always equal to 1. The default can be interpreted as the bias term in INLINEFORM0 / INLINEFORM1 , which captures the information unrelated to the labels/features. While there are no document labels or word features, with the default, MetaLDA is equivalent in model to asymmetric-asymmetric LDA of BIBREF23 .Compared Models and Parameter Settings	We evaluate the performance of the following models:MetaLDA and its variants: the proposed model and its variants. Here we use MetaLDA to indicate the model considering both document labels and word features. Several variants of MetaLDA with document labels and word features separately were also studied, which are shown in Table TABREF32 . These variants differ in the method of estimating INLINEFORM0 and INLINEFORM1 . All the models listed in Table TABREF32 were implemented on top of Mallet. The hyper-parameters INLINEFORM2 and INLINEFORM3 were set to INLINEFORM4 .LDA BIBREF0 : the baseline model. The Mallet implementation of SparseLDA BIBREF30 is used.LLDA, Labelled LDA BIBREF11 and PLLDA, Partially Labelled LDA BIBREF9 : two models that make use of multiple document labels. The original implementation is used.DMR, LDA with Dirichlet Multinomial Regression BIBREF8 : a model that can use multiple document labels. The Mallet implementation of DMR based on SparseLDA was used. Following Mallet, we set the mean of INLINEFORM0 to 0.0 and set the variances of INLINEFORM1 for the default label and the document labels to 100.0 and 1.0 respectively.WF-LDA, Word Feature LDA BIBREF16 : a model with word features. We implemented it on top of Mallet and used the default settings in Mallet for the optimisation.LF-LDA, Latent Feature LDA BIBREF5 : a model that incorporates word embeddings. The original implementation was used. Following the paper, we used 1500 and 500 MCMC iterations for initialisation and sampling respectively and set INLINEFORM0 to 0.6, and used the original 50-dimensional GloVe word embeddings as word features.GPU-DMM, Generalized Pólya Urn DMM BIBREF7 : a model that incorporates word semantic similarity. The original implementation was used. The word similarity was generated from the distances of the word embeddings. Following the paper, we set the hyper-parameters INLINEFORM0 and INLINEFORM1 to 0.1 and 0.7 respectively, and the symmetric document Dirichlet prior to INLINEFORM2 .PTM, Pseudo document based Topic Model BIBREF18 : a model for short text analysis. The original implementation was used. Following the paper, we set the number of pseudo documents to 1000 and INLINEFORM0 to 0.1.All the models, except where noted, the symmetric parameters of the document and the topic Dirichlet priors were set to 0.1 and 0.01 respectively, and 2000 MCMC iterations are used to train the models.Perplexity Evaluation	Perplexity is a measure that is widely used BIBREF23 to evaluate the modelling accuracy of topic models. The lower the score, the higher the modelling accuracy. To compute perplexity, we randomly selected some documents in a dataset as the training set and the remaining as the test set. We first trained a topic model on the training set to get the word distributions of each topic INLINEFORM0 ( INLINEFORM1 ). Each test document INLINEFORM2 was split into two halves containing every first and every second words respectively. We then fixed the topics and trained the models on the first half to get the topic proportions ( INLINEFORM3 ) of test document INLINEFORM4 and compute perplexity for predicting the second half. In regard to MetaLDA, we fixed the matrices INLINEFORM5 and INLINEFORM6 output from the training procedure. On the first half of test document INLINEFORM7 , we computed the Dirichlet prior INLINEFORM8 with INLINEFORM9 and the labels INLINEFORM10 of test document INLINEFORM11 (See Step UID12 ), and then point-estimated INLINEFORM12 . We ran all the models 5 times with different random number seeds and report the average scores and the standard deviations.In testing, we may encounter words that never occur in the training documents (a.k.a., unseen words or out-of-vocabulary words). There are two strategies for handling unseen words for calculating perplexity on test documents: ignoring them or keeping them in computing the perplexity. Here we investigate both strategies:In this experiment, the perplexity is computed only on the words that appear in the training vocabulary. Here we used 80% documents in each dataset as the training set and the remaining 20% as the test set.Tables TABREF48 and TABREF49 show: the average perplexity scores with standard deviations for all the models. Note that: (1) The scores on AN with 150 and 200 topics are not reported due to overfitting observed in all the compared models. (2) Given the size of NYT, the scores of 200 and 500 topics are reported. (3) The number of latent topics in LLDA must equal to the number of document labels. (4) For PLLDA, we varied the number of topics per label from 5 to 50 (2 and 5 topics on NYT). The number of topics in PPLDA is the product of the numbers of labels and topics per label.The results show that MetaLDA outperformed all the competitors in terms of perplexity on nearly all the datasets, showing the benefit of using both document and word meta information. Specifically, we have the following remarks:By looking at the models using only the document-level meta information, we can see the significant improvement of these models over LDA, which indicates that document labels can play an important role in guiding topic modelling. Although the performance of the two variants of MetaLDA with document labels and DMR is comparable, our models runs much faster than DMR, which will be studied later in Section SECREF67 .It is interesting that PLLDA with 50 topics for each label has better perplexity than MetaLDA with 200 topics in the 20NG dataset. With the 20 unique labels, the actual number of topics in PLLDA is 1000. However, if 10 topics for each label in PLLDA are used, which is equivalent to 200 topics in MetaLDA, PLLDA is outperformed by MetaLDA significantly.At the word level, MetaLDA-def-wf performed the best among the models with word features only. Moreover, our model has obvious advantage in running speed (see Table TABREF66 ). Furthermore, comparing MetaLDA-def-wf with MetaLDA-def-def and MetaLDA-0.1-wf with LDA, we can see using the word features indeed improved perplexity.The scores show that the improvement gained by MetaLDA over LDA on the short text datasets is larger than that on the regular text datasets. This is as expected because meta information serves as complementary information in MetaLDA and can have more significant impact when the data is sparser.It can be observed that models usually gained improved perplexity, if INLINEFORM0 is sampled/optimised, in line with BIBREF23 .On the AN dataset, there is no statistically significant difference between MetaLDA and DMR. On NYT, a similar trend is observed: the improvement in the models with the document labels over LDA is obvious but not in the models with the word features. Given the number of the document labels (194 of AN and 545 of NYT), it is possible that the document labels already offer enough information and the word embeddings have little contribution in the two datasets.To test the hypothesis that the incorporation of meta information in MetaLDA can significantly improve the modelling accuracy in the cases where the corpus is sparse, we varied the proportion of documents used in training from 20% to 80% and used the remaining for testing. It is natural that when the proportion is small, the number of unseen words in testing documents will be large. Instead of simply excluding the unseen words in the previous experiments, here we compute the perplexity with unseen words for LDA, DMR, WF-LDA and the proposed MetaLDA. For perplexity calculation, INLINEFORM0 for each topic INLINEFORM1 and each token INLINEFORM2 in the test documents is needed. If INLINEFORM3 occurs in the training documents, INLINEFORM4 can be directly obtained. While if INLINEFORM5 is unseen, INLINEFORM6 can be estimated by the prior: INLINEFORM7 . For LDA and DMR which do not use word features, INLINEFORM8 ; For WF-LDA and MetaLDA which are with word features, INLINEFORM9 is computed with the features of the unseen token. Following Step UID9 , for MetaLDA, INLINEFORM10 .Figure FIGREF51 shows the perplexity scores on Reuters, 20NG, TMN and WS with 200, 200, 100 and 50 topics respectively. MetaLDA outperformed the other models significantly with a lower proportion of training documents and relatively higher proportion of unseen words. The gap between MetaLDA and the other three models increases while the training proportion decreases. It indicates that the meta information helps MetaLDA to achieve better modelling accuracy on predicting unseen words.Topic Coherence Evaluation	We further evaluate the semantic coherence of the words in a topic learnt by LDA, PTM, DMR, LF-LDA, WF-LDA, GPU-DMM and MetaLDA. Here we use the Normalised Pointwise Mutual Information (NPMI) BIBREF31 , BIBREF32 to calculate topic coherence score for topic INLINEFORM0 with top INLINEFORM1 words: INLINEFORM2 , where INLINEFORM3 is the probability of word INLINEFORM4 , and INLINEFORM5 is the joint probability of words INLINEFORM6 and INLINEFORM7 that co-occur together within a sliding window. Those probabilities were computed on an external large corpus, i.e., a 5.48GB Wikipedia dump in our experiments. The NPMI score of each topic in the experiments is calculated with top 10 words ( INLINEFORM8 ) by the Palmetto package. Again, we report the average scores and the standard deviations over 5 random runs.It is known that conventional topic models directly applied to short texts suffer from low quality topics, caused by the insufficient word co-occurrence information. Here we study whether or not the meta information helps MetaLDA improve topic quality, compared with other topic models that can also handle short texts. Table TABREF65 shows the NPMI scores on the three short text datasets. Higher scores indicate better topic coherence. All the models were trained with 100 topics. Besides the NPMI scores averaged over all the 100 topics, we also show the scores averaged over top 20 topics with highest NPMI, where “rubbish” topics are eliminated, following BIBREF22 . It is clear that MetaLDA performed significantly better than all the other models in WS and AN dataset in terms of NPMI, which indicates that MetaLDA can discover more meaningful topics with the document and word meta information. We would like to point out that on the TMN dataset, even though the average score of MetaLDA is still the best, the score of MetaLDA has overlapping with the others' in the standard deviation, which indicates the difference is not statistically significant.Running Time	In this section, we empirically study the efficiency of the models in term of per-iteration running time. The implementation details of our MetaLDA are as follows: (1) The SparseLDA framework BIBREF30 reduces the complexity of LDA to be sub-linear by breaking the conditional of LDA into three “buckets”, where the “smoothing only” bucket is cached for all the documents and the “document only” bucket is cached for all the tokens in a document. We adopted a similar strategy when implementing MetaLDA. When only the document meta information is used, the Dirichlet parameters INLINEFORM0 for different documents in MetaLDA are different and asymmetric. Therefore, the “smoothing only” bucket has to be computed for each document, but we can cache it for all the tokens, which still gives us a considerable reduction in computing complexity. However, when the word meta information is used, the SparseLDA framework no longer works in MetaLDA as the INLINEFORM1 parameters for each topic and each token are different. (2) By adapting the DistributedLDA framework BIBREF33 , our MetaLDA implementation runs in parallel with multiple threads, which makes MetaLDA able to handle larger document collections. The parallel implementation was used on the NYT dataset.The per-iteration running time of all the models is shown in Table TABREF66 . Note that: (1) On the Reuters and WS datasets, all the models ran with a single thread on a desktop PC with a 3.40GHz CPU and 16GB RAM. (2) Due to the size of NYT, we report the running time for the models that are able to run in parallel. All the parallelised models ran with 10 threads on a cluster with a 14-core 2.6GHz CPU and 128GB RAM. (3) All the models were implemented in JAVA. (4) As the models with meta information add extra complexity to LDA, the per-iteration running time of LDA can be treated as the lower bound.At the document level, both MetaLDA-df-0.01 and DMR use priors to incorporate the document meta information and both of them were implemented in the SparseLDA framework. However, our variant is about 6 to 8 times faster than DMR on the Reuters dataset and more than 10 times faster on the WS dataset. Moreover, it can be seen that the larger the number of topics, the faster our variant is over DMR. At the word level, similar patterns can be observed: our MetaLDA-0.1-wf ran significantly faster than WF-LDA and LF-LDA especially when more topics are used (20-30 times faster on WS). It is not surprising that GPU-DMM has comparable running speed with our variant, because only one topic is allowed for each document in GPU-DMM. With both document and word meta information, MetaLDA still ran several times faster than DMR, LF-LDA, and WF-LDA. On NYT with the parallel settings, MetaLDA maintains its efficiency advantage as well.Conclusion	In this paper, we have presented a topic modelling framework named MetaLDA that can efficiently incorporate document and word meta information. This gains a significant improvement over others in terms of perplexity and topic quality. With two data augmentation techniques, MetaLDA enjoys full local conjugacy, allowing efficient Gibbs sampling, demonstrated by superiority in the per-iteration running time. Furthermore, without losing generality, MetaLDA can work with both regular texts and short texts. The improvement of MetaLDA over other models that also use meta information is more remarkable, particularly when the word-occurrence information is insufficient. As MetaLDA takes a particular approach for incorporating meta information on topic models, it is possible to apply the same approach to other Bayesian probabilistic models, where Dirichlet priors are used. Moreover, it would be interesting to extend our method to use real-valued meta information directly, which is the subject of future work.Acknowledgement	Lan Du was partially supported by Chinese NSFC project under grant number 61402312. Gang Liu was partially supported by Chinese PostDoc Fund under grant number LBH-Q15031.","['Which real world datasets do they experiment on?', 'Which other models that incorporate meta information do they compare against?', 'How do they measure topic quality?', 'Which data augmentation techniques do they use?']","['Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable']"
53,"Addressee and Response Selection in Multi-Party Conversations with Speaker Interaction RNNs	In this paper, we study the problem of addressee and response selection in multi-party conversations. Understanding multi-party conversations is challenging because of complex speaker interactions: multiple speakers exchange messages with each other, playing different roles (sender, addressee, observer), and these roles vary across turns. To tackle this challenge, we propose the Speaker Interaction Recurrent Neural Network (SI-RNN). Whereas the previous state-of-the-art system updated speaker embeddings only for the sender, SI-RNN uses a novel dialog encoder to update speaker embeddings in a role-sensitive way. Additionally, unlike the previous work that selected the addressee and response separately, SI-RNN selects them jointly by viewing the task as a sequence prediction problem. Experimental results show that SI-RNN significantly improves the accuracy of addressee and response selection, particularly in complex conversations with many speakers and responses to distant messages many turns in the past.	Introduction	Real-world conversations often involve more than two speakers. In the Ubuntu Internet Relay Chat channel (IRC), for example, one user can initiate a discussion about an Ubuntu-related technical issue, and many other users can work together to solve the problem. Dialogs can have complex speaker interactions: at each turn, users play one of three roles (sender, addressee, observer), and those roles vary across turns.In this paper, we study the problem of addressee and response selection in multi-party conversations: given a responding speaker and a dialog context, the task is to select an addressee and a response from a set of candidates for the responding speaker. The task requires modeling multi-party conversations and can be directly used to build retrieval-based dialog systems BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 .The previous state-of-the-art Dynamic-RNN model from BIBREF4 ouchi-tsuboi:2016:EMNLP2016 maintains speaker embeddings to track each speaker status, which dynamically changes across time steps. It then produces the context embedding from the speaker embeddings and selects the addressee and response based on embedding similarity. However, this model updates only the sender embedding, not the embeddings of the addressee or observers, with the corresponding utterance, and it selects the addressee and response separately. In this way, it only models who says what and fails to capture addressee information. Experimental results show that the separate selection process often produces inconsistent addressee-response pairs.To solve these issues, we introduce the Speaker Interaction Recurrent Neural Network (SI-RNN). SI-RNN redesigns the dialog encoder by updating speaker embeddings in a role-sensitive way. Speaker embeddings are updated in different GRU-based units depending on their roles (sender, addressee, observer). Furthermore, we note that the addressee and response are mutually dependent and view the task as a joint prediction problem. Therefore, SI-RNN models the conditional probability (of addressee given the response and vice versa) and selects the addressee and response pair by maximizing the joint probability.On a public standard benchmark data set, SI-RNN significantly improves the addressee and response selection accuracy, particularly in complex conversations with many speakers and responses to distant messages many turns in the past. Our code and data set are available online.Related Work	We follow a data-driven approach to dialog systems. BIBREF5 singh1999reinforcement, BIBREF6 henderson2008hybrid, and BIBREF7 young2013pomdp optimize the dialog policy using Reinforcement Learning or the Partially Observable Markov Decision Process framework. In addition, BIBREF8 henderson2014second propose to use a predefined ontology as a logical representation for the information exchanged in the conversation. The dialog system can be divided into different modules, such as Natural Language Understanding BIBREF9 , BIBREF10 , Dialog State Tracking BIBREF11 , BIBREF12 , and Natural Language Generation BIBREF13 . Furthermore, BIBREF14 wen2016network and BIBREF15 bordes2017learning propose end-to-end trainable goal-oriented dialog systems.Recently, short text conversation has been popular. The system receives a short dialog context and generates a response using statistical machine translation or sequence-to-sequence networks BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 . In contrast to response generation, the retrieval-based approach uses a ranking model to select the highest scoring response from candidates BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . However, these models are single-turn responding machines and thus still are limited to short contexts with only two speakers. As for larger context, BIBREF22 lowe2015ubuntu propose the Next Utterance Classification (NUC) task for multi-turn two-party dialogs. BIBREF4 ouchi-tsuboi:2016:EMNLP2016 extend NUC to multi-party conversations by integrating the addressee detection problem. Since the data is text based, they use only textual information to predict addressees as opposed to relying on acoustic signals or gaze information in multimodal dialog systems BIBREF23 , BIBREF24 .Furthermore, several other papers are recently presented focusing on modeling role-specific information given the dialogue contexts BIBREF25 , BIBREF26 , BIBREF27 . For example, BIBREF25 meng2017towards combine content and temporal information to predict the utterance speaker. By contrast, our SIRNN explicitly utilizes the speaker interaction to maintain speaker embeddings and predicts the addressee and response by joint selection.Addressee and Response Selection	 BIBREF4 ouchi-tsuboi:2016:EMNLP2016 propose the addressee and response selection task for multi-party conversation. Given a responding speaker INLINEFORM0 and a dialog context INLINEFORM1 , the task is to select a response and an addressee. INLINEFORM2 is a list ordered by time step: INLINEFORM3 where INLINEFORM0 says INLINEFORM1 to INLINEFORM2 at time step INLINEFORM3 , and INLINEFORM4 is the total number of time steps before the response and addressee selection. The set of speakers appearing in INLINEFORM5 is denoted INLINEFORM6 . As for the output, the addressee is selected from INLINEFORM7 , and the response is selected from a set of candidates INLINEFORM8 . Here, INLINEFORM9 contains the ground-truth response and one or more false responses. We provide some examples in Table TABREF30 (Section SECREF6 ).Dynamic-RNN Model	In this section, we briefly review the state-of-the-art Dynamic-RNN model BIBREF4 , which our proposed model is based on. Dynamic-RNN solves the task in two phases: 1) the dialog encoder maintains a set of speaker embeddings to track each speaker status, which dynamically changes with time step INLINEFORM0 ; 2) then Dynamic-RNN produces the context embedding from the speaker embeddings and selects the addressee and response based on embedding similarity among context, speaker, and utterance.Figure FIGREF4 (Left) illustrates the dialog encoder in Dynamic-RNN on an example context. In this example, INLINEFORM0 says INLINEFORM1 to INLINEFORM2 , then INLINEFORM3 says INLINEFORM4 to INLINEFORM5 , and finally INLINEFORM6 says INLINEFORM7 to INLINEFORM8 . The context INLINEFORM9 will be: DISPLAYFORM0 with the set of speakers INLINEFORM0 .For a speaker INLINEFORM0 , the bold letter INLINEFORM1 denotes its embedding at time step INLINEFORM2 . Speaker embeddings are initialized as zero vectors and updated recurrently as hidden states of GRUs BIBREF28 , BIBREF29 . Specifically, for each time step INLINEFORM3 with the sender INLINEFORM4 and the utterance INLINEFORM5 , the sender embedding INLINEFORM6 is updated recurrently from the utterance: INLINEFORM7 where INLINEFORM0 is the embedding for utterance INLINEFORM1 . Other speaker embeddings are updated from INLINEFORM2 . The speaker embeddings are updated until time step INLINEFORM3 .To summarize the whole dialog context INLINEFORM0 , the model applies element-wise max pooling over all the speaker embeddings to get the context embedding INLINEFORM1 : DISPLAYFORM0 The probability of an addressee and a response being the ground truth is calculated based on embedding similarity. To be specific, for addressee selection, the model compares the candidate speaker INLINEFORM0 , the dialog context INLINEFORM1 , and the responding speaker INLINEFORM2 : DISPLAYFORM0 where INLINEFORM0 is the final speaker embedding for the responding speaker INLINEFORM1 , INLINEFORM2 is the final speaker embedding for the candidate addressee INLINEFORM3 , INLINEFORM4 is the logistic sigmoid function, INLINEFORM5 is the row-wise concatenation operator, and INLINEFORM6 is a learnable parameter. Similarly, for response selection, DISPLAYFORM0 where INLINEFORM0 is the embedding for the candidate response INLINEFORM1 , and INLINEFORM2 is a learnable parameter.The model is trained end-to-end to minimize a joint cross-entropy loss for the addressee selection and the response selection with equal weights. At test time, the addressee and the response are separately selected to maximize the probability in Eq EQREF12 and Eq EQREF13 .Speaker Interaction RNN	While Dynamic-RNN can track the speaker status by capturing who says what in multi-party conversation, there are still some issues. First, at each time step, only the sender embedding is updated from the utterance. Therefore, other speakers are blind to what is being said, and the model fails to capture addressee information. Second, while the addressee and response are mutually dependent, Dynamic-RNN selects them independently. Consider a case where the responding speaker is talking to two other speakers in separate conversation threads. The choice of addressee is likely to be either of the two speakers, but the choice is much less ambiguous if the correct response is given, and vice versa. Dynamic-RNN often produces inconsistent addressee-response pairs due to the separate selection. See Table TABREF30 for examples.In contrast to Dynamic-RNN, the dialog encoder in SI-RNN updates embeddings for all the speakers besides the sender at each time step. Speaker embeddings are updated depending on their roles: the update of the sender is different from the addressee, which is different from the observers. Furthermore, the update of a speaker embedding is not only from the utterance, but also from other speakers. These are achieved by designing variations of GRUs for different roles. Finally, SI-RNN selects the addressee and response jointly by maximizing the joint probability.[t] Dialog Encoder in SI-RNN [1] Input INLINEFORM0 : INLINEFORM1 INLINEFORM2 where INLINEFORM3 // Initialize speaker embeddings INLINEFORM4 INLINEFORM5 //Update speaker embeddings INLINEFORM6 // Update sender, addressee, observers INLINEFORM7 INLINEFORM8 INLINEFORM9 // Compute utterance embedding INLINEFORM10 INLINEFORM11 // Update sender embedding INLINEFORM12 // Update addressee embedding INLINEFORM13 // Update observer embeddings INLINEFORM14 INLINEFORM15 // Return final speaker embeddings Output INLINEFORM16 for INLINEFORM17 Utterance Encoder	To encode an utterance INLINEFORM0 of INLINEFORM1 words, we use a RNN with Gated Recurrent Units BIBREF28 , BIBREF29 : INLINEFORM2 where INLINEFORM0 is the word embedding for INLINEFORM1 , and INLINEFORM2 is the INLINEFORM3 hidden state. INLINEFORM4 is initialized as a zero vector, and the utterance embedding is the last hidden state, i.e. INLINEFORM5 .Dialog Encoder	Figure FIGREF4 (Right) shows how SI-RNN encodes the example in Eq EQREF9 . Unlike Dynamic-RNN, SI-RNN updates all speaker embeddings in a role-sensitive manner. For example, at the first time step when INLINEFORM0 says INLINEFORM1 to INLINEFORM2 , Dynamic-RNN only updates INLINEFORM3 using INLINEFORM4 , while other speakers are updated using INLINEFORM5 . In contrast, SI-RNN updates each speaker status with different units: INLINEFORM6 updates the sender embedding INLINEFORM7 from the utterance embedding INLINEFORM8 and the addressee embedding INLINEFORM9 ; INLINEFORM10 updates the addressee embedding INLINEFORM11 from INLINEFORM12 and INLINEFORM13 ; INLINEFORM14 updates the observer embedding INLINEFORM15 from INLINEFORM16 .Algorithm SECREF4 gives a formal definition of the dialog encoder in SI-RNN. The dialog encoder is a function that takes as input a dialog context INLINEFORM0 (lines 1-5) and returns speaker embeddings at the final time step (lines 28-30). Speaker embeddings are initialized as INLINEFORM1 -dimensional zero vectors (lines 6-9). Speaker embeddings are updated by iterating over each line in the context (lines 10-27).Role-Sensitive Update	In this subsection, we explain in detail how INLINEFORM0 / INLINEFORM1 / INLINEFORM2 update speaker embeddings according to their roles at each time step (Algorithm SECREF4 lines 19-26).As shown in Figure FIGREF17 , INLINEFORM0 / INLINEFORM1 / INLINEFORM2 are all GRU-based units. INLINEFORM3 updates the sender embedding from the previous sender embedding INLINEFORM4 , the previous addressee embedding INLINEFORM5 , and the utterance embedding INLINEFORM6 : INLINEFORM7 The update, as illustrated in the upper part of Figure FIGREF17 , is controlled by three gates. The INLINEFORM0 gate controls the previous sender embedding INLINEFORM1 , and INLINEFORM2 controls the previous addressee embedding INLINEFORM3 . Those two gated interactions together produce the sender embedding proposal INLINEFORM4 . Finally, the update gate INLINEFORM5 combines the proposal INLINEFORM6 and the previous sender embedding INLINEFORM7 to update the sender embedding INLINEFORM8 . The computations in INLINEFORM9 (including gates INLINEFORM10 , INLINEFORM11 , INLINEFORM12 , the proposal embedding INLINEFORM13 , and the final updated embedding INLINEFORM14 ) are formulated as: INLINEFORM15  where INLINEFORM0 INLINEFORM1 are learnable parameters. INLINEFORM2 uses the same formulation with a different set of parameters, as illustrated in the middle of Figure FIGREF17 . In addition, we update the observer embeddings from the utterance. INLINEFORM3 is implemented as the traditional GRU unit in the lower part of Figure FIGREF17 . Note that the parameters in INLINEFORM4 / INLINEFORM5 / INLINEFORM6 are not shared. This allows SI-RNN to learn role-dependent features to control speaker embedding updates. The formulations of INLINEFORM7 and INLINEFORM8 are similar.Joint Selection	The dialog encoder takes the dialog context INLINEFORM0 as input and returns speaker embeddings at the final time step, INLINEFORM1 . Recall from Section SECREF7 that Dynamic-RNN produces the context embedding INLINEFORM2 using Eq EQREF11 and then selects the addressee and response separately using Eq EQREF12 and Eq EQREF13 .In contrast, SI-RNN performs addressee and response selection jointly: the response is dependent on the addressee and vice versa. Therefore, we view the task as a sequence prediction process: given the context and responding speaker, we first predict the addressee, and then predict the response given the addressee. (We also use the reversed prediction order as in Eq EQREF21 .)In addition to Eq EQREF12 and Eq EQREF13 , SI-RNN is also trained to model the conditional probability as follows. To predict the addressee, we calculate the probability of the candidate speaker INLINEFORM0 to be the ground-truth given the ground-truth response INLINEFORM1 (available during training time): DISPLAYFORM0 The key difference from Eq EQREF12 is that Eq EQREF19 is conditioned on the correct response INLINEFORM0 with embedding INLINEFORM1 . Similarly, for response selection, we calculate the probability of a candidate response INLINEFORM2 given the ground-truth addressee INLINEFORM3 : DISPLAYFORM0 At test time, SI-RNN selects the addressee-response pair from INLINEFORM0 to maximize the joint probability INLINEFORM1 : DISPLAYFORM0  In Eq EQREF21 , we decompose the joint probability into two terms: the first term selects the response given the context, and then selects the addressee given the context and the selected response; the second term selects the addressee and response in the reversed order.Experimental Setup	Data Set. We use the Ubuntu Multiparty Conversation Corpus BIBREF4 and summarize the data statistics in Table TABREF24 . same The whole data set (including the Train/Dev/Test split and the false response candidates) is publicly available. The data set is built from the Ubuntu IRC chat room where a number of users discuss Ubuntu-related technical issues. The log is organized as one file per day corresponding to a document INLINEFORM0 . Each document consists of (Time, SenderID, Utterance) lines. If users explicitly mention addressees at the beginning of the utterance, the addresseeID is extracted. Then a sample, namely a unit of input (the dialog context and the current sender) and output (the addressee and response prediction) for the task, is created to predict the ground-truth addressee and response of this line. Note that samples are created only when the addressee is explicitly mentioned for clear, unambiguous ground-truth labels. False response candidates are randomly chosen from all other utterances within the same document. Therefore, distractors are likely from the same sub-conversation or even from the same sender but at different time steps. This makes it harder than BIBREF22 lowe2015ubuntu where distractors are randomly chosen from all documents. If no addressee is explicitly mentioned, the addressee is left blank and the line is marked as a part of the context.Baselines. Apart from Dynamic-RNN, we also include several other baselines. Recent+TF-IDF always selects the most recent speaker (except the responding speaker INLINEFORM0 ) as the addressee and chooses the response to maximize the tf-idf cosine similarity with the context. We improve it by using a slightly different addressee selection heuristic (Direct-Recent+TF-IDF): select the most recent speaker that directly talks to INLINEFORM1 by an explicit addressee mention. We select from the previous 15 utterances, which is the longest context among all the experiments. This works much better when there are multiple concurrent sub-conversations, and INLINEFORM2 responds to a distant message in the context. We also include another GRU-based model Static-RNN from BIBREF4 ouchi-tsuboi:2016:EMNLP2016. Unlike Dynamic-RNN, speaker embeddings in Static-RNN are based on the order of speakers and are fixed. Furthermore, inspired by BIBREF30 zhou16multi and BIBREF19 serban2016building, we implement Static-Hier-RNN, a hierarchical version of Static-RNN. It first builds utterance embeddings from words and then uses high-level RNNs to process utterance embeddings.Implementation Details For a fair comparison, we follow the hyperparameters from BIBREF4 ouchi-tsuboi:2016:EMNLP2016, which are chosen based on the validation data set. We take a maximum of 20 words for each utterance. We use 300-dimensional GloVe word vectors, which are fixed during training. SI-RNN uses 50-dimensional vectors for both speaker embeddings and hidden states. Model parameters are initialized with a uniform distribution between -0.01 and 0.01. We set the mini-batch size to 128. The joint cross-entropy loss function with 0.001 L2 weight decay is minimized by Adam BIBREF31 . The training is stopped early if the validation accuracy is not improved for 5 consecutive epochs. All experiments are performed on a single GTX Titan X GPU. The maximum number of epochs is 30, and most models converge within 10 epochs.Results and Discussion	For fair and meaningful quantitative comparisons, we follow BIBREF4 ouchi-tsuboi:2016:EMNLP2016's evaluation protocols. SI-RNN improves the overall accuracy on the addressee and response selection task. Two ablation experiments further analyze the contribution of role-sensitive units and joint selection respectively. We then confirm the robustness of SI-RNN with the number of speakers and distant responses. Finally, in a case study we discuss how SI-RNN handles complex conversations by either engaging in a new sub-conversation or responding to a distant message.Overall Result. As shown in Table TABREF23 , SI-RNN significantly improves upon the previous state-of-the-art. In particular, addressee selection (ADR) benefits most, with different number of candidate responses (denoted as RES-CAND): around 12% in RES-CAND INLINEFORM0 and more than 10% in RES-CAND INLINEFORM1 . Response selection (RES) is also improved, suggesting role-sensitive GRUs and joint selection are helpful for response selection as well. The improvement is more obvious with more candidate responses (2% in RES-CAND INLINEFORM2 and 4% in RES-CAND INLINEFORM3 ). These together result in significantly better accuracy on the ADR-RES metric as well.Ablation Study. We show an ablation study in the last rows of Table TABREF23 . First, we share the parameters of INLINEFORM0 / INLINEFORM1 / INLINEFORM2 . The accuracy decreases significantly, indicating that it is crucial to learn role-sensitive units to update speaker embeddings. Second, to examine our joint selection, we fall back to selecting the addressee and response separately, as in Dynamic-RNN. We find that joint selection improves ADR and RES individually, and it is particularly helpful for pair selection ADR-RES.Number of Speakers. Numerous speakers create complex dialogs and increased candidate addressee, thus the task becomes more challenging. In Figure FIGREF27 (Upper), we investigate how ADR accuracy changes with the number of speakers in the context of length 15, corresponding to the rows with T=15 in Table TABREF23 . Recent+TF-IDF always chooses the most recent speaker and the accuracy drops dramatically as the number of speakers increases. Direct-Recent+TF-IDF shows better performance, and Dynamic-RNNis marginally better. SI-RNN is much more robust and remains above 70% accuracy across all bins. The advantage is more obvious for bins with more speakers.Addressing Distance. Addressing distance is the time difference from the responding speaker to the ground-truth addressee. As the histogram in Figure FIGREF27 (Lower) shows, while the majority of responses target the most recent speaker, many responses go back five or more time steps. It is important to note that for those distant responses, Dynamic-RNN sees a clear performance decrease, even worse than Direct-Recent+TF-IDF. In contrast, SI-RNN handles distant responses much more accurately.sameCase Study. Examples in Table TABREF30 show how SI-RNN can handle complex multi-party conversations by selecting from 10 candidate responses. In both examples, the responding speakers participate in two or more concurrent sub-conversations with other speakers.Example (a) demonstrates the ability of SI-RNN to engage in a new sub-conversation. The responding speaker “wafflejock"" is originally involved in two sub-conversations: the sub-conversation 1 with “codepython"", and the ubuntu installation issue with “theoletom"". While it is reasonable to address “codepython"" and “theoletom"", the responses from other baselines are not helpful to solve corresponding issues. TF-IDF prefers the response with the “install"" key-word, yet the response is repetitive and not helpful. Dynamic-RNN selects an irrelevant response to “codepython"". SI-RNN chooses to engage in a new sub-conversation by suggesting a solution to “releaf"" about Ubuntu dedicated laptops.Example (b) shows the advantage of SI-RNN in responding to a distant message. The responding speaker “nicomachus"" is actively engaged with “VeryBewitching"" in the sub-conversation 1 and is also loosely involved in the sub-conversation 2: “chingao"" mentions “nicomachus"" in the most recent utterance. SI-RNN remembers the distant sub-conversation 1 and responds to “VeryBewitching"" with a detailed answer. Direct-Recent+TF-IDF selects the ground-truth addressee because “VeryBewitching"" talks to “nicomachus"", but the response is not helpful. Dynamic-RNN is biased to the recent speaker “chingao"", yet the response is not relevant.Conclusion	SI-RNN jointly models who says what to whom by updating speaker embeddings in a role-sensitive way. It provides state-of-the-art addressee and response selection, which can instantly help retrieval-based dialog systems. In the future, we also consider using SI-RNN to extract sub-conversations in the unlabeled conversation corpus and provide a large-scale disentangled multi-party conversation data set.Acknowledgements	We thank the members of the UMichigan-IBM Sapphire Project and all the reviewers for their helpful feedback. This material is based in part upon work supported by IBM under contract 4915012629. Any opinions, findings, conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of IBM.","['what is the average number of speakers in the dataset?', 'by how much is accuracy improved?', 'what are the previous state of the art systems?']","['26.8', 'In addressee selection around 12% in RES-CAND = 2 and 10% in RES-CAND = 10, in candidate responses around 2% in RES-CAND = 2 and 4% in RES-CAND = 10', 'Unacceptable']"
54,"Multi-Module System for Open Domain Chinese Question Answering over Knowledge Base	For the task of open domain Knowledge Based Question Answering in CCKS2019, we propose a method combining information retrieval and semantic parsing. This multi-module system extracts the topic entity and the most related relation predicate from a question and transforms it into a Sparql query statement. Our method obtained the F1 score of 70.45% on the test data.	Introduction	We introduce an open domain question answering system based on Chinese knowledge graph in this paper. We analyze the questions and find that most of the answers to the questions are within two hops. Therefore, we only solve the problem within two hops to reduce the complexity of the system. The system consists of a topic entity selection module, a relationship recognition module and an answer selection module. Firstly, we construct a scoring mechanism to select the core entity of the question; Then we score the relationship in the two-hop subgraph of the topic entity; Finally, we build a classier to judge whether a question is simple or complicated, so that we can choose the final relationship and generate sparql query.Related Work	There are two main approaches in Knowledge Graph based Question Answering(KBQA) : semantic parsing based and retrieval based.Semantic Parsing based approach is a linguistic method that transforms natural language into logic forms and queries them in the knowledge graph through corresponding semantic representations, such as lambda-Caculus, to arrive at an answer. Semantic parsing-based methods, including semantic parsing based on Lambda Dependency-Based Compositional Semantics (Lambda-DCS) BIBREF0, BIBREF1, semantic parsing based on Combinatory Categorical Grammars (CCG), semantic parsing based on Neural Machine Translation(NMT), and semantic parsing based on deep learning BIBREF2 released by Microsoft in 2015.Retrieval based approach could be regarded as a sorting algorithm for the answer: given the input question Q and the knowledge graph KB, by scoring and sorting the entities in the KB, the entity or entity set with the highest score is selected as the answer. It mainly includes feature-based method BIBREF3, extracting features from the input question Q and the answer candidate A, generating feature vectors, and training the classifier; vector-representation based method BIBREF4, the input question Q and the answer candidate A are represented as two vectors (distributed embedding) respectively, and vector distance is calculated for scoring; CNN network based method BIBREF5, the feature extraction is performed by a convolutional neural network; Gated-GNN based methodBIBREF6, etc.We combine the above two methods. On the one hand, we use the retrieve based method to sort KB relationships and entities, and on the other hand, we use the most related relationship and entity to generate the sparql statement to query the final answer.The Proposed Model	Our model is mainly divided into three parts, namely Topic Entity Recognition, Relation Recognition and Answer Selection. The overall model is shown in Figure FIGREF1.After entering a question, the model first finds the topic entity in the sentence. Here we used the Entity-mention file provided by the contest organizer and some external tools such as paddle-paddle. Then in the relationship recognition module, the relationship of the question (also called the predicate) is found by extracting the subgraph of the topic entity in the knowledge graph. The ranking of all relationships is obtained by a similarity scoring model. Finally, in the answer selection module, according to the simple-complex problem classifier and some rules, the final answer is obtained.The Proposed Model ::: Topic Entity Recognition	Topic entity is the core entity of the corresponding KB query. Since most of the problems are within two hops, so we only need to find a core entity and then search for its subgraph to find the answer of the question. The extraction of the topic entity is divided into two parts. Firstly, all the entities are extracted through the NER module. Then, the entities are scored by some rules and the entity with the highest score is selected as the topic entity.The Proposed Model ::: Topic Entity Recognition ::: NER Module	We chose the pre-trained NER models released by Baidu Cloud and Paddle-paddle. These two models have their own preponderance in entity recognition, so we decide to combine the results of both. Unlike the common NER, we don't just extract the entities labeled 'LOC', 'ORG', 'PER' and 'TIME'. Since the entity in question is not always one of the four, we also extract 'n' (noun), 'nr' (person name), 'ns' (place name), 'nt' (institution name), 'nw' (work name). Besides, considering that there are many financial problems in this data set, we have compiled a small dictionary of vocabulary in the financial field on the public website in order to ensure that the model has strong adaptability.The Proposed Model ::: Topic Entity Recognition ::: Stop-words	In Chinese, many relationships exist in the form of nouns, so they are extracted in the NER module, such as `creator' and `inventor'. However, our topic entity should not use relationship as the core node, so we maintain a stop-words dictionary, which records the relationship in the form of nouns. Considering that some of the words also have the possibility of being a real entity, we set a rule that when a sentence has not been checked by any entity after passing the previous NER module , we reconsider detecting the words in the dictionary.The Proposed Model ::: Topic Entity Recognition ::: Scoring Module	After the NER module we get a series of entities, we select all possible entities according to the correspondence in the entity-mention dictionary. If an entity does not appear in the dictionary, we further query it in the knowledge base, and if it still does not, discard it. Then we constructed a scoring strategy for all the candidate entities generated above. The specific score rules are as follows:Score1 The Length of Entity. An entity with a longer length is usually given a higher score.Score2 The Out-degree of Entity in the KB. An entity with higher out-degree is more likely to be the topic entity. Meanwhile, it is more efficient to query the nodes pointed to other entity in the knowledge base than to be pointed.Score3 The Distance Between the Entity and Interrogative Word. We define ['who','what','where','how','how much','how many'] as interrogative words. An entity gets higher score if more close to the above words.Score4 Char Overlap between Entity and Question. The more overlap chars shared between entity and question, the bigger probability that the entity be a topic entity.Score5 Word Overlap between Entity-mention and Question. If an entity and its corresponding mention have more overlap with the problem, it is more likely to be the topic entity.Score6 NER Label of Entity. If an entity is recognized as special noun like 'ORG', it has more probability to be a topic entity than simple noun. Additionally, if an entity is a person name or financial related, we assign it higher score.Score7 Similarity between Entity-mention and Question. We simply fine-tune bert model to measure how well an entity matches a question. Matching rate is the corresponding score.After calculating the above scores, we normalize and add them, the entity with the highest score is selected as the topic entity. Figure FIGREF6 shows the flow of extraction. The scores of the first two entities are higher because they capture more keywords in the question and the reason why the first entity is higher than the second is that its out-degree is much higher, which means that it is more well-known.The Proposed Model ::: Relation Recognition	The relationship in the question generally refers to the predicate in a sentence, and also the relationship between two entities in the knowledge base. The correct relationship should be the relationship between the topic entity and the answer entity.The Proposed Model ::: Relation Recognition ::: Subgraph Extraction	We mainly use retrieval based method in recognizing relationships. By observing the dataset, we found that among the 2,298 questions in the training set, there were 1,160 questions that could be solved through one-hop in the knowledge graph, and 912 questions through two-hop, accounting for 90.17% of all data. Thus, We only consider the problem within two-hop in later experiments.For each topic entity, we extract the subgraphs within its two-hop in the knowledge base, and each relationships in the subgraph could be the target relationship.The Proposed Model ::: Relation Recognition ::: Scoring Module	After getting all the candidate relationships, we constructed a scoring strategy. The specific score rules are as follows:Score_relation_similarity Similarity between Relation and Question. We use the BERT-Base, Chinese BIBREF7 pre-trained model to initialize word vectors. Then fine-tune it through a similarity model BIBREF8. Question and relationship are first fed into the word embedding layer, mapping words to a fixed-dimensional word vector. It is then sent to the biLSTM layer, whose output is averaged at each step. The obtained result is sent to a fully connected layer, and finally the semantic embedding is obtained. The cosine similarity of the two semantic embeddings is calculated to represent the similarity between the question and relation.Score_object_similarity Similarity between Object and Question. This model is similar to the above relation similarity model. Although the similarity between relation and question is the main indicator of evaluation, the similarity between object and question also plays a supporting role. For example, the question ""What TV series did actor A and actor B play together?"", if actor A is recognized as the topic entity, and actor B appears as an object of a two-hop relation(actor A ==act in==> TV serie X <==act in== actor B), the score of this relation chain should be higher than other relations. An example is shown in figure FIGREF10.Score_char_overlap Char Overlap between Relation and Question. The more overlap chars shared between entity and question, the bigger probability that the relation be a correct relation. This score is added primarily to prevent the model from relying too heavily on the bert similarity model.The final score of the relationship is the weighted addition of the above scores.The Proposed Model ::: Answer Selection	After getting the topic entity and relationship scoring of the question, we need to generate the final sparql query and find the answer from the knowledge graph.The Proposed Model ::: Answer Selection ::: simple-complex Question Classifier	As mentioned before, we learned a classifier since we only consider one-hop (simple) and two-hop (complex) problems. Before performing the final sparql generation, we use the classifier to determine whether it is a simple or complex problem. If it is a simple problem, select the highest one-hop relationship as the answer. Conversely, if it is a complex problem, select the highest two-hop relationship.We use a bert classifier to implement this classification model.The Proposed Model ::: Answer Selection ::: Sparql Generation	We need to generate the corresponding sparql statement based on the topic entity and the most relevant relation, and find the result from the knowledge graph.We consider the relationship of the five structures shown in Figure FIGREF14. For the question ""What is the representative work of Monica Bellucci?"", we found the topic entity <Monica Bellucci>, it is classified as a simple one-hop question, the highest score relation is <representative work>, and the sparql statement is ""select ?x where <Monica Bellucci> <representative work> ?x"".In addition, we have added some other rules, such as:(1) When there is a gender-related vocabulary in the question, add “?z <gender> <male/female>” to sparql as restriction.(2) When time appears in the question, it is unified into the format of “YYYY-MM-DD” (consistent with the knowledge graph).(3) When the two-hop object appears in the sentence, select the intermediate result ?y is the final result. Still consider question “What TV series did actor A and actor B play together?”, the highest score is “actor A ==act in==> TV serie X <==act in== actor B”, but ?y(TV serie X ) rather than ?x(actor B) is the correct result. Thus, the sparql should be “select ?y where <actor A> <act in> ?y. <actor B> <act in> ?y.”Experiments and Results ::: Dataset	We use CCKS 2019 dataset to evaluate our approach. The dataset is published by the CCKS 2019 task 6, which includes a knowledge base, an entity-mention file, and Q&A pairs for training, validation, and testing. The knowledge base has more than 30 million triples (We use gstore to manage the knowledge base), the training set has 2298 question and answer pairs, the dev set has 766 questions, and the test set has 766 questions. Since we don't have the correct answer to the dev set, in order to evaluate the model performance during the experiments, we randomly selected 100 Q&A pairs from the training set as the real development set.Experiments and Results ::: Topic Entity Recognition	We test the effects of different scores in the scoring mechanism. Table TABREF18 shows some results. Baseline refers to the sum of the scores excluded similarity score and out-degree score. We find that the impact of out-degree score is mainly reflected in confused entities. As shown in Figure FIGREF6, there are actually two singers named 'Wangfei'. At this time, we need to combine common sense to select the more likely one. Generally speaking, the more well-known entities correspond to more information (that is, the introduction more detailed). The score of similarity gives a higher weight to the entity more relevant to the sentence.Experiments and Results ::: Relation Recognition	When constructing the training set for the similarity scoring model, since the positive samples are significantly smaller than the negative samples, we performed 5 oversamplings on the positive samples and 5 randomly selected training sets from all negative samples of each data. The positive and negative relationship of the one-hop relationship is easier to understand. The positive sample of the two-hop relationship is the concatenation of the correct one-hop and two-hop relationship, and the wrong relationship is not put into the negative sample because it may interfere with the one-hop relationship score. In addition, entities in all questions are replaced with <e> in order to reduce entity interference. We tried several common models, the results of relation scoring model is shown in table TABREF20. Bert model has the highest accuracy of 95.7%.Experiments and Results ::: Answer Selection	The simple-complex model is a simple binary classifier, it has an accuracy rate of 91%. Final Answer Selection results are shown in table TABREF22. We evaluated the model using accuracy indicator. The baseline model, which is the bert relation similarity model mentioned above, has an accuracy of 68% over 100 dev data. After adding the object similarity score and sparql rules, the accuracy is increased to 75%.Since the correct answer to test set has not yet been released, we are unable to verify the accuracy of each model. According to the final version submitted on the website, our model has a F1-score of 70.45% in test set.Conclusion	We introduce an open domain question answering system based on Chinese knowledge graph in this paper. The system consists of a topic entity selection module, a relationship recognition module and an answer selection module. Our method obtained an F1 score of 70.45% on the test data.","['What is the state-of-the-art model in this task?', 'How does this result compare to other methods KB QA in CCKS2019?']","['Unacceptable', 'Unacceptable']"
55,"Analyzing ASR pretraining for low-resource speech-to-text translation	Previous work has shown that for low-resource source languages, automatic speech-to-text translation (AST) can be improved by pretraining an end-to-end model on automatic speech recognition (ASR) data from a high-resource language. However, it is not clear what factors --e.g., language relatedness or size of the pretraining data-- yield the biggest improvements, or whether pretraining can be effectively combined with other methods such as data augmentation. Here, we experiment with pretraining on datasets of varying sizes, including languages related and unrelated to the AST source language. We find that the best predictor of final AST performance is the word error rate of the pretrained ASR model, and that differences in ASR/AST performance correlate with how phonetic information is encoded in the later RNN layers of our model. We also show that pretraining and data augmentation yield complementary benefits for AST.	Introduction	Low-resource automatic speech-to-text translation (AST) has recently gained traction as a way to bring NLP tools to under-represented languages. An end-to-end approach BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6 is particularly appealing for source languages with no written form, or for endangered languages where translations into a high-resource language may be easier to collect than transcriptions BIBREF7. However, building high-quality end-to-end AST with little parallel data is challenging, and has led researchers to explore how other sources of data could be used to help.A number of methods have been investigated. Several of these use transcribed source language audio and/or translated source language text in a multitask learning scenario BIBREF8, BIBREF3, BIBREF5 or to pre-train parts of the model before fine-tuning on the end-to-end AST task BIBREF3. Others assume, as we do here, that no additional source language resources are available, in which case transfer learning using data from language(s) other than the source language is a good option. In particular, several researchers have shown that low-resource AST can be improved by pretraining on an ASR task in some other language, then transferring the encoder parameters to initialize the AST model. For example, Bansal et al. BIBREF4 showed that pre-training on either English or French ASR improved their Spanish-English AST system (trained on 20 hours of parallel data) and Tian BIBREF9 got improvements on an 8-hour Swahili-English AST dataset using English ASR pretraining.Overall these results show that pretraining helps, but leave open the question of what factors affect the degree of improvement. For example, does language relatedness play a role, or simply the amount of pretraining data? Bansal et al. showed bigger AST gains as the amount of English pretraining data increased from 20 to 300 hours, and also found a slightly larger improvement when pretraining on 20 hours of English versus 20 hours of French, but they pointed out that the Spanish data contains many English code-switched words, which could explain the latter result. In related work on multilingual pretraining for low-resource ASR, Adams et al. BIBREF10 showed that pre-training on more languages helps, but it is not clear whether the improvement is due to including more languages, or just more data.To begin to tease apart these issues, we focus here on monolingual pretraining for low-resource AST, and investigate two questions. First, can we predict what sort of pretraining data is best for a particular AST task? Does it matter if the pretraining language is related to the AST source language (defined here as part of the same language family, since phonetic similarity is difficult to measure), or is the amount of pretraining data (or some other factor) more important? Second, can pretraining be effectively combined with other methods, such as data augmentation, in order to further improve AST results?To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. BIBREF4, but pretrain the encoder using a number of different ASR datasets: the 150-hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data. We find that pretraining on a larger amount of data from an unrelated language is much better than pretraining on a smaller amount of data from a related language. Moreover, even when controlling for the amount of data, the WER of the ASR model from pretraining seems to be a better predictor of final AST performance than does language relatedness. Indeed, we show that there is a very strong correlation between the WER of the pretraining model and BLEU score of the final AST model—i.e., the best pretraining strategy may simply be to use datasets and methods that will yield the lowest ASR WER during pretraining. However, we also found that AST results can be improved further by augmenting the AST data using standard speed perturbation techniques BIBREF11. Our best results using non-English pretraining data improve the test set BLEU scores of an AST system trained on 20 hours of parallel data from 10.2 to 14.3, increasing to 15.8 with data augmentation.Finally, we analyze the representations learned by the models and show that better performance seems to correlate with the extent to which phonetic information is encoded in a linearly separable way in the later RNN layers.Methodology	For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure FIGREF1: the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2. Details of the architecture and training parameters are described in Section SECREF9.After pretraining an ASR model, we transfer only its encoder parameters to the AST task. Previous experiments BIBREF4 showed that the encoder accounts for most of the benefits of transferring the parameters. Transferring also the decoder and attention mechanism does bring some improvements, but is only feasible when the ASR pretraining language is the same as the AST target language, which is not true in most of our experiments.In addition to pretraining, we experimented with data augmentation. Specifically, we augmented the AST data using Kaldi's BIBREF12 3-way speed perturbation, adding versions of the AST data where the audio is sped down and up by a factor of 0.9 and 1.1, respectively.To evaluate ASR performance we compute the word error rate (WER). To evaluate AST performance we calculate the 4-gram BLEU score BIBREF13 on four reference translations.Experimental Setup ::: Parallel data	For the AST models, we use Spanish-English parallel data from Fisher corpus BIBREF14, containing 160 hours of Spanish telephone speech translated into English text. To simulate low-resource settings, we randomly downsample the original corpus to 20 hours of training data. Each of the dev and test sets comprise 4.5 hours of speech.Experimental Setup ::: Pretraining data	Since we focus on investigating factors that might affect the AST improvements over the baseline when pretraining, we have chosen ASR datasets for pretraining that contrast in the number of hours and/or in the language similarity with Spanish. Statistics for each dataset are in the left half of Table TABREF7, with further details below.To look at a range of languages with similar amounts of data, we used GlobalPhone corpora from seven languages BIBREF15, each with around 20 hours of speech: Mandarin Chinese (zh), Croatian (hr), Czech (cs), French (fr), Polish (pl), Portuguese (pt), and Swedish (sv). French and Portuguese, like the source language (Spanish), belong to the Romance family of languages, while the other languages are less related—especially Chinese, which is not an Indo-European language. GlobalPhone consists of read speech recorded using similar conditions across languages, and the transcriptions for Chinese are Romanized, with annotated word boundaries.To explore the effects of using a large amount of pretraining data from an unrelated language, we used the AISHELL-1 corpus of Mandarin Chinese BIBREF16, which contains 150 hours of read speech. Transcriptions with annotated word boundaries are available in both Hanzi (Chinese characters) and Romanized versions, and we built models with each. To compare to the GlobalPhone data, we also created a 20-hour subset of the Romanized AISHELL (zh-ai-small) by randomly selecting utterances from a subset of the speakers (81, roughly the number present in most of the GlobalPhone datasets).Finally, to reproduce one of the experiments from BIBREF4, we pre-trained one model using 300 hours of Switchboard English BIBREF17. This data is the most similar to the AST speech data in terms of style and channel (both are conversational telephone speech). However, as noted by BIBREF4, the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages.Experimental Setup ::: Preprocessing	We compute 13-dim MFCCs and cepstral mean and variance normalization along speakers using Kaldi BIBREF12 on our ASR and AST audio. To shorten the training time, we trimmed utterances from the AST data to 16 seconds (or 12 seconds for the 160h augmented dataset).To account for unseen words in the test data, we model the ASR and AST text outputs via sub-word units using byte-pair encoding (BPE) BIBREF18. We do this separately for each dataset as BPE works best as a language-specific tool (i.e. it depends on the frequency of different subword units, which varies with the language). We use 1k merge operations in all cases except Hanzi, where there are around 3000 symbols initially (vs around 60 in the other datasets). For Hanzi we ran experiments with both 1k and 15k merge operations. For Chinese Romanized transcriptions we removed tone diacritics.Experimental Setup ::: Model architecture and training	Following the architecture and training procedure described in BIBREF4, input speech features are fed into a stack of two CNN layers. In each CNN layer we stride the input with a factor of 2 along time, apply ReLU activation BIBREF19 followed by batch normalization BIBREF20. The CNN output is fed into a three-layer bi-directional long short-term memory network (LSTM) BIBREF21, with 512 hidden layer dimensions. For decoding, we use the predicted token 20% of the time and the training token 80% of the time BIBREF22 as input to a 128-dimensional embedding layer followed by a three-layer LSTM, with 256 hidden layer dimensions, and combine this with the output from the attention mechanism BIBREF23 to predict the word at the current time step.We use code and hyperparameter settings from BIBREF4: the Adam optimizer BIBREF24 with an initial learning rate of 0.001 and decay it by a factor of 0.5 based on the dev set BLEU score. When training AST models, we regularize using dropout BIBREF25 with a ratio of $0.3$ over the embedding and LSTM layers BIBREF26; weight decay with a rate of $0.0001$; and, after the first 20 epochs, 30% of the time we replace the predicted output word by a random word from the target vocabulary. At test time we use beam decoding with a beam size of 5 and length normalization BIBREF27 with a weight of 0.6.Results and Discussion ::: Baseline and ASR results	Our baseline 20-hour AST system obtains a BLEU score of 10.3 (Table TABREF7, first row), 0.5 BLEU point lower than that reported by BIBREF4. This discrepancy might be due to differences in subsampling from the 160-hour AST dataset to create the 20-hour subset, or from Kaldi parameters when computing the MFCCs.WERs for our pre-trained models (Table TABREF7) vary from 22.5 for the large AISHELL dataset with Romanized transcript to 80.5 for Portuguese GlobalPhone. These are considerably worse than state-of-the-art ASR systems (e.g., Kaldi recipes can achieve WER of 7.5 on AISHELL and 26.5 on Portuguese GlobalPhone), but we did not optimize our architecture or hyperparameters for the ASR task since our main goal is to analyze the relationship between pretraining and AST performance (and in order to use pretraining, we must use a seq2seq model with the architecture as for AST).Results and Discussion ::: Pretraining the AST task on ASR models	AST results for our pre-trained models are given in Table TABREF7. Pretraining improves AST performance in every case, with improvements ranging from 0.2 (pt-gp) to 4.3 (zh-ai-large). These results make it clear that language relatedness does not play a strong role in predicting AST improvements, since on the similar-sized GlobalPhone datasets, the two languages most related to Spanish (French and Portuguese) yield the highest and lowest improvements, respectively. Moreover, pretraining on the large Chinese dataset yields a bigger improvement than either of these—4.3 BLEU points. This is nearly as much as the 6 point improvement reported by BIBREF4 when pretraining on 100 hours of English data, which is especially surprising given not only that Chinese is very different from Spanish, but also that the Spanish data contains some English words.This finding seems to suggest that data size is more important than language relatedness for predicting the effects of pretraining. However, there are big differences even amongst the languages with similar amounts of pretraining data. Analyzing our results further, we found a striking correlation between the WER of the initial ASR model and the BLEU score of the AST system pretrained using that model, as shown in Figure FIGREF11. Therefore, although pretraining data size clearly influences AST performance, this appears to be mainly due to its effect on WER of the ASR model. We therefore hypothesize that WER is a better direct predictor of AST performance than either data size or language relatedness.Results and Discussion ::: Multilingual pretraining	Although our main focus is monolingual pretraining, we also looked briefly at multilingual pretraining, inspired by recent work on multilingual ASR BIBREF28, BIBREF29 and evidence that multilingual pretraining followed by fine-tuning on a distinct target language can improve ASR on the target language BIBREF10, BIBREF30, BIBREF31. These experiments did not directly compare pretraining using a similar amount of monolingual data, but such a comparison was done by BIBREF32, BIBREF33 in their work on learning feature representations for a target language with no transcribed data. They found a benefit for multilingual vs monolingual pretraining given the same amount of data.Following up on this work, we tried pretraining using 124 hours of multilingual data (all GlobalPhone languages except Chinese), roughly the amount of data in our large Chinese models. We combined all the data together and trained an ASR model using a common target BPE with 6k merge operations, then transferred only the encoder to the AST model. However, we did not see a benefit to the multilingual training (Table TABREF7, final row); in fact the resulting AST model was slightly worse than the zh-ai-large model (BLEU of 13.3 vs 14.6). Other configurations of multilingual training might still outperform their monolingual counterparts, but we leave this investigation as future work.Results and Discussion ::: Augmenting the parallel data	Table TABREF16 (top) shows how data augmentation affects the results of the baseline 20h AST system, as well as three of the best-performing pretrained models from Table TABREF7. For these experiments only, we changed the learning rates of the augmented-data systems so that all models took about the same amount of time to train (see Figure FIGREF17). Despite a more aggressive learning schedule, the performance of the augmented-data systems surpasses that of the baseline and pretrained models, even those trained on the largest ASR sets (150-hr Chinese and 300-hr English).For comparison to other work, Table TABREF16 (bottom) gives results for AST models trained on the full 160 hours of parallel data, including models with both pretraining and data augmentation. For the latter, we used the original learning schedule, but had to stop training early due to time constraints (after 15 days, compared to 8 days for complete training of the non-augmented 160h models). We find that both pretraining and augmentation still help, providing a combined gain of 3.8 (3.2) BLEU points over the baseline on the dev (test) set.Analyzing the models' representations	Finally, we hope to gain some understanding into why pretraining on ASR helps with AST, and specifically how the neural network representations change during pretraining and fine-tuning. We follow BIBREF34 and BIBREF9, who built diagnostic classifiers BIBREF35 to examine the representation of phonetic information in end-to-end ASR and AST systems, respectively. Unlike BIBREF34, BIBREF9, who used non-linear classifiers, we use a linear classifier to predict phone labels from the internal representations of the trained ASR or AST model.Using a linear classifier allows us to make more precise claims: if the classifier performs better using the representation from a particular layer, we can say that layer represents the phonetic information in a more linearly separable way. Using a nonlinear classifier raises questions about how to choose the complexity of the classifier itself, and therefore makes any results difficult to interpret.We hypothesized that pretraining allows the models to abstract away from nonlinguistic acoustic differences, and to better represent phonetic information: crucially, both in the trained language and in other languages. To test this hypothesis, we used two phone-labelled datasets distinct from all our ASR and AST datasets: the English TIMIT corpus (a language different to all of our trained models, with hand-labeled phones) and the Spanish GlobalPhone corpus (the same language as our AST source language, with phonetic forced-alignments produced using Kaldi). We randomly sampled utterances from these and passed them through the trained encoders, giving us a total of about 600k encoded frames. We used 400k of these to train logistic regression models to predict the phone labels, and tested on the remaining 200k frames.Separate logistic regression models were trained on the representations from each layer of the encoder. Since convolutional layers have a stride of 2, the number of frames decreases at each convolutional layer. To label the frames after a convolutional layer we eliminated every other label (and corresponding frame) from the original label sequence. For example, given label sequence S$_{\text{1}}$ = aaaaaaann at input layer, we get sequence S$_{\text{2}}$ = aaaan at the first convolutional layer and sequence S$_{\text{3}}$ = aan at the second convolutional layer and at the following recurrent layers.Results for the two classification data sets (Figure FIGREF18) show very similar patterns. In both the ASR and the AST models, the pretraining data seems to make little difference to phonetic encoding at the early layers, and classification accuracy peaks at the second CNN layer. However, the RNN layers show a clear trend where phone classification accuracy drops off more slowly for models with better ASR/AST performance (i.e., zh $>$ fr $>$ pt). That is, the later RNN layers more transparently encode language-universal phonetic information.Phone classification accuracy in the RNN layers drops for both English and Spanish after fine-tuning on the AST data. This is slightly surprising for Spanish, since the fine-tuning data (unlike the pretraining data) is actually Spanish speech. However, we hypothesize that for AST, higher layers of the encoder may be recruited more to encode semantic information needed for the translation task, and therefore lose some of the linear separability in the phonetic information. Nevertheless, we still see the same pattern where better end-to-end models have higher classification accuracy in the later layers.Conclusions	This paper explored what factors help pretraining for low-resource AST. We performed careful comparisons to tease apart the effects of language relatedness and data size, ultimately finding that rather than either of these, the WER of the pre-trained ASR model is likely the best direct predictor of AST performance. Given equivalent amounts of data, we did not find multilingual pretraining to help more than monolingual pretraining, but we did find an added benefit from using speed perturbation to augment the AST data. Finally, analysis of the pretrained models suggests that those models with better WER are transparently encoding more language-universal phonetic information in the later RNN layers, and this appears to help with AST.","['What sizes were their datasets?', 'How many layers does their model have?', ""What is their model's architecture?"", 'What languages did they use?']","['ast-20h: 20 hours,\nzh-ai-small: 20 hours,\nzh-ai-large: 150 hours,\nzh-ai-hanzi: 150 hours,\nhr-gp: 12 hours,\nsv-gp: 18 hours,\npl-gp: 19 hours,\npt-gp: 23 hours,\nfr-gp: 25 hours,\nzh-gp: 26 hours,\ncs-gp: 27 hours,\nmultilin6: 124 hours', '10 ', 'Unacceptable', 'Unacceptable']"
56,"Dual Co-Matching Network for Multi-choice Reading Comprehension	Multi-choice reading comprehension is a challenging task that requires complex reasoning procedure. Given passage and question, a correct answer need to be selected from a set of candidate answers. In this paper, we propose \textbf{D}ual \textbf{C}o-\textbf{M}atching \textbf{N}etwork (\textbf{DCMN}) which model the relationship among passage, question and answer bidirectionally. Different from existing approaches which only calculate question-aware or option-aware passage representation, we calculate passage-aware question representation and passage-aware answer representation at the same time. To demonstrate the effectiveness of our model, we evaluate our model on a large-scale multiple choice machine reading comprehension dataset (i.e. RACE). Experimental result show that our proposed model achieves new state-of-the-art results.	Introduction	Machine reading comprehension and question answering has becomes a crucial application problem in evaluating the progress of AI system in the realm of natural language processing and understanding BIBREF0 . The computational linguistics communities have devoted significant attention to the general problem of machine reading comprehension and question answering.However, most of existing reading comprehension tasks only focus on shallow QA tasks that can be tackled very effectively by existing retrieval-based techniques BIBREF1 . For example, recently we have seen increased interest in constructing extractive machine reading comprehension datasets such as SQuAD BIBREF2 and NewsQA BIBREF3 . Given a document and a question, the expected answer is a short span in the document. Question context usually contains sufficient information for identifying evidence sentences that entail question-answer pairs. For example, 90.2% questions in SQuAD reported by Min BIBREF4 are answerable from the content of a single sentence. Even in some multi-turn conversation tasks, the existing models BIBREF5 mostly focus on retrieval-based response matching.In this paper, we focus on multiple-choice reading comprehension datasets such as RACE BIBREF6 in which each question comes with a set of answer options. The correct answer for most questions may not appear in the original passage which makes the task more challenging and allow a rich type of questions such as passage summarization and attitude analysis. This requires a more in-depth understanding of a single document and leverage external world knowledge to answer these questions. Besides, comparing to traditional reading comprehension problem, we need to fully consider passage-question-answer triplets instead of passage-question pairwise matching.In this paper, we propose a new model, Dual Co-Matching Network, to match a question-answer pair to a given passage bidirectionally. Our network leverages the latest breakthrough in NLP: BERT BIBREF7 contextual embedding. In the origin BERT paper, the final hidden vector corresponding to first input token ([CLS]) is used as the aggregation representation and then a standard classification loss is computed with a classification layer. We think this method is too rough to handle the passage-question-answer triplet because it only roughly concatenates the passage and question as the first sequence and uses question as the second sequence, without considering the relationship between the question and the passage. So we propose a new method to model the relationship among the passage, the question and the candidate answer.Firstly we use BERT as our encode layer to get the contextual representation of the passage, question, answer options respectively. Then a matching layer is constructed to get the passage-question-answer triplet matching representation which encodes the locational information of the question and the candidate answer matched to a specific context of the passage. Finally we apply a hierarchical aggregation method over the matching representation from word-level to sequence-level and then from sequence level to document-level. Our model improves the state-of-the-art model by 2.6 percentage on the RACE dataset with BERT base model and further improves the result by 3 percentage with BERT large model.Model	For the task of multi-choice reading comprehension, the machine is given a passage, a question and a set of candidate answers. The goal is to select the correct answer from the candidates. P, Q, and A are used to represent the passage, the question and a candidate answer respectively. For each candidate answer, our model constructs a question-aware passage representation, a question-aware passage representation and a question-aware passage representation. After a max-pooling layer, the three representations are concatenated as the final representation of the candidate answer. The representations of all candidate answers are then used for answer selection.In section ""Encoding layer"" , we introduce the encoding mechanism. Then in section ""Conclusions"" , we introduce the calculation procedure of the matching representation between the passage, the question and the candidate answer. In section ""Aggregation layer"" , we introduce the aggregation method and the objective function.Encoding layer	This layer encodes each token in passage and question into a fixed-length vector including both word embedding and contextualized embedding. We utilize the latest result from BERT BIBREF7 as our encoder and the final hidden state of BERT is used as our final embedding. In the origin BERT BIBREF7 , the procedure of processing multi-choice problem is that the final hidden vector corresponding to first input token ([CLS]) is used as the aggregation representation of the passage, the question and the candidate answer, which we think is too simple and too rough. So we encode the passage, the question and the candidate answer respectively as follows: $$\begin{split}
\textbf {H}^p=&BERT(\textbf {P}),\textbf {H}^q=BERT(\textbf {Q}) \\
&\textbf {H}^a=BERT(\textbf {A})
\end{split}$$   (Eq. 3) where $\textbf {H}^p \in R^{P \times l}$ , $\textbf {H}^q \in R^{Q \times l}$ and $\textbf {H}^a \in R^{A \times l}$ are sequences of hidden state generated by BERT. $P$ , $Q$ , $A$ are the sequence length of the passage, the question and the candidate answer respectively. $l$ is the dimension of the BERT hidden state.Matching layer	To fully mine the information in a {P, Q, A} triplet , We make use of the attention mechanism to get the bi-directional aggregation representation between the passage and the answer and do the same process between the passage and the question. The attention vectors between the passage and the answer are calculated as follows: $$\begin{split}
\textbf {W}&=SoftMax(\textbf {H}^p({H^{a}G + b})^T), \\
\textbf {M}^{p}&=\textbf {W}\textbf {H}^{a},
\textbf {M}^{a}=\textbf {W}^T\textbf {H}^{p},
\end{split}$$   (Eq. 5) where $G \in R^{l \times l}$ and $b \in R^{A \times l}$ are the parameters to learn. $\textbf {W} \in R^{P \times A}$ is the attention weight matrix between the passage and the answer. $\textbf {M}^{p} \in R^{P \times l}$ represent how each hidden state in passage can be aligned to the answe rand $\textbf {M}^{a} \in R^{A \times l}$ represent how the candidate answer can be aligned to each hidden state in passage. In the same method, we can get $\textbf {W}^{\prime } \in R^{P \times Q}$ and $\textbf {M}^{q} \in R^{Q \times l}$ for the representation between the passage and the question.To integrate the original contextual representation, we follow the idea from BIBREF8 to fuse $\textbf {M}^{a}$ with original $\textbf {H}^p$ and so is $\textbf {M}^{p}$ . The final representation of passage and the candidate answer is calculated as follows: $$\begin{split}
\textbf {S}^{p}&=F([\textbf {M}^{a} - \textbf {H}^{a}; \textbf {M}^{a} \cdot \textbf {H}^{a}]W_1 + b_1),\\
\textbf {S}^{a}&=F([\textbf {M}^{p} - \textbf {H}^{p}; \textbf {M}^{p} \cdot \textbf {H}^{p}]W_2 + b_2),\\
\end{split}$$   (Eq. 6) where $W_1, W_2 \in R^{2l \times l}$ and $b_1 \in R^{P \times l}, b_2 \in R^{(A) \times l}$ are the parameters to learn. $[ ; ]$ is the column-wise concatenation and $-, \cdot $ are the element-wise subtraction and multiplication between two matrices. Previous work in BIBREF9 , BIBREF10 shows this method can build better matching representation. $F$ is the activation function and we choose $ReLU$ activation function there. $\textbf {S}^{p} \in R^{P \times l}$ and $\textbf {S}^{a} \in R^{A \times l}$ are the final representations of the passage and candidate answer. In the question side, we can get $\textbf {S}^{p^{\prime }} \in R^{P \times l}$ and $\textbf {S}^{q} \in R^{Q \times l}$ in the same calculation method.Aggregation layer	To get the final representation for each candidate answer, a row-wise max pooling operation is used to $\textbf {S}^{p}$ and $\textbf {S}^{a}$ . Then we get $\textbf {C}^{p} \in R^l$ and $\textbf {C}^{a} \in R^l$ respectively. In the question side, $\textbf {C}^{p^{\prime }} \in R^l$ and $\textbf {C}^{q} \in R^l$ are calculated. Finally, we concatenate all of them as the final output $\textbf {C} \in R^{4l}$ for each {P, Q, A} triplet. $$\begin{split}
\textbf {C}^{p} = &Pooling(\textbf {S}^{p}),
\textbf {C}^{a} = Pooling(\textbf {S}^{a}),\\
\textbf {C}^{p^{\prime }} = &Pooling(\textbf {S}^{p^{\prime }}),
\textbf {C}^{q} = Pooling(\textbf {S}^{q}),\\
\textbf {C} &= [\textbf {C}^{p}; \textbf {C}^{a};\textbf {C}^{p^{\prime }};\textbf {C}^{q}]
\end{split}$$   (Eq. 9) For each candidate answer choice $i$ , its matching representation with the passage and question can be represented as $\textbf {C}_i$ . Then our loss function is computed as follows: $$\begin{split}
L(\textbf {A}_i|\textbf {P,Q}) = -log{\frac{exp(V^T\textbf {C}_i)}{\sum _{j=1}^N{exp(V^T\textbf {C}_j)}}},
\end{split}$$   (Eq. 10) where $V \in R^l$ is a parameter to learn.Experiment	We evaluate our model on RACE dataset BIBREF6 , which consists of two subsets: RACE-M and RACE-H. RACE-M comes from middle school examinations while RACE-H comes from high school examinations. RACE is the combination of the two.We compare our model with the following baselines: MRU(Multi-range Reasoning) BIBREF12 , DFN(Dynamic Fusion Networks) BIBREF11 , HCM(Hierarchical Co-Matching) BIBREF8 , OFT(OpenAI Finetuned Transformer LM) BIBREF13 , RSM(Reading Strategies Model) BIBREF14 . We also compare our model with the BERT baseline and implement the method described in the original paper BIBREF7 , which uses the final hidden vector corresponding to the first input token ([CLS]) as the aggregate representation followed by a classification layer and finally a standard classification loss is computed.Results are shown in Table 2 . We can see that the performance of BERT $_{base}$ is very close to the previous state-of-the-art and BERT $_{large}$ even outperforms it for 3.7%. But experimental result shows that our model is more powerful and we further improve the result for 2.2% computed to BERT $_{base}$ and 2.2% computed to BERT $_{large}$ .Conclusions	In this paper, we propose a Dual Co-Matching Network, DCMN, to model the relationship among the passage, question and the candidate answer bidirectionally. By incorporating the latest breakthrough, BERT, in an innovative way, our model achieves the new state-of-the-art in RACE dataset, outperforming the previous state-of-the-art model by 2.2% in RACE full dataset.","['Do they evaluate their model on datasets other than RACE?', ""What is their model's performance on RACE?""]","['Yes, they also evaluate on the ROCStories\n(Spring 2016) dataset which collects 50k five sentence commonsense stories. ', ""Model's performance ranges from 67.0% to 82.8%.""]"
57,"Tackling Online Abuse: A Survey of Automated Abuse Detection Methods	Abuse on the Internet represents an important societal problem of our time. Millions of Internet users face harassment, racism, personal attacks, and other types of abuse on online platforms. The psychological effects of such abuse on individuals can be profound and lasting. Consequently, over the past few years, there has been a substantial research effort towards automated abuse detection in the field of natural language processing (NLP). In this paper, we present a comprehensive survey of the methods that have been proposed to date, thus providing a platform for further development of this area. We describe the existing datasets and review the computational approaches to abuse detection, analyzing their strengths and limitations. We discuss the main trends that emerge, highlight the challenges that remain, outline possible solutions, and propose guidelines for ethics and explainability	Introduction	With the advent of social media, anti-social and abusive behavior has become a prominent occurrence online. Undesirable psychological effects of abuse on individuals make it an important societal problem of our time. Munro munro2011 studied the ill-effects of online abuse on children, concluding that children may develop depression, anxiety, and other mental health problems as a result of their encounters online. Pew Research Center, in its latest report on online harassment BIBREF0 , revealed that INLINEFORM0 of adults in the United States have experienced abusive behavior online, of which INLINEFORM1 have faced severe forms of harassment, e.g., that of sexual nature. The report goes on to say that harassment need not be experienced first-hand to have an impact: INLINEFORM2 of American Internet users admitted that they stopped using an online service after witnessing abusive and unruly behavior of their fellow users. These statistics stress the need for automated abuse detection and moderation systems. Therefore, in the recent years, a new research effort on abuse detection has sprung up in the field of NLP.That said, the notion of abuse has proven elusive and difficult to formalize. Different norms across (online) communities can affect what is considered abusive BIBREF1 . In the context of natural language, abuse is a term that encompasses many different types of fine-grained negative expressions. For example, Nobata et al. nobata use it to collectively refer to hate speech, derogatory language and profanity, while Mishra et al. mishra use it to discuss racism and sexism. The definitions for different types of abuse tend to be overlapping and ambiguous. However, regardless of the specific type, we define abuse as any expression that is meant to denigrate or offend a particular person or group. Taking a course-grained view, Waseem et al. W17-3012 classify abuse into broad categories based on explicitness and directness. Explicit abuse comes in the form of expletives, derogatory words or threats, while implicit abuse has a more subtle appearance characterized by the presence of ambiguous terms and figures of speech such as metaphor or sarcasm. Directed abuse targets a particular individual as opposed to generalized abuse, which is aimed at a larger group such as a particular gender or ethnicity. This categorization exposes some of the intricacies that lie within the task of automated abuse detection. While directed and explicit abuse is relatively straightforward to detect for humans and machines alike, the same is not true for implicit or generalized abuse. This is illustrated in the works of Dadvar et al. davdar and Waseem and Hovy waseemhovy: Dadvar et al. observed an inter-annotator agreement of INLINEFORM0 on their cyber-bullying dataset. Cyber-bullying is a classic example of directed and explicit abuse since there is typically a single target who is harassed with personal attacks. On the other hand, Waseem and Hovy noted that INLINEFORM1 of all the disagreements in annotation of their dataset occurred on the sexism class. Sexism is typically both generalized and implicit.In this paper, we survey the methods that have been developed for automated detection of online abuse, analyzing their strengths and weaknesses. We first describe the datasets that exist for abuse. Then we review the various detection methods that have been investigated by the NLP community. Finally, we conclude with the main trends that emerge, highlight the challenges that remain, outline possible solutions, and propose guidelines for ethics and explainability. To the best of our knowledge, this is the first comprehensive survey in this area. We differ from previous surveys BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 in the following respects: 1) we discuss the categorizations of abuse based on coarse-grained vs. fine-grained taxonomies; 2) we present a detailed overview of datasets annotated for abuse; 3) we provide an extensive review of the existing abuse detection methods, including ones based on neural networks (omitted by previous surveys); 4) we discuss the key outstanding challenges in this area; and 5) we cover aspects of ethics and explainability.Annotated datasets	Supervised learning approaches to abuse detection require annotated datasets for training and evaluation purposes. To date, several datasets manually annotated for abuse have been made available by researchers. These datasets differ in two respects:In what follows, we review several commonly-used datasets manually annotated for abuse.Dataset descriptions. The earliest dataset published in this domain was compiled by Spertus smokey. It consisted of INLINEFORM0 private messages written in English from the web-masters of controversial web resources such as NewtWatch. These messages were marked as flame (containing insults or abuse; INLINEFORM1 ), maybe flame ( INLINEFORM2 ), or okay ( INLINEFORM3 ). We refer to this dataset as data-smokey. Yin et al. Yin09detectionof constructed three English datasets and annotated them for harassment, which they defined as “systematic efforts by a user to belittle the contributions of other users"". The samples were taken from three social media platforms: Kongregate ( INLINEFORM4 posts; INLINEFORM5 harassment), Slashdot ( INLINEFORM6 posts; INLINEFORM7 harassment), and MySpace ( INLINEFORM8 posts; INLINEFORM9 harassment). We refer to the three datasets as data-harass. Several datasets have been compiled using samples taken from portals of Yahoo!, specifically the News and Finance portals. Djuric et al. djuric created a dataset of INLINEFORM10 user comments in English from the Yahoo! Finance website that were editorially labeled as either hate speech ( INLINEFORM11 ) or clean (data-yahoo-fin-dj). Nobata et al. nobata produced four more datasets with comments from Yahoo! News and Yahoo! Finance, each labeled abusive or clean: 1) data-yahoo-fin-a: INLINEFORM12 comments, 7.0% abusive; 2) data-yahoo-news-a: INLINEFORM13 comments, 16.4% abusive; 3) data-yahoo-fin-b: INLINEFORM14 comments, 3.4% abusive; and 4) data-yahoo-news-b: INLINEFORM15 comments, 9.7% abusive.Several groups have investigated abusive language in Twitter. Waseem and Hovy waseemhovy created a corpus of INLINEFORM0 tweets, each annotated as one of racism ( INLINEFORM1 ), sexism, ( INLINEFORM2 ) or neither (data-twitter-wh). We note that although certain tweets in the dataset lack surface-level abusive traits (e.g., @Mich_McConnell Just “her body” right?), they have nevertheless been marked as racist or sexist as the annotators took the wider discourse into account; however, such discourse information or annotation is not preserved in the dataset. Inter-annotator agreement was reported at INLINEFORM3 , with a further insight that INLINEFORM4 of all the disagreements occurred on the sexism class alone. Waseem waseem later released a dataset of INLINEFORM5 tweets annotated as racism ( INLINEFORM6 ), sexism ( INLINEFORM7 ), both ( INLINEFORM8 ), or neither (data-twitter-w). data-twitter-w and data-twitter-wh have INLINEFORM9 tweets in common. It should, however, be noted that the inter-annotator agreement between the two datasets is low (mean pairwise INLINEFORM10 ) BIBREF6 .Davidson et al. davidson created a dataset of approximately INLINEFORM0 tweets, manually annotated as one of racist ( INLINEFORM1 ), offensive but not racist ( INLINEFORM2 ), or clean ( INLINEFORM3 ). We note, however, that their data sampling procedure relied on the presence of certain abusive words and, as a result, the distribution of classes does not follow a real-life distribution. Recently, Founta et al. founta crowd-sourced a dataset (data-twitter-f) of INLINEFORM4 tweets, of which INLINEFORM5 were annotated as normal, INLINEFORM6 as spam, INLINEFORM7 as hateful and INLINEFORM8 as abusive. The OffensEval 2019 shared task used a recently released dataset of INLINEFORM9 tweets BIBREF7 , each hierarchically labeled as: offensive ( INLINEFORM10 ) or not, whether the offence is targeted ( INLINEFORM11 ) or not, and whether it targets an individual ( INLINEFORM12 ), a group ( INLINEFORM13 ) or otherwise ( INLINEFORM14 ).Wulczyn et al. wulczyn annotated English Talk page comments from a dump of the full history of Wikipedia and released three datasets: one focusing on personal attacks ( INLINEFORM0 comments; INLINEFORM1 abusive), one on aggression ( INLINEFORM2 comments), and one on toxicity ( INLINEFORM3 comments; INLINEFORM4 abusive) (data-wiki-att, data-wiki-agg, and data-wiki-tox respectively). data-wiki-agg contains the exact same comments as data-wiki-att but annotated for aggression – the two datasets show a high correlation in the nature of abuse (Pearson's INLINEFORM5 ). Gao and Huang gao2017detecting released a dataset of INLINEFORM6 Fox News user comments (data-fox-news) annotated as hateful ( INLINEFORM7 ) or non-hateful. The dataset preserves context information for each comment, including user's screen-name, all comments in the same thread, and the news article for which the comment is written.Some researchers investigated abuse in languages other than English. Van Hee et al. vanhee gathered INLINEFORM0 Dutch posts from ask.fm to form a dataset on cyber-bullying (data-bully; INLINEFORM1 cyber-bullying cases). Pavlopoulos et al. pavlopoulos-emnlp released a dataset of ca. INLINEFORM2 comments in Greek provided by the news portal Gazzetta (data-gazzetta). The comments were marked as accept or reject, and are divided into 6 splits with similar distributions (the training split is the largest one: INLINEFORM3 accepted and INLINEFORM4 rejected comments). As part of the GermEval shared task on identification of offensive language in German tweets BIBREF8 , a dataset of INLINEFORM5 tweets was released, of which INLINEFORM6 were labeled as abuse, INLINEFORM7 as insult, INLINEFORM8 as profanity, and INLINEFORM9 as other. Around the same time, INLINEFORM10 Facebook posts and comments, each in Hindi (in both Roman and Devanagari script) and English, were released (data-facebook) as part of the COLING 2018 shared task on aggression identification BIBREF9 . INLINEFORM11 of the comments were covertly aggressive, INLINEFORM12 overtly aggressive and INLINEFORM13 non-aggressive. We note, however, that some issues were raised by the participants regarding the quality of the annotations. The HatEval 2019 shared task (forthcoming) focuses on detecting hate speech against immigrants and women using a dataset of INLINEFORM14 tweets in Spanish and INLINEFORM15 in English annotated hierarchically as hateful or not; and, in turn, as aggressive or not, and whether the target is an individual or a group.Remarks. In their study, Ross et al. ross stressed the difficulty in reliably annotating abuse, which stems from multiple factors, such as the lack of “standard” definitions for the myriad types of abuse, differences in annotators' cultural background and experiences, and ambiguity in the annotation guidelines. That said, Waseem et al. W17-3012 and Nobata et al. nobata observed that annotators with prior expertise provide good-quality annotations with high levels of agreement. We note that most datasets contain discrete labels only; abuse detection systems trained on them would be deprived of the notion of severity, which is vital in real-world settings. Also, most datasets cover few types of abuse only. Salminen et al. salminen2018anatomy suggest fine-grained annotation schemes for deeper understanding of abuse; they propose 29 categories that include both types of abuse and their targets (e.g., humiliation, religion).Feature engineering based approaches	In this section, we describe abuse detection methods that rely on hand-crafted rules and manual feature engineering. The first documented abuse detection method was designed by Spertus smokey who used a heuristic rule-based approach to produce feature vectors for the messages in the data-smokey dataset, followed by a decision tree generator to train a classification model. The model achieved a recall of INLINEFORM0 on the flame messages, and INLINEFORM1 on the non-flame ones in the test set. Spertus noted some limitations of adopting a heuristic rule-based approach, e.g., the inability to deal with sarcasm, and vulnerability to errors in spelling, punctuation and grammar. Yin et al. Yin09detectionof developed a method for detecting online harassment. Working with the three data-harass datasets, they extracted local features (tf–idf weights of words), sentiment-based features (tf–idf weights of foul words and pronouns) and contextual features (e.g., similarity of a post to its neighboring posts) to train a linear support vector machine (svm) classifier. The authors concluded that important contextual indicators (such as harassment posts generally being off-topic) cannot be captured by local features alone. Their approach achieved INLINEFORM2 F INLINEFORM3 on the MySpace dataset, INLINEFORM4 F INLINEFORM5 on the Slashdot dataset, and INLINEFORM6 F INLINEFORM7 on the Kongregate dataset.Razavi et al. razavi were the first to adopt lexicon-based abuse detection. They constructed an insulting and abusing language dictionary of words and phrases, where each entry had an associated weight indicating its abusive impact. They utilized semantic rules and features derived from the lexicon to build a three-level Naive Bayes classification system and apply it to a dataset of INLINEFORM0 messages ( INLINEFORM1 flame and the rest okay) extracted from the Usenet newsgroup and the Natural Semantic Module company's employee conversation thread ( INLINEFORM2 accuracy). Njagi et al. gitari also employed such a lexicon-based approach and, more recently, Wiegand et al. wiegand proposed an automated framework for generating such lexicons. While methods based on lexicons performed well on explicit abuse, the researchers noted their limitations on implicit abuse.Bag-of-words (bow) features have been integral to several works on abuse detection. Sood et al. sood2012 showed that an svm trained on word bi-gram features outperformed a word-list baseline utilizing a Levenshtein distance-based heuristic for detecting profanity. Their best classifier (combination of SVMs and word-lists) yielded an F INLINEFORM0 of INLINEFORM1 . Warner and Hirschberg warner employed a template-based strategy alongside Brown clustering to extract surface-level bow features from a dataset of paragraphs annotated for antisemitism, and achieved an F INLINEFORM2 of INLINEFORM3 using svms. Their approach is unique in that they framed the task as a word-sense disambiguation problem, i.e., whether a term carried an anti-semitic sense or not. Other examples of bow-based methods are those of Dinakar et al. dinakar2011modeling, Burnap and Williams burnap and Van Hee et al. vanhee who use word n-grams in conjunction with other features, such as typed-dependency relations or scores based on sentiment lexicons, to train svms ( INLINEFORM4 F INLINEFORM5 on the data-bully dataset). Recenlty, Salminen et al. salminen2018anatomy showed that a linear SVM using tf–idf weighted n-grams achieves the best performance (average F INLINEFORM6 of INLINEFORM7 ) on classification of hateful comments (from a YouTube channel and Facebook page of an online news organization) as one of 29 different hate categories (e.g., accusation, promoting violence, humiliation, etc.).Several researchers have directly incorporated features and identity traits of users in order to model the likeliness of abusive behavior from users with certain traits, a process known as user profiling. Dadvar et al. davdar included the age of users alongside other traditional lexicon-based features to detect cyber-bullying, while Galán-García et al. galan2016supervised utilized the time of publication, geo-position and language in the profile of Twitter users. Waseem and Hovy waseemhovy exploited gender of Twitter users alongside character n-gram counts to improve detection of sexism and racism in tweets from data-twitter-wh (F INLINEFORM0 increased from INLINEFORM1 to INLINEFORM2 ). Using the same setup, Unsvåg and Gambäck unsvaag2018effects showed that the inclusion of social network-based (i.e., number of followers and friends) and activity-based (i.e., number of status updates and favorites) information of users alongside their gender further enhances performance ( INLINEFORM3 gain in F INLINEFORM4 ).Neural network based approaches	In this section, we review the approaches to abuse detection that utilize or rely solely on neural networks. We also include methods that use embeddings generated from a neural architecture within an otherwise non-neural framework.Distributed representations. Djuric et al. djuric were the first to adopt a neural approach to abuse detection. They utilized paragraph2vec BIBREF10 to obtain low-dimensional representations for comments in data-yahoo-fin-dj, and train a logistic regression (lr) classifier. Their model outperformed other classifiers trained on bow-based representations (auc INLINEFORM0 vs. INLINEFORM1 ). In their analysis, the authors noted that words and phrases in hate speech tend to be obfuscated, leading to high dimensionality and large sparsity of bow representations; classifiers trained on such representations often over-fit in training.Building on the work of Djuric et al., Nobata et al. nobata evaluated the performance of a large range of features on the Yahoo! datasets (data-yahoo-*) using a regression model: (1) word and character n-grams; (2) linguistic features, e.g., number of polite/hate words and punctuation count; (3) syntactic features, e.g., parent and grandparent of node in a dependency tree; (4) distributional-semantic features, e.g., paragraph2vec comment representations. Although the best results were achieved with all features combined (F INLINEFORM0 INLINEFORM1 on data-yahoo-fin-a, INLINEFORM2 on data-yahoo-news-a), character n-grams on their own contributed significantly more than other features due to their robustness to noise (i.e., obfuscations, misspellings, unseen words). Experimenting with the data-yahoo-fin-dj dataset, Mehdad and Tetreault mehdad investigated whether character-level features are more indicative of abuse than word-level ones. Their results demonstrated the superiority of character-level features, showing that svm classifiers trained on Bayesian log-ratio vectors of average counts of character n-grams outperform the more intricate approach of Nobata et al. nobata in terms of AUC ( INLINEFORM3 vs. INLINEFORM4 ) as well as other rnn-based character and word-level models.Samghabadi et al. W17-3010 utilized a similar set of features as Nobata et al. and augmented it with hand-engineered ones such as polarity scores derived from SentiWordNet, measures based on the LIWC program, and features based on emoticons. They then applied their method to three different datasets: data-wiki-att, a Kaggle dataset annotated for insult, and a dataset of questions and answers (each labeled as invective or neutral) that they created by crawling ask.fm. Distributional-semantic features combined with the aforementioned features constituted an effective feature space for the task ( INLINEFORM0 , INLINEFORM1 , INLINEFORM2 F INLINEFORM3 on data-wiki-att, Kaggle, ask.fm respectively). In line with the findings of Nobata et al. and Mehdad and Tetreault, character n-grams performed well on these datasets too.Deep learning in abuse detection. With the advent of deep learning, many researchers have explored its efficacy in abuse detection. Badjatiya et al. badjatiya evaluated several neural architectures on the data-twitter-wh dataset. Their best setup involved a two-step approach wherein they use a word-level long-short term memory (lstm) model, to tune glove or randomly-initialized word embeddings, and then train a gradient-boosted decision tree (gbdt) classifier on the average of the tuned embeddings in each tweet. They achieved the best results using randomly-initialized embeddings (weighted F INLINEFORM0 of INLINEFORM1 ). However, working with a similar setup, Mishra et al. mishra recently reported that glove initialization provided superior performance; a mismatch is attributed to the fact that Badjatiya et al. tuned the embeddings on the entire dataset (including the test set), hence allowing for the randomly-initialized ones to overfit.Park and Fung parkfung utilized character and word-level cnns to classify comments in the dataset that they formed by combining data-twitter-w and data-twitter-wh. Their experiments demonstrated that combining the two levels of granularity using two input channels achieves the best results, outperforming a character n-gram lr baseline (weighted F INLINEFORM0 from INLINEFORM1 to INLINEFORM2 ). Several other works have also demonstrated the efficacy of cnns in detecting abusive social media posts BIBREF11 . Some researchers BIBREF12 , BIBREF13 have shown that sequentially combining cnns with gated recurrent unit (gru) rnns can enhance performance by taking advantage of properties of both architectures (e.g., 1-2% increase in F INLINEFORM3 compared to only using cnns).Pavlopoulos et al. pavlopoulos,pavlopoulos-emnlp applied deep learning to the data-wiki-att, data-wiki-tox, and data-gazzetta datasets. Their most effective setups were: (1) a word-level gru followed by an lr layer; (2) setup 1 extended with an attention mechanism on words. Both setups outperformed a simple word-list baseline and the character n-gram lr classifier (detox) of Wulczyn et al. wulczyn. Setup 1 achieved the best performance on data-wiki-att and data-wiki-tox (auc INLINEFORM0 and INLINEFORM1 respectively), while setup 2 performed the best on data-gazzetta (auc INLINEFORM2 ). The attention mechanism was additionally able to highlight abusive words and phrases within the comments, exhibiting a high level of agreement with annotators on the task. Lee et al. W18-5113 worked with a subset of the data-twitter-f dataset and showed that a word-level bi-gru along with latent topic clustering (whereby topic information is extracted from the hidden states of the gru BIBREF14 ) yielded the best weighted F INLINEFORM3 ( INLINEFORM4 ).The GermEval shared task on identification of offensive language in German tweets BIBREF8 saw submission of both deep learning and feature engineering approaches. The winning system BIBREF15 (macro F INLINEFORM0 of INLINEFORM1 ) employed multiple character and token n-gram classifiers, as well as distributional semantic features obtained by averaging word embeddings. The second best approach BIBREF16 (macro F INLINEFORM2 INLINEFORM3 ), on the other hand, employed an ensemble of cnns, the outputs of which were fed to a meta classifier for final prediction. Most of the remaining submissions BIBREF17 , BIBREF18 used deep learning with cnns and rnns alongside techniques such as transfer learning (e.g., via machine translation or joint representation learning for words across languages) from abuse-annotated datasets in other languages (mainly English). Wiegand et al. wiegand2018overview noted that simple deep learning approaches themselves were quite effective, and the addition of other techniques did not necessarily provide substantial improvements.Kumar et al. kumar2018benchmarking noted similar trends in the shared task on aggression identification on data-facebook. The top approach on the task's English dataset BIBREF19 comprised rnns and cnns along with transfer learning via machine translation (macro F INLINEFORM0 of INLINEFORM1 ). The top approach for Hindi BIBREF20 utilized lexical features based on word and character n-grams (F INLINEFORM2 62.92%).Recently, Aken et al. van2018challenges performed a systematic comparison of neural and non-neural approaches to toxic comment classification, finding that ensembles of the two were most effective.User profiling with neural networks. More recently, researchers have employed neural networks to extract features for users instead of manually leveraging ones like gender, location, etc. as discussed before. Working with the data-gazzetta dataset, Pavlopoulos et al. W17-4209 incorporated user embeddings into Pavlopoulos' setup 1 pavlopoulos,pavlopoulos-emnlp described above. They divided all the users whose comments are included in data-gazzetta into 4 types based on proportion of abusive comments (e.g., red users if INLINEFORM0 comments and INLINEFORM1 abusive comments), yellow (users with INLINEFORM2 comments and INLINEFORM3 abusive comments), green (users with INLINEFORM4 comments and INLINEFORM5 abusive comments), and unknown (users with INLINEFORM6 comments). They then assigned unique randomly-initialized embeddings to users and added them as additional input to the lr layer, alongside representations of comments obtained from the gru, increasing auc from INLINEFORM7 to INLINEFORM8 . Qian et al. N18-2019 used lstms for modeling inter and intra-user relationships on data-twitter-wh, with sexist and racist tweets combined into one category. The authors applied a bi-lstm to users' recent tweets in order to generate intra-user representations that capture their historic behavior. To improve robustness against noise present in tweets, they also used locality sensitive hashing to form sets semantically similar to user tweets. They then trained a policy network to select tweets from such sets that a bi-lstm could use to generate inter-user representations. When these inter and intra-user representations were utilized alongside representations of tweets from an lstm baseline, performance increased significantly (from INLINEFORM9 to INLINEFORM10 F INLINEFORM11 ).Mishra et al. mishra constructed a community graph of all users whose tweets are included in the data-twitter-wh dataset. Nodes in the graph were users while edges the follower-following relationship between them on Twitter. They then applied node2vec BIBREF21 to this graph to generate user embeddings. Inclusion of these embeddings into character n-gram based baselines yielded state of the art results on data-twitter-wh (F INLINEFORM0 increased from INLINEFORM1 and INLINEFORM2 to INLINEFORM3 and INLINEFORM4 on the racism and sexism classes respectively). The gains were attributed to the fact that user embeddings captured not only information about online communities, but also some elements of the wider conversation amongst connected users in the graph. Ribeiro et al. ribeiro and Mishra et al. mishragcn applied graph neural networks BIBREF22 , BIBREF23 to social graphs in order to generate user embeddings (i.e., profiles) that capture not only their surrounding community but also their linguistic behavior.Discussion	Current trends. English has been the dominant language so far in terms of focus, followed by German, Hindi and Dutch. However, recent efforts have focused on compilation of datasets in other languages such as Slovene and Croatian BIBREF24 , Chinese BIBREF25 , Arabic BIBREF26 , and even some unconventional ones such as Hinglish BIBREF27 . Most of the research to date has been on racism, sexism, personal attacks, toxicity, and harassment. Other types of abuse such as obscenity, threats, insults, and grooming remain relatively unexplored. That said, we note that the majority of methods investigated to date and described herein are (in principle) applicable to a range of abuse types.While the recent state of the art approaches rely on word-level cnns and rnns, they remain vulnerable to obfuscation of words BIBREF28 . Character n-gram, on the other hand, remain one of the most effective features for addressing obfuscation due to their robustness to spelling variations. Many researchers to date have exclusively relied on text based features for abuse detection. But recent works have shown that personal and community-based profiling features of users significantly enhance the state of the art.Ethical challenges. Whilst the research community has started incorporating features from user profiling, there has not yet been a discussion of ethical guidelines for doing so. To encourage such a discussion, we lay out four ethical considerations in the design of such approaches. First, the profiling approach should not compromise the privacy of the user. So a researcher might ask themselves such questions as: is the profiling based on identity traits of users (e.g., gender, race etc.) or solely on their online behavior? And is an appropriate generalization from (identifiable) user traits to population-level behavioural trends performed? Second, one needs to reflect on the possible bias in the training procedure: is it likely to induce a bias against users with certain traits? Third, the visibility aspect needs to be accounted for: is the profiling visible to the users, i.e., can users directly or indirectly observe how they (or others) have been profiled? And finally, one needs to carefully consider the purpose of such profiling: is it intended to take actions against users, or is it more benign (e.g. to better understand the content produced by them and make task-specific generalizations)? While we do not intend to provide answers to these questions within this survey, we hope that the above considerations can help to start a debate on these important issues.Labeling abuse. Labeling experiences as abusive provides powerful validation for victims of abuse and enables observers to grasp the scope of the problem. It also creates new descriptive norms (suggesting what types of behavior constitute abuse) and exposes existing norms and expectations around appropriate behavior. On the other hand, automated systems can invalidate abusive experiences, particularly for victims whose experiences do not lie within the realm of `typical' experiences BIBREF29 . This points to a critical issue: automated systems embody the morals and values of their creators and annotators BIBREF30 , BIBREF29 . It is therefore imperative that we design systems that overcome such issues. For e.g., some recent works have investigated ways to mitigate gender bias in models BIBREF31 , BIBREF32 .Abuse over time and across domains. New abusive words and phrases continue to enter the language BIBREF33 . This suggests that abuse is a constantly changing phenomenon. Working with the data-yahoo-*-b datasets, Nobata et al. nobata found that a classifier trained on more recent data outperforms one trained on older data. They noted that a prominent factor in this is the continuous evolution of the Internet jargon. We would like to add that, given the situational and topical nature of abuse BIBREF1 , contextual features learned by detection methods may become irrelevant over time.A similar trend also holds for abuse detection across domains. Wiegand et al. wiegand showed that the performance of state of the art classifiers BIBREF34 , BIBREF35 decreases substantially when tested on data drawn from domains different to those in the training set. Wiegand et al. attributed the trend to lack of domain-specific learning. Chandrasekharan et al. chandrasekharan2017bag propose an approach that utilizes similarity scores between posts to improve in-domain performance based on out-of-domain data. Possible solutions for improving cross-domain abuse detection can be found in the literature of (adversarial) multi-task learning and domain adaptation BIBREF36 , BIBREF37 , BIBREF38 , and also in works such as that of Sharifirad et al. jafarpour2018boosting who utilize knowledge graphs to augment the training of a sexist tweet classifier. Recently, Waseem et al. waseem2018bridging and Karan and Šnajder karan2018cross exploited multi-task learning frameworks to train models that are robust across data from different distributions and data annotated under different guidelines.Modeling wider conversation. Abuse is inherently contextual; it can only be interpreted as part of a wider conversation between users on the Internet. This means that individual comments can be difficult to classify without modeling their respective contexts. However, the vast majority of existing approaches have focused on modeling the lexical, semantic and syntactic properties of comments in isolation from other comments. Mishra et al. mishra have pointed out that some tweets in data-twitter-wh do not contain sufficient lexical or semantic information to detect abuse even in principle, e.g., @user: Logic in the world of Islam http://t.co/xxxxxxx, and techniques for modeling discourse and elements of pragmatics are needed. To address this issue, Gao and Huang gao2017detecting, working with data-fox-news, incorporate features from two sources of context: the title of the news article for which the comment was posted, and the screen name of the user who posted it. Yet this is only a first step towards modeling the wider context in abuse detection; more sophisticated techniques are needed to capture the history of the conversation and the behavior of the users as it develops over time. NLP techniques for modeling discourse and dialogue can be a good starting point in this line of research. However, since posts on social media often includes data of multiple modalities (e.g., a combination of images and text), abuse detection systems would also need to incorporate a multi-modal component.Figurative language. Figurative devices such as metaphor and sarcasm are common in natural language. They tend to be used to express emotions and sentiments that go beyond the literal meaning of words and phrases BIBREF39 . Nobata et al. nobata (among others, e.g., Aken et al. van2018challenges) noted that sarcastic comments are hard for abuse detection methods to deal with since surface features are not sufficient; typically the knowledge of the context or background of the user is also required. Mishra mishrathesis found that metaphors are more frequent in abusive samples as opposed to non-abusive ones. However, to fully understand the impact of figurative devices on abuse detection, datasets with more pronounced presence of these are required.Explainable abuse detection. Explainability has become an important aspect within NLP, and within AI generally. Yet there has been no discussion of this issue in the context of abuse detection systems. We hereby propose three properties that an explainable abuse detection system should aim to exhibit. First, it needs to establish intent of abuse (or the lack of it) and provide evidence for it, hence convincingly segregating abuse from other phenomena such as sarcasm and humour. Second, it needs to capture abusive language, i.e., highlight instances of abuse if present, be they explicit (i.e., use of expletives) or implicit (e.g., dehumanizing comparisons). Third, it needs to identify the target(s) of abuse (or the absence thereof), be it an individual or a group. These properties align well with the categorizations of abuse we discussed in the introduction. They also aptly motivate the advances needed in the field: (1) developments in areas such as sarcasm detection and user profiling for precise segregation of abusive intent from humor, satire, etc.; (2) better identification of implicit abuse, which requires improvements in modeling of figurative language; (3) effective detection of generalized abuse and inference of target(s), which require advances in areas such as domain adaptation and conversation modeling.Conclusions	Online abuse stands as a significant challenge before society. Its nature and characteristics constantly evolve, making it a complex phenomenon to study and model. Automated abuse detection methods have seen a lot of development in recent years: from simple rule-based methods aimed at identifying directed, explicit abuse to sophisticated methods that can capture rich semantic information and even aspects of user behavior. By comprehensively reviewing the investigated methods to date, our survey aims to provide a platform for future research, facilitating progress in this important area. While we see an array of challenges that lie ahead, e.g., modeling extra-propositional aspects of language, user behavior and wider conversation, we believe that recent progress in the areas of semantics, dialogue modeling and social media analysis put the research community in a strong position to address them. Summaries of public datasets In table TABREF4 , we summarize the datasets described in this paper that are publicly available and provide links to them. A discussion of metrics The performance results we have reported highlight that, throughout work on abuse detection, different researchers have utilized different evaluation metrics for their experiments – from area under the receiver operating characteristic curve (auroc) BIBREF79 , BIBREF48 to micro and macro F INLINEFORM0 BIBREF28 – regardless of the properties of their datasets. This makes the presented techniques more difficult to compare. In addition, as abuse is a relatively infrequent phenomenon, the datasets are typically skewed towards non-abusive samples BIBREF6 . Metrics such as auroc may, therefore, be unsuitable since they may mask poor performance on the abusive samples as a side-effect of the large number of non-abusive samples BIBREF52 . Macro-averaged precision, recall, and F INLINEFORM1 , as well as precision, recall, and F INLINEFORM2 on specifically the abusive classes, may provide a more informative evaluation strategy; the primary advantage being that macro-averaged metrics provide a sense of effectiveness on the minority classes BIBREF73 . Additionally, area under the precision-recall curve (auprc) might be a better alternative to auroc in imbalanced scenarios BIBREF46 . ","['Did the survey provide insight into features commonly found to be predictive of abusive content on online platforms?', 'Is deep learning the state-of-the-art method in automated abuse detection', 'What datasets were used in this work?', 'How is abuse defined for the purposes of this research?']","['Unacceptable', 'Unacceptable', 'DATA-TWITTER-WH, DATA-TWITTER-W, DATA-TWITTER-DAVID, DATA-TWITTER-F,  DATA-WIKI-ATT,  DATA-WIKI-AGG,  DATA-WIKI-TOX,  DATA-FOX-NEWS,  DATA-GAZZETTA, DATA-FACEBOOK, Arabic News,  GermEval,  Ask.fm.', 'Unacceptable']"
58,"An Interactive Machine Translation Framework for Modernizing Historical Documents	Due to the nature of human language, historical documents are hard to comprehend by contemporary people. This limits their accessibility to scholars specialized in the time period in which the documents were written. Modernization aims at breaking this language barrier by generating a new version of a historical document, written in the modern version of the document's original language. However, while it is able to increase the document's comprehension, modernization is still far from producing an error-free version. In this work, we propose a collaborative framework in which a scholar can work together with the machine to generate the new version. We tested our approach on a simulated environment, achieving significant reductions of the human effort needed to produce the modernized version of the document.	Introduction	In recent years, awareness of the importance of preserving our cultural heritage has increased. Historical documents are an important part of that heritage. In order to preserve them, there is an increased need in creating digital text versions which can be search and automatically processed BIBREF0. However, their linguistic properties create additional difficulties: due to the lack of a spelling convention, orthography changes depending on the time period and author. Furthermore, human language evolves with the passage of time, increasing the difficulty of the document's comprehension. Thus, historical documents are mostly accessible to scholars specialized in the time period in which each document was written.Modernization tackles the language barrier in order to increase the accessibility of historical documents. To achieve this, it generates a new version of a historical document in the modern version of the language in which the document was originally written (fi:Shakespeare shows an example of modernizing a document). However, while modernization has been successful in order to increase the comprehension of historical documents BIBREF1, BIBREF2, it is still far from creating error-free modern versions. Therefore, this task still needs to be carried out by scholars.Interactive machine translation (IMT) fosters human–computer collaborations to generate error-free translations in a productive way BIBREF4, BIBREF5. In this work, we proposed to apply one of these protocols to historical documents modernization. We strive for creating an error-free modern version of a historical document, decreasing the human effort needed to achieve this goal.The rest of this document is structured as follows: se:work introduces the related work. Then, in se:IMT we present our protocol. se:exp describes the experiments conducted in order to assess our proposal. The results of those experiments are presented and discussed in se:res. Finally, in se:conc, conclusions are drawn.Related Work	While the lack of a spelling convention has been extensively researched for years BIBREF6, BIBREF7, BIBREF8, modernization of historical documents is a younger field. BIBREF1 organized a shared task in order to translate historical text to contemporary language. The main goal of this shared task was to tackle the spelling problem. However, they also approached document modernization using a set of rules. BIBREF9 proposed a modernization approach based on statistical machine translation (SMT). A neural machine translation (NMT) approach was proposed by BIBREF2. Finally, BIBREF10 extracted parallel phrases from an original parallel corpus and used them as an additional training data for their NMT approach.Despise the promising results achieved in last years, machine translation (MT) is still far from producing high-quality translations BIBREF11. Therefore, a human agent has to supervise these translation in a post-editing stage. IMT was introduced with the goal of combining the knowledge of a human translator and the efficiency of an MT system. Although many protocols have been proposed in recent years BIBREF12, BIBREF13, BIBREF14, BIBREF15, the prefix-based remains as one of the most successful approaches BIBREF5, BIBREF16, BIBREF17. In this approach, the user corrects the leftmost wrong word from the translation hypothesis, inherently validating a correct prefix. With each new correction, the system generates a suffix that completes the prefix to produce a new translation.Interactive Machine Translation	Classical IMT approaches relay on the statistical formalization of the MT problem. Given a source sentence $\mathbf {x}$, SMT aims at finding its most likely translation $\hat{\mathbf {y}}$ BIBREF18:For years, the prevailing approach to compute this expression have been phrase-based models BIBREF19. These models rely on a log-linear combination of different models BIBREF20: namely, phrase-based alignment models, reordering models and language models; among others BIBREF21, BIBREF22. However, more recently, this approach has shifted into neural models (see se:NMT).Interactive Machine Translation ::: Prefix-based Interactive Machine Translation	Prefix-based IMT proposed a user–computer collaboration that starts with the system proposing an initial translation $\mathbf {y}$ of length $I$. Then, the user corrects the leftmost wrong word $y_i$, inherently validating all preceding words. These words form a validated prefix $\tilde{\mathbf {y}}_p$, that includes the corrected word $\tilde{y}_i$. The system reacts to this user feedback, generating a suffix $\hat{\mathbf {y}}_s$ that completes $\tilde{\mathbf {y}}_p$ to obtain a new translation of $\mathbf {x}:\hat{\mathbf {y}}~=~\tilde{\mathbf {y}}_p\,\hat{\mathbf {y}}_s$. This process is repeated until the user accepts the complete system suggestion. fi:IMT illustrates this protocol.BIBREF5 formalized the suffix generation as follows:which can be straightforwardly rewritten as:This equation is very similar to eq:SMT: at each iteration, the process consists in a regular search in the translations space but constrained by the prefix $\tilde{\mathbf {y}}_p$.Interactive Machine Translation ::: Neural Machine Translation	In NMT, eq:SMT is modeled by a neural network with parameters $\mathbf {\Theta }$:This neural network usually follows an encoder-decoder architecture, featuring recurrent networks BIBREF23, BIBREF24, convolutional networks BIBREF25 or attention mechanisms BIBREF26. Model parameters are jointly estimated on large parallel corpora, using stochastic gradient descent BIBREF27, BIBREF28. At decoding time, the system obtains the most likely translation using a beam search method.Interactive Machine Translation ::: Prefix-based Interactive Neural Machine Translation	The prefix-based IMT protocol (see se:PBIMT) can be naturally included into NMT systems since sentences are generated from left to right. In order to take into account the user's feedback and generate compatible hypothesis, the search space must be constraint. Given a prefix $\tilde{\mathbf {y}}_p$, only a single path accounts for it. The branching of the search process starts once this path has been covered. Introducing the validated prefix $\tilde{\mathbf {y}}_p$, eq:NMT becomes:which implies a search over the space of translations, but constrained by the validated prefix $\tilde{\mathbf {y}}_p$ BIBREF15.Experiments	In this section, we present our experimental conditions, including translation systems, corpora and evaluation metrics.Experiments ::: MT Systems	SMT systems were trained with Moses BIBREF29, following the standard procedure: we estimated a 5-gram language model—smoothed with the improved KneserNey method—using SRILM BIBREF30, and optimized the weights of the log-linear model with MERT BIBREF31.We built our NMT systems using NMT-Keras BIBREF32. We used long short-term memory units BIBREF33, with all model dimensions set to 512. We trained the system using Adam BIBREF34 with a fixed learning rate of $0.0002$ and a batch size of 60. We applied label smoothing of $0.1$ BIBREF35. At inference time, we used beam search with a beam size of 6. We applied joint byte pair encoding to all corpora BIBREF36, using $32,000$ merge operations.Statistical IMT systems were implemented following the procedure of word graph exploration and generation of a best suffix for a given prefix described by BIBREF5. Neural IMT systems were built using the interactive branch of NMT-Keras.Experiments ::: Corpora	The first corpus used in our experimental session was the Dutch Bible BIBREF1. This corpus consists in a collection of different versions of the Dutch Bible: a version from 1637, another from 1657, another from 1888 and another from 2010. Except for the 2010 version, which is missing the last books, all versions contain the same texts. Moreover, since the authors mentioned that the translation from this last version is not very reliable and, considering that Dutch has not evolved significantly between 1637 and 1657, we decided to only use the 1637 version—considering this as the original document—and the 1888 version—considering 19$^{\mathrm {th}}$ century Dutch as modern Dutch.We selected El Quijote BIBREF2 as our second corpus. This corpus contains the famous 17$^{\mathrm {th}}$ century Spanish novel by Miguel de Cervantes, and its correspondent 21$^{\mathrm {st}}$ century version. Finally, we used El Conde Lucanor BIBREF2 as a third corpus. This data set contains the original 14$^{\mathrm {th}}$ century Spanish novel by Don Juan Manuel, and its correspondent 21$^{\mathrm {st}}$ century version. Due to the small size of the corpus, we decided to use it only as a test. Additionally, unable to find a suitable training corpus, we used the systems built for El Quijote—despite the original documents belonging to different time periods—in order to modernize El Conde Lucanor.ta:corp presents the corpora statistics.Experiments ::: Metrics	In order to measure the gains in human effort reduction, we made use of the following metrics:BIBREF37: measures the number of words edited by the user, normalized by the number of words in the final translation.BIBREF5: measures the number of mouse actions made by the user, normalized by the number of characters in the final translation.Additionally, to evaluate the quality of the modernization and the difficulty of each task, we made use of the following well-known metrics:BiLingual Evaluation Understudy (BLEU) BIBREF38: computes the geometric average of the modified n-gram precision, multiplied by a brevity factor that penalizes short sentences.Translation Error Rate (TER) BIBREF39: computes the number of word edit operations (insertion, substitution, deletion and swapping), normalized by the number of words in the final translation.We used sacreBLEU BIBREF40 for ensuring consistent BLEU scores. For determining whether two systems presented statistically significant differences, we applied approximate randomization tests BIBREF41, with $10,000$ repetitions and using a $p$-value of $0.05$.Experiments ::: User Simulation	Due to the high costs of an evaluation involving human agents, we carried out an automatic evaluation with simulated users whose desired modernizations correspond to the reference sentences.At each iteration, the user corrects the leftmost wrong word from the system's hypothesis. With this correction, a new prefix is validated. The associated cost of this correction is of one mouse action and one word stroke. The system, then, reacts to this feedback, generating a new suffix that completes the prefix to conform a new hypothesis. This process is repeated until hypothesis and reference are the same.Results	ta:quality presents the quality of the modernization. Both SMT and NMT approaches were able to significantly improved the baseline. That is, the modernized documents are easier to comprehend by a contemporary reader than the original documents. An exception to this is El Conde Lucanor. The SMT approach yielded significant improvements in terms of TER, but was worse in terms of BLEU. Moreover, the NMT approach yielded worst results in terms of both BLEU and TER. Most likely, this results are due to having used the systems trained with El Quijote for modernizing El Conde Lucanor (see se:corp).When comparing the SMT and NMT approaches, we observe that SMT yielded the best results in all cases. This behavior was already perceived by BIBREF2 and is, most likely, due to the small size of the training corpora—a well-known problem in NMT. However, while the goal of modernization is making historical documents as easier to comprehend by contemporary people as possible, our goal is different. In this work, our goal is to obtain an error-free modern copy of a historical document. To achieve this, we proposed an interactive collaboration between a human expert and our modernizing system, in order to reduce the effort needed to generate such copy. ta:effort presents the experimental results.Both SMT and NMT approaches yielded significant reductions of the human effort needed to modernize the Dutch Bible (up to 48 points in terms of WSR and 8 in terms of MAR) and El Quijote (up to 7 points in terms of WSR and 1 of MAR). For El Conde Lucanor, however, both approaches resulted in an increased of the effort need to generate an error-free modern version. This behavior was to be expected since the modernization quality for El Conde Lucanor was very low. Therefore, the system consistently generated wrong suffixes, resulting in the user having to make more corrections.Regarding the performance of both approaches, SMT achieved the highest effort reduction. This was reasonably expected since its modernization quality was better. However, in past neural IMT works BIBREF15, the neural IMT approach was able to yield further improvements despite having a lower translation quality than its SMT counterpart. Most likely, the reason of this is that, due to the small training corpora, the neural model was not able to reach its best performance, Nonetheless, we should address this in a future work.Results ::: Qualitative Analysis	fi:exIMT shows an example of modernizing a sentence from El Quijote with the interactive SMT approach. While the system's initial suggestion contains five errors, with the IMT protocol, the user only needs to make three corrections. With each correction, the system is able to improve its suggestions, reducing the total effort needed to achieve an error-free modernization. Note that this example has been chosen for illustrative purposes of a correct functioning of the system. The average sentences from El Quijote are longer, and there are times in which the system fails to take the human knowledge into account, resulting in an increase of the number of corrections. Nonetheless, as seen in se:res, overall the system is able to significantly decrease the human effort.fi:exINMT contains an example of modernizing the same sentence as in fi:exIMT, using the interactive NMT approach. This is an example in which the system fails to take into account the user's corrections, resulting in an increase of the human effort. It is specially worth noting the introduction of non-existing words such as durdos and duradas. This problem was probably caused by an incorrect segmentation of a word, via the byte pair encoding process, and should be address in a future work. Nonetheless, as seen in se:res, overall the system is able to significantly decrease the human effort.Conclusions and Future Work	In this work, we proposed a collaborative user–computer approach to create an error-free modern version of a historical document. We tested this proposal on a simulated environment, achieving significant reductions of the human effort. We built our modernization protocol based on both SMT and NMT approaches to prefix-based IMT. Although both systems yielded significant improvements for two data sets out of three, the SMT approach yielded the best results—both in terms of the human reduction and in the modernization quality of the initial system.As a future work, we want to further research the behavior of the neural systems. For that, we would like to explore techniques for enriching the training corpus with additional data, and the incorrect generation of words due to subwords. We would also like to develop new protocols based on successful IMT approaches. Finally, we should test our proposal with real users to obtain actual measures of the effort reduction.Acknowledgments	The research leading to these results has received funding from the European Union through Programa Operativo del Fondo Europeo de Desarrollo Regional (FEDER) from Comunitat Valencia (2014–2020) under project Sistemas de frabricación inteligentes para la indústria 4.0 (grant agreement IDIFEDER/2018/025); and from Ministerio de Economía y Competitividad (MINECO) under project MISMIS-FAKEnHATE (grant agreement PGC2018-096212-B-C31). We gratefully acknowledge the support of NVIDIA Corporation with the donation of a GPU used for part of this research.","['What previous approaches are presented for comparison?', 'What kind of data is used to train the model?', 'Does proposed approach use neural networks?', 'What machine learning techniques are used in the model architecture?', 'What language(s) is the model tested on?']","['Baseline system corresponds to considering the original document as the modernized version. They used two approaches SMT and NMT and compared to the baseline, SMT showed best results.', 'Modern and historical versions of literature like the Bible and a Spanish novel.', 'Unacceptable', 'Unacceptable', 'Dutch and Spanish']"
59,"Generating Black-Box Adversarial Examples for Text Classifiers Using a Deep Reinforced Model	Recently, generating adversarial examples has become an important means of measuring robustness of a deep learning model. Adversarial examples help us identify the susceptibilities of the model and further counter those vulnerabilities by applying adversarial training techniques. In natural language domain, small perturbations in the form of misspellings or paraphrases can drastically change the semantics of the text. We propose a reinforcement learning based approach towards generating adversarial examples in black-box settings. We demonstrate that our method is able to fool well-trained models for (a) IMDB sentiment classification task and (b) AG's news corpus news categorization task with significantly high success rates. We find that the adversarial examples generated are semantics-preserving perturbations to the original text.	Introduction	Adversarial examples are generally minimal perturbations applied to the input data in an effort to expose the regions of the input space where a trained model performs poorly. Prior works BIBREF0, BIBREF1 have demonstrated the ability of an adversary to evade state-of-the-art classifiers by carefully crafting attack examples which can be even imperceptible to humans. Following such approaches, there has been a number of techniques aimed at generating adversarial examples BIBREF2, BIBREF3. Depending on the degree of access to the target model, an adversary may operate in one of the two different settings: (a) black-box setting, where an adversary doesn't have access to target model's internal architecture or its parameters, (b) white-box setting, where an adversary has access to the target model, its parameters, and input feature representations. In both these settings, the adversary cannot alter the training data or the target model itself. Depending on the purpose of the adversary, adversarial attacks can be categorized as (a) targeted attack and (b) non-targeted attack. In a targeted attack, the output category of a generated example is intentionally controlled to a specific target category with limited change in semantic information. While a non-targeted attack doesn't care about the category of misclassified results.Most of the prior work has focused on image classification models where adversarial examples are obtained by introducing imperceptible changes to pixel values through optimization techniques BIBREF4, BIBREF5. However, generating natural language adversarial examples can be challenging mainly due to the discrete nature of text samples. Continuous data like image or speech is much more tolerant to perturbations compared to text BIBREF6. In textual domain, even a small perturbation is clearly perceptible and can completely change the semantics of the text. Another challenge for generating adversarial examples relates to identifying salient areas of the text where a perturbation can be applied successfully to fool the target classifier. In addition to fooling the target classifier, the adversary is designed with different constraints depending on the task and its motivations BIBREF7. In our work, we focus on constraining our adversary to craft examples with semantic preservation and minimum perturbations to the input text.Given different settings of the adversary, there are other works that have designed attacks in “gray-box” settings BIBREF8, BIBREF9, BIBREF10. However, the definitions of “gray-box” attacks are quite different in each of these approaches. In this paper, we focus on “black-box” setting where we assume that the adversary possesses a limited set of labeled data, which is different from the target's training data, and also has an oracle access to the system, i.e., one can query the target classifier with any input and get its corresponding predictions. We propose an effective technique to generate adversarial examples in a black-box setting. We develop an Adversarial Example Generator (AEG) model that uses a reinforcement learning framing to generate adversarial examples. We evaluate our models using a word-based BIBREF11 and character-based BIBREF12 text classification model on benchmark classification tasks: sentiment classification and news categorization. The adversarial sequences generated are able to effectively fool the classifiers without changing the semantics of the text. Our contributions are as follows:We propose a black-box non-targeted attack strategy by combining ideas of substitute network and adversarial example generation. We formulate it as a reinforcement learning task.We introduce an encoder-decoder that operates over words and characters of an input text and empowers the model to introduce word and character-level perturbations.We adopt a self-critical sequence training technique to train our model to generate examples that can fool or increase the probability of misclassification in text classifiers.We evaluate our models on two different datasets associated with two different tasks: IMDB sentiment classification and AG's news categorization task. We run ablation studies on various components of the model and provide insights into decisions of our model.Related Work	Generating adversarial examples to bypass deep learning classification models have been widely studied. In a white-box setting, some of the approaches include gradient-based BIBREF13, BIBREF6, decision function-based BIBREF2 and spatial transformation based perturbation techniquesBIBREF3. In a black-box setting, several attack strategies have been proposed based on the property of transferability BIBREF1. Papernot et al. BIBREF14, BIBREF15 relied on this transferability property where adversarial examples, generated on one classifier, are likely to cause another classifier to make the same mistake, irrespective of their architecture and training dataset. In order to generate adversarial samples, a local substitute model was trained with queries to the target model. Many learning systems allow query accesses to the model. However, there is little work that can leverage query-based access to target models to construct adversarial samples and move beyond transferability. These studies have primarily focused on image-based classifiers and cannot be directly applied to text-based classifiers.While there is limited literature for such approaches in NLP systems, there have been some studies that have exposed the vulnerabilities of neural networks in text-based tasks like machine translations and question answering. Belinkov and Bisk BIBREF16 investigated the sensitivity of neural machine translation (NMT) to synthetic and natural noise containing common misspellings. They demonstrate that state-of-the-art models are vulnerable to adversarial attacks even after a spell-checker is deployed. Jia et al. BIBREF17 showed that networks trained for more difficult tasks, such as question answering, can be easily fooled by introducing distracting sentences into text, but these results do not transfer obviously to simpler text classification tasks. Following such works, different methods with the primary purpose of crafting adversarial example have been explored. Recently, a work by Ebrahimi et al. BIBREF18 developed a gradient-based optimization method that manipulates discrete text structure at its one-hot representation to generate adversarial examples in a white-box setting. In another white-box based attack, Gong et al. BIBREF19 perturbed the word embedding of given text examples and projected them to the nearest neighbour in the embedding space. This approach is an adaptation of perturbation algorithms for images. Though the size and quality of embedding play a critical role, this targeted attack technique ensured that the generated text sequence is intelligible.Alzantot et al. BIBREF20 proposed a black-box targeted attack using a population-based optimization via genetic algorithm BIBREF21. The perturbation procedure consists of random selection of words, finding their nearest neighbours, ranking and substitution to maximize the probability of target category. In this method, random word selection in the sequence to substitute were full of uncertainties and might be meaningless for the target label when changed. Since our model focuses on black-box non-targeted attack using an encoder-decoder approach, our work is closely related to the following techniques in the literature: Wong (2017) BIBREF22, Iyyer et al. BIBREF23 and Gao et al. BIBREF24. Wong (2017) BIBREF22 proposed a GAN-inspired method to generate adversarial text examples targeting black-box classifiers. However, this approach was restricted to binary text classifiers. Iyyer et al. BIBREF23 crafted adversarial examples using their proposed Syntactically Controlled Paraphrase Networks (SCPNs). They designed this model for generating syntactically adversarial examples without compromising on the quality of the input semantics. The general process is based on the encoder-decoder architecture of SCPN. Gao et al. BIBREF24 implemented an algorithm called DeepWordBug that generates small text perturbations in a black box setting forcing the deep learning model to make mistakes. DeepWordBug used a scoring function to determine important tokens and then applied character-level transformations to those tokens. Though the algorithm successfully generates adversarial examples by introducing character-level attacks, most of the introduced perturbations are constricted to misspellings. The semantics of the text may be irreversibly changed if excessive misspellings are introduced to fool the target classifier. While SCPNs and DeepWordBug primary rely only on paraphrases and character transformations respectively to fool the classifier, our model uses a hybrid word-character encoder-decoder approach to introduce both paraphrases and character-level perturbations as a part of our attack strategy. Our attacks can be a test of how robust the text classification models are to word and character-level perturbations.Proposed Attack Strategy	Let us consider a target model $T$ and $(x,l)$ refers to the samples from the dataset. Given an instance $x$, the goal of the adversary is to generate adversarial examples $x^{\prime }$ such that $T(x^{\prime }) \ne l$, where $l$ denotes the true label i.e take one of the $K$ classes of the target classification model. The changes made to $x$ to get $x^{\prime }$ are called perturbations. We would like to have $x^{\prime }$ close to the original instance $x$. In a black box setting, we do not have knowledge about the internals of the target model or its training data. Previous work by Papernot et al. BIBREF14 train a separate substitute classifier such that it can mimic the decision boundaries of the target classifier. The substitute classifier is then used to craft adversarial examples. While these techniques have been applied for image classification models, such methods have not been explored extensively for text.We implement both the substitute network training and adversarial example generation using an encoder-decoder architecture called Adversarial Examples Generator (AEG). The encoder extracts the character and word information from the input text and produces hidden representations of words considering its sequence context information. A substitute network is not implemented separately but applied using an attention mechanism to weigh the encoded hidden states based on their relevance to making predictions closer to target model outputs. The attention scores provide certain level of interpretability to the model as the regions of text that need to perturbed can be identified and visualized. The decoder uses the attention scores obtained from the substitute network, combines it with decoder state information to decide if perturbation is required at this state or not and finally emits the text unit (a text unit may refer to a word or character). Inspired by a work by Luong et al. BIBREF25, the decoder is a word and character-level recurrent network employed to generate adversarial examples. Before the substitute network is trained, we pretrain our encoder-decoder model on common misspellings and paraphrase datasets to empower the model to produce character and word perturbations in the form of misspellings or paraphrases. For training substitute network and generation of adversarial examples, we randomly draw data that is disjoint from the training data of the black-box model since we assume the adversaries have no prior knowledge about the training data or the model. Specifically, we consider attacking a target classifier by generating adversarial examples based on unseen input examples. This is done by dividing the dataset into training, validation and test using 60-30-10 ratio. The training data is used by the target model, while the unseen validation samples are used with necessary data augmentation for our AEG model. We further improve our model by using a self-critical approach to finally generate better adversarial examples. The rewards are formulated based on the following goals: (a) fool the target classifier, (b) minimize the number of perturbations and (c) preserve the semantics of the text. In the following sections, we explain the encoder-decoder model and then describe the reinforcement learning framing towards generation of adversarial examples.Proposed Attack Strategy ::: Background and Notations	Most of the sequence generation models follow an encoder-decoder framework BIBREF26, BIBREF27, BIBREF28 where encoder and decoder are modelled by separate recurrent neural networks. Usually these models are trained using a pair of text $(x,y)$ where $x=[x_1, x_2..,x_n]$ is the input text and the $y=[y_1, y_2..,y_m]$ is the target text to be generated. The encoder transforms an input text sequence into an abstract representation $h$. While the decoder is employed to generate the target sequence using the encoded representation $h$. However, there are several studies that have incorporated several modifications to the standard encoder-decoder framework BIBREF29, BIBREF25, BIBREF30.Proposed Attack Strategy ::: Background and Notations ::: Encoder	Based on Bahdanau et al. BIBREF29, we encode the input text sequence using bidirectional gated recurrent units (GRUs) to encode the input text sequence $x$. Formally, we obtain an encoded representation given by: $\overleftrightarrow{h_t}= \overleftarrow{h_t} + \overrightarrow{h_t}$.Proposed Attack Strategy ::: Background and Notations ::: Decoder	The decoder is a forward GRU implementing an attention mechanism to recognize the units of input text sequence relevant for the generation of the next target work. The decoder GRU generates the next text unit at time step $j$ by conditioning on the current decoder state $s_j$, context vector $c_j$ computed using attention mechanism and previously generated text units. The probability of decoding each target unit is given by:where $f_d$ is used to compute a new attentional hidden state $\tilde{s_j}$. Given the encoded input representations $\overleftrightarrow{H}=\lbrace \overleftrightarrow{h_1}, ...,\overleftrightarrow{h_n}\rbrace $ and the previous decoder GRU state $s_{j-1}$, the context vector at time step $j$ is computed as: $c_j= Attn(\overleftrightarrow{H}, s_{j-1})$. $Attn(\cdot ,\cdot )$ computes a weight $\alpha _{jt}$ indicating the degree of relevance of an input text unit $x_t$ for predicting the target unit $y_j$ using a feed-forward network $f_{attn}$. Given a parallel corpus $D$, we train our model by minimizing the cross-entropy loss: $J=\sum _{(x,y)\in D}{-log p(y|x)}$.Adversarial Examples Generator (AEG) Architecture	In this task of adversarial example generation, we have black-box access to the target model; the generator is not aware of the target model architecture or parameters and is only capable of querying the target model with supplied inputs and obtaining the output predictions. To enable the model to have capabilities to generate word and character perturbations, we develop a hybrid encoder-decoder model, Adversarial Examples Generator (AEG), that operates at both word and character level to generate adversarial examples. Below, we explain the components of this model which have been improved to handle both word and character information from the text sequence.Adversarial Examples Generator (AEG) Architecture ::: Encoder	The encoder maps the input text sequence into a sequence of representations using word and character-level information. Our encoder (Figure FIGREF10) is a slight variant of Chen et al.BIBREF31. This approach providing multiple levels of granularity can be useful in order to handle rare or noisy words in the text. Given character embeddings $E^{(c)}=[e_1^{(c)}, e_2^{(c)},...e_{n^{\prime }}^{(c)}]$ and word embeddings $E^{(w)}=[e_1^{(w)}, e_2^{(w)},...e_{n}^{(w)}]$ of the input, starting ($p_t$) and ending ($q_t$) character positions at time step $t$, we define inside character embeddings as: $E_I^{(c)}=[e_{p_t}^{(c)},...., e_{q_t}^{(c)}]$ and outside embeddings as: $E_O^{(c)}=[e_{1}^{(c)},....,e_{p_t-1}^{(c)}; e_{q_t+1}^{(c)},...,e_{n^{\prime }}^{(c)}]$. First, we obtain the character-enhanced word representation $\overleftrightarrow{h_t}$ by combining the word information from $E^{(w)}$ with the character context vectors. Character context vectors are obtained by attending over inside and outside character embeddings. Next, we compute a summary vector $S$ over the hidden states $\overleftrightarrow{h_t}$ using an attention layer expressed as $Attn(\overleftrightarrow{H})$. To generate adversarial examples, it is important to identify the most relevant text units that contribute towards the target model's prediction and then use this information during the decoding step to introduce perturbation on those units. Hence, the summary vector is optimized using target model predictions without back propagating through the entire encoder. This acts as a substitute network that learns to mimic the predictions of the target classifier.Adversarial Examples Generator (AEG) Architecture ::: Decoder	Our AEG should be able to generate both character and word level perturbations as necessary. We achieve this by modifying the standard decoder BIBREF29, BIBREF30 to have two-level decoder GRUs: word-GRU and character-GRU (see Figure FIGREF14). Such hybrid approaches have been studied to achieve open vocabulary NMT in some of the previous work like Wu et al. BIBREF32 and Luong et al. BIBREF25. Given the challenge that all different word misspellings cannot fit in a fixed vocabulary, we leverage the power of both words and characters in our generation procedure. The word-GRU uses word context vector $c_j^{(w)}$ by attending over the encoder hidden states $\overleftrightarrow{h_t}$. Once the word context vector $c_j^{(w)}$ is computed, we introduce a perturbation vector $v_{p}$ to impart information about the need for any word or character perturbations at this decoding step. We construct this vector using the word-GRU decoder state $s_j^{(w)}$, context vector $c_j^{(w)}$ and summary vector $S$ from the encoder as:We modify the the Equation (DISPLAY_FORM8) as: $\tilde{s}_j^{(w)}=f_{d}^{(w)}([c_j^{(w)};s_j^{(w)};v_{p}])$. The character-GRU will decide if the word is emitted with or without misspellings. We don't apply step-wise attention for character-GRU, instead we initialize it with the correct context. The ideal candidate representing the context must combine information about: (a) the word obtained from $c_j^{(w)}, s_j^{(w)}$, (b) its character alignment with the input characters derived from character context vector $c_j^{(c)}$ with respect to the word-GRU's state and (c) perturbation embedded in $v_p$. This yields,Thus, $\tilde{s}_j^{(c)}$ is initialized to the character-GRU only for the first hidden state. With this mechanism, both word and character level information can be used to introduce necessary perturbations.Training ::: Supervised Pretraining with Teacher Forcing	The primary purpose of pretraining AEG is to enable our hybrid encoder-decoder to encode both character and word information from the input example and produce both word and character-level transformations in the form of paraphrases or misspellings. Though the pretraining helps us mitigate the cold-start issue, it does not guarantee that these perturbed texts will fool the target model. There are large number of valid perturbations that can be applied due to multiple ways of arranging text units to produce paraphrases or different misspellings. Thus, minimizing $J_{mle}$ is not sufficient to generate adversarial examples.Training ::: Supervised Pretraining with Teacher Forcing ::: Dataset Collection	In this paper, we use paraphrase datasets like PARANMT-50M corpusBIBREF33, Quora Question Pair dataset and Twitter URL paraphrasing corpus BIBREF34. These paraphrase datasets together contains text from various sources: Common Crawl, CzEng1.6, Europarl, News Commentary, Quora questions, and Twitter trending topic tweets. We do not use all the data for our pretraining. We randomly sample 5 million parallel texts and augment them using simple character-transformations (eg. random insertion, deletion or replacement) to words in the text. The number of words that undergo transformation is capped at 10% of the total number of words in the text. We further include examples which contain only character-transformations without paraphrasing the original input.Training ::: Supervised Pretraining with Teacher Forcing ::: Training Objective	AEG is pre-trained using teacher-forcing algorithm BIBREF35 on the dataset explained in Section SECREF3. Consider an input text: “movie was good” that needs to be decoded into the following target perturbed text: “film is gud”. The word “gud” might be out-of-vocabulary indicated by $<oov>$. Hence, we compute the loss incurred by word-GRU decoder, $J^{(w)}$, when predicting {“film”, “is”, “$<oov>$”} and loss incurred by character-GRU decoder, $J^{(c)}$, when predicting {`f', `i',`l', `m', `_'},{`i',`s','_'},{`g', `u',`d',`_'}. Therefore, the training objective in Section SECREF7 is modified into:Training ::: Training with Reinforcement learning	We fine-tune our model to fool a target classifier by learning a policy that maximizes a specific discrete metric formulated based on the constraints required to generate adversarial examples. In our work, we use the self-critical approach of Rennie et al. BIBREF36 as our policy gradient training algorithm.Training ::: Training with Reinforcement learning ::: Self-critical sequence training (SCST)	In SCST approach, the model learns to gather more rewards from its sampled sequences that bring higher rewards than its best greedy counterparts. First, we compute two sequences: (a) $y^{\prime }$ sampled from the model's distribution $p(y^{\prime }_j|y^{\prime }_{<j},h)$ and (b) $\hat{y}$ obtained by greedily decoding ($argmax$ predictions) from the distribution $p(\hat{y}_j|\hat{y}_{<j},h)$ Next, rewards $r(y^{\prime }_j),r(\hat{y}_j)$ are computed for both the sequences using a reward function $r(\cdot )$, explained in Section SECREF26. We train the model by minimizing:Here $r(\hat{y})$ can be viewed as the baseline reward. This approach, therefore, explores different sequences that produce higher reward compared to the current best policy.Training ::: Training with Reinforcement learning ::: Rewards	The reward $r(\hat{y})$ for the sequence generated is a weighted sum of different constraints required for generating adversarial examples. Since our model operates at word and character levels, we therefore compute three rewards: adversarial reward, semantic similarity and lexical similarity reward. The reward should be high when: (a) the generated sequence causes the target model to produce a low classification prediction probability for its ground truth category, (b) semantic similarity is preserved and (c) the changes made to the original text are minimal.Training ::: Training with Reinforcement learning ::: Rewards ::: Adversarial Reward	Given a target model $T$, it takes a text sequence $y$ and outputs prediction probabilities $P$ across various categories of the target model. Given an input sample $(x, l)$, we compute a perturbation using our AEG model and produce a sequence $y$. We compute the adversarial reward as $R_{A}=(1-P_l)$, where the ground truth $l$ is an index to the list of categories and $P_l$ is the probability that the perturbed generated sequence $y$ belongs to target ground truth $l$. Since we want the target classifier to make mistakes, we promote it by rewarding higher when the sequences produce low target probabilities.Training ::: Training with Reinforcement learning ::: Rewards ::: Semantic Similarity	Inspired by the work of Li et al. BIBREF37, we train a deep matching model that can represent the degree of match between two texts. We use character based biLSTM models with attention BIBREF38 to handle word and character level perturbations. The matching model will help us compute the the semantic similarity $R_S$ between the text generated and the original input text.Training ::: Training with Reinforcement learning ::: Rewards ::: Lexical Similarity	Since our model functions at both character and word level, we compute the lexical similarity. The purpose of this reward is to keep the changes as minimal as possible to just fool the target classifier. Motivated by the recent work of Moon et al. BIBREF39, we pretrain a deep neural network to compute approximate Levenshtein distance $R_{L}$ composed of character based bi-LSTM model. We replicate that model by generating a large number of text with perturbations in the form of insertions, deletions or replacements. We also include words which are prominent nicknames, abbreviations or inconsistent notations to have more lexical similarity. This is generally not possible using direct Levenshtein distance computation. Once trained, it can produce a purely lexical embedding of the text without semantic allusion. This can be used to compute the lexical similarity between the generated text $y$ and the original input text $x$ for our purpose.Finally, we combine all these three rewards using:where $\gamma _A, \gamma _S, \gamma _L$ are hyperparameters that can be modified depending upon the kind of textual generations expected from the model. The changes inflicted by different reward coefficients can be seen in Section SECREF44.Training ::: Training Details	We trained our models on 4 GPUs. The parameters of our hybrid encoder-decoder were uniformly initialized to $[-0.1, 0.1]$. The optimization algorithm used is Adam BIBREF40. The encoder word embedding matrices were initialized with 300-dimensional Glove vectors BIBREF41. During reinforcement training, we used plain stochastic gradient descent with a learning rate of 0.01. Using a held-out validation set, the hyper-parameters for our experiments are set as follows: $\gamma _A=1, \gamma _S=0.5, \gamma _L=0.25$.Experiments	In this section, we describe the evaluation setup used to measure the effectiveness of our model in generating adversarial examples. The success of our model lies in its ability to fool the target classifier. We pretrain our models with dataset that generates a number of character and word perturbations. We elaborate on the experimental setup and the results below.Experiments ::: Setup	We conduct experiments on different datasets to verify if the accuracy of the deep learning models decrease when fed with the adversarial examples generated by our model. We use benchmark sentiment classification and news categorization datasets and the details are as follows:Sentiment classification: We trained a word-based convolutional model (CNN-Word) BIBREF11 on IMDB sentiment dataset . The dataset contains 50k movie reviews in total which are labeled as positive or negative. The trained model achieves a test accuracy of 89.95% which is relatively close to the state-of-the-art results on this dataset.News categorization: We perform our experiments on AG's news corpus with a character-based convolutional model (CNN-Char) BIBREF12. The news corpus contains titles and descriptions of various news articles along with their respective categories. There are four categories: World, Sports, Business and Sci/Tech. The trained CNN-Char model achieves a test accuracy of 89.11%.Table TABREF29 summarizes the data and models used in our experiments. We compare our proposed model with the following black-box non-targeted attacks:Random: We randomly select a word in the text and introduce some perturbation to that word in the form of a character replacement or synonymous word replacement. No specific strategy to identify importance of words.NMT-BT: We generate paraphrases of the sentences of the text using a back-translation approach BIBREF23. We used pretrained English$\leftrightarrow $German translation models to obtain back-translations of input examples.DeepWordBug BIBREF24: A scoring function is used to determine the important tokens to change. The tokens are then modified to evade a target model.No-RL: We use our pretrained model without the reinforcement learning objective.The performance of these methods are measured by the percentage fall in accuracy of these models on the generated adversarial texts. Higher the percentage dip in the accuracy of the target classifier, more effective is our model.Experiments ::: Quantitative Analysis	We analyze the effectiveness of our approach by comparing the results from using two different baselines against character and word-based models trained on different datasets. Table TABREF40 demonstrates the capability of our model. Without the reinforcement learning objective, the No-RL model performs better than the back-translation approach(NMT-BT). The improvement can be attributed to the word and character perturbations introduced by our hybrid encoder-decoder model as opposed to only paraphrases in the former model. Our complete AEG model outperforms all the other models with significant drop in accuracy. For the CNN-Word, DeepWordBug decreases the accuracy from 89.95% to 28.13% while AEG model further reduces it to 18.5%.It is important to note that our model is able to expose the weaknesses of the target model irrespective of the nature of the model (either word or character level). It is interesting that even simple lexical substitutions and paraphrases can break such models on both datasets we tested. Across different models, the character-based models are less susceptible to adversarial attacks compared to word-based models as they are able to handle misspellings and provide better generalizations.Experiments ::: Human Evaluation	We also evaluated our model based on human judgments. We conducted an experiment where the workers were presented with randomly sampled 100 adversarial examples generated by our model which were successful in fooling the target classifier. The examples were shuffled to mitigate ordering bias, and every example was annotated by three workers. The workers were asked to label the sentiment of the sampled adversarial example. For every adversarial example shown, we also showed the original text and asked them to rate their similarity on a scale from 0 (Very Different) to 3 (Very Similar). We found that the perturbations produced by our model do not affect the human judgments significantly as $94.6\%$ of the human annotations matched with the ground-truth label of the original text. The average similarity rating of $1.916$ also indicated that the generated adversarial sequences are semantics-preserving.Experiments ::: Ablation Studies	In this section, we make different modifications to our encoder and decoder to weigh the importance of these techniques: (a) No perturbation vector (No Pert) and finally (b) a simple character based decoder (Char-dec) but involves perturbation vector. Table TABREF40 shows that the absence of hybrid decoder leads to a significant drop in the performance of our model. The main reason we believe is that hybrid decoder is able to make targeted attacks on specific words which otherwise is lost while generating text using a pure-character based decoder. In the second case case, the most important words associated with the prediction of the target model are identified by the summary vector. When the perturbation vector is used, it carries forward this knowledge and decides if a perturbation should be performed at this step or not. This can be verified even in Figure FIGREF43, where the regions of high attention get perturbed in the text generated.Experiments ::: Qualitative Analysis	We qualitatively analyze the results by visualizing the attention scores and the perturbations introduces by our model. We further evaluate the importance of hyperparameters $\gamma _{(.)}$ in the reward function. We set only one of the hyperparameters closer to 1 and set the remaining closer to zero to see how it affects the text generation. The results can be seen in Figure FIGREF43. Based on a subjective qualitative evaluation, we make the following observations:Promisingly, it identifies the most important words that contribute to particular categorization. The model introduces misspellings or word replacements without significant change in semantics of the text.When the coefficient associated only with adversarial reward goes to 1, it begins to slowly deviate though not completely. This is motivated by the initial pretraining step on paraphrases and perturbations.Conclusion	In this work, we have introduced a $AEG$, a model capable of generating adversarial text examples to fool the black-box text classification models. Since we do not have access to gradients or parameters of the target model, we modelled our problem using a reinforcement learning based approach. In order to effectively baseline the REINFORCE algorithm for policy-gradients, we implemented a self-critical approach that normalizes the rewards obtained by sampled sentences with the rewards obtained by the model under test-time inference algorithm. By generating adversarial examples for target word and character-based models trained on IMDB reviews and AG's news dataset, we find that our model is capable of generating semantics-preserving perturbations that leads to steep decrease in accuracy of those target models. We conducted ablation studies to find the importance of individual components of our system. Extremely low values of the certain reward coefficient constricts the quantitative performance of the model can also lead to semantic divergence. Therefore, the choice of a particular value for this model should be motivated by the demands of the context in which it is applied. One of the main challenges of such approaches lies in the ability to produce more synthetic data to train the generator model in the distribution of the target model's training data. This can significantly improve the performance of our model. We hope that our method motivates a more nuanced exploration into generating adversarial examples and adversarial training for building robust classification models.","['Do they manually check all adversarial examples that fooled some model for potential valid examples?', 'Are all generated examples semantics-preserving perturbations to the original text?', 'What is success rate of fooling tested models in experiments?', ""What models are able to be fooled for AG's news corpus news categorization task by this approach?"", 'What models are able to be fooled for IMDB sentiment classification task by this approach?', 'Do they use already trained model on some task in their reinforcement learning approach?', 'How does proposed reinforcement learning based approach generate adversarial examples in black-box settings?']","['Unacceptable', 'While the models aim to generate examples which preserve the semantics of the text with minimal perturbations, the Random model randomly replaces a character, which may not preserve the semantics. ', ""Authors best attacking model resulted in dip in the accuracy of CNN-Word (IMDB) by 79.43% and CNN-Char (AG's News) model by 72.16%"", 'Unacceptable', 'A word-based convolutional neural network (CNN-Word)', 'Unacceptable', 'Unacceptable']"
60,"And That's A Fact: Distinguishing Factual and Emotional Argumentation in Online Dialogue	We investigate the characteristics of factual and emotional argumentation styles observed in online debates. Using an annotated set of""factual""and""feeling""debate forum posts, we extract patterns that are highly correlated with factual and emotional arguments, and then apply a bootstrapping methodology to find new patterns in a larger pool of unannotated forum posts. This process automatically produces a large set of patterns representing linguistic expressions that are highly correlated with factual and emotional language. Finally, we analyze the most discriminating patterns to better understand the defining characteristics of factual and emotional arguments.	Introduction	Human lives are being lived online in transformative ways: people can now ask questions, solve problems, share opinions, or discuss current events with anyone they want, at any time, in any location, on any topic. The purposes of these exchanges are varied, but a significant fraction of them are argumentative, ranging from hot-button political controversies (e.g., national health care) to religious interpretation (e.g., Biblical exegesis). And while the study of the structure of arguments has a long lineage in psychology BIBREF0 and rhetoric BIBREF1 , large shared corpora of natural informal argumentative dialogues have only recently become available.Natural informal dialogues exhibit a much broader range of argumentative styles than found in traditional work on argumentation BIBREF2 , BIBREF0 , BIBREF3 , BIBREF4 . Recent work has begun to model different aspects of these natural informal arguments, with tasks including stance classification BIBREF5 , BIBREF6 , argument summarization BIBREF7 , sarcasm detection BIBREF8 , and work on the detailed structure of arguments BIBREF9 , BIBREF10 , BIBREF11 . Successful models of these tasks have many possible applications in sentiment detection, automatic summarization, argumentative agents BIBREF12 , and in systems that support human argumentative behavior BIBREF13 .Our research examines factual versus feeling argument styles, drawing on annotations provided in the Internet Argument Corpus (IAC) BIBREF6 . This corpus includes quote-response pairs that were manually annotated with respect to whether the response is primarily a factual or feeling based argument, as Section SECREF2 describes in more detail. Figure FIGREF1 provides examples of responses in the IAC (paired with preceding quotes to provide context), along with the response's factual vs. feeling label.factual responses may try to bolster their argument by providing statistics related to a position, giving historical or scientific background, or presenting specific examples or data. There is clearly a relationship between a proposition being factual versus objective or veridical, although each of these different labelling tasks may elicit differences from annotators BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 .The feeling responses may seem to lack argumentative merit, but previous work on argumentation describes situations in which such arguments can be effective, such as the use of emotive arguments to draw attention away from the facts, or to frame a discussion in a particular way BIBREF18 , BIBREF19 . Furthermore, work on persuasion suggest that feeling based arguments can be more persuasive in particular circumstances, such as when the hearer shares a basis for social identity with the source (speaker) BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 . However none of this work has documented the linguistic patterns that characterize the differences in these argument types, which is a necessary first step to their automatic recognition or classification. Thus the goal of this paper is to use computational methods for pattern-learning on conversational arguments to catalog linguistic expressions and stylistic properties that distinguish Factual from Emotional arguments in these on-line debate forums.Section SECREF2 describes the manual annotations for factual and feeling in the IAC corpus. Section SECREF5 then describes how we generate lexico-syntactic patterns that occur in both types of argument styles. We use a weakly supervised pattern learner in a bootstrapping framework to automatically generate lexico-syntactic patterns from both annotated and unannotated debate posts. Section SECREF3 evaluates the precision and recall of the factual and feeling patterns learned from the annotated texts and after bootstrapping on the unannotated texts. We also present results for a supervised learner with bag-of-word features to assess the difficulty of this task. Finally, Section SECREF4 presents analyses of the linguistic expressions found by the pattern learner and presents several observations about the different types of linguistic structures found in factual and feeling based argument styles. Section SECREF5 discusses related research, and Section SECREF6 sums up and proposes possible avenues for future work.Pattern Learning for Factual and Emotional Arguments	We first describe the corpus of online debate posts used for our research, and then present a bootstrapping method to identify linguistic expressions associated with factual and feeling arguments.Data	The IAC corpus is a freely available annotated collection of 109,553 forum posts (11,216 discussion threads). In such forums, conversations are started by posting a topic or a question in a particular category, such as society, politics, or religion BIBREF6 . Forum participants can then post their opinions, choosing whether to respond directly to a previous post or to the top level topic (start a new thread). These discussions are essentially dialogic; however the affordances of the forum such as asynchrony, and the ability to start a new thread rather than continue an existing one, leads to dialogic structures that are different than other multiparty informal conversations BIBREF25 . An additional source of dialogic structure in these discussions, above and beyond the thread structure, is the use of the quote mechanism, which is an interface feature that allows participants to optionally break down a previous post into the components of its argument and respond to each component in turn.The IAC includes 10,003 Quote-Response (Q-R) pairs with annotations for factual vs. feeling argument style, across a range of topics. Figure FIGREF4 shows the wording of the survey question used to collect the annotations. Fact vs. Feeling was measured as a scalar ranging from -5 to +5, because previous work suggested that taking the means of scalar annotations reduces noise in Mechanical Turk annotations BIBREF26 . Each of the pairs was annotated by 5-7 annotators. For our experiments, we use only the response texts and assign a binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class. We did not use the responses with scores between -1 and 1 because they had a very weak Fact/Feeling assessment, which could be attributed to responses either containing aspects of both factual and feeling expression, or neither. The resulting set contains 3,466 fact and 2,382 feeling posts. We randomly partitioned the fact/feel responses into three subsets: a training set with 70% of the data (2,426 fact and 1,667 feeling posts), a development (tuning) set with 20% of the data (693 fact and 476 feeling posts), and a test set with 10% of the data (347 fact and 239 feeling posts). For the bootstrapping method, we also used 11,560 responses from the unannotated data.Bootstrapped Pattern Learning	The goal of our research is to gain insights into the types of linguistic expressions and properties that are distinctive and common in factual and feeling based argumentation. We also explore whether it is possible to develop a high-precision fact vs. feeling classifier that can be applied to unannotated data to find new linguistic expressions that did not occur in our original labeled corpus.To accomplish this, we use the AutoSlog-TS system BIBREF27 to extract linguistic expressions from the annotated texts. Since the IAC also contains a large collection of unannotated texts, we then embed AutoSlog-TS in a bootstrapping framework to learn additional linguistic expressions from the unannotated texts. First, we briefly describe the AutoSlog-TS pattern learner and the set of pattern templates that we used. Then, we present the bootstrapping process to learn more Fact/Feeling patterns from unannotated texts.To learn patterns from texts labeled as fact or feeling arguments, we use the AutoSlog-TS BIBREF27 extraction pattern learner, which is freely available for research. AutoSlog-TS is a weakly supervised pattern learner that requires training data consisting of documents that have been labeled with respect to different categories. For our purposes, we provide AutoSlog-TS with responses that have been labeled as either fact or feeling.AutoSlog-TS uses a set of syntactic templates to define different types of linguistic expressions. The left-hand side of Figure FIGREF8 shows the set of syntactic templates defined in the AutoSlog-TS software package. PassVP refers to passive voice verb phrases (VPs), ActVP refers to active voice VPs, InfVP refers to infinitive VPs, and AuxVP refers to VPs where the main verb is a form of “to be” or “to have”. Subjects (subj), direct objects (dobj), noun phrases (np), and possessives (genitives) can be extracted by the patterns. AutoSlog-TS applies the Sundance shallow parser BIBREF28 to each sentence and finds every possible match for each pattern template. For each match, the template is instantiated with the corresponding words in the sentence to produce a specific lexico-syntactic expression. The right-hand side of Figure FIGREF8 shows an example of a specific lexico-syntactic pattern that corresponds to each general pattern template.In addition to the original 17 pattern templates in AutoSlog-TS (shown in Figure FIGREF8 ), we defined 7 new pattern templates for the following bigrams and trigrams: Adj Noun, Adj Conj Adj, Adv Adv, Adv Adv Adv, Adj Adj, Adv Adj, Adv Adv Adj. We added these n-gram patterns to provide coverage for adjective and adverb expressions because the original templates were primarily designed to capture noun phrase and verb phrase expressions.The learning process in AutoSlog-TS has two phases. In the first phase, the pattern templates are applied to the texts exhaustively, so that lexico-syntactic patterns are generated for (literally) every instantiation of the templates that appear in the corpus. In the second phase, AutoSlog-TS uses the labels associated with the texts to compute statistics for how often each pattern occurs in each class of texts. For each pattern INLINEFORM0 , we collect P(factual INLINEFORM1 INLINEFORM2 ) and P(feeling INLINEFORM3 INLINEFORM4 ), as well as the pattern's overall frequency in the corpus.Since the IAC data set contains a large number of unannotated debate forum posts, we embedd AutoSlog-TS in a bootstrapping framework to learn additional patterns. The flow diagram for the bootstrapping system is shown in Figure FIGREF10 .Initially, we give the labeled training data to AutoSlog-TS, which generates patterns and associated statistics. The next step identifies high-precision patterns that can be used to label some of the unannotated texts as factual or feeling. We define two thresholds: INLINEFORM0 to represent a minimum frequency value, and INLINEFORM1 to represent a minimum probability value. We found that using only a small set of patterns (when INLINEFORM2 is set to a high value) achieves extremely high precision, yet results in a very low recall. Instead, we adopt a strategy of setting a moderate probability threshold to identify reasonably reliable patterns, but labeling a text as factual or feeling only if it contains at least a certain number different patterns for that category, INLINEFORM3 . In order to calibrate the thresholds, we experimented with a range of threshold values on the development (tuning) data and identified INLINEFORM4 =3, INLINEFORM5 =.70, and INLINEFORM6 =3 for the factual class, and INLINEFORM7 =3, INLINEFORM8 =.55, and INLINEFORM9 =3 for the feeling class as having the highest classification precision (with non-trivial recall).The high-precision patterns are then used in the bootstrapping framework to identify more factual and feeling texts from the 11,561 unannotated posts, also from 4forums.com. For each round of bootstrapping, the current set of factual and feeling patterns are matched against the unannotated texts, and posts that match at least 3 patterns associated with a given class are assigned to that class. As shown in Figure FIGREF10 , the Bootstrapped Data Balancer then randomly selects a balanced subset of the newly classified posts to maintain the same proportion of factual vs. feeling documents throughout the bootstrapping process. These new documents are added to the set of labeled documents, and the bootstrapping process repeats. We use the same threshold values to select new high-precision patterns for all iterations.Evaluation	We evaluate the effectiveness of the learned patterns by applying them to the test set of 586 posts (347 fact and 239 feeling posts, maintaining the original ratio of fact to feel data in train). We classify each post as factual or feeling using the same procedure as during bootstrapping: a post is labeled as factual or feeling if it matches at least three high-precision patterns for that category. If a document contains three patterns for both categories, then we leave it unlabeled. We ran the bootstrapping algorithm for four iterations.The upper section of Table TABREF11 shows the Precision and Recall results for the patterns learned during bootstrapping. The Iter 0 row shows the performance of the patterns learned only from the original, annotated training data. The remaining rows show the results for the patterns learned from the unannotated texts during bootstrapping, added cumulatively. We show the results after each iteration of bootstrapping.Table TABREF11 shows that recall increases after each bootstrapping iteration, demonstrating that the patterns learned from the unannotated texts yield substantial gains in coverage over those learned only from the annotated texts. Recall increases from 22.8% to 40.9% for fact, and from 8.0% to 18.8% for feel. The precision for the factual class is reasonably good, but the precision for the feeling class is only moderate. However, although precision typically decreases during boostrapping due to the addition of imperfectly labeled data, the precision drop during bootstrapping is relatively small.We also evaluated the performance of a Naive Bayes (NB) classifier to assess the difficulty of this task with a traditional supervised learning algorithm. We trained a Naive Bayes classifier with unigram features and binary values on the training data, and identified the best Laplace smoothing parameter using the development data. The bottom row of Table TABREF11 shows the results for the NB classifier on the test data. These results show that the NB classifier yields substantially higher recall for both categories, undoubtedly due to the fact that the classifier uses all unigram information available in the text. Our pattern learner, however, was restricted to learning linguistic expressions in specific syntactic constructions, usually requiring more than one word, because our goal was to study specific expressions associated with factual and feeling argument styles. Table TABREF11 shows that the lexico-syntactic patterns did obtain higher precision than the NB classifier, but with lower recall.Table TABREF14 shows the number of patterns learned from the annotated data (Iter 0) and the number of new patterns added after each bootstrapping iteration. The first iteration dramatically increases the set of patterns, and more patterns are steadily added throughout the rest of bootstrapping process.The key take-away from this set of experiments is that distinguishing factual and feeling argumets is clearly a challenging task. There is substantial room for improvement for both precision and recall, and surprisingly, the feeling class seems to be harder to accurately recognize than the factual class. In the next section, we examine the learned patterns and their syntactic forms to better understand the language used in the debate forums.Analysis	Table TABREF13 provides examples of patterns learned for each class that are characteristic of that class. We observe that patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases. In contrast, the patterns associated with feeling based arguments are often based on the speaker's own beliefs or claims, perhaps assuming that they themselves are credible BIBREF20 , BIBREF24 , or they involve assessment or evaluations of the arguments of the other speaker BIBREF29 . They are typically also very creative and diverse, which may be why it is hard to get higher accuracies for feeling classification, as shown by Table TABREF11 .Figure FIGREF15 shows the distribution of syntactic forms (templates) among all of the high-precision patterns identified for each class during bootstrapping. The x-axes show the syntactic templates and the y-axes show the percentage of all patterns that had a specific syntactic form. Figure FIGREF15 counts each lexico-syntactic pattern only once, regardless of how many times it occurred in the data set. Figure FIGREF15 counts the number of instances of each lexico-syntactic pattern. For example, Figure FIGREF15 shows that the Adj Noun syntactic form produced 1,400 different patterns, which comprise 22.6% of the distinct patterns learned. Figure FIGREF15 captures the fact that there are 7,170 instances of the Adj Noun patterns, which comprise 17.8% of all patterns instances in the data set.For factual arguments, we see that patterns with prepositional phrases (especially NP Prep) and passive voice verb phrases are more common. Instantiations of NP Prep are illustrated by FC1, FC5, FC8, FC10 in Table TABREF13 . Instantiations of PassVP are illustrated by FC2 and FC4 in Table TABREF13 . For feeling arguments, expressions with adjectives and active voice verb phrases are more common. Almost every high probability pattern for feeling includes an adjective, as illustrated by every pattern except FE8 in Table TABREF13 . Figure FIGREF15 shows that three syntactic forms account for a large proportion of the instances of high-precision patterns in the data: Adj Noun, NP Prep, and ActVP.Next, we further examine the NP Prep patterns since they are so prevalent. Figure FIGREF19 shows the percentages of the most frequently occurring prepositions found in the NP Prep patterns learned for each class. Patterns containing the preposition ""of"" make up the vast majority of prepositional phrases for both the fact and feel classes, but is more common in the fact class. In contrast, we observe that patterns with the preposition “for” are substantially more common in the feel class than the fact class.Table TABREF20 shows examples of learned NP Prep patterns with the preposition ""of"" in the fact class and ""for"" in the feel class. The ""of"" preposition in the factual arguments often attaches to objective terminology. The ""for"" preposition in the feeling-based arguments is commonly used to express advocacy (e.g., demand for) or refer to affected population groups (e.g., treatment for). Interestingly, these phrases are subtle indicators of feeling-based arguments rather than explicit expressions of emotion or sentiment.Related Work	Related research on argumentation has primarily worked with different genres of argument than found in IAC, such as news articles, weblogs, legal briefs, supreme court summaries, and congressional debates BIBREF2 , BIBREF30 , BIBREF31 , BIBREF0 , BIBREF3 , BIBREF4 . The examples from IAC in Figure FIGREF1 illustrate that natural informal dialogues such as those found in online forums exhibit a much broader range of argumentative styles. Other work has on models of natural informal arguments have focused on stance classification BIBREF32 , BIBREF5 , BIBREF6 , argument summarization BIBREF7 , sarcasm detection BIBREF8 , and identifying the structure of arguments such as main claims and their justifications BIBREF9 , BIBREF10 , BIBREF11 .Other types of language data also typically contains a mixture of subjective and objective sentences, e.g. Wiebe et al. wiebeetal2001a,wiebeetalcl04 found that 44% of sentences in a news corpus were subjective. Our work is also related to research on distinguishing subjective and objective text BIBREF33 , BIBREF34 , BIBREF14 , including bootstrapped pattern learning for subjective/objective sentence classification BIBREF15 . However, prior work has primarily focused on news texts, not argumentation, and the notion of objective language is not exactly the same as factual. Our work also aims to recognize emotional language specifically, rather than all forms of subjective language. There has been substantial work on sentiment and opinion analysis (e.g., BIBREF35 , BIBREF36 , BIBREF37 , BIBREF38 , BIBREF39 , BIBREF40 ) and recognition of specific emotions in text BIBREF41 , BIBREF42 , BIBREF43 , BIBREF44 , which could be incorporated in future extensions of our work. We also hope to examine more closely the relationship of this work to previous work aimed at the identification of nasty vs. nice arguments in the IAC BIBREF45 , BIBREF8 .Conclusion	In this paper, we use observed differences in argumentation styles in online debate forums to extract patterns that are highly correlated with factual and emotional argumentation. From an annotated set of forum post responses, we are able extract high-precision patterns that are associated with the argumentation style classes, and we are then able to use these patterns to get a larger set of indicative patterns using a bootstrapping methodology on a set of unannotated posts.From the learned patterns, we derive some characteristic syntactic forms associated with the fact and feel that we use to discriminate between the classes. We observe distinctions between the way that different arguments are expressed, with respect to the technical and more opinionated terminologies used, which we analyze on the basis of grammatical forms and more direct syntactic patterns, such as the use of different prepositional phrases. Overall, we demonstrate how the learned patterns can be used to more precisely gather similarly-styled argument responses from a pool of unannotated responses, carrying the characteristics of factual and emotional argumentation style.In future work we aim to use these insights about argument structure to produce higher performing classifiers for identifying factual vs. feeling argument styles. We also hope to understand in more detail the relationship between these argument styles and the heurstic routes to persuasion and associated strategies that have been identified in previous work on argumentation and persuasion BIBREF2 , BIBREF0 , BIBREF4 .Acknowledgments	This work was funded by NSF Grant IIS-1302668-002 under the Robust Intelligence Program. The collection and annotation of the IAC corpus was supported by an award from NPS-BAA-03 to UCSC and an IARPA Grant under the Social Constructs in Language Program to UCSC by subcontract from the University of Maryland.","['Do they report results only on English data?', 'What are the most discriminating patterns which are analyzed?', 'What bootstrapping methodology was used to find new patterns?', 'What patterns were extracted which were correlated with emotional arguments?', 'What patterns were extracted which were correlated with factual arguments?', 'How were the factual and feeling forum posts annotated?']","['Unacceptable', 'Patterns containing the preposition ""of"" make up the vast majority of prepositional phrases for both the fact and feel classes and  patterns with the preposition “for” are substantially more common in the feel class than the fact class.', 'Unacceptable', 'Examples of extracted patters with high probability that contain of are: MARRIAGE FOR, STANDING FOR, SAME FOR, TREATMENT FOR, DEMAND FOR, ATTENTION FOR, ADVOCATE FOR, NO EVIDENCE FOR, JUSTIFICATION FOR, EXCUSE FOR', 'Unacceptable', 'Unacceptable']"
61,"Clinical Information Extraction via Convolutional Neural Network	We report an implementation of a clinical information extraction tool that leverages deep neural network to annotate event spans and their attributes from raw clinical notes and pathology reports. Our approach uses context words and their part-of-speech tags and shape information as features. Then we hire temporal (1D) convolutional neural network to learn hidden feature representations. Finally, we use Multilayer Perceptron (MLP) to predict event spans. The empirical evaluation demonstrates that our approach significantly outperforms baselines.	Introduction	In the past few years, there has been much interest in applying neural network based deep learning techniques to solve all kinds of natural language processing (NLP) tasks. From low level tasks such as language modeling, POS tagging, named entity recognition, and semantic role labeling BIBREF0 , BIBREF1 , to high level tasks such as machine translation, information retrieval, semantic analysis BIBREF2 , BIBREF3 , BIBREF4 and sentence relation modeling tasks such as paraphrase identification and question answering BIBREF5 , BIBREF6 , BIBREF7 . Deep representation learning has demonstrated its importance for these tasks. All the tasks get performance improvement via learning either word level representations or sentence level representations.In this work, we brought deep representation learning technologies to the clinical domain. Specifically, we focus on clinical information extraction, using clinical notes and pathology reports from the Mayo Clinic. Our system will identify event expressions consisting of the following components:The input of our system consists of raw clinical notes or pathology reports like below:And output annotations over the text that capture the key information such as event mentions and attributes. Table TABREF7 illustrates the output of clinical information extraction in details.To solve this task, the major challenge is how to precisely identify the spans (character offsets) of the event expressions from raw clinical notes. Traditional machine learning approaches usually build a supervised classifier with features generated by the Apache clinical Text Analysis and Knowledge Extraction System (cTAKES) . For example, BluLab system BIBREF8 extracted morphological(lemma), lexical(token), and syntactic(part-of-speech) features encoded from cTAKES. Although using the domain specific information extraction tools can improve the performance, learning how to use it well for clinical domain feature engineering is still very time-consuming. In short, a simple and effective method that only leverage basic NLP modules and achieves high extraction performance is desired to save costs.To address this challenge, we propose a deep neural networks based method, especially convolution neural network BIBREF0 , to learn hidden feature representations directly from raw clinical notes. More specifically, one method first extract a window of surrounding words for the candidate word. Then, we attach each word with their part-of-speech tag and shape information as extra features. Then our system deploys a temporal convolution neural network to learn hidden feature representations. Finally, our system uses Multilayer Perceptron (MLP) to predict event spans. Note that we use the same model to predict event attributes.Constructing High Quality Training Dataset	The major advantage of our system is that we only leverage NLTK tokenization and a POS tagger to preprocess our training dataset. When implementing our neural network based clinical information extraction system, we found it is not easy to construct high quality training data due to the noisy format of clinical notes. Choosing the proper tokenizer is quite important for span identification. After several experiments, we found ""RegexpTokenizer"" can match our needs. This tokenizer can generate spans for each token via sophisticated regular expression like below,Neural Network Classifier	Event span identification is the task of extracting character offsets of the expression in raw clinical notes. This subtask is quite important due to the fact that the event span identification accuracy will affect the accuracy of attribute identification. We first run our neural network classifier to identify event spans. Then, given each span, our system tries to identify attribute values.Temporal Convolutional Neural Network	The way we use temporal convlution neural network for event span and attribute classification is similar with the approach proposed by BIBREF0 . Generally speaking, we can consider a word as represented by INLINEFORM0 discrete features INLINEFORM1 , where INLINEFORM2 is the dictionary for the INLINEFORM3 feature. In our scenario, we just use three features such as token mention, pos tag and word shape. Note that word shape features are used to represent the abstract letter pattern of the word by mapping lower-case letters to “x”, upper-case to “X”, numbers to “d”, and retaining punctuation. We associate to each feature a lookup table. Given a word, a feature vector is then obtained by concatenating all lookup table outputs. Then a clinical snippet is transformed into a word embedding matrix. The matrix can be fed to further 1-dimension convolutional neural network and max pooling layers. Below we will briefly introduce core concepts of Convoluational Neural Network (CNN).Temporal Convolution applies one-dimensional convolution over the input sequence. The one-dimensional convolution is an operation between a vector of weights INLINEFORM0 and a vector of inputs viewed as a sequence INLINEFORM1 . The vector INLINEFORM2 is the filter of the convolution. Concretely, we think of INLINEFORM3 as the input sentence and INLINEFORM4 as a single feature value associated with the INLINEFORM5 -th word in the sentence. The idea behind the one-dimensional convolution is to take the dot product of the vector INLINEFORM6 with each INLINEFORM7 -gram in the sentence INLINEFORM8 to obtain another sequence INLINEFORM9 : DISPLAYFORM0 Usually, INLINEFORM0 is not a single value, but a INLINEFORM1 -dimensional word vector so that INLINEFORM2 . There exist two types of 1d convolution operations. One was introduced by BIBREF9 and also known as Time Delay Neural Networks (TDNNs). The other one was introduced by BIBREF0 . In TDNN, weights INLINEFORM3 form a matrix. Each row of INLINEFORM4 is convolved with the corresponding row of INLINEFORM5 . In BIBREF0 architecture, a sequence of length INLINEFORM6 is represented as: DISPLAYFORM0 where INLINEFORM0 is the concatenation operation. In general, let INLINEFORM1 refer to the concatenation of words INLINEFORM2 . A convolution operation involves a filter INLINEFORM3 , which is applied to a window of INLINEFORM4 words to produce the new feature. For example, a feature INLINEFORM5 is generated from a window of words INLINEFORM6 by: DISPLAYFORM0 where INLINEFORM0 is a bias term and INLINEFORM1 is a non-linear function such as the hyperbolic tangent. This filter is applied to each possible window of words in the sequence INLINEFORM2 to produce the feature map: DISPLAYFORM0 where INLINEFORM0 .We also employ dropout on the penultimate layer with a constraint on INLINEFORM0 -norms of the weight vector. Dropout prevents co-adaptation of hidden units by randomly dropping out a proportion INLINEFORM1 of the hidden units during forward-backpropagation. That is, given the penultimate layer INLINEFORM2 , instead of using: DISPLAYFORM0 for output unit INLINEFORM0 in forward propagation, dropout uses: DISPLAYFORM0 where INLINEFORM0 is the element-wise multiplication operator and INLINEFORM1 is a masking vector of Bernoulli random variables with probability INLINEFORM2 of being 1. Gradients are backpropagated only through the unmasked units. At test step, the learned weight vectors are scaled by INLINEFORM3 such that INLINEFORM4 , and INLINEFORM5 is used to score unseen sentences. We additionally constrain INLINEFORM6 -norms of the weight vectors by re-scaling INLINEFORM7 to have INLINEFORM8 whenever INLINEFORM9 after a gradient descent step.Dataset	We use the Clinical TempEval corpus as the evaluation dataset. This corpus was based on a set of 600 clinical notes and pathology reports from cancer patients at the Mayo Clinic. These notes were manually de-identified by the Mayo Clinic to replace names, locations, etc. with generic placeholders, but time expression were not altered. The notes were then manually annotated with times, events and temporal relations in clinical notes. These annotations include time expression types, event attributes and an increased focus on temporal relations. The event, time and temporal relation annotations were distributed separately from the text using the Anafora standoff format. Table TABREF19 shows the number of documents, event expressions in the training, development and testing portions of the 2016 THYME data.Evaluation Metrics	All of the tasks were evaluated using the standard metrics of precision(P), recall(R) and INLINEFORM0 : DISPLAYFORM0 where INLINEFORM0 is the set of items predicted by the system and INLINEFORM1 is the set of items manually annotated by the humans. Applying these metrics of the tasks only requires a definition of what is considered an ""item"" for each task. For evaluating the spans of event expressions, items were tuples of character offsets. Thus, system only received credit for identifying events with exactly the same character offsets as the manually annotated ones. For evaluating the attributes of event expression types, items were tuples of (begin, end, value) where begin and end are character offsets and value is the value that was given to the relevant attribute. Thus, systems only received credit for an event attribute if they both found an event with correct character offsets and then assigned the correct value for that attribute BIBREF10 .Hyperparameters and Training Details	We want to maximize the likelihood of the correct class. This is equivalent to minimizing the negative log-likelihood (NLL). More specifically, the label INLINEFORM0 given the inputs INLINEFORM1 is predicted by a softmax classifier that takes the hidden state INLINEFORM2 as input: DISPLAYFORM0 After that, the objective function is the negative log-likelihood of the true class labels INLINEFORM0 : DISPLAYFORM0 where INLINEFORM0 is the number of training examples and the superscript INLINEFORM1 indicates the INLINEFORM2 th example.We use Lasagne deep learning framework. We first initialize our word representations using publicly available 300-dimensional Glove word vectors . We deploy CNN model with kernel width of 2, a filter size of 300, sequence length is INLINEFORM0 , number filters is INLINEFORM1 , stride is 1, pool size is INLINEFORM2 , cnn activation function is tangent, MLP activation function is sigmoid. MLP hidden dimension is 50. We initialize CNN weights using a uniform distribution. Finally, by stacking a softmax function on top, we can get normalized log-probabilities. Training is done through stochastic gradient descent over shuffled mini-batches with the AdaGrad update rule BIBREF11 . The learning rate is set to 0.05. The mini-batch size is 100. The model parameters were regularized with a per-minibatch L2 regularization strength of INLINEFORM3 .Results and Discussions	Table TABREF28 shows results on the event expression tasks. Our initial submits RUN 4 and 5 outperformed the memorization baseline on every metric on every task. The precision of event span identification is close to the max report. However, our system got lower recall. One of the main reason is that our training objective function is accuracy-oriented. Table TABREF29 shows results on the phase 2 subtask.Conclusions	In this paper, we introduced a new clinical information extraction system that only leverage deep neural networks to identify event spans and their attributes from raw clinical notes. We trained deep neural networks based classifiers to extract clinical event spans. Our method attached each word to their part-of-speech tag and shape information as extra features. We then hire temporal convolution neural network to learn hidden feature representations. The entire experimental results demonstrate that our approach consistently outperforms the existing baseline methods on standard evaluation datasets.Our research proved that we can get competitive results without the help of a domain specific feature extraction toolkit, such as cTAKES. Also we only leverage basic natural language processing modules such as tokenization and part-of-speech tagging. With the help of deep representation learning, we can dramatically reduce the cost of clinical information extraction system development.","['By how much did their model outperform baselines?', 'Which baselines did they compare against?', 'What was their performance on this task?', 'What dataset did they use to evaluate?', 'How did they obtain part-of-speech tags?']","['Answer with content missing: (Table 3) Best proposed result has F1 score of 0.844, 0.813, 0.870, 0.842, 0.844 compared to 0.855, 0.789, 0.852, 0.792, 0.833 on span, modality, degree, polarity and type respectively.', 'memorization, median report, max report', 'Their average F1 score was 0.874 on span detection; 08115 on contextual modality detection; 0.8695 on degree detection; 0.839 on polarity detection; 0.844 on type detection', 'Unacceptable', 'Answer with content missing: (We then use ”PerceptronTagger” as our part-ofspeech tagger due to its fast tagging speed) PerceptronTagger.']"
62,"Hungarian Layer: Logics Empowered Neural Architecture	Neural architecture is a purely numeric framework, which fits the data as a continuous function. However, lacking of logic flow (e.g. \textit{if, for, while}), traditional algorithms (e.g. \textit{Hungarian algorithm, A$^*$ searching, decision tress algorithm}) could not be embedded into this paradigm, which limits the theories and applications. In this paper, we reform the calculus graph as a dynamic process, which is guided by logic flow. Within our novel methodology, traditional algorithms could empower numerical neural network. Specifically, regarding the subject of sentence matching, we reformulate this issue as the form of task-assignment, which is solved by Hungarian algorithm. First, our model applies BiLSTM to parse the sentences. Then Hungarian layer aligns the matching positions. Last, we transform the matching results for soft-max regression by another BiLSTM. Extensive experiments show that our model outperforms other state-of-the-art baselines substantially.	Introduction	Paraphrase identification is an important topic in artificial intelligence and this task justifies whether two sentences expressed in various forms are semantically similar, BIBREF0 . For example, “On Sunday, the boy runs in the yard” and “The child runs outside at the weekend” are identified as paraphrase. This task directly benefits many industrial applications, such as plagiarism identification BIBREF0 , machine translation BIBREF1 and removing redundancy questions in Quora website BIBREF2 . Recently, there emerge many methods, such as ABCNN BIBREF3 , Siamese LSTM BIBREF2 and L.D.C BIBREF4 .Conventionally, neural methodology aligns the sentence pair and then generates a matching score for paraphrase identification, BIBREF4 , BIBREF2 . Regarding the alignment, we conjecture that the aligned unmatched parts are semantically critical, where we define the corresponded word pairs with low similarity as aligned unmatched parts. For an example: “On Sunday, the boy runs in the yard” and “The child runs inside at the weekend”, the matched parts (i.e. (Sunday, weekend), (boy, child), run) barely make contribution to the semantic sentence similarity, but the unmatched parts (i.e. “yard” and “inside”) determine these two sentences are semantically dissimilar. For another example: “On Sunday, the boy runs in the yard” and “The child runs outside at the weekend”, the aligned unmatched parts (i.e. “yard” and “outside”) are semantically similar, which makes the two sentences paraphrase. In conclusion, if the aligned unmatched parts are semantically consistent, the two sentences are paraphrase, otherwise they are non-paraphrase.Traditional alignment methods take advantage of attention mechanism BIBREF2 , which is a soft-max weighting technique. Weighting technique could pick out the most similar/dissimilar parts, but is weak in modeling the aligned unmatched parts, which are the crucial evidence to identify paraphrase. For the input sentences in Figure FIGREF1 , the weight between “Sunday” and “run” is lower than the weight between “yard” and “inside”, but the former weight is not the evidence of paraphrase/non-paraphrase, because the former two words that are most dissimilar should not be aligned for an inappropriate comparison.To extract the aligned unmatched parts, in this paper, we embed Hungarian algorithm BIBREF5 into neural architecture as Hungarian layer (Algorithm SECREF7 ). Illustrated in Figure FIGREF1 , the alignment in sentence matching could be formulated as the task-assignment problem, which is tackled by Hungarian algorithm. Simply, Hungarian algorithm works out the theoretically optimal alignment relationship in an exclusive manner and the exclusiveness characterizes the aligned unmatched parts. For the example in Figure FIGREF1 , because Hungarian layer allocates the aligned pairs with exclusiveness, the matched parts (i.e (Sunday, weekend), (boy, child), run) are aligned firstly, then the word “yard” would be assigned to the word “inside” with a negative similarity, making a strong evidence for discrimination.Specifically, our model performs this task in three steps. First, our model applies BiLSTM to parse the input sentences into hidden representations. Then, Hungarian layer leverages the hidden representations to extract the aligned unmatched parts. Last, we apply cosine similarity to metric the aligned unmatched parts for a final discrimination. Regarding the training process of Hungarian layer, we modify the back-propagation algorithm in both directions. In the forward pass, Hungarian layer works out the alignment relationship, according to which, the computational graph is dynamically constructed, as demonstrated in Figure FIGREF13 . Once the computational graph has been dynamically constructed, the backward propagation could be performed as usual in a conventional graph.We conduct our experiments on the public benchmark dataset of “Quora Question Pairs” for the task of paraphrase identification. Experimental results demonstrate that our model outperforms other baselines extensively and significantly, which verifies our theory about the aligned unmatched parts and illustrates the effectiveness of our methodology.Contributions. (1.) We offer a new perspective for paraphrase identification, which focuses on the aligned unmatched parts of two sentences. Accordingly, we propose the Hungarian layer to extract the aligned unmatched parts. The proposed method can achieve hard and exclusive alignments between two sequences, while we can learn parameters by end-to-end back-propagation. (2.) Our model outperforms other baselines extensively, verifying the effectiveness of our theory and method.Organization. In Section 2, we survey the related work of paraphrase identification and dynamic differentiable computational graphs. In Section 3, we introduce our neural architecture. In Section 4, we conduct the experiments. In Section 5, we conclude our paper and publish our codes.Related Work	We have surveyed this task and categorized related papers into three lines.Non-Neural Architecture for Paraphrase Identification	The topic of paraphrase identification raises in the last decade. The development has been through four stages before neural architectures: word specific, syntactic tree specific, semantic matching and probabilistic graph modeling.Firstly, BIBREF6 focuses on simple surface-form matching between bag-of-words, which produces poor accuracy, because of word ambiguities and syntactic complexity. Therefore, syntactic analysis is introduced into this task for semantic understanding, such as deeper semantic analysis BIBREF7 , quasi-synchronous grammars BIBREF8 and tree edit distance BIBREF9 . Notably, most of these methods compare the grammar tree (e.g. syntactic tree, dependency tree, etc.) of sentence pair. Further, semantic information such as negation, hypernym, synonym and antonym is integrated into this task for a better prediction precision, BIBREF10 . Finally, BIBREF11 leverages a semi-Markov CRF to align phrases rather than words, which consumes too many resources for industrial applications.In summary, the advantage of this branch, which roots the foundation in linguistics, is semantically interpretable, while the disadvantage is too simple to understand complex language phenomenon.Neural Architecture for Paraphrase Identification: Independent Sentence Encoder	With the popularity of deep neural network, some neural architectures are proposed to analyze the complex language phenomenon in a data-fitting way, which promotes the performance. First of all, the neural network extracts the abstracted features from each sentence independently, then measures the similarity of the abstracted feature pair. There list two frameworks: CNN-based and RAE-based.Commonly, CNN could be treated as n-gram method, which corresponds to language model. Specifically, BIBREF12 applies a bi-gram CNN to jointly model source and target sequences. BIBREF13 achieves a better performance by following this work. BIBREF14 has proposed a RAE based model to characterize phrase-level representation, which promotes simple pooling method, BIBREF15 . Multi-perspective methods BIBREF2 take the advantage of multiple metric aspects to boost the accuracy.In summary, the advantage of this branch is to model complex and ambiguous linguistic phenomenon in a black-box style. However, the disadvantage is that the encoder could not adjust the abstracted representations according to the correlation of sentence pair, making an imperfect matching process.Neural Architecture for Paraphrase Identification: Interdependent Sentence Encoder	To emphasize the correlation of sentence pair in encoder, the researchers propose the attention-based neural architectures, which guide the encoding process according to the corresponding part. There introduce the representative methods: ABCNN BIBREF3 and L.D.C BIBREF2 .ABCNN is a CNN-based model. In a single stage, this model computes the attention similarity matrix for the convolution layer, then sums out each row and column as the weighs of pooling layer. The output of convolution layer is weighted by pooling layer in an average manner as the output of this stage. ABCNN could stack at most three stages. This method achieves satisfactory performance in many tasks, because of modeling correlation in sentence encoder. L.D.C model BIBREF4 is an attention-based method, which decomposes the hidden representations into similar and dissimilar parts, then respectively processes each parts to generate the final result. Notably, L.D.C is the state-of-the-art method.In summary, the advantage of this branch is to model alignment or correlation in the encoding process. However, the disadvantage is to focus on the matched parts, rather than the unmatched parts, which are critical in this task as previously discussed.Dynamic Differentiable Computational Graphs	Neural Turing Machine (NTM) BIBREF16 , BIBREF17 is a seminal work to implement instrument-based algorithm in the neural architecture, which attempts to express algorithms by simulating memory and controller. However, NTM leverages the weighting technique, which involves too much noise and makes the learned algorithm fuzzy. Thus, we propose a hard way to embed algorithms into neural architectures.There also exist some papers for dynamical computational graph construction. At the lower level, pointer-switch networks BIBREF18 are a kind of dynamic differentiable neural model. At the higher level, some architecture search models BIBREF19 , BIBREF20 construct new differentiable computational graphs dynamically at every iteration.Methodology	First, we introduce the basic components of our neural architecture. Then, we analyze the training process of Hungarian layer, that how to dynamically construct the computational graph.Neural Architecture	Our neural architecture is illustrated in Figure FIGREF6 . Basically our model is composed by four components, namely, word embedding, bi-directional LSTM (BiLSTM), Hungarian layer and cosine similarity.Word Embedding. The goal of this layer is to represent each word INLINEFORM0 in every sentence INLINEFORM1 with INLINEFORM2 -dimensional semantic vectors. The word representations, which are pre-trained by GloVe BIBREF21 , are unmodified within the learning procedure. The inputs of this layer are a pair of sentences as word sequences INLINEFORM3 and INLINEFORM4 , while the outputs are corresponding embedding matrices as INLINEFORM5 and INLINEFORM6 .Bi-Directional LSTM (BiLSTM). The purpose of this layer is to transform lexical representations to hidden contextual representations. For hidden contextual encoding, we employ a parameter-shared bi-directional LSTM (BiLSTM) BIBREF22 to parse the word embeddings into hidden representations, mathematically as: DISPLAYFORM0 where INLINEFORM0 is the INLINEFORM1 -th hidden representation and INLINEFORM2 corresponds to the INLINEFORM3 -th word embedding in the source/target sentence or INLINEFORM4 / INLINEFORM5 .Hungarian Layer. This layer, which is the matching component of our model, extracts the aligned unmatched parts from the source and target sentences. This layer is composed by two sequential stages.Algorithm SECREF7 demonstrates the first stage. The objective of this stage is to align the source and target hidden representations. The inputs of this stage are INLINEFORM0 source hidden representation vectors INLINEFORM1 and INLINEFORM2 target hidden representation vectors INLINEFORM3 , while the outputs of this stage are INLINEFORM4 aligned hidden representation vector pairs INLINEFORM5 , assuming INLINEFORM6 , where INLINEFORM7 corresponds to the INLINEFORM8 -th aligned source/target hidden representation vector, respectively.Specifically in this stage, there are totally three steps. First, the input hidden representations are crossly dotted to generate the pairwise similarity matrix INLINEFORM0 . Then, Hungarian algorithm works out the aligned source-target position pairs INLINEFORM1 with this similarity matrix. For example in Figure FIGREF1 , assuming the left/top sentence indicates the source/target sequence, the aligned source-target position pairs are listed as INLINEFORM2 . Last, the input hidden representation vectors INLINEFORM3 are re-organized into the aligned source-target hidden representation vector pairs INLINEFORM4 , according to the aligned source-target position pairs INLINEFORM5 .The second stage attempts to extract the aligned unmatched parts by weighting the aligned hidden representations INLINEFORM0 from the first stage. Required by extracting the unmatched parts, if two aligned representations are matched, the weight for them should be small, otherwise, large dissimilarity leads to large weight. For this reason, we introduce cosine dissimilarity, mathematically as: DISPLAYFORM0 where INLINEFORM0 is the INLINEFORM1 -th aligned cosine dissimilarity and INLINEFORM2 is the INLINEFORM3 -th aligned cosine similarity from the first stage. Thus, the aligned hidden representations are concatenated and then weighted by cosine dissimilarity: DISPLAYFORM0 where INLINEFORM0 is the INLINEFORM1 -th output of Hungarian layer, INLINEFORM2 is the INLINEFORM3 -th aligned source/target hidden representation generated by Algorithm SECREF7 and INLINEFORM4 is the scalar-vector multiplication. Actually in the practical setting, most of cosine dissimilarity approach 0 and the remaining hidden representations indicate the aligned unmatched parts.[t] Hungarian Layer: First Stage [1] Source and target sentence hidden representations: INLINEFORM0 and INLINEFORM1 . INLINEFORM2 , where INLINEFORM3 and INLINEFORM4 mean the INLINEFORM5 -th aligned hidden representations for source and target respectively, and INLINEFORM6 means the corresponding similarity. Generate the pairwise similarity matrix: INLINEFORM7 where INLINEFORM0 is the dot product and INLINEFORM1 is the length of vector. Perform Hungarian algorithm BIBREF5 to assign the aligned position pairs INLINEFORM2 , where INLINEFORM3 is INLINEFORM4 -th aligned source/target position of the sentence pair. INLINEFORM5 , where INLINEFORM6 is the length of source sentence. Compute INLINEFORM7 , where INLINEFORM8 is the pairwise similarity for INLINEFORM9 -th matched position. return INLINEFORM10 , where INLINEFORM11 corresponds to the INLINEFORM12 -th aligned source/target hidden representation, while INLINEFORM13 is the INLINEFORM14 -th aligned source-target position pair, INLINEFORM15 are the input source/target hidden representation vectors and INLINEFORM16 is the INLINEFORM17 -th aligned cosine similarity.Cosine Similarity. Last, we average the concatenated hidden representations as the final sentence representation INLINEFORM0 , which is a conventional procedure in neural natural language processing, BIBREF4 . Then, we employ a cosine similarity as the output: DISPLAYFORM0 where INLINEFORM0 is the matching score, INLINEFORM1 is the length of vector and INLINEFORM2 / INLINEFORM3 is the corresponding source/target part of the final sentence representation INLINEFORM4 . Thus, our output ranges in INLINEFORM5 , where INLINEFORM6 means the two sentences are similar/paraphrase, and INLINEFORM7 means otherwise. For further evaluation of accuracy, we also apply a threshold learned in the development dataset to binary the cosine similarity as paraphrase/non-paraphrase. Notably, the introduction of concatenation layer facilitates the inference and training of Hungarian layer.Training Hungarian Layer	Previously discussed, Hungarian algorithm is embedded into neural architecture, making a challenge for learning process. We tackle this issue by modifying the back-propagation algorithm in a dynamically graph-constructing manner. In the forward pass, we dynamically construct the links between Hungarian layer and the next layer, according to the aligned position pairs, while in the backward process, the back-propagation is performed through the dynamically constructed links. Next, we illustratively exemplify how the computational graph is dynamically constructed in Hungarian layer as Figure FIGREF13 shows.As Figure FIGREF13 shows, in the forward propagation, Hungarian algorithm works out the aligned position pairs, according to which, neural components are dynamically connected to the next layer. For the example of Figure FIGREF13 , the 1st source and 2nd target word representations are jointly linked to the 1st aligned position of concatenation layer. Once the computational graph has been dynamically constructed in the forward pass, the backward process could propagate through the dynamically constructed links between layers, without any branching and non-differentiated issues. For the example in Figure FIGREF13 , the backward pass firstly propagates to the 1st aligned position of concatenation layer, then respectively propagates to 1st source and 2nd target word representations. In this way, the optimization framework could still adjust the parameters of neural architectures in an end-to-end manner.Experiment	In this section, we verify our model performance on the famous public benchmark dataset of “Quora Question Pairs”. First, we introduce the experimental settings, in Section 4.1. Then, in Section 4.2, we conduct the performance evaluation. Last, in order to further test our assumptions, that the aligned unmatched parts are semantically critical, we conduct a case study for illustration in Section 4.3.Experimental Setting	We initialize the word embedding with 300-dimensional GloVe BIBREF21 word vectors pre-trained in the 840B Common Crawl corpus BIBREF21 . For the out-of-vocabulary (OOV) words, we directly apply zero vector as word representation. Regarding the hyper-parameters, we set the hidden dimension as 150 for each BiLSTM. To train the model, we leverage AdaDelta BIBREF23 as our optimizer, with hyper-parameters as moment factor INLINEFORM0 and INLINEFORM1 . We train the model until convergence, but at most 30 rounds. We apply the batch size as 512.Performance Evaluation	Dataset. Actually, to demonstrate the effectiveness of our model, we perform our experiments on the famous public benchmark dataset of “Quora Question Pairs” . For a fair comparison, we follow the splitting rules of BIBREF2 . Specifically, there are over 400,000 question pairs in this dataset, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other or not. We randomly select 5,000 paraphrases and 5,000 non-paraphrases as the development set, and sample another 5,000 paraphrases and 5,000 non-paraphrases as the test set. We keep the remaining instances as the training set. Baselines. To make a sufficient comparison, we choose five state-of-the-art baselines: Siamese CNN, Multi-Perspective CNN, Siamese LSTM, Multi-Perspective LSTM, and L.D.C. Specifically, Siamese CNN and LSTM encode the two input sentences into two sentence vectors by CNN and LSTM, respectively, BIBREF24 . Based on the two sentence vectors, a cosine similarity is leveraged to make the final decision. Multi-Perspective methods leverage different metric aspects to promote the performance, BIBREF2 . L.D.C model BIBREF4 is an attention-based method, which decomposes the hidden representations into similar and dissimilar parts. L.D.C is a powerful model which achieves the state-of-the-art performance.We have tested L.D.C. and our model five times to evaluate the mean and variance, then perform the test for statistical significance. INLINEFORM0 We apply t-test and INLINEFORM1 . Thus, the improvement is statistically significant.Results. Our results are reported in Table TABREF17 . We can conclude that:Our method outperforms all the baselines, which illustrates the effectiveness of our model.In order to evaluate the reliability of the comparison between L.D.C and our model, the results are tested for statistical significance using t-test. In this case, we obtain a p-value = 0.003 INLINEFORM0 0.01. Therefore, the null hypothesis that values are drawn from the same population (i.e., the accuracies of two approaches are virtually equivalent) can be rejected, which means that the improvement is statistically significant.Compared with Siamese LSTM BIBREF24 , which lacks the matching layer, our model could precisely align the input sentences. Thus, our method promotes the performance.Compared with L.D.C. BIBREF4 , which is an attention-based method and still analyzes the dissimilar part, our model could exactly extract the aligned unmatched parts rather than the fuzzy dissimilar parts. Thus, our performance is better.Notably, L.D.C. is a very complex model, which is beaten by our simple model within a statistically significant improvement. This comparison illustrates our model is indeed simple but effective. Thus it is very suitable for industrial applications.Case Study	We have conducted a case study in the practical setting of “Quora Question Pairs” with our model for paraphrase identification. Illustrated in Figure FIGREF18 , the slashed grids correspond to the aligned matched parts, while the crossed ones indicate the aligned unmatched parts. Notably, we mark the pairwise similarity below INLINEFORM0 as unmatched in this case study.For the example of (a), there exist two input sentences: “What is your review of Hidden Figures -LRB- 2016 movie -RRB-” and “What are your impressions of Hidden Figures -LRB- 2017 movie -RRB-”. From our case analysis, most of the aligned parts are matched, while minor aligned unmatched parts are similar. Thus, our method justifies the two sentences as paraphrase. This is accorded to our assumption.For the example of (b), there exist two input sentences: “Why is saltwater taffy candy imported in Austria” and “Why is salt water taffy candy unknown in Japan”. There are two unmatched parts that “imported/unknown” and “Austria/Japan”, which are conflicted. Thus, the case is classified as non-paraphrase.For the example of (c), the two sentences are: “How can I stop being addicted to love” and “How can I stop being so addicted to my phone”. From our case analysis, there is an extreme conflict that “love/phone”, making this case non-paraphrase, according to our assumption.For the example of (d), the two sentences are: “Is a(n) APK file just a hidden app” and “Where do APK files get stored in Android Studio”. As we know, there are too many conflicts in this case, making a very dissimilar score as non-paraphrase.In summary, this case study justifies our assumption that “the aligned unmatched parts are semantically critical”.Conclusion	In this paper, we leverage Hungarian algorithm to design Hungarian layer, which extracts the aligned matched and unmatched parts exclusively from the sentence pair. Then our model is designed by assuming the aligned unmatched parts are semantically critical. Experimental results on benchmark datasets verify our theory and demonstrate the effectiveness of our proposed method.","['Do they ensure the that the architecture is differentiable everywhere after adding the Hungarian layer?', 'Which dataset(s) do they train on?', 'By how much does their model outperform state-of-the-art baselines?']","['Unacceptable', 'Unacceptable', '0.78% over the best state-of-the-art baseline']"
63,"Radical-level Ideograph Encoder for RNN-based Sentiment Analysis of Chinese and Japanese	The character vocabulary can be very large in non-alphabetic languages such as Chinese and Japanese, which makes neural network models huge to process such languages. We explored a model for sentiment classification that takes the embeddings of the radicals of the Chinese characters, i.e, hanzi of Chinese and kanji of Japanese. Our model is composed of a CNN word feature encoder and a bi-directional RNN document feature encoder. The results achieved are on par with the character embedding-based models, and close to the state-of-the-art word embedding-based models, with 90% smaller vocabulary, and at least 13% and 80% fewer parameters than the character embedding-based models and word embedding-based models respectively. The results suggest that the radical embedding-based approach is cost-effective for machine learning on Chinese and Japanese.	Introduction	Word embeddings have been widely used for natural language processing (NLP) tasks BIBREF0 . However, the large word vocabulary makes word embeddings expensive to train. Some people argue that we can model languages at the character-level BIBREF1 . For alphabetic languages such as English, where the characters are much fewer than the words, the character embeddings achieved the state-of-the-art results with much fewer parameters.Unfortunately, for the other languages that use non-alphabetic systems, the character vocabulary can be also large. Moreover, Chinese and Japanese, two of the most widely used non-alphabetic languages, especially contain large numbers of ideographs: hanzi of Chinese and kanji of Japanese. The character vocabulary can be as scalable as the word vocabulary (e.g., see the datasets introduced in Section SECREF11 ). Hence the conventional character embedding-based method is not able to give us a slim vocabulary on Chinese and Japanese.For convenience, let us collectively call hanzi and kanji as Chinese characters. Chinese characters are ideographs composed with semantic and phonetic radicals, both of which are available for character recognition, and the semantic information may be embedded in Chinese characters by the semantic radicals BIBREF2 . Besides, though the character vocabulary is huge, the number of the radicals is much fewer. Accordingly, we explored a model that represents the Chinese characters by the sequence of the radicals. We applied our proposed model to sentiment classification tasks on Chinese and Japanese and achieved the follows:Methodology	The architecture of our proposed model is as shown in Fig. FIGREF3 . It looks similar to the character-aware neural language model proposed by BIBREF1 , but we represent a word by the sequence of radical embeddings instead of character embeddings. Besides, unlike the former model, there are no highway layers in the proposed model, because we find that highway layers do not bring significant improvements to our proposed model (see Section SECREF31 ).Representation of Characters: Sequences of Radical-level Embeddings	For every character, we use a sequence of INLINEFORM0 radical-level embeddings to represent it. They are not treated as in a bag because the position of each radical is related to how it is informative BIBREF3 . For a Chinese character, it is the sequence of the radical embeddings. When it comes to the other characters, including kanas of Japanese, alphabets, digits, punctuation marks, and special characters, it is the sequence comprised of the corresponding character embedding and INLINEFORM1 zero vectors. We zero-pad the radical sequences of all the characters to align the lengths.From Character Radicals to Word Features: CNN Encoder	CNNs BIBREF4 have been used for various NLP tasks and shown effective BIBREF0 , BIBREF5 , BIBREF1 . For NLP, CNNs are able to extract the temporal features, reduce the parameters, alleviate over-fitting and improve generalization ability. We also take advantage of the weight sharing technology of CNNs to learn the shared features of the characters.Let INLINEFORM0 be the radical-level vocabulary that contains Chinese character radicals, kanas of Japanese, alphabets, digits, punctuation marks, and special characters, INLINEFORM1 be the matrix of all the radical-level embeddings, INLINEFORM2 be the dimension of each radical-level embedding. We have introduced that each character is represented by a sequence of radical-level embeddings of length INLINEFORM3 . Thus a word INLINEFORM4 composed of INLINEFORM5 characters is represented by a matrix INLINEFORM6 , each column of which is a radical-level embedding.We apply convolution between INLINEFORM0 and several filters (convolution kernels). For each filter INLINEFORM1 , we apply a nonlinear activation function INLINEFORM2 and a max-pooling on the output to obtain a feature vector. Let INLINEFORM3 be the stride, INLINEFORM4 be the window, INLINEFORM5 be the hidden weight of a filter, respectively. The feature vector of INLINEFORM6 obtained by INLINEFORM7 is given by: DISPLAYFORM0 where INLINEFORM0 is the convolution operator. The pooling window is INLINEFORM1 to obtain the most important information of word INLINEFORM2 .We have two kinds of filters: (1) the filters with stride INLINEFORM0 to obtain radical-level features; (2) the filters with stride INLINEFORM1 to obtain character-level features.After the max-pooling layer, we concatenate and flatten all of the outputs through all of the filters as the feature vector of the word. Let INLINEFORM0 be the number of the output channel of each filter. If we use totally INLINEFORM1 filters, each output of which is INLINEFORM2 , the output feature of INLINEFORM3 is INLINEFORM4 . Here, we assume that the number of the output channels of every filter is the same, but we tailor it for each filter in the experiments following BIBREF1 .From Word Features to Document Features: Bi-directional Long Short-term Memory RNN Encoder	An RNN is a kind of neural networks designed to learn sequential data. The output of an RNN unit at time INLINEFORM0 depends on the output at time INLINEFORM1 . Bi-directional RNNs BIBREF6 are able to extract the past and future information for each node in a sequence, have shown effective for Machine Translation BIBREF7 and Machine Comprehension BIBREF8 .A Long Short-term Memory (LSTM) BIBREF9 Unit is a kind of unit for RNN that keeps information from long range context. We use a bi-directional RNN of LSTM to encode the document feature from the sequence of the word features.An LSTM unit contains a forget gate INLINEFORM0 to decide whether to keep the memory, an input gate INLINEFORM1 to decide whether to update the memory and an output gate INLINEFORM2 to control the output. Let INLINEFORM3 be the output of a LSTM unit at time INLINEFORM4 , INLINEFORM5 be the candidate cell state at time INLINEFORM6 , INLINEFORM7 be the cell state at time INLINEFORM8 . They are given by: DISPLAYFORM0 where, INLINEFORM0 and INLINEFORM1 are the element-wise sigmoid function and multiplication operator.Our proposed model contains two RNN layers that read document data from different directions. Let INLINEFORM0 be a document composed of INLINEFORM1 words. One of the RNN layers reads the document from the first word to the INLINEFORM2 th word, the other reads the document from the INLINEFORM3 th word to the first word. Let INLINEFORM4 be the final output of the former RNN layer and INLINEFORM5 be the final output of the latter. We concatenate INLINEFORM6 and INLINEFORM7 as the document feature. After that, we apply an affine transformation and a softmax to obtain the prediction of the sentiment labels: DISPLAYFORM0 where INLINEFORM0 , INLINEFORM1 is the concatenation operator. INLINEFORM2 is the estimated label of the document, INLINEFORM3 is one of the labels in the label set INLINEFORM4 .We minimize the cross entropy loss to train the model. Let INLINEFORM0 be the set of all the documents, and INLINEFORM1 be the true label of document INLINEFORM2 , the loss is given by: DISPLAYFORM0 Datasets	In the experiments, we used a Chinese dataset and a Japanese dataset. We used the publicly available Ctrip review data pack for Chinese. They are comprised of travel reviews crawled from ctrip.com . We used a subset of 10,000 reviews in the pack. We randomly select 8,000 and 2,000 from it for training and test, respectively. The Japanese dataset is provided by Rakuten, Inc. It contains 64,000,000 reviews of the products in Rakuten Ichiba . The reviews are labeled with 6-point evaluation of 0-5. We labeled the reviews with less than 3 points as the negative samples, and the others as the positive samples. We randomly chose 10,000 reviews to align the size of the Chinese datasets, 8,000 and 2,000 from it for training and test, respectively.The detailed information of the datasets is shown in Table TABREF15 . The character vocabularies are as scalable as the word vocabularies but the radical-level vocabularies are much smaller. The character vocabulary of the Rakuten dataset is even larger than its own word vocabulary. 94% of the Rakuten dataset are Chinese characters while the Ctrip dataset contains 74% Chinese characters. Chinese characters account for fewer percentage in Ctrip data, probably because the Ctrip data is not well stripped.Baselines	We compared the proposed model with the follows:The character-aware neural language model BIBREF1 : It is an RNN language model that takes character embeddings as the inputs, encodes them with CNNs and then input them to RNNs for prediction. It achieved the state-of-the-art as a language model on alphabetic languages. We let it predict the sentiment labels instead of words.Bi-directional RNN BIBREF6 with word embeddings: It is a classical bi-directional RNN classifier, basic but effective. We also employed LSTM for it, and input the word embeddings.Hierarchical attention networks BIBREF10 : It is the state-of-the-art RNN-based document classifier. Following their method, the documents were segmented into shorter sentences of 100 words, and hierarchically encoded with bi-directional RNNs.FastText BIBREF11 : It is the state-of-the-art baseline for text classification, which simply takes n-gram features and classifies sentences by hierarchical softmax. We used the word embedding version but did not use the bigram version because the other models for comparison do not use bigram inputs.Hyperparameters	The setup of the hyperparameters in our experiments is shown in Table TABREF22 . They were tuned on the development set of 4,000 reviews, 2,000 from another subset in the public Ctrip data pack, and the other 2,000 randomly chosen from the review data of Rakuten Ichiba. We aligned the sizes of the feature vectors of the words and the documents in different models for a fair comparison. All the embeddings are initialized randomly with the uniform distribution.All of the models were trained by RMSprop BIBREF12 with mini-batches of 100 samples. The learning rate and decay term were set as 0.001 and 0.9 respectively, also tuned on the development set.Text Preprocess	We segmented the documents into words by Jieba and Juman++ BIBREF13 , respectively for Chinese and Japanese. We zero-padded the length of the sentences, words, and radical sequences of the characters as 500, 4 and 3, respectively.We split the Chinese characters in CJK Unified Ideographs of ISO/IEC 10646-1:2000 character set, until there is no component can be split further, according to CHISE Character Structure Information Database . Then the Chinese character is represented by the sequence of the radicals from the left to the right, the top to the bottom as shown in Fig. FIGREF26 . The sequences are zero-padded to the same length. For an unknown Chinese character not in the set, we treat it as a special character.Results	The number of parameters, test accuracy, and cross entropy loss of each model are as shown in Fig. FIGREF28 . The proposed model has 13% fewer parameters than the character embedding-based model, 91% and 82% fewer parameters than the word embedding-based models for Ctrip dataset and Rakuten dataset, respectively. The accuracy is statistically the same as the character embedding-based model, approximately 98% of the word embedding-based model. The losses of the models are also close. The hierarchical attention networks and fastText achieved approximately 11% and 19% lower loss on Ctrip dataset. But on Rakuten dataset whose percentage of Chinese characters is higher, the differences between them and the proposed model drops to 0% and 9% respectively.The Proposed Model Is the Most Cost-effective	The performance of the proposed model is not significantly different from the character embedding-based baseline, and very close to the word embedding-based baselines, with a smaller vocabulary and fewer parameters. It indicates that radical-embeddings are at least as effective as the character-embeddings for Chinese and Japanese, but require less space. It suggests that for Chinese and Japanese, the radical embeddings are more cost-effective than the character embeddings.The CNN Encoder Is Efficient	Even though the character vocabulary is as scalable as the word vocabulary on Chinese and Japanese, the character embedding-based method with CNN encoder can still reduce approximately 90% and 80% parameters for the Chinese and Japanese datasets, respectively. The CNNs allow low-dimension inputs, and share weights in the procedure of encoding the intpus to the high-dimension word features. It is probably the reason that it can save parameters although the sizes of the vocabularies are similar.Highway Layers Are Not Effective For Us	 BIBREF1 reported that the highway networks BIBREF14 are effective for RNN language models. A highway layer is tailored to adaptively switch between a full-connected layer and a “highway"" that directly outputs the input. We also studied that whether it is effective for our proposed model in the sentiment classification task. Following BIBREF1 , we attempted to input the flattened concatenated output of the max-pooling layer to a highway layer that employs ReLU before we input it to RNN. The change of the performance is as shown in Fig. FIGREF32 .We observed no significant improvement. Probably for two-class sentiment classification, a full-connected layer with ReLU is not necessary between the CNN encoder and the bi-directional RNN encoder, hence the highway network learned to pass the inputs directly to the outputs all the time.Related Works	The computational cost brought by the large word vocabulary is a classical problem when neural networks are employed for NLP. In the earliest works, people limited the size of the vocabulary, which is not able to exploit the potential generalization ability on the rare words BIBREF15 . It has made people explore alternative methods for the softmax function to efficiently train all the words, e.g., hierarchical softmax BIBREF16 , noise-contrastive estimation BIBREF17 and negative sampling BIBREF18 . However, the temporal complexity of the softmax function is not the only thing suffering the high-dimension vocabulary. Scalable word vocabulary leads to a large embedding layer, hence huge neural network with millions of parameters, which costs quite a few gigabytes to store. BIBREF19 proposed a convolutional neural network (CNN) that takes characters as the input for text classification and outperforms the previous models for large datasets. They showed the character-level CNNs are effective for text classification without the need for words. BIBREF1 introduced a recurrent neural network (RNN) language model that takes character embeddings encoded by convolutional layers as the input. Their model has much fewer parameters than the models using word embeddings, and reached the performance of the state-of-the-art on English, and outperformed baselines on morphologically rich languages. However, for Chinese and Japanese, the character vocabulary is also large, and the character embeddings are blind to the semantic information of the radicals.Conclusion and Outlook	We have proposed a model that takes radicals of characters as the inputs for sentiment classification on Chinese and Japanese, whose character vocabulary can be as scalable as word vocabulary. Our proposed model is as powerful as the character embedding-based model, and close to the word embedding-based model for the sentiment classification task, with much smaller vocabulary and fewer parameters. The results show that the radical embeddings are cost-effective for Chinese and Japanese. They are useful for the circumstances where the storage is limited.There are still a lot to do on radical embeddings. For example, a radical may be related to the meaning sometimes, but express the pronunciation at other times. We will work on dealing with such phenomena for machine learning in the future.The authors would like to thank Rakuten, Inc. and the Advanced Language Information Forum (ALAGIN) for generously providing us the Rakuten Ichiba data.",['what are the state of the art models?'],['Unacceptable']
64,"Exploring Domain Shift in Extractive Text Summarization	Although domain shift has been well explored in many NLP applications, it still has received little attention in the domain of extractive text summarization. As a result, the model is under-utilizing the nature of the training data due to ignoring the difference in the distribution of training sets and shows poor generalization on the unseen domain. With the above limitation in mind, in this paper, we first extend the conventional definition of the domain from categories into data sources for the text summarization task. Then we re-purpose a multi-domain summarization dataset and verify how the gap between different domains influences the performance of neural summarization models. Furthermore, we investigate four learning strategies and examine their abilities to deal with the domain shift problem. Experimental results on three different settings show their different characteristics in our new testbed. Our source code including \textit{BERT-based}, \textit{meta-learning} methods for multi-domain summarization learning and the re-purposed dataset \textsc{Multi-SUM} will be available on our project: \url{http://pfliu.com/TransferSum/}.	Introduction	Text summarization has been an important research topic due to its widespread applications. Existing research works for summarization mainly revolve around the exploration of neural architectures BIBREF0, BIBREF1 and design of training constraints BIBREF2, BIBREF3. Apart from these, several works try to integrate document characteristics (e.g. domain) to enhance the model performance BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9 or make interpretable analysis towards existing neural summarization models BIBREF10.Despite their success, only a few literature BIBREF11, BIBREF12 probes into the exact influence domain can bring, while none of them investigates the problem of domain shift, which has been well explored in many other NLP tasks. This absence poses some challenges for current neural summarization models: 1) How will the domain shift exactly affect the performance of existing neural architectures? 2) How to take better advantage of the domain information to improve the performance for current models? 3) Whenever a new model is built which can perform well on its test set, it should also be employed to unseen domains to make sure that it learns something useful for summarization, instead of overfitting its source domains.The most important reason for the lack of approaches that deal with domain shift might lay in the unawareness of different domain definitions in text summarization. Most literature limits the concept of the domain into the document categories or latent topics and uses it as the extra loss BIBREF6, BIBREF7 or feature embeddings BIBREF8, BIBREF9. This definition presumes that category information will affect how summaries should be formulated. However, such information may not always be obtained easily and accurately. Among the most popular five summarization datasets, only two of them have this information and only one can be used for training. Besides, the semantic categories do not have a clear definition. Both of these prevent previous work from the full use of domains in existing datasets or building a new multi-domain dataset that not only can be used for multi-domain learning but also is easy to explore domain connection across datasets.In this paper, we focus on the extractive summarization and demonstrate that news publications can cause data distribution differences, which means that they can also be defined as domains. Based on this, we re-purpose a multi-domain summarization dataset MULTI-SUM and further explore the issue of domain shift.Methodologically, we employ four types of models with their characteristics under different settings. The first model is inspired by the joint training strategy, and the second one builds the connection between large-scale pre-trained models and multi-domain learning. The third model directly constructs a domain-aware model by introducing domain type information explicitly. Lastly, we additionally explore the effectiveness of meta-learning methods to get better generalization. By analyzing their performance under in-domain, out-of-domain, and cross-dataset, we provide a preliminary guideline in Section SECREF31 for future research in multi-domain learning of summarization tasks.Our contributions can be summarized as follows:We analyze the limitation of the current domain definition in summarization tasks and extend it into article publications. We then re-purpose a dataset MULTI-SUM to provide a sufficient multi-domain testbed (in-domain and out-of-domain).To the best of our knowledge, this is the first work that introduces domain shift to text summarization. We also demonstrate how domain shift affects the current system by designing a verification experiment.Instead of pursuing a unified model, we aim to analyze how different choices of model designs influence the generalization ability of dealing with the domain shift problem, shedding light on the practical challenges and provide a set of guidelines for future researchers.Domains in Text Summarization	In this section, we first describe similar concepts used as the domain in summarization tasks. Then we extend the definition into article sources and verify its rationality through several indicators that illustrate the data distribution on our re-purposed multi-domain summarization dataset.Domains in Text Summarization ::: Common Domain Definition	Although a domain is often defined by the content category of a text BIBREF17, BIBREF18 or image BIBREF19, the initial motivation for a domain is a metadata attribute which is used in order to divide the data into parts with different distributions BIBREF20.For text summarization, the differences between data distribution are often attributed to the document categories, such as sports or business, or the latent topics within articles, which can be caught by classical topic models like Latent Dirichlet Allocation (LDA) BIBREF21. Although previous works have shown that taking consideration of those distribution differences can improve summarization models performance BIBREF7, BIBREF8, few related them with the concept of the domain and investigated the summarization tasks from a perspective of multi-domain learning.Domains in Text Summarization ::: Publications as Domain	In this paper, we extend the concept into the article sources, which can be easily obtained and clearly defined.Domains in Text Summarization ::: Publications as Domain ::: Three Measures	We assume that the publications of news may also affect data distribution and thus influence the summarization styles. In order to verify our hypothesis, we make use of three indicators (Coverage, Density and Compression) defined by BIBREF16 to measure the overlap and compression between the (document, summary) pair. The coverage and the density are the word and the longest common subsequence (LCS) overlaps, respectively. The compression is the length ratio between the document and the summary.Domains in Text Summarization ::: Publications as Domain ::: Two Baselines	We also calculate two strong summarization baselines for each publication. The LEAD baseline concatenates the first few sentences as the summary and calculates its ROUGE score. This baseline shows the lead bias of the dataset, which is an essential factor in news articles. The Ext-Oracle baseline evaluates the performance of the ground truth labels and can be viewed as the upper bound of the extractive summarization models BIBREF1, BIBREF9.Domains in Text Summarization ::: Publications as Domain ::: MULTI-SUM	The recently proposed dataset Newsroom BIBREF16 is used, which was scraped from 38 major news publications. We select top ten publications (NYTimes, WashingtonPost, FoxNews, TheGuardian, NYDailyNews, WSJ, USAToday, CNN, Time and Mashable) and process them in the way of BIBREF22. To obtain the ground truth labels for extractive summarization task, we follow the greedy approach introduced by BIBREF1. Finally, we randomly divide ten domains into two groups, one for training and the other for test. We call this re-purposed subset of Newsroom MULTI-SUM to indicate it is specially designed for multi-domain learning in summarization tasks.From Table TABREF6, we can find that data from those news publications vary in indicators that are closely relevant to summarization. This means that (document, summary) pairs from different publications will have unique summarization formation, and models might need to learn different semantic features for different publications. Furthermore, we follow the simple experiment by BIBREF23 to train a classifier for the top five domains. A simple classification model with GloVe initializing words can also achieve 74.84% accuracy (the chance is 20%), which ensures us that there is a built-in bias in each publication. Therefore, it is reasonable to view one publication as a domain and use our multi-publication MULTI-SUM as a multi-domain dataset.Analytical Experiment for Domain Shift	Domain shift refers to the phenomenon that a model trained on one domain performs poorly on a different domainBIBREF19, BIBREF24. To clearly verify the existence of domain shift in the text summarization, we design a simple experiment on MULTI-SUM dataset.Concretely, we take turns choosing one domain and use its training data to train the basic model. Then, we use the testing data of the remaining domains to evaluate the model with the automatic metric ROUGE BIBREF25ROUGE-2 and ROUGE-L show similar trends and their results are attached in Appendix.Analytical Experiment for Domain Shift ::: Basic Model	Like a few recent approaches, we define extractive summarization as a sequence labeling task. Formally, given a document $S$ consisting of $n$ sentences $s_1, \cdots , s_n$, the summaries are extracted by predicting a sequence of label $Y = y_1, \cdots , y_n$ ($y_i \in \lbrace 0,1\rbrace $) for the document, where $y_i = 1$ represents the $i$-th sentence in the document should be included in the summaries.In this paper, we implement a simple but powerful model based on the encoder-decoder architecture. We choose CNN as the sentence encoder following prior works BIBREF26 and employ the popular modular Transformer BIBREF27 as the document encoder. The detailed settings are described in Section SECREF28.Analytical Experiment for Domain Shift ::: Results	From Table TABREF14, we find that the values are negative except the diagonal, which indicates models trained and tested on the same domain show the great advantage to those trained on other domains. The significant performance drops demonstrate that the domain shift problem is quite serious in extractive summarization tasks, and thus pose challenges to current well-performed models, which are trained and evaluated particularly under the strong hypothesis: training and test data instances are drawn from the identical data distribution. Motivated by this vulnerability, we investigate the domain shift problem under both multi-domain training and evaluation settings.Multi-domain Summarization	With the above observations in mind, we are seeking an approach which can alleviate the domain shift problem effectively in text summarization. Specifically, the model should not only perform well on source domains where it is trained on, but also show advantage on the unseen target domains. This involves the tasks of multi-domain learning and domain adaptation. Here, we begin with several simple approaches for multi-domain summarization based on multi-domain learning.Multi-domain Summarization ::: Four Learning Strategies	To facilitate the following description, we first set up mathematical notations. Assuming that there are $K$ related domains, we refer to $D_k$ as a dataset with $N_k$ samples for domain $k$. $D_k = \lbrace (S_i^{(k)},Y_i^{(k)})\rbrace _{i=1}^{N_k}$, where $S_i^{(k)}$ and $Y_i^{(k)}$ represent a sequence of sentences and the corresponding label sequence from a document of domain $k$, respectively. The goal is to estimate the conditional probability $P(Y|S)$ by utilizing the complementarities among different domains.Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{I}_{Base}$@!END@	This is a simple but effective model for multi-domain learning, in which all domains are aggregated together and will be further used for training a set of shared parameters. Notably, domains in this model are not explicitly informed of their differences.Therefore, the loss function of each domain can be written as:where Basic denotes our CNN-Transformer encoder framework (As described in Section SECREF15). $\theta ^{(s)}$ means that all domains share the same parameters.Analysis: The above model benefits from the joint training strategy, which can allow a monolithic model to learn shared features from different domains. However, it is not sufficient to alleviate the domain shift problem, because two potential limitations remain: 1) The joint model is not aware of the differences across domains, which would lead to poor performance on in-task evaluation since some task-specific features shared by other tasks. 2) Negative transferring might happened on new domains. Next, we will study three different approaches to address the above problems.Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{II}_{BERT}$@!END@	More recently, unsupervised pre-training has achieved massive success in NLP community BIBREF28, BIBREF29, which usually provides tremendous external knowledge. However, there are few works on building the connection between large-scale pre-trained models and multi-domain learning. In this model, we explore how the external knowledge unsupervised pre-trained models bring can contribute to multi-domain learning and new domain adaption .We achieve this by pre-training our basic model $Model^{I}_{Base}$ with BERT BIBREF28, which is one of the most successful learning frameworks. Then we investigate if BERT can provide domain information and bring the model good domain adaptability. To avoid introducing new structures, we use the feature-based BERT with its parameters fixed.Analysis: This model instructs the processing of multi-domain learning by utilizing external pre-trained knowledge. Another perspective is to address this problem algorithmically.Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{III}_{Tag}$@!END@	The domain type can also be introduced directly as a feature vector, which can augment learned representations with domain-aware ability.Specifically, each domain tag $C^{(k)}$ will be embedded into a low dimensional real-valued vector and then be concatenated with sentence embedding $\mathbf {s^{(k)}_i}$. The loss function can be formulated as:It is worth noting that, on unseen domains, the information of real domain tags is not available. Thus we design a domain tag `$\mathfrak {X}$' for unknown domains and randomly relabeled examples with it during training. Since the real tag of the data tagged with `$\mathfrak {X}$' may be any source domain, this embedding will force the model to learn the shared features and makes it more adaptive to unseen domains. In the experiment, this improves the performance on both source domains and target domains.Analysis: This domain-aware model makes it possible to learn domain-specific features, while it still suffers from the negative transfer problem since private and shared features are entangled in shared space BIBREF31, BIBREF32. Specifically, each domain has permission to modify shared parameters, which makes it easier to update parameters along different directions.-0.7cmMulti-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{IV}_{Meta}$@!END@	In order to overcome the above limitations, we try to bridge the communication gap between different domains when updating shared parameters via meta-learning BIBREF33, BIBREF34, BIBREF35.Here, the introduced communicating protocol claims that each domain should tell others what its updating details (gradients) are. Through its different updating behaviors of different domains can be more consistent.Formally, given a main domain $A$ and an auxiliary domain $B$, the model will first compute the gradients of A $\nabla _{\theta } \mathcal {L}^{A}$ with regard to the model parameters $\theta $. Then the model will be updated with the gradients and calculate the gradients of B.Our objective is to produce maximal performance on sample $(S^{(B)},Y^{(B)})$:So, the loss function for each domain can be finally written as:where $\gamma $ $(0 \le \gamma \le 1)$ is the weight coefficient and $\mathcal {L}$ can be instantiated as $\mathcal {L}_{I}$ (Eqn. DISPLAY_FORM19), $\mathcal {L}_{II}$ or $\mathcal {L}_{III}$ (Eqn. DISPLAY_FORM23).Analysis: To address the multi-domain learning task and the adaptation to new domains, Model$^{II}_{BERT}$, Model$^{III}_{Tag}$, Model$^{IV}_{Meta}$ take different angles. Specifically, Model$^{II}_{BERT}$ utilizes a large-scale pre-trained model while Model$^{III}_{Tag}$ proposes to introduce domain type information explicitly. Lastly, Model$^{IV}_{Meta}$ is designed to update parameters more consistently, by adjusting the gradient direction of the main domain A with the auxiliary domain B during training. This mechanism indeed purifies the shared feature space via filtering out the domain-specific features which only benefit A.Experiment	We investigate the effectiveness of the above four strategies under three evaluation settings: in-domain, out-of-domain and cross-dataset. These settings make it possible to explicitly evaluate models both on the quality of domain-aware text representation and on their adaptation ability to derive reasonable representations in unfamiliar domains.Experiment ::: Experiment Setup	We perform our experiments mainly on our multi-domain MULTI-SUM dataset. Source domains are defined as the first five domains (in-domain) in Table TABREF6 and the other domains (out-of-domain) are totally invisible during training. The evaluation under the in-domain setting tests the model ability to learn different domain distribution on a multi-domain set and later out-of-domain investigates how models perform on unseen domains. We further make use of CNN/DailyMail as a cross-dataset evaluation environment to provide a larger distribution gap.We use Model$^{I}_{Basic}$ as a baseline model, build Model$^{II}_{BERT}$ with feature-based BERT and Model$^{III}_{Tag}$ with domain embedding on it. We further develop Model$^{III}_{Tag}$ as the instantiation of Model$^{IV}_{Meta}$. For the detailed dataset statistics, model settings and hyper-parameters, the reader can refer to Appendix.-12ptExperiment ::: Quantitative Results	We compare our models by ROUGE-1 scores in Table TABREF29. Note that we select two sentences for MULTI-SUM domains and three sentences for CNN/Daily Mail due to the different average lengths of reference summaries.Experiment ::: Quantitative Results ::: Model@!START@$^{I}_{Basic}$@!END@ vs Model@!START@$^{III}_{Tag}$@!END@	From Table TABREF29, we observe that the domain-aware model outperforms the monolithic model under both in-domain and out-of-domain settings. The significant improvement of in-domain demonstrates domain information is effective for summarization models trained on multiple domains. Meanwhile, the superior performance on out-of-domain further illustrates that, the awareness of domain difference also benefits under the zero-shot setting. This might suggest that the domain-aware model could capture domain-specific features by domain tags and have learned domain-invariant features at the same time, which can be transferred to unseen domains.Experiment ::: Quantitative Results ::: Model@!START@$^{I}_{Basic}$@!END@ vs Model@!START@$^{IV}_{Meta}$@!END@	Despite a little drop under in-domain setting, the narrowed performance gap, as shown in $\Delta R$ of Table TABREF29, indicates Model$^{IV}_{Meta}$ has better generalization ability as a compensation. The performance decline mainly lies in the more consistent way to update parameters, which purifies shared feature space at the expense of filtering out some domain-specific features. The excellent results under cross-dataset settings further suggest the meta-learning strategy successfully improve the model transferability not only among the domains of MULTI-SUM but also across different datasets.Experiment ::: Quantitative Results ::: Model@!START@$^{II}_ {BERT}$@!END@	Supported by the smaller $\Delta R$ compared with Model$^{I}_{Base}$, we can draw the conclusion that BERT shows some domain generalization ability within MULTI-SUM. However, this ability is inferior to Model$^{III}_{Tag}$ and Model$^{IV}_{Meta}$, which further leads to the worse performance on cross-dataset. Thus we cannot attribute its success in MULTI-SUM to the ability to address multi-domain learning nor domain adaptation. Instead, we suppose the vast external knowledge of BERT provides its superior ability for feature extraction. That causes Model$^{II}_ {BERT}$ to overfit MULTI-SUM and perform excellently across all domains, but fails on the more different dataset CNN/Daily Mail.This observation also suggests that although unsupervised pre-trained models are powerful enough BIBREF30, still, it can not take place the role of supervised learning methods (i.e. Model$^{III}_{Tag}$ and Model$^{IV}_{Meta}$), which is designed specifically for addressing multi-domain learning and new domain adaptation.Experiment ::: Quantitative Results ::: Analysis of Different Model Choices	To summarize, Model$^{III}_ {Tag} $ is a simple and efficient method, which can achieve good performance under in-domain setting and shows certain generalization ability on the unseen domain. Model$^{IV}_ {Meta} $ shows the best generalization ability at the cost of relatively lower in-domain performance. Therefore, using Model$^{IV}_ {Meta} $ is not a good choice if in-domain performance matters for end users. Model$^{II}_ {BERT} $ can achieve the best performance under in-domain settings at expense of training time and shows worse generalization ability than Model$^{IV}_ {Meta} $. If the training time is not an issue, Model$^{II}_ {BERT} $ could be a good supplement for other methods.Experiment ::: Results on CNN/DailyMail	Inspired by such observations, we further employ our four learning strategies to the mainstream summarization dataset CNN/DailyMail BIBREF22, which also includes two different data sources: CNN and DailyMail. We use the publication as the domain and train our models on its 28w training set. As Table TABREF30 shows, our basic model has comparable performance with other extractive summarization models. Besides, the publication tags can improve ROUGE scores significantly by 0.13 points in ROUGE-1 and the meta learning strategy does not show many advantages when dealing with in-domain examples, what we have expected. BERT with tags achieves the best performance, although the performance increment is not as much as what publication tags bring to the basic model, which we suppose that BERT itself has contained some degree of domain information.Experiment ::: Qualitative Analysis	We furthermore design several experiments to probe into some potential factors that might contribute to the superior performance of domain-aware models over the monolithic basic model.Experiment ::: Qualitative Analysis ::: Label Position	Sentence position is a well known and powerful feature, especially for extractive summarization BIBREF40 . We compare the relative position of sentences selected by our models with the ground truth labels on source domains to investigate how well these models fit the distribution and whether they can distinguish between domains. We select the most representative models Model$^{I}_{Base}$ and Model$^{III}_{Tag}$ illustrated in Figure FIGREF34 .The percentage of the first sentence on FoxNews is significantly higher than others: (1) Unaware of different domains, Model$^{I}_{Base}$ learns a similar distribution for all domains and is seriously affected by this extreme distribution. In its density histogram, the probability of the first sentence being selected is much higher than the ground truth on the other four domains. (2) Compared with Model$^{I}_{Base}$, domain-aware models are more robust by learning different relative distributions for different domains. Model$^{III}_{Tag}$ constrains the extreme trend especially obviously on CNN and Mashable.-2cmExperiment ::: Qualitative Analysis ::: Weight @!START@$\gamma $@!END@ for Model@!START@$^{IV}_{Meta}$@!END@	We investigate several $\gamma $ to further probe into the performance of Model$^{IV}_{Meta}$. In Eqn. DISPLAY_FORM27, $\gamma $ is the weight coefficient of main domain A. When $\gamma =0$, the model ignores A and focuses on the auxiliary domain B and when $\gamma =1$ it is trained only on the loss of main domain A (the same as the instantiation Model$^{III}_{Tag}$). As Figure FIGREF43 shows, with the increase of $\gamma $, the Rouge scores rise on in-domain while decline on out-of-domain and cross-dataset. The performances under in-domain settings prove that the import of the auxiliary domain hurts the model ability to learn domain-specific features. However, results under both out-of-domain and cross-dataset settings indicate the loss of B, which is informed of A's gradient information, helps the model to learn more general features, thus improving the generalization ability.Related Work	We briefly outline connections and differences to the following related lines of research.Related Work ::: Domains in Summarization	There have been several works in summarization exploring the concepts of domains. BIBREF11 explored domain-specific knowledge and associated it as template information. BIBREF12 investigated domain adaptation in abstractive summarization and found the content selection is transferable to a new domain. BIBREF41 trained a selection mask for abstractive summarization and proved it has excellent adaptability. However, previous works just investigated models trained on a single domain and did not explore multi-domain learning in summarization.Related Work ::: Multi-domain Learning (MDL) & Domain Adaptation (DA)	We focus on the testbed that requires both training and evaluating performance on a set of domains. Therefore, we care about two questions: 1) how to learn a model when the training set contains multiple domains – involving MDL. 2) how to adapt the multi-domain model to new domains – involving DA. Beyond the investigation of some effective approaches like existing works, we have first verified how domain shift influences the summarization tasks.Related Work ::: Semi-supervised Pre-training for Zero-shot Transfer	It has a long history of fine-tuning downstream tasks with supervised or unsupervised pre-trained models BIBREF42, BIBREF28, BIBREF29. However, there is a rising interest in applying large-scale pre-trained models to zero-shot transfer learning BIBREF30. Different from the above works, we focus on addressing domain shift and generalization problem. One of our explored methods is semi-supervised pre-training, which combines supervised and unsupervised approaches to achieve zero-shot transfer.Conclusion	In this paper, we explore publication in the context of the domain and investigate the domain shift problem in summarization. When verified its existence, we propose to build a multi-domain testbed for summarization that requires both training and measuring performance on a set of domains. Under these new settings, we propose four learning schemes to give a preliminary explore in characteristics of different learning strategies when dealing with multi-domain summarization tasks.Acknowledgment	We thank Jackie Chi Kit Cheung for useful comments and discussions. The research work is supported by National Natural Science Foundation of China (No. 61751201 and 61672162), Shanghai Municipal Science and Technology Commission (16JC1420401 and 17JC1404100), Shanghai Municipal Science and Technology Major Project(No.2018SHZDZX01)and ZJLab.","['What settings did they experiment with?', 'what domains are explored in this paper?', 'what multi-domain dataset is repurposed?', 'what four learning strategies are investigated?']","['Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable']"
65,"CAp 2017 challenge: Twitter Named Entity Recognition	The paper describes the CAp 2017 challenge. The challenge concerns the problem of Named Entity Recognition (NER) for tweets written in French. We first present the data preparation steps we followed for constructing the dataset released in the framework of the challenge. We begin by demonstrating why NER for tweets is a challenging problem especially when the number of entities increases. We detail the annotation process and the necessary decisions we made. We provide statistics on the inter-annotator agreement, and we conclude the data description part with examples and statistics for the data. We, then, describe the participation in the challenge, where 8 teams participated, with a focus on the methods employed by the challenge participants and the scores achieved in terms of F$_1$ measure. Importantly, the constructed dataset comprising $\sim$6,000 tweets annotated for 13 types of entities, which to the best of our knowledge is the first such dataset in French, is publicly available at \url{http://cap2017.imag.fr/competition.html} .	Introduction	The proliferation of the online social media has lately resulted in the democratization of online content sharing. Among other media, Twitter is very popular for research and application purposes due to its scale, representativeness and ease of public access to its content. However, tweets, that are short messages of up to 140 characters, pose several challenges to traditional Natural Language Processing (NLP) systems due to the creative use of characters and punctuation symbols, abbreviations ans slung language.Named Entity Recognition (NER) is a fundamental step for most of the information extraction pipelines. Importantly, the terse and difficult text style of tweets presents serious challenges to NER systems, which are usually trained using more formal text sources such as newswire articles or Wikipedia entries that follow particular morpho-syntactic rules. As a result, off-the-self tools trained on such data perform poorly BIBREF0 . The problem becomes more intense as the number of entities to be identified increases, moving from the traditional setting of very few entities (persons, organization, time, location) to problems with more. Furthermore, most of the resources (e.g., software tools) and benchmarks for NER are for text written in English. As the multilingual content online increases, and English may not be anymore the lingua franca of the Web. Therefore, having resources and benchmarks in other languages is crucial for enabling information access worldwide.In this paper, we propose a new benchmark for the problem of NER for tweets written in French. The tweets were collected using the publicly available Twitter API and annotated with 13 types of entities. The annotators were native speakers of French and had previous experience in the task of NER. Overall, the generated datasets consists of INLINEFORM0 tweets, split in training and test parts.The paper is organized in two parts. In the first, we discuss the data preparation steps (collection, annotation) and we describe the proposed dataset. The dataset was first released in the framework of the CAp 2017 challenge, where 8 systems participated. Following, the second part of the paper presents an overview of baseline systems and the approaches employed by the systems that participated. We conclude with a discussion of the performance of Twitter NER systems and remarks for future work.Challenge Description	In this section we describe the steps taken during the organisation of the challenge. We begin by introducing the general guidelines for participation and then proceed to the description of the dataset.Guidelines for Participation	The CAp 2017 challenge concerns the problem of NER for tweets written in French. A significant milestone while organizing the challenge was the creation of a suitable benchmark. While one may be able to find Twitter datasets for NER in English, to the best of our knowledge, this is the first resource for Twitter NER in French. Following this observation, our expectations for developing the novel benchmark are twofold: first, we hope that it will further stimulate the research efforts for French NER with a focus on in user-generated text social media. Second, as its size is comparable with datasets previously released for English NER we expect it to become a reference dataset for the community.The task of NER decouples as follows: given a text span like a tweet, one needs to identify contiguous words within the span that correspond to entities. Given, for instance, a tweet “Les Parisiens supportent PSG ;-)” one needs to identify that the abbreviation “PSG” refers to an entity, namely the football team “Paris Saint-Germain”. Therefore, there two main challenges in the problem. First one needs to identify the boundaries of an entity (in the example PSG is a single word entity), and then to predict the type of the entity. In the CAp 2017 challenge one needs to identify among 13 types of entities: person, musicartist, organisation, geoloc, product, transportLine, media, sportsteam, event, tvshow, movie, facility, other in a given tweets. Importantly, we do not allow the entities to be hierarchical, that is contiguous words belong to an entity as a whole and a single entity type is associated per word. It is also to be noted that some of the tweets may not contain entities and therefore systems should not be biased towards predicting one or more entities for each tweet.Lastly, in order to enable participants from various research domains to participate, we allowed the use of any external data or resources. On one hand, this choice would enable the participation of teams who would develop systems using the provided data or teams with previously developed systems capable of setting the state-of-the-art performance. On the other hand, our goal was to motivate approaches that would apply transfer learning or domain adaptation techniques on already existing systems to adapt them for the task of NER for French tweets.The Released Dataset	For the purposes of the CAp 2017 challenge we constructed a dataset for NER of French tweets. Overall, the dataset comprises 6,685 annotated tweets with the 13 types of entities presented in the previous section. The data were released in two parts: first, a training part was released for development purposes (dubbed “Training” hereafter). Then, to evaluate the performance of the developed systems a “Test” dataset was released that consists of 3,685 tweets. For compatibility with previous research, the data were released tokenized using the CoNLL format and the BIO encoding.To collect the tweets that were used to construct the dataset we relied on the Twitter streaming API. The API makes available a part of Twitter flow and one may use particular keywords to filter the results. In order to collect tweets written in French and obtain a sample that would be unbiased towards particular types of entities we used common French words like articles, pronouns, and prepositions: “le”,“la”,“de”,“il”,“elle”, etc.. In total, we collected 10,000 unique tweets from September 1st until September the 15th of 2016.Complementary to the collection of tweets using the Twitter API, we used 886 tweets provided by the “Société Nationale des Chemins de fer Français” (SNCF), that is the French National Railway Corporation. The latter subset is biased towards information in the interest of the corporation such as train lines or names of train stations. To account for the different distribution of entities in the tweets collected by SNCF we incorporated them in the data as follows:For the training set, which comprises 3,000 tweets, we used 2,557 tweets collected using the API and 443 tweets of those provided by SNCF.For the test set, which comprises 3,685 consists we used 3,242 tweets from those collected using the API and the remaining 443 tweets from those provided by SNCF.Annotation	In the framework of the challenge, we were required to first identify the entities occurring in the dataset and, then, annotate them with of the 13 possible types. Table TABREF12 provides a description for each type of entity that we made available both to the annotators and to the participants of the challenge.Mentions (strings starting with @) and hashtags (strings starting with #) have a particular function in tweets. The former is used to refer to persons while the latter to indicate keywords. Therefore, in the annotation process we treated them using the following protocol: A hashtag or a mention should be annotated as an entity if:For a hashtag or a mention to be annotated both conditions are to be met. Figure FIGREF16 elaborates on that:We measure the inter-annotator agreement between the annotators based on the Cohen's Kappa (cf. Table TABREF15 ) calculated on the first 200 tweets of the training set. According to BIBREF1 our score for Cohen's Kappa (0,70) indicates a strong agreement.In the example given in Figure FIGREF20 :[name=M1, matrix of nodes, row sep=10pt, column sep=3pt,ampersand replacement=&] schema) [text=black] Il; & schema-spezifisch) [text=black] rejoint; & nutzerinfo) [text=frenchrose] Pierre; & host) [text=frenchrose] Fabre;query) [text=black] comme; & fragment) [text=black] directeur; & text=black] des; & text=black] marques;ducray) [text=magenta] Ducray; & text=black] et; & a) [text=magenta] A;& text=magenta] -; & derma) [text=magenta] Derma;; [overbrace style] (nutzerinfo.north west) – (host.north east) node [overbrace text style,rectangle,draw,color=white,rounded corners,inner sep=4pt, fill=frenchrose] Group; [underbrace style] (ducray.south west) – (ducray.south east) node [underbrace text style,rectangle,draw=black,color=white,rounded corners,inner sep=4pt, fill=magenta] Brand; [underbrace style] (a.south west) – (derma.south east) node [underbrace text style,rectangle,draw,color=white,rounded corners,inner sep=4pt, fill=magenta] Brand;A given entity must be annotated with one label. The annotator must therefore choose the most relevant category according to the semantics of the message. We can therefore find in the dataset an entity annotated with different labels. For instance, Facebook can be categorized as a media (“notre page Facebook"") as well as an organization (“Facebook acquires acquiert Nascent Objects"").Event-named entities must include the type of the event. For example, colloque (colloquium) must be annotated in “le colloque du Réveil français est rejoint par"".Abbreviations must be annotated. For example, LMP is the abbreviation of “Le Meilleur Patissier"" which is a tvshow.As shown in Figure 1, the training and the test set have a similar distribution in terms of named entity types. The training set contains 2,902 entities among 1,656 unique entities (i.e. 57,1%). The test set contains 3,660 entities among 2,264 unique entities (i.e. 61,8%). Only 15,7% of named entities are in both datasets (i.e. 307 named entities). Finally we notice that less than 2% of seen entities are ambiguous on the testset.Description of the Systems	Overall, the results of 8 systems were submitted for evaluation. Among them, 7 submitted a paper discussing their implementation details. The participants proposed a variety of approaches principally using Deep Neural Networks (DNN) and Conditional Random Fields (CRF). In the rest of the section we provide a short overview for the approaches used by each system and discuss the achieved scores.Submission 1 BIBREF2 The system relies on a recurrent neural network (RNN). More precisely, a bi-directional GRU network is used and a CRF layer is adde on top of the network to improve label prediction given information from the context of a word, that is the previous and next tags.Submission 2 BIBREF3 The system follows a state-of-the-art approach by using a CRF for to tag sentences with NER tags. The authors develop a set of features divided into six families (orthographic, morphosyntactic, lexical, syntactic, polysemic traits, and language-modeling traits).Submission 3 BIBREF4 , ranked first, employ CRF as a learning model. In the feature engineering process they use morphosyntactic features, distributional ones as well as word clusters based on these learned representations.Submission 4 BIBREF5 The system also relies on a CRF classifier operating on features extracted for each word of the tweet such as POS tags etc. In addition, they employ an existing pattertn mining NER system (mXS) which is not trained for tweets. The addition of the system's results in improving the recall at the expense of precision.Submission 5 BIBREF6 The authors propose a bidirectional LSTM neural network architecture embedding words, capitalization features and character embeddings learned with convolutional neural networks. This basic model is extended through a transfer learning approach in order to leverage English tweets and thus overcome data sparsity issues.Submission 6 BIBREF7 The approach proposed here used adaptations for tailoring a generic NER system in the context of tweets. Specifically, the system is based on CRF and relies on features provided by context, POS tags, and lexicon. Training has been done using CAP data but also ESTER2 and DECODA available data. Among possible combinations, the best one used CAP data only and largely relied on a priori data.Submission 7 Lastly, BIBREF8 uses a rule based system which performs several linguistic analysis like morphological and syntactic as well as the extraction of relations. The dictionaries used by the system was augmented with new entities from the Web. Finally, linguistics rules were applied in order to tag the detected entities.Results	Table TABREF22 presents the ranking of the systems with respect to their F1-score as well as the precision and recall scores.The approach proposed by BIBREF4 topped the ranking showing how a standard CRF approach can benefit from high quality features. On the other hand, the second best approach does not require heavy feature engineering as it relies on DNNs BIBREF2 .We also observe that the majority of the systems obtained good scores in terms of F1-score while having important differences in precision and recall. For example, the Lattice team achieved the highest precision score.Conclusion	In this paper we presented the challenge on French Twitter Named Entity Recognition. A large corpus of around 6,000 tweets were manyally annotated for the purposes of training and evaluation. To the best of our knowledge this is the first corpus in French for NER in short and noisy texts. A total of 8 teams participated in the competition, employing a variety of state-of-the-art approaches. The evaluation of the systems helped us to reveal the strong points and the weaknesses of these approaches and to suggest potential future directions. ","['What method did the highest scoring team use?', 'What descriptive statistics are provided about the data?', 'What was the level of inter-annotator agreement?', 'What questions were asked in the annotation process?', 'Why is NER for tweets more challenging as the number of entities increases?', 'What data preparation steps were used to construct the dataset?']","['CRF model that used morphosyntactic and distributional features, as well as word clusters based on these learned representations.', 'the number of entities, unique entities in the training and test sets', 'Average Cohen’s Kappa score of inter-annotator agreement was 0.655', 'determine entities and annotate them based on the description that matched the type of entity', 'tweets contain informal text with multilingual content that becomes more difficult to classify when there are more options to choose from', 'The tweets were gathered using Twitter API plus tweets provided by the French National Railway Corporation; the tweets were split into training and test sets, and then annotated.']"
66,"ACUTE-EVAL: Improved Dialogue Evaluation with Optimized Questions and Multi-turn Comparisons	While dialogue remains an important end-goal of natural language research, the difficulty of evaluation is an oft-quoted reason why it remains troublesome to make real progress towards its solution. Evaluation difficulties are actually two-fold: not only do automatic metrics not correlate well with human judgments, but also human judgments themselves are in fact difficult to measure. The two most used human judgment tests, single-turn pairwise evaluation and multi-turn Likert scores, both have serious flaws as we discuss in this work.  ::: We instead provide a novel procedure involving comparing two full dialogues, where a human judge is asked to pay attention to only one speaker within each, and make a pairwise judgment. The questions themselves are optimized to maximize the robustness of judgments across different annotators, resulting in better tests. We also show how these tests work in self-play model chat setups, resulting in faster, cheaper tests. We hope these tests become the de facto standard, and will release open-source code to that end.	Introduction	Dialogue between human and machine is an important end-goal of natural language research. The open-ended nature of generating sequences in a multi-turn setup naturally makes the task difficult to evaluate – with full evaluation possessing many of the difficulties of the task itself as it requires deep understanding of the content of the conversation. As in many other natural language generation (NLG) tasks, automatic metrics have not been shown to have a clear correlation with human evaluations BIBREF0, BIBREF1. This means the current standard for all dialogue research involves human trials, which slows down research and greatly increases the cost of model development.Unfortunately, human judgments are themselves difficult to measure. The two most used approaches, single-turn pairwise evaluation BIBREF2, BIBREF3, and multi-turn Likert scores BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8 have serious limitations. Single-turn pairwise evaluation provides the benefits and simplicity of an A/B test, allowing for cheap and fast annotations, with comparisons that are robust to annotator score bias, but fail to take into account the multi-turn aspect of conversations. To give a trivial example, such comparisons fail to capture whether the model would repeat itself in a multi-turn conversation because they only look at one turn; repetition is a known issue that humans dislike BIBREF6.Multi-turn Likert scores require the annotator to have a multi-turn conversation and then provide an integer score, which is more costly and time-consuming to run but evaluates full conversations more accurately. The integer scores however suffer from differing bias and variance per annotator, which researchers have tried to mitigate BIBREF9, but nevertheless due to its lack of sensitivity often yields comparisons that are not statistically significant. Furthermore, due to strong anchoring effects during model evaluation, i.e. that annotators are affected by the first systems they evaluate, Likert comparisons are generally not comparable across multiple papers. This mandates that evaluations of new models be simultaneously collected with baselines, further increasing the cost of developing additional models BIBREF6.In this work we introduce Acute-eval, a method that combines the benefits, and attempts to mitigate the deficiencies, of the above two approaches by introducing a pairwise relative comparison setup for multi-turn dialogues. In each trial, we show the annotator two whole conversations, with the second speaker in each conversation highlighted, as the judgment should be independent of the quality of the first speaker, see Figure FIGREF1. We then show a carefully worded question with two choices: speaker A or B, where the question measures a desired quality such as which speaker is more engaging, interesting or knowledgeable. Our experiments show that annotators perform well in this setup, and that our method can reveal subtle but significant differences between conversational models that other approaches, such as multi-turn Likert, cannot.Overall, our work provides the following contributions:A new evaluation method with a clear mechanism that provides fast, cheap iteration. This evaluation method allows efficient reuse of data from prior papers, allowing new models to be evaluated independently of baselines, and dramatically lowers the cost of annotation.We optimize question choices to find those with the highest agreement, increasing confidence in the desired test. We provide the wording of the questions that we found to work best for several questions of interest (most engaging, human, interesting or knowledgeable conversationalist) for further research use.We provide an explicit benchmark comparison between current best performing retrieval and generative models on two recent tasks, PersonaChat BIBREF5 and Wizard of Wikipedia BIBREF7 for several question choices, revealing the current state-of-the-art, and to be used for benchmarking on these tasks in the future.We show that our test can be applied to self-chats rather than human-model conversation logs, which can reveal problems with existing models at a cheaper price, and provides high agreement with the human-model evaluations.We will release the code for running these tests.Related Work	Dialogue tasks have traditionally been separated into two areas: goal-oriented and chitchat. Goal-oriented tasks typically have a clearer evaluation, e.g. task completion can be measured if the correct actions are taken BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14. Chitchat tasks are more open ended, and instead feature conversations without a precise goal that can be automatically evaluated. For example, conversations where two speaking partners are discussing interests BIBREF5 or topics BIBREF7. We study the latter in this work.Evaluation of chitchat tasks with automatic metrics is difficult precisely because of their open-ended nature. For example, the answer to the question “What are you doing tonight?” has many possible answers, each with little word overlap. This means standard metrics for tasks like question-answering or machine translation do not work well, and have poor correlation with human judgments BIBREF0, BIBREF15. Nevertheless, a number of studies do report automatic metrics, without human studies BIBREF16, BIBREF17. Researchers have made attempts to improve automatic evaluation, trying methods such as adversarial evaluation BIBREF18, learning a scoring model BIBREF1, or a learnt ensemble of automatic metrics BIBREF19, but their value is as yet not fully understood.Currently the standard approach in chitchat dialogue is to perform human evaluations BIBREF2, BIBREF20, BIBREF21, BIBREF4, BIBREF5, BIBREF7, typically reporting a judgment such as conversation quality or appropriateness via a Likert scale or pairwise comparison. While conversations are naturally multi-turn, pairwise setups typically consider single turn evaluations, taking the “gold” dialogue history from human-human logs, and only consider altering a single utterance. A more complete multi-turn evaluation is typically measured with a Likert scale (usually 1-4 or 1-5) after the conversation takes place. Some works such as BIBREF6 ask a series of questions relating to different aspects of conversational ability. There are some notable variants from these standard setups. BIBREF22 provide a method that combines continuous scales and relative assessments, but in single-turn, rather than multi-turn evaluation. BIBREF19 compare human evaluations to automatic metrics computed on self-chats. Note that we also use self-chats in this work, but we evaluate these with humans, rather than automatic metrics.Finally, this work expands upon some of the ideas present in BIBREF6. In that work, a test for interestingness of a specificity-controlled model conducted with pairwise chat logs was mentioned, similar to the ones used here, but was not the focus of their work. In our work, we conduct a full study of novel variants of this approach, consider optimizing the questions for robust measurements over four types of questions, utilize self-chat logs in addition to human-bot logs, and benchmark state-of-the-art models across two recent tasks.Method: Acute-eval	To compare two dialogue models, model A and model B, our evaluation asks humans to directly compare side-by-side multi-turn dialogues conducted by these models. See Figure FIGREF1 for an example.Our method is thus the following: (1) collect conversation logs for model A; similarly for model B. (2) In a number of trials, ask annotators to make binary judgments between sampled pairs from the logs, and collate the results to determine the winner, either A or B, and the statistical significance.We consider different approaches to step (1) and (2) below.Method: Acute-eval ::: Human-Model chats	Our standard setup is to compare conversation logs between models and humans. In each evaluation trial we then show a human annotator two of the previously obtained conversations, one of model $A$ conversing with a human, and one of model $B$ conversing with a (possibly different) human. The annotator sees the conversations side by side on the same screen, with the two models' utterances highlighted in different colors, and the human utterances in gray to minimally distract from the models.The annotator is posed a question phrasing (e.g. “which speaker is more knowledgeable” or “which speaker sounds more human?”), and asked to make a binary choice between model $A$ and model $B$. They are strongly encouraged to provide a short text justification for their choice. We collect $N$ trials of such pairwise judgments, and use them to decide which model wins. Statistical significance can be computed using a binomial test.Method: Acute-eval ::: Self-Chats	Human-model conversation logs are themselves time-consuming and expensive to collect, which limits rapid iterative model development. We investigate if it is possible to remove the human from the conversation, and only use human annotators in the final pairwise conversation evaluation step. The concept of self-chats BIBREF21, BIBREF19, whereby a model talks to itself, playing the roles of both speaking partners, has been previously explored in other contexts. Such logs are easy to collect for models A and B, involving simply running inference for both speaker roles. We then use these logs in the Acute-eval pairwise comparison setup as described above.Method: Acute-eval ::: Question Optimization	So far, we have not detailed the actual question(s) asked of the annotators. The framing and phrasing of questions in surveys is known to greatly affect the direction of responses, and therefore, in the case of evaluation, inter-annotator agreement. Though this has been noted in prior work BIBREF1, we have found no systematic experimentation on question formulation or task presentation. We therefore aim to propose and evaluate multiple potential question wordings to achieve higher agreement.To do this, we build an initial test that compares human-human logs with human-model logs where the model is a relatively low quality baseline model. The aim is that there should be a clear and agreeable difference between human and model which is visible to human annotators. We ask annotators to make judgments between these two, where we choose pairs where the human should be judged as superior.We then run independent trials with different question phrasing, and find the questions with highest inter-annotator agreement. The winning questions can then be used in future experiments by ourselves, and other researchers. Although having high inter-annotator agreement does not guarantee that crowdworkers interpret the question as intended, it increases the chance the question is understood uniformly. That is, the researcher still has to exercise care in the formulation of the question so that they believe it measures the quantity they are interested in. In our experiments we find questions with high-agreement rate over four axes: engagingness, interestingness, knowledge and humanness.Method: Acute-eval ::: Annotation Quality	We use crowdworkers for our annotations. We recommend limiting the number of annotations a single worker may complete to be only a few pairs (in our experiments, if we are making $N$ model comparisons then we allow $N$ annotations). In preliminary trials, we found that limiting the influence of any one worker was important for replicability, but that results were highly consistent across multiple runs with this limitation.Additionally, the first comparison any worker is asked to annotate consists of a conversation between a weak baseline model and human, and a human-human conversation. If a worker fails to rate the human-human conversation as better, we remove their annotations from the results, in order to remove poor quality annotators. We additionally remove workers who never give a reason for their choice. Note that adding such worker quality tests to pairwise annotation tasks is straightforward where the gold annotation is known, while it is harder for Likert tests which have integer scores. One may also increase the number of quality-control annotations to decrease the likelihood of fraudulent workers, but we found using a single control question had a reasonable cost-noise ratio.Each specific pair of conversations is shown at most once, given that there are at least as many possible pairs of conversations as desired annotations. If there are more conversations available for each model than desired annotations, each conversation is shown at most once - that is, in only one annotation. We found that maximizing the diversity of pairs improved robustness of our evaluation across multiple replication experiments.Experiments	We perform experiments on two tasks, PersonaChat and Wizard of Wikipedia, which evaluate different aspects of conversational ability. We first optimize the questions to maximize worker agreement, and then benchmark existing state-of-the-art models on each task.Experiments ::: PersonaChat task	PersonaChat BIBREF5 is a chitchat dialogue task involving two participants (two humans or a human and a bot). Each participant is given a persona – a short collection of personal traits such as I'm left handed or My favorite season is spring – and are instructed to get to know each other by chatting naturally using their designated personas, for 6–8 turns. The original dataset contains nearly 9000 human-human training conversations; most models are pretrained with a larger corpus, and then fine-tuned on this set.PersonaChat was the subject of the NeurIPS 2018 ConvAI2 Challenge BIBREF8, in which competitor's models were first evaluated with respect to automatic metrics, and then with respect to human judgment via human-bot chats followed by the question “How much did you enjoy talking to this user?"" on a scale of 1–4. A total of 9 systems were evaluated using human annotators, 100 conversations for each. In this work, we leverage the human-model chat logs from the ConvAI2 competition for three models: Lost in Conversation (LIC), which won the competition, and Hugging Face (HF; BIBREF23, BIBREF23) which won the automatic evaluation track, and the KVMemNN BIBREF24 baseline released by the competition organizers (KV; BIBREF8, BIBREF8). LIC and HF are large pretrained and fine-tuned generative Transformer models, while KV is a retrieval model with no pretraining.Secondly, we also compare to recently published models from BIBREF6. The authors studied the effects of controllable generation. and showed that Repetition-controlled (RC), Inquisitive (INQ), and Interesting (INT) models obtained the highest human Likert scores in their study, however their comparison to models from other studies is not direct. We thus compare to these models as well; we use the human-model conversation logs from their work, 100 for each model.Finally, we also compare to the Polyencoder model (PE, BIBREF25, BIBREF25), a recent state-of-the-art retrieval model. It is a type of large Transformer architecture pretrained on Reddit, which learns a small number of global features to represent the input so that retrieval can be computed efficiently. As no conversation logs were provided in that work, we additionally collect human-model conversations for that model.Overall, we benchmark 7 models, and compare them to human (H) performance in a number of different settings: with human-model and self-chat over three questions: engagingness, humamnness and interestingness.Experiments ::: Wizard of Wikipedia task	Wizard of Wikipedia BIBREF7 is a chitchat dialogue task where two speakers discuss a topic in depth, chosen from 1247 topics. One speaker (termed the Wizard) is meant to be both engaging and knowledgeable on the topics, and has access to an information retrieval system over Wikipedia to supplement their own knowledge. The other speaker (the Apprentice) is meant to be curious and eager to learn about the topic. The original dataset contains over 18,000 human-human dialogues, and has been used to train various kinds of models to imitate the human wizards. These include the Memory Network Transformer, in both generative and retrieval versions that employs the retrieved knowledge by attending over it before producing an utterance (GK and RK respectively), and baselines that do not have access to the knowledge (GU and RU). See Figure FIGREF25 for an example chat. We use the human-model logs from that paper (100 conversations for each model) on unseen test topics and evaluate them against humans (H), using both engagingness and knowledgeability questions. We note the original paper tested engagingness only.Experiments ::: Question Optimization	We are interested in evaluating models in terms of four axes: engagingness, interestingness, knowledge and humanness. In order to find the questions with highest inter-annotator agreement, we run multiple trials of experiments according to the setup described below. Each trial tests the effectiveness of a single question and consists of the same set of multi-turn conversation logs, presented to the human annotators. We test 13 questions: three regarding engagingness, four regarding interestingness, three regarding humanness, and three regarding knowledgeability (see Table TABREF11).We compare human-human logs with human-model logs where the model is a relatively low quality baseline model, with the aim that there should be a clear and agreeable difference between human and model which is visible to human annotators. For PersonaChat we use a greedy generative baseline, and for Wizard we use the GU (generative unknowledgeable) model. Both of these baselines exhibit strong repetitive behavior which is known to be highly disfavored by crowdworkers BIBREF6. We select a single handpicked conversation pair for each of the tasks, and collect $\sim $20 annotations per question.We calculate the inter-annotator agreement for each question. The question achieving the highest inter-annotator agreement is selected for use in the rest of our experiments. The specific question phrasing and the texts accompanying the option for Speaker 1 (i.e. the left-hand conversation) are listed in Table TABREF11 along with inter-annotator agreements. As can be seen, the phrasing of the question is important, with poor phrasing choices leading to much lower agreement levels, e.g. 86.7% agreement in the best case for interestingness, and 69.6% in the worst case.As a preliminary sanity check, we ran A/A tests over each of the engagingness, interestingness, and humanness best questions, with the same model appearing as both Speaker 1 and 2. All three tests came back close to 50-50.Overall, we see this question optimization step as an important pre-requisite for our main experiments, and use the best discovered phrasing in each case. We encourage further research to use them as well.Experiments ::: Benchmarking: Evaluation of State-of-the-art ::: PersonaChat	We first compare all 7 models and humans on the PersonaChat task using Acute-eval over the human-model chats using the optimized engagingness question. In total, we evaluate 28 paired comparisons. Results are given in Table TABREF18. Bold win percentages indicate significance.We first observe that the models form a clean well-ordered set, and there are no rock-paper-scissors effects, giving an order Human $>$ PE $>$ LIC $>$ INT $>$ HF $>$ INQ $>$ KV $>$ RC. In general, these results agree closely with the known Likert comparisons made in prior papers, shown in Table TABREF19. Similar conclusions are derived for the interestingness and humanness questions as well, see Tables TABREF26 and TABREF24, note the model ordering is slightly different for those questions. BIBREF6 previously showed that different models often exhibit different rankings for different metrics, and Acute-eval results remain largely consistent with Likert.A surprising result for the community is that the retrieval model PE outperforms all generative models, as the community has focused heavily on building generative models, e.g. almost all 23 entrants to the ConvAI2 competition BIBREF8. Now that the current best performing models have been benchmarked against each other we hope future research will use the same approach so the state-of-the-art can be clearly tracked.Experiments ::: Benchmarking: Evaluation of State-of-the-art ::: Self-Chat	We perform Acute-eval over self-chats instead of human-model chats. We compare all models and humans (via human-human chats) in an otherwise identical setup to the human-bot evaluation for PersonaChat. Results are given in Table TABREF20.We observe very similar conclusions to human-model chats in terms of winning models, making this a viable cheaper alternative to collecting human-model conversations, thus being considerably cheaper to collect. This approach also appears to require relatively fewer annotations/person-hours in this case to achieve statistical significance. One important caveat is the performance of the HF model. HF self-chats surface degeneracies in the model itself, and do not look natural (see Figure FIGREF22 for examples), explaining its poor performance compared to all other models. All other models do not exhibit this behavior and apart from HF, are ordered by humans exactly the same as for human-bot chats. For example, see Figure FIGREF23 for PE engaging in self-chat more successfully. However, due to the inadequacies of a specific model, in this case HF, conclusions from self-chat performance results must therefore be handled with care, but we believe are a reasonable choice for early experiments in the model development cycle, enabling faster research iteration.One concern with self-chat is that powerful models could easily cheat, and simply recall training examples with perfect accuracy. In practice, we found that none of the models exhibit this behavior: $<$1% of the Polyencoder's call-response utterance pairs produced during self-chats come directly from the training set. The worst offender, INQ, has roughly 10% of pairs coming from training, but this stems from it using the same generic greeting and response in nearly all conversations (“Hello, how are you doing today?”, “I am doing well, how about yourself?”).Experiments ::: Benchmarking: Evaluation of State-of-the-art ::: Wizard of Wikipedia	We similarly compare all 4 models and humans on the optimized engaging and knowledge questions. The results are given in Tables TABREF27 and TABREF28. We again find retrieval models outperform generative models, with knowledge attention (GK) clearly helping the generative models, but with RU and RK very close.Results largely agree between the two questions, except retrieval with knowledge (RK) more clearly beats the generative version (GK) than retrieval without (RU) when the question is about knowledge. For the engagingness question, where it makes sense that this is less important, there is little difference between knowledge or not.Experiments ::: Benchmarking: Evaluation of State-of-the-art ::: Comparison to Likert	We compare Acute-eval to multi-turn Likert for both tasks by computing pairwise Likert differences, where known, from the original papers. We do not compare across papers as evaluation setups differ. Values are provided in Tables TABREF19, TABREF26, TABREF24 and TABREF27. While the tests generally agree, Acute-eval can be a more sensitive test, which more often yields significance. On Wizard of Wikipedia where all Likert matchups are known, 8 of the pairwise matchups are significant for our test with human-model chats, while 6 are significant for Likert. On PersonaChat for the interestingness question, 6 of 10 matchups are significant for Acute-eval, including all known Likert matchups, which only has 2 of 3 that are significant. For the humanness question, 5 of 10 matchups are significant for Acute-eval, including all known Likert matchups, which only has 2 of 3 that are significant. For the engagingness question, 5 of the 9 Likert matchups are significant. All 9 are significant for Acute-eval when using self-chats; 3 are significant for human-model chats.We compare the cost effectiveness of Likert to Acute-eval human-model and self-chat comparisons in Figure FIGREF30. Shown is the PersonaChat Engagingness question comparing RC and INT models, a fairly tight matchup. We show the % chance of achieving significance when drawing pairs of dialogues at random, plotting with respect to person-hours spent annotating. In this case Likert fails to achieve significance, likely due to bias and variance issues with integer scores. Acute-eval human-model and self-chat pairwise tests perform well, achieving significance; self-chat requires fewer person-hours.Conclusion	Studying the ability of machines to communicate with humans is an important long-term goal of AI research. Unfortunately, measuring progress towards that goal has been hampered by the trustworthiness of evaluation itself. Current human evaluation methods such as multi-turn Likert are expensive to run, have annotator bias and variance problems, and can fail to yield statistical significance.In this work we have contributed a novel evaluation method that alleviates some of these problems. By optimizing questions and performing comparisons on pairs of human-bot dialogues we arrive at more sensitive statistical tests when benchmarking current state-of-the models. Utilizing self-chat bot evaluations we can often improve sensitivity, while yielding even cheaper evaluations. We will publicly release the code for our tests, and recommend them to be used in future research studies in order to push forward the state of the art.","['Which dialogue data do they use to evaluate on?', 'How much faster are pairwise annotations than other annotations?']","['Datasets from PersonaChat and Wizard of Wikipedia tasks.', 'by 5 times']"
67,"Application of Pre-training Models in Named Entity Recognition	Named Entity Recognition (NER) is a fundamental Natural Language Processing (NLP) task to extract entities from unstructured data. The previous methods for NER were based on machine learning or deep learning. Recently, pre-training models have significantly improved performance on multiple NLP tasks. In this paper, firstly, we introduce the architecture and pre-training tasks of four common pre-training models: BERT, ERNIE, ERNIE2.0-tiny, and RoBERTa. Then, we apply these pre-training models to a NER task by fine-tuning, and compare the effects of the different model architecture and pre-training tasks on the NER task. The experiment results showed that RoBERTa achieved state-of-the-art results on the MSRA-2006 dataset.	Introduction	Named Entity Recognition (NER) is a basic and important task in Natural Language Processing (NLP). It aims to recognize and classify named entities, such as person names and location namesBIBREF0. Extracting named entities from unstructured data can benefit many NLP tasks, for example Knowledge Graph (KG), Decision-making Support System (DSS), and Question Answering system. Researchers used rule-based and machine learning methods for the NER in the early yearsBIBREF1BIBREF2. Recently, with the development of deep learning, deep neural networks have improved the performance of NER tasksBIBREF3BIBREF4. However, it may still be inefficient to use deep neural networks because the performance of these methods depends on the quality of labeled data in training sets while creating annotations for unstructured data is especially difficultBIBREF5. Therefore, researchers hope to find an efficient method to extract semantic and syntactic knowledge from a large amount of unstructured data, which is also unlabeled. Then, apply the semantic and syntactic knowledge to improve the performance of NLP task effectively.Recent theoretical developments have revealed that word embeddings have shown to be effective for improving many NLP tasks. The Word2Vec and Glove models represent a word as a word embedding, where similar words have similar word embeddingsBIBREF6. However, the Word2Vec and Glove models can not solve the problem of polysemy. Researchers have proposed some pre-training models, such as BERT, ERNIE, and RoBERTa, to learn contextualized word embeddings from unstructured text corpusBIBREF7BIBREF8BIBREF9. These models not only solve the problem of polysemy but also obtain more accurate word representations. Therefore, researchers pay more attention to how to apply these pre-training models to improve the performance of NLP tasks.The purpose of this paper is to introduce the structure and pre-training tasks of four common pre-trained models (BERT, ERNIE, ERNIE2.0-tiny, RoBERTa), and how to apply these models to a NER task by fine-tuning. Moreover, we also conduct experiments on the MSRA-2006 dataset to test the effects of different pre-training models on the NER task, and discuss the reasons for these results from the model architecture and pre-training tasks respectively.Related work ::: Named Entity Recognition	Named entity recognition (NER) is the basic task of the NLP, such as information extraction and data mining. The main goal of the NER is to extract entities (persons, places, organizations and so on) from unstructured documents. Researchers have used rule-based and dictionary-based methods for the NERBIBREF1. Because these methods have poor generalization properties, researchers have proposed machine learning methods, such as Hidden Markov Model (HMM) and Conditional Random Field (CRF)BIBREF2BIBREF10. But machine learning methods require a lot of artificial features and can not avoid costly feature engineering. In recent years, deep learning, which is driven by artificial intelligence and cognitive computing, has been widely used in multiple NLP fields. Huang $et$ $al$. BIBREF3 proposed a model that combine the Bidirectional Long Short-Term Memory (BiLSTM) with the CRF. It can use both forward and backward input features to improve the performance of the NER task. Ma and Hovy BIBREF11 used a combination of the Convolutional Neural Networks (CNN) and the LSTM-CRF to recognize entities. Chiu and Nichols BIBREF12 improved the BiLSTM-CNN model and tested it on the CoNLL-2003 corpus.Related work ::: Pre-training model	As mentioned above, the performance of deep learning methods depends on the quality of labeled training sets. Therefore, researchers have proposed pre-training models to improve the performance of the NLP tasks through a large number of unlabeled data. Recent research on pre-training models has mainly focused on BERT. For example, R. Qiao $et$ $al$. and N. Li $et$ $al$. BIBREF13BIBREF14 used BERT and ELMO respectively to improve the performance of entity recognition in chinese clinical records. E. Alsentzer $et$ $al$. , L. Yao $et$ $al$. and K. Huang $et$ $al$. BIBREF15BIBREF16BIBREF17 used domain-specific corpus to train BERT(the model structure and pre-training tasks are unchanged), and used this model for a domain-specific task, obtaining the result of SOTA.Methods	In this section, we first introduce the four pre-trained models (BERT, ERNIE, ERNIE 2.0-tiny, RoBERTa), including their model structures and pre-training tasks. Then we introduce how to use them for the NER task through fine-tuning.Methods ::: BERT	BERT is a pre-training model that learns the features of words from a large amount of corpus through unsupervised learningBIBREF7.There are different kinds of structures of BERT models. We chose the BERT-base model structure. BERT-base's architecture is a multi-layer bidirectional TransformerBIBREF18. The number of layers is $L=12$, the hidden size is $H=768$, and the number of self-attention heads is $A=12$BIBREF7.Unlike ELMO, BERT's pre-training tasks are not some kind of N-gram language model prediction tasks, but the ""Masked LM (MLM)"" and ""Next Sentence Prediction (NSP)"" tasks. For MLM, like a $Cloze$ task, the model mask 15% of all tokens in each input sequence at random, and predict the masked token. For NSP, the input sequences are sentence pairs segmented with [SEQ]. Among them, only 50% of the sentence pairs are positive samples.Methods ::: ERNIE	ERNIE is also a pre-training language model. In addition to a basic-level masking strategy, unlike BERT, ERNIE using entity-level and phrase-level masking strategies to obtain the language representations enhanced by knowledge BIBREF8.ERNIE has the same model structure as BERT-base, which uses 12 Transformer encoder layers, 768 hidden units and 12 attention heads.As mentioned above, ERNIE using three masking strategies: basic-level masking, phrase-level masking, and entity-level masking. the basic-level making is to mask a character and train the model to predict it. Phrase-level and entity-level masking are to mask a phrase or an entity and predict the masking part. In addition, ERNIE also performs the ""Dialogue Language Model (DLM)"" task to judge whether a multi-turn conversation is real or fake BIBREF8.Methods ::: ERNIE2.0-tiny	ERNIE2.0 is a continual pre-training framework. It could incrementally build and train a large variety of pre-training tasks through continual multi-task learning BIBREF19.ERNIE2.0-tiny compresses ERNIE 2.0 through the method of structure compression and model distillation. The number of Transformer layers is reduced from 12 to 3, and the number of hidden units is increased from 768 to 1024.ERNIE2.0-tiny's pre-training task is called continual pre-training. The process of continual pre-training including continually constructing unsupervised pre-training tasks with big data and updating the model via multi-task learning. These tasks include word-aware tasks, structure-aware tasks, and semantic-aware tasks.Methods ::: RoBERTa	RoBERTa is similar to BERT, except that it changes the masking strategy and removes the NSP taskBIBREF9.Like ERNIE, RoBERTa has the same model structure as BERT, with 12 Transformer layers, 768 hidden units, and 12 self-attention heads.RoBERTa removes the NSP task in BERT and changes the masking strategy from static to dynamicBIBREF9. BERT performs masking once during data processing, resulting in a single static mask. However, RoBoERTa changes masking position in every epoch. Therefore, the pre-training model will gradually adapt to different masking strategies and learn different language representations.Methods ::: Applying Pre-training Models	After the pre-training process, pre-training models obtain abundant semantic knowledge from unlabeled pre-training corpus through unsupervised learning. Then, we use the fine-tuning approach to apply pre-training models in downstream tasks. As shown in Figure 1, we add the Fully Connection (FC) layer and the CRF layer after the output of pre-training models. The vectors output by pre-training models can be regarded as the representations of input sentences. Therefore, we use a fully connection layer to obtain the higher level and more abstract representations. The tags of the output sequence have strong restrictions and dependencies. For example, ""I-PER"" must appear after ""B-PER"". Conditional Random Field, as an undirected graphical model, can obtain dependencies between tags. We add the CRF layer to ensure the output order of tags.Experiments and Results	We conducted experiments on Chinese NER datasets to demonstrate the effectiveness of the pre-training models specified in section III. For the dataset, we used the MSRA-2006 published by Microsoft Research Asia.The experiments were conducted on the AI Studio platform launched by the Baidu. This platform has a build-in deep learning framework PaddlePaddle and is equipped with a V100 GPU. The pre-training models mentioned above were downloaded by PaddleHub, which is a pre-training model management toolkit. It is also launched by the Baidu. For hyper-parameter configuration, we adjusted them according to the performance on development sets. In this article, the number of the epoch is 2, the learning rate is 5e-5, and the batch size is 16.The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.Discussion	This section discusses the experimental results in detail. We will analyze the different model structures and pre-training tasks on the effect of the NER task.First of all, it is shown that the deeper the layer, the better the performance. All pre-training models have 12 Transformer layers, except ERNIE2.0-tiny. Although Ernie2.0-tiny increases the number of hidden units and improves the pre-training task with continual pre-training, 3 Transformer layers can not extract semantic knowledge well. The F1 value of ERNIE-2.0-tiny is even lower than the baseline model.Secondly, for pre-training models with the same model structure, RoBERTa obtained the result of SOTA. BERT and ERNIE retain the sentence pre-training tasks of NSP and DLM respectively, while RoBERTa removes the sentence-level pre-training task because Liu $et$ $al$. BIBREF9 hypothesizes the model can not learn long-range dependencies. The results confirm the above hypothesis. For the NER task, sentence-level pre-training tasks do not improve performance. In contrast, RoBERTa removes the NSP task and improves the performance of entity recognition. As described by Liu $et$ $al$. BIBREF9, the NSP and the MLP are designed to improve the performance on specific downstream tasks, such as the SQuAD 1.1, which requires reasoning about the relationships between pairs of sentences. However, the results show that the NER task does not rely on sentence-level knowledge, and using sentence-level pre-training tasks hurts performance because the pre-training models may not able to learn long-range dependencies.Moreover, as mentioned before, RoBERTa could adapt to different masking strategies and acquires richer semantic representations with the dynamic masking strategy. In contrast, BERT and ERNIE use the static masking strategy in every epoch. In addition, the results in this paper show that the F1 value of ERNIE is slightly lower than BERT. We infer that ERNIE may introduce segmentation errors when performing entity-level and phrase-level masking.Conclusion	In this paper, we exploit four pre-training models (BERT, ERNIE, ERNIE2.0-tiny, RoBERTa) for the NER task. Firstly, we introduce the architecture and pre-training tasks of these pre-training models. Then, we apply the pre-training models to the target task through a fine-tuning approach. During fine-tuning, we add a fully connection layer and a CRF layer after the output of pre-training models. Results showed that using the pre-training models significantly improved the performance of recognition. Moreover, results provided a basis that the structure and pre-training tasks in RoBERTa model are more suitable for NER tasks.In future work, investigating the model structure of different downstream tasks might prove important.Acknowledgment	This research was funded by the major special project of Anhui Science and Technology Department (Grant: 18030801133) and Science and Technology Service Network Initiative (Grant: KFJ-STS-ZDTP-079).","['what evaluation metrics did they use?', 'what was the baseline?', ""what were roberta's results?"", 'which was the worst performing model?']","['Precision, recall and F1 score.', 'Unacceptable', 'Unacceptable', 'Unacceptable']"
68,"SubGram: Extending Skip-gram Word Representation with Substrings	Skip-gram (word2vec) is a recent method for creating vector representations of words (""distributed word representations"") using a neural network. The representation gained popularity in various areas of natural language processing, because it seems to capture syntactic and semantic information about words without any explicit supervision in this respect. We propose SubGram, a refinement of the Skip-gram model to consider also the word structure during the training process, achieving large gains on the Skip-gram original test set.	Introduction	Vector representations of words learned using neural networks (NN) have proven helpful in many algorithms for image annotation BIBREF0 or BIBREF1 , language modeling BIBREF2 , BIBREF3 and BIBREF4 or other natural language processing (NLP) tasks BIBREF5 or BIBREF6 .Traditionally, every input word of an NN is stored in the “one-hot” representation, where the vector has only one element set to one and the rest of the vector are zeros. The size of the vector equals to the size of the vocabulary. The NN is trained to perform some prediction, e.g. to predict surrounding words given a word of interest. Instead of using this prediction capacity in some task, the practice is to extract the output of NN's hidden layer of each word (called distributed representation) and directly use this deterministic mapping INLINEFORM0 of word forms to the vectors of real numbers as the word representation.The input one-hot representation of words has two weaknesses: the bloat of the size of the vector with more words in vocabulary and the inability to provide any explicit semantic or syntactic information to the NN.The learned distributed representation of words relies on much shorter vectors (e.g. vocabularies containing millions words are represented in vectors of a few hundred elements) and semantic or syntactic information is often found to be implicitly present (“embedded”) in the vector space. For example, the Euclidean distance between two words in the vector space may be related to semantic or syntactic similarity between them.Skip-gram Model	The authors of BIBREF7 created a model called Skip-gram, in which linear vector operations allow to find related words with surprisingly good results. For instance INLINEFORM0 gives a value close to INLINEFORM1 .In this paper, we extend Skip-gram model with the internal word structure and show how it improves the performance on embedding morpho-syntactic information.The Skip-gram model defined in BIBREF7 is trained to predict context words of the input word. Given a corpus INLINEFORM0 of words INLINEFORM1 and their context words INLINEFORM2 (i.e. individual words INLINEFORM3 appearing close the original word INLINEFORM4 ), it considers the conditional probabilities INLINEFORM5 . The training finds the parameters INLINEFORM6 of INLINEFORM7 to maximize the corpus probability: DISPLAYFORM0 The Skip-gram model is a classic NN, where activation functions are removed and hierarchical soft-max BIBREF8 is used instead of soft-max normalization. The input representation is one-hot so the activation function is not needed on hidden layer, there is nothing to be summed up. This way, the model is learned much faster than comparable non-linear NNs and lends itself to linear vector operations possibly useful for finding semantically or syntactically related words.Related Work	In BIBREF9 was proposed to append part-of-speech (POS) tags to each word and train Skip-gram model on the new vocabulary. This avoided conflating, e.g. nouns and verbs, leading to a better performance, at the cost of (1) the reliance on POS tags and their accurate estimation and (2) the increased sparsity of the data due to the larger vocabulary.The authors in BIBREF10 used character-level input to train language models using a complex setup of NNs of several types. Their model was able to assign meaning to out-of-vocabulary words based on the closest neighbor. One disadvantage of the model is its need to run the computation on a GPU for a long time.The authors of BIBREF11 proposed an extension of Skip-gram model which uses character similarity of words to improve performance on syntactic and semantic tasks. They are using a set of similar words as additional features for the NN. Various similarity measures are tested: Levenshtein, longest common substring, morpheme and syllable similarity.The authors of BIBREF12 added the information about word's root, affixes, syllables, synonyms, antonyms and POS tags to continuous bag-of-words model (CBOW) proposed by BIBREF7 and showed how these types of knowledge lead to better word embeddings. The CBOW model is a simpler model with usually worse performance than Skip-gram.SubGram	We propose a substring-oriented extension of Skip-gram model which induces vector embeddings from character-level structure of individual words. This approach gives the NN more information about the examined word with no drawbacks in data sparsity or reliance on explicit linguistic annotation.We append the characters and $ to the word to indicate its beginning and end. In order to generate the vector of substrings, we take all character bigrams, trigrams etc. up to the length of the word. This way, even the word itself is represented as one of the substrings. For the NN, each input word is then represented as a binary vector indicating which substrings appear in the word.The original Skip-gram model BIBREF7 uses one-hot representation of a word in vocabulary as the input vector. This representation makes training fast because no summation or normalization is needed. The weights INLINEFORM0 of the input word INLINEFORM1 can be directly used as the output of hidden layer INLINEFORM2 (and as the distributed word representation): INLINEFORM3 In our approach, we provide the network with a binary vector representing all substrings of the word. To compute the input of hidden layer we decided to use mean value as it is computationally simpler than sigmoid: DISPLAYFORM0 where INLINEFORM0 is the number of substrings of the word INLINEFORM1 .Evaluation and Data Sets	We train our NN on words and their contexts extracted from the English wikipedia dump from May 2015. We have cleaned the data by replacing all numbers with 0 and removing special characters except those usually present in the English text like dots, brackets, apostrophes etc. For the final training data we have randomly selected only 2.5M segments (mostly sentences). It consist of 96M running words with the vocabulary size of 1.09M distinct word forms.We consider only the 141K most frequent word forms to simplify the training. The remaining word forms fall out of vocabulary (OOV), so the original Skip-gram cannot provide them with any vector representation. Our SubGram relies on known substrings and always provides at least some approximation.We test our model on the original test set BIBREF7 . The test set consists of 19544 “questions”, of which 8869 are called “semantic” and 10675 are called “syntactic” and further divided into 14 types, see Table TABREF4 . Each question contains two pairs of words ( INLINEFORM0 ) and captures relations like “What is to `woman' ( INLINEFORM1 ) as `king' ( INLINEFORM2 ) is to `man' ( INLINEFORM3 )?”, together with the expected answer `queen' ( INLINEFORM4 ). The model is evaluated by finding the word whose representation is the nearest (cosine similarity) to the vector INLINEFORM5 . If the nearest neighbor is INLINEFORM6 , we consider the question answered correctly.In this work, we use Mikolov's test set which is used in many papers. After a closer examination we came to the conclusion, that it does not test what the broad terms “syntactic” and “semantic relations” suggest. “Semantics” is covered by questions of only 3 types: predict a city based on a country or state, currency name from the country and the feminine variant of nouns denoting family relations. The authors of BIBREF13 showed, that many other semantic relationships could be tested, e.g. walk-run, dog-puppy, bark-dog, cook-eat and others.“Syntactic” questions cover a wider range of relations at the boundary of morphology and syntax. The problem is that all questions of a given type are constructed from just a few dozens of word pairs, comparing pairs with each other. Overall, there are 313 distinct pairs throughout the whole syntactic test set of 10675 questions, which means only around 35 different pairs per question set. Moreover, of the 313 pairs, 286 pairs are regularly formed (e.g. by adding the suffix `ly' to change an adjective into the corresponding adverb). Though it has to be mentioned that original model could not use this kind of information.We find such a small test set unreliable to answer the question whether the embedding captures semantic and syntactic properties of words.Rule-Based Baseline Approach	Although the original test set has been used to compare results in several papers, no-one tried to process it with some baseline approach. Therefore, we created a very simple set of rules for comparison on the syntactic part of the test set. The rules cover only the most frequent grammatical phenomenona.[noitemsep]adjective-to-adverb: Add ly at the end of the adjective.opposite: Add un at the beginning of positive form.comparative: If the adjective ends with y, replace it with ier. If it ends with e, add r. Otherwise add er at the end.superlative: If the adjective ends with y, replace it with iest. If it ends with e, add st. Otherwise add est at the end.present-participle: If the verb ends with e, replace it with ing, otherwise add ing at the end.nationality-adjective: Add n at the end, e.g. Russia INLINEFORM0 Russian.past-tense: Remove ing and add ed at the end of the verb.plural: Add s at the end of the word.plural-verbs: If the word ends with a vowel, add es at the end, else add s.Our Test Set	We have decided to create more general test set which would consider more than 35 pairs per question set. Since we are interested in morphosyntactic relations, we extended only the questions of the “syntactic” type with exception of nationality adjectives which is already covered completely in original test set.We constructed the pairs more or less manually, taking inspiration in the Czech side of the CzEng corpus BIBREF14 , where explicit morphological annotation allows to identify various pairs of Czech words (different grades of adjectives, words and their negations, etc.). The word-aligned English words often shared the same properties. Another sources of pairs were acquired from various webpages usually written for learners of English. For example for verb tense, we relied on a freely available list of English verbs and their morphological variations. We have included 100–1000 different pairs for each question set. The questions were constructed from the pairs similarly as by Mikolov: generating all possible pairs of pairs. This leads to millions of questions, so we randomly selected 1000 instances per question set, to keep the test set in the same order of magnitude. Additionally, we decided to extend set of questions on opposites to cover not only opposites of adjectives but also of nouns and verbs.In order to test our extension of Skip-gram on out-of-vocabulary words, we created an additional subset of our test set with questions where at least one of INLINEFORM0 and INLINEFORM1 is not among the known word forms. Note that the last word INLINEFORM2 must be in vocabulary in order to check if the output vector is correct.Experiments and Results	We used a Python implementation of word2vec as the basis for our SubGram, which we have made freely available .We limit the vocabulary, requiring each word form to appear at least 10 times in the corpus and each substring to appear at least 500 times in the corpus. This way, we get the 141K unique words mentioned above and 170K unique substrings (+141K words, as we downsample words separately).Our word vectors have the size of 100. The size of the context window is 5.The accuracy is computed as the number of correctly answered questions divided by the total number of questions in the set. Because the Skip-gram cannot answer questions containing OOV words, we also provide results with such questions excluded from the test set (scores in brackets).Table TABREF18 and Table TABREF19 report the results. The first column shows the rule-based approach. The column “Released Skip-gram” shows results of the model released by Mikolov and was trained on a 100 billion word corpus from Google News and generates 300 dimensional vector representation. The third column shows Skip-gram model trained on our training data, the same data as used for the training of the SubGram. Last column shows the results obtained from our SubGram model.Comparing Skip-gram and SubGram on the original test set (Table TABREF18 ), we see that our SubGram outperforms Skip-gram in several morpho-syntactic question sets but over all performs similarly (42.5% vs. 42.3%). On the other hand, it does not capture the tested semantic relations at all, getting a zero score on average.When comparing models on our test set (Table TABREF19 ), we see that given the same training set, SubGram significantly outperforms Skip-gram model (22.4% vs. 9.7%). The performance of Skip-gram trained on the much larger dataset is higher (43.5%) and it would be interesting to see the SubGram model, if we could get access to such training data. Note however, that the Rule-based baseline is significantly better on both test sets.The last column suggests that the performance of our model on OOV words is not very high, but it is still an improvement over flat zero of the Skip-gram model. The performance on OOVs is expected to be lower, since the model has no knowledge of exceptions and can only benefit from regularities in substrings.Future Work	We are working on a better test set for word embeddings which would include many more relations over a larger vocabulary especially semantics relations. We want to extend the test set with Czech and perhaps other languages, to see what word embeddings can bring to languages morphologically richer than English.As shown in the results, the rule based approach outperform NN approach on this type of task, therefore we would like to create a hybrid system which could use rules and part-of-speech tags. We will also include morphological tags in the model as proposed in BIBREF9 but without making the data sparse.Finally, we plan to reimplement SubGram to scale up to larger training data.Conclusion	We described SubGram, an extension of the Skip-gram model that considers also substrings of input words. The learned embeddings then better capture almost all morpho-syntactic relations tested on test set which we extended from original described in BIBREF7 . This test set is released for the public use.An useful feature of our model is the ability to generate vector embeddings even for unseen words. This could be exploited by NNs also in different tasks.Acknowledgment	This work has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement no. 645452 (QT21), the grant GAUK 8502/2016, and SVV project number 260 333.This work has been using language resources developed, stored and distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2015071).","['Did they use the same dataset as Skip-gram to train?', 'How much were the gains they obtained?']","['Unacceptable', 'between 21-57% in several morpho-syntactic questions']"
69,"Similarity measure for Public Persons	For the webportal""Who is in the News!""with statistics about the appearence of persons in written news we developed an extension, which measures the relationship of public persons depending on a time parameter, as the relationship may vary over time. On a training corpus of English and German news articles we built a measure by extracting the persons occurrence in the text via pretrained named entity extraction and then construct time series of counts for each person. Pearson correlation over a sliding window is then used to measure the relation of two persons.	Motivation	“Who is in the News!” is a webportal with statistics and plots about the appearence of persons in written news articles. It counts how often public persons are mentioned in news articles and can be used for research or journalistic purposes. The application is indexing articles published by “Reuters” agency on their website . With the interactive charts users can analyze different timespans for the mentiones of public people and look for patterns in the data. The portal is bulit with the Python microframework “Dash"" which uses the plattform “Plotly"" for the interactive charts.Playing around with the charts shows some interresting patterns like the one in the example of Figure FIGREF5 . This figure suggests that there must be some relationship between this two persons. In this example it is obvious because the persons are both german politicians and candidates for the elections.This motivated us to look for suitalbe measures to caputure how persons are related to each other, which then can be used to exted the webportal with charts showing the person to person relationships. Relationship and distance between persons have been analyzed for decades, for example BIBREF0 looked at distance in the famous experimental study “the Small World Problem”. They inspected the graph of relationships between different persons and set the “distance” to the shortest path between them.Other approaches used large text corpora for trying to find connections and relatedness by making statistics over the words in the texts. This of course only works for people appearing in the texts and we will discuss this in section SECREF2 . All these methods do not cover the changes of relations of the persons over time, that may change over the years. Therefore the measure should have a time parameter, which can be set to the desired time we are investigating.We have developed a method for such a measure and tested it on a set of news articles for the United States and Germany. In Figure FIGREF6 you see how the relation changes in an example of the German chancellor ”Angela Merkel” and her opponent on the last elections “Martin Schulz”. It starts around 0 in 2015 and goes up to about 0.75 in 2017 as we can expect looking at the high correlated time series chart in Figure FIGREF5 from the end of 2017.Related work	There are several methods which represent words as vectors of numbers and try to group the vectors of similar words together in vector space. Figure FIGREF8 shows a picture which represents such a high dimensional space in 2D via multidimensional scaling BIBREF1 . The implementation was done with Scikit Learn BIBREF2 , BIBREF3 , BIBREF4 . Word vectors are the building blocks for a lot of applications in areas like search, sentiment analysis and recommendation systems.The similarity and therefore the distance between words is calculated via the cosine similarity of the associated vectors, which gives a number between -1 and 1. The word2vec tool was implemented by BIBREF5 , BIBREF6 , BIBREF7 and trained over a Google News dataset with about 100 billion words. They use global matrix factorization or local context window methods for the training of the vectors.A trained dictionary for more than 3 million words and phrases with 300-dim vectors is provided for download. We used the Python library Gensim from BIBREF8 for the calculation of the word distances of the multidimensional scaling in Figure FIGREF8 . BIBREF9 combine the global matrix factorization and local context window methods in the ""GloVe"" method for word representation . BIBREF10 worked on a corpus of newspaper articles and developed a method for unsupervised relation discovery between named entities of different types by looking at the words between each pair of named etities. By measuring the similarity of this context words they can also discover the type of relatoionship. For example a person entity and an organization entity can have the relationship “is member of”. For our application this interesting method can not be used because we need additional time information. BIBREF11 developed models for supervised learning with kernel methods and support vector machines for relation extraction and tested them on problems of person-affiliation and organization-location relations, but also without time parameter.Dataset and Data Collection	We collected datasets of news articles in English and German language from the news agency Reuters (Table TABREF13 ). After a data cleaning step, which was deleting meta information like author and editor name from the article, title, body and date were stored in a local database and imported to a Pandas data frame BIBREF12 . The English corpus has a dictionary of length 106.848, the German version has a dictionary of length 163.788.For each article we extracted with the Python library “Spacy” the named entities labeled as person. “Spacy” was used because of its good performance BIBREF13 and it has pre-trained language models for English, German and others. The entity recognition is not perfect, so we have errors in the lists of persons. In a post processing step the terms from a list of common errors are removed. The names of the persons appear in different versions like “Donald Trump” or “Trump”. We map all names to the shorter version i.e. “Trump” in this example.In Figure FIGREF15 you can see the time series of the mentions of “Trump” in the news, with a peak at the 8th of November 2016 the day of the election. It is also visible that the general level is changing with the election and is on higher level since then.Taking a look at the histograms of the most frequent persons in some timespan shows the top 20 persons in the English news articles from 2016 to 2018 (Figure FIGREF16 ). As expected the histogram has a distribution that follows Zipfs law BIBREF14 , BIBREF15 .From the corpus data a dictionary is built, where for each person the number of mentions of this person in the news per day is recorded. This time series data can be used to build a model that covers time as parameter for the relationship to other persons.Building the Model	Figure FIGREF18 shows that the mentions of a person and the correlation with the mentions of another person varies over time. We want to capture this in our relation measure. So we take a time window of INLINEFORM0 days and look at the time series in the segment back in time as shown in the example of Figure FIGREF5 .For this vectors of INLINEFORM0 numbers for persons we can use different similarity measures. This choice has of course an impact of the results in applications BIBREF16 . A first choice could be the cosine similarity as used in the word2vec implementations BIBREF5 . We propose a different calculation for our setup, because we want to capture the high correlation of the series even if they are on different absolute levels of the total number of mentions, as in the example of Figure FIGREF19 .We propose to use the Pearson correlation coefficient instead. We can shift the window of calculation over time and therefore get the measure of relatedness as a function of time.Results	Figure FIGREF6 shows a chart of the Pearson correlation coefficient computed over a sliding window of 30 days from 2015-01-01 to 2018-02-26 for the persons “Merkel” and “Schulz”. The measure clearly covers the change in their relationship during this time period. We propose that 30 days is a good value for the time window, because on one hand it is large enough to have sufficient data for the calculation of the correlation, on the other hand it is sensitive enough to reflect changes over time. But the optimal value depends on the application for which the measure is used.An example from the US news corpus shows the time series of “Trump” and “Obama” in Figure FIGREF18 and a zoom in to the first month of 2018 in Figure FIGREF19 . It shows that a high correlation can be on different absolute levels. Therefore we used Pearson correlation to calculate the relation of two persons. You can find examples of the similarities of some test persons from December 2017 in Table TABREF17 The time series of the correlations looks quite “noisy” as you can see in Figure FIGREF6 , because the series of the mentions has a high variance. To reflect the change of the relation of the persons in a more stable way, you can take a higher value for the size of the calculation window of the correlation between the two series. In the example of Figure FIGREF20 we used a calculation window of 120 days instead of 30 days.Future Work	It would be interesting to test the ideas with a larger corpus of news articles for example the Google News articles used in the word2vec implementation BIBREF5 .The method can be used for other named entities such as organizations or cities but we expect not as much variation over time periods as with persons. And similarities between different types of entities would we interesting. So as the relation of a person to a city may chance over time.","['Did they build a dataset?', 'Do they compare to other methods?', 'How large is the dataset?']","['Unacceptable', 'Unacceptable', '70287']"
70,"Zero-Shot Adaptive Transfer for Conversational Language Understanding	Conversational agents such as Alexa and Google Assistant constantly need to increase their language understanding capabilities by adding new domains. A massive amount of labeled data is required for training each new domain. While domain adaptation approaches alleviate the annotation cost, prior approaches suffer from increased training time and suboptimal concept alignments. To tackle this, we introduce a novel Zero-Shot Adaptive Transfer method for slot tagging that utilizes the slot description for transferring reusable concepts across domains, and enjoys efficient training without any explicit concept alignments. Extensive experimentation over a dataset of 10 domains relevant to our commercial personal digital assistant shows that our model outperforms previous state-of-the-art systems by a large margin, and achieves an even higher improvement in the low data regime.	Introduction	Recently, there is a surge of excitement in adding numerous new domains to conversational agents such as Alexa, Google Assistant, Cortana and Siri to support a myriad of use cases. However, building a slot tagger, which is a key component for natural language understanding (NLU) BIBREF0 , for a new domain requires massive amounts of labeled data, hindering rapid development of new skills. To address the data-intensiveness problem, domain adaptation approaches have been successfully applied. Previous approaches are roughly categorized into two groups: data-driven approaches BIBREF1 , BIBREF2 and model-driven approaches BIBREF3 , BIBREF4 .In the data-driven approach, new target models are trained by combining target domain data with relevant data from a repository of arbitrary labeled datasets using domain adaptation approaches such as feature augmentation BIBREF1 . A disadvantage of this approach is the increase in training time as the amount of reusable data grows. The reusable data might contain hundreds of thousands of samples, making iterative refinement prohibitive. In contrast, the model-driven approach utilizes “expert"" models for summarizing the data for reusable slots BIBREF3 , BIBREF4 . The outputs of the expert models are directly used when training new domains, allowing for faster training. A drawback of this approach is that it requires explicit concept alignments which itself is not a trivial task, potentially missing lots of reusable concepts. Additionally, it's not easy to generalize these models to new, unseen slots.In this paper, we present a new domain adaptation technique for slot tagging inspired by recent advances in zero-shot learning. Traditionally, slot tagging is formulated as a sequence labeling task using the BIO representation (Figure 1 ). Our approach formulates this problem as detecting spans that contain values for each slot as shown in Figure 1 . For implicit transfer of reusable concepts across domains, we represent slots in a shared latent semantic space by embedding the slot description. With the shared latent space, domain adaptation can simply be done by fine-tuning a base model, which is trained on massive data, with a handful of target domain data without any explicit concept alignments. A similar idea of utilizing zero-shot learning for slot tagging has been proven to work in semi-supervised settings BIBREF5 . Our zero-shot model architecture differs from this by adding: 1) an attention layer to produce the slot-aware representations of input words, 2) a CRF layer to better satisfy global consistency constraints, 3) character-level embeddings to incorporate morphological information. Despite its simplicity, we show that our model outperforms all existing methods including the previous zero-shot learning approach in domain adaptation settings.We first describe our approach called Zero-Shot Adaptive Transfer model (ZAT) in detail. We then describe the dataset we used for our experiments. Using this data, we conduct experiments comparing our ZAT model with a set of state-of-the-art models: Bag-of-Expert (BoE) models and their non-expert counterparts BIBREF4 , and the Concept Tagger model BIBREF5 , showing that our model can lead to significant F1-score improvements. This is followed by an in-depth analysis of the results. We then provide a survey of related work and concluding remarks.Adaptive Transfer	Our Zero-Shot Adaptive Transfer model for slot tagging is a hierarchical model with six layers (Figure 2 ).Data	For our experiments, we collected data from a set of ten diverse domains. Table 1 shows the domains along with some statistics and sample utterances. Since these are new domains for our digital assistant, we did not have enough data for these domains in our historical logs. Therefore, the data was collected using crowdsourcing from human judges. For each domain, several prompts were created to crowdsource utterances for a variety of intents. These utterances were then annotated through our standard data annotation pipeline after several iterations of measuring interannotator agreement and refining the annotation guidelines. We collected at least 5000 instances for each domain, with more data collected for some domains based on business priority.For each of the domains, we sampled 80% of the data as training and 10% each as dev and test sets. Further samples of 2000, 1000, and 500 training samples were taken to compare our approach with previous methods. All samples were obtained by stratified sampling based on the annotated intents of the utterances.Baseline Systems	In order to compare our method against the state-of-the-art models, we compare against the models presented in BIBREF4 , including the BoE models and their non-BoE variants. We also compare our method with another zero-shot model for slot tagging BIBREF5 in domain adaptation settings.Following BIBREF4 , we concatenate the output of 25 dimensional character-level bidirectional LSTMs with pre-trained word embeddings to obtain morphology-sensitive embeddings. We then use a 100 dimensional word-level bidirectional LSTM layer to obtain contextualized word representations. Finally, the output of this layer is passed on to a dense feed forward layer with a softmax activation to predict the label probabilities for each word. We train using stochastic gradient descent with Adam BIBREF11 . To avoid overfitting, we also apply dropout to the output of each layer, with a default dropout keep probability of 0.8.The LSTM-BoE architecture is similar to the LSTM model with the exception that we use the output vectors of the word-level bidirectional LSTM layer of each expert model to obtain enriched word embeddings. Specifically, let $e_1 ... e_k \in E$ be the set of reusable expert domains. For each expert $e_j$ , we train a separate LSTM model. Let $h^{e_j}_i$ be the word-level bi-directional LSTM output for expert $e_j$ on word $w_i$ . When training on a target domain, for each word $w_i$ , we first compute a BoE representation for this word as $h^E = \sum _{e_i \in E} h^{e_j}_i$ . The input to the word-level LSTM for word $w_i$ in the target domain is now a concatenation of the character-level LSTM outputs, the pre-trained word embedding, and the BoE representation.Following BIBREF4 , We use two expert domains containing reusable slots: timex and location. The timex domain consists of utterances containing the slots $date$ , $time$ and $duration$ . The location domain consists of utterances containing $location$ , $location\_type$ and $place\_name$ slots. Both of these types of slots appear in more than 20 of a set of 40 domains developed for use in our commercial personal assistant, making them ideal candidates for reuse. Data for these domains was sampled from the input utterances from our commercial digital assistant. Each reusable domain contains about a million utterances. There is no overlap between utterances in the target domains used for our experiments and utterances in the reusable domains. The data for the reusable domains is sampled from other domains available to the digital assistant, not including our target domains. Models trained on the timex and location data have F1-scores of 96% and 89% respectively on test data from their respective domains.We use a standard linear-chain CRF architecture with n-gram and context features. In particular, for each token, we use unigram, bigram and trigram features, along with previous and next unigrams, bigrams, and trigrams for context length of up to 3 words. We also use a skip bigram feature created by concatenating the current unigram and skip-one unigram. We train our CRF using stochastic gradient descent with L1 regularization to prevent overfitting. The L1 coefficient was set to 0.1 and we use a learning rate of 0.1 with exponential decay for learning rate scheduling BIBREF12 .Similar to the LSTM-BoE model, we first train a CRF model $c_j$ for each of the reusable expert domains $e_j \in E$ . When training on a target domain, for every query word $w_i$ , a one-hot label vector $l^j_i$ is emitted by each expert CRF model $c_j$ . The length of the label vector $l^j_i$ is the number of labels in the expert domain, with the value corresponding to the label predicted by $c_j$ for word $w_i$ set to 1, and values for all other labels set to 0. For each word, the label vectors for all the expert CRF models are concatenated and provided as features for the target domain CRF training, along with the n-gram features.For comparison with a state-of-the-art zero-shot model, we implement the concept tagger (CT) BIBREF5 . The CT model consists of a single 256 dimensional bidirectional LSTM layer that takes pre-trained word embeddings as input to produce contextual word representations. This is followed by a feed forward layer where the contextual word representations are combined with a slot encoding to produce vectors of 128 dimensions. The slot encoding is the average vector of the word embeddings for the slot description. This feeds into another 128 dimensional bi-directional LSTM layer followed by a softmax layer that outputs the prediction for that slot.Domain Adaptation using Zero-Shot Model	For domain adaptation with zero-shot models, we first construct a joint training dataset by combining the training datasets of size 2000 from all domains except for a target domain. We then train a base model on the joint dataset. We sample input examples during training and evaluation for each slot to include both positive examples (which have the slot) and negative examples (which do not have the slot) with a ratio of 1 to 3. After the base model is trained, domain adaptation is simply done by further training the base model on varying amounts of the training data of the target domain. Note that the size of the joint dataset for each target domain is 18,000, which is dramatically smaller than millions of examples used for training expert models in the BoE approach. Furthermore, there are a lot of utterances in the joint dataset where no slots from the target domain is present.Comparative Results	Table 2 shows the F1-scores obtained by the different methods for each of the 10 domains. LSTM based models in general perform better than the CRF based models. Both the CRF-BoE and LSTM-BoE outperform the basic CRF and LSTM models. Both zero-shot models, CT and ZAT, again surpass the BoE models. ZAT has a statistically significant mean improvement of $4.04$ , $5.37$ and $3.27$ points over LSTM-BoE with training size 500, 1000 and 2000, respectively. ZAT also shows a statistically significant average improvement of $2.58$ , $2.44$ and $2.5$ points over CT, another zero-shot model with training size 500, 1000 and 2000, respectively. Looking at results for individual domains, the highest improvement for BoE models are seen for transportation and travel. This can be explained by these domains having a high frequency of $timex$ and $location$ slots. But BoE models show a regression in the shopping domain, and a reason could be the low frequency of expert slots. In contrast, ZAT consistently outperforms non-adapted models (CRF and LSTM) by a large margin. This is because ZAT can benefit from other reusable slots than $timex$ and $location$ . Though not as popular as $5.37$0 and $5.37$1 , slots such as $5.37$2 , $5.37$3 , $5.37$4 , and $5.37$5 appear across many domains.We plot the averaged performances on varying amounts of training data for each target domain in Figure 3 . Note that the improvements are even higher for the experiments with smaller training data. In particular, ZAT shows an improvement of $14.67$ in absolute F1-score over CRF when training with 500 instances. ZAT achieves an F1-score of 76.04% with only 500 training instances, while even with 2000 training instances the CRF model achieves an F1-score of only 75%. Thus the ZAT model achieves better F1-score with only one-fourth the training data.Table 3 shows the performances of CT and ZAT when no target domain data is available. Both models are able to achieve reasonable zero-shot performance for most domains, and ZAT shows an average improvement of $5.07$ over CT.Model Variants	In Table 4 , we ablate our full model by removing the CRF layer ( $-CRF$ ) and character-level word embeddings ( $-CHAR$ ). Without CRF, the model suffers a loss of 1%-1.8% points. The character-level word embeddings are also important: without this, the performance is down by 0.5%-2.7%. We study the impact of fine-tuning the pre-trained word embeddings ( $+WEFT$ ). When there is no target domain data available, fine-tuning hurts performance. But, with a moderate amount of target domain data, fine-tuning improves performance.Analysis	To better understand our model, in Figure 7 , we visualize the attention weights for the input sentence ""Can I wear jeans to a casual dinner?"" with different slots: (a) category, (b) item, and (c) time. From (a) and (b), it is clear that the attention is concentrated on the relevant words of the input and slot description. In contrast, there is no salient attention when the slot is not present in the input sentence.To analyze the impact of context, we compute the error rate with respect to span start position in the input sentence. Figure 4 shows that error rate tends to degrade for span start positions further from the beginning. This highlights opportunities to reduce a significant amount of errors by considering previous context.As shown in Figure 5 , our model makes more errors for longer spans. This can be improved by consulting spans detected by parsers or other span-based models such as coreference resolution systems BIBREF13 .Finally, we compute the percentage of POS tags that are tied to labeling errors. Figure 6 shows POS tags which occurs more than 10,000 times and contributes to more than 10% of errors. It is not surprising that there are many errors for ADJ, ADV and NOUN. Our system suffers in handling conjunctive structures, for instance “Help me find my $[black\text{ }and\text{ }tan]_{described\_as}$ $[jacket]_{item}$ ”, and parsing information can be helpful at enforcing structural consistencies. The NUM category is associated with a variety of concepts and diverse surface forms. Thus it is a probably good idea to have an expert model focusing on the NUM category.Related Work	A number of deep learning approaches have been applied to the problem of language understanding in recent years BIBREF14 , BIBREF15 , BIBREF16 . For a thorough overview of deep learning methods in conversational language understanding, we refer the readers to BIBREF17 .As the digital assistants increase in sophistication, an increasing number of slot models have to be trained, making scalability of these models a concern. Researchers have explored several directions for data efficient training of new models. One of the directions has been multi-task learning, where a joint model across multiple tasks and domains might be learned BIBREF18 , BIBREF19 , BIBREF20 . As a recent example, BIBREF21 presented an approach for multi-task learning across the tasks of language understanding and dialog state tracking. BIBREF22 presented a multi-task learning approach for language understanding that consists of training a shared representation over multiple domains, with additional fine-tuning applied for new target domains by replacing the affine transform and softmax layers.Another direction has been domain adaptation and transfer learning methods. Early focus was on data driven adaptation techniques where data from multiple source domains was combined BIBREF1 . Such data-driven approaches offer model improvements at the cost of increased training time. More recently, model-driven approaches have shown success BIBREF3 , BIBREF4 . These approaches follow the strategy of first training expert models on the source data, and then using the output of these models when training new target models. A benefit of these approaches over data-driven adaptation techniques is the improved training time that scales well as the number of source domains increase.However, both these transfer learning approaches require concept alignment to map the new labels to existing ones, and cannot generalize to unseen labels. This has led researchers to investigate zero-shot learning techniques, where a model is learned against label representations as opposed to a fixed set of labels.Several researchers have explored zero-shot models for domain and intent classification. BIBREF23 described a zero-shot model for domain classification of input utterances by using query click logs to learn domain label representations. BIBREF24 also learn a zero-shot model for domain classification. BIBREF25 learn a zero-shot model for intent classification using a DSSM style model for learning semantic representations for intents.Slot tagging using zero-shot models has also been explored. BIBREF26 presented a zero-shot approach for slot tagging based on a knowledge base and word representations learned from unlabeled data. BIBREF5 also applied zero-shot learning to slot-filling by implicitly linking slot representations across domains by using the label descriptions of the slots. Our method is similar to their approach, but we use an additional attention layer to produce the slot-aware representations of input words, leading to better performance as demonstrated by our empirical results.More recently, zero-shot learning has also been applied to other tasks. For example, BIBREF27 apply zero-shot learning for training language understanding models for multiple languages and show good results. BIBREF28 presented a zero-shot model for question generation from knowledge graphs, and BIBREF29 presented a model for zero-shot transfer learning for event extraction.Conclusion	In this paper, we introduce a novel Zero-Shot Adaptive Transfer method for slot tagging that utilizes the slot description for transferring reusable concepts across domains to avoid some drawbacks of prior approaches such as increased training time and suboptimal concept alignments. Experiment results show that our model performs significantly better than state-of-the-art systems by a large margin of 7.24% in absolute F1-score when training with 2000 instances per domain, and achieves an even higher improvement of 14.57% when only 500 training instances are used. We provide extensive analysis of the results to shed light on future work. We plan to extend our model to consider more context and utilize exogenous resources like parsing information.",['How large the improvement margin is?'],"['+7.24 for train size of 2000, +11.03 for train size of 1000, and +14.67 for train size of 500']"
71,"Unfolding and Shrinking Neural Machine Translation Ensembles	Ensembling is a well-known technique in neural machine translation (NMT) to improve system performance. Instead of a single neural net, multiple neural nets with the same topology are trained separately, and the decoder generates predictions by averaging over the individual models. Ensembling often improves the quality of the generated translations drastically. However, it is not suitable for production systems because it is cumbersome and slow. This work aims to reduce the runtime to be on par with a single system without compromising the translation quality. First, we show that the ensemble can be unfolded into a single large neural network which imitates the output of the ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On Japanese-English we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system.	Introduction	The top systems in recent machine translation evaluation campaigns on various language pairs use ensembles of a number of NMT systems BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . Ensembling BIBREF7 , BIBREF8 of neural networks is a simple yet very effective technique to improve the accuracy of NMT. The decoder makes use of INLINEFORM0 NMT networks which are either trained independently BIBREF9 , BIBREF2 , BIBREF3 , BIBREF4 or share some amount of training iterations BIBREF10 , BIBREF1 , BIBREF5 , BIBREF6 . The ensemble decoder computes predictions from each of the individual models which are then combined using the arithmetic average BIBREF9 or the geometric average BIBREF5 .Ensembling consistently outperforms single NMT by a large margin. However, the decoding speed is significantly worse since the decoder needs to apply INLINEFORM0 NMT models rather than only one. Therefore, a recent line of research transfers the idea of knowledge distillation BIBREF11 , BIBREF12 to NMT and trains a smaller network (the student) by minimizing the cross-entropy to the output of the ensemble system (the teacher) BIBREF13 , BIBREF14 . This paper presents an alternative to knowledge distillation as we aim to speed up decoding to be comparable to single NMT while retaining the boost in translation accuracy from the ensemble. In a first step, we describe how to construct a single large neural network which imitates the output of an ensemble of multiple networks with the same topology. We will refer to this process as unfolding. GPU-based decoding with the unfolded network is often much faster than ensemble decoding since more work can be done on the GPU. In a second step, we explore methods to reduce the size of the unfolded network. This idea is justified by the fact that ensembled neural networks are often over-parameterized and have a large degree of redundancy BIBREF15 , BIBREF16 , BIBREF17 . Shrinking the unfolded network leads to a smaller model which consumes less space on the disk and in the memory; a crucial factor on mobile devices. More importantly, the decoding speed on all platforms benefits greatly from the reduced number of neurons. We find that the dimensionality of linear embedding layers in the NMT network can be reduced heavily by low-rank matrix approximation based on singular value decomposition (SVD). This suggest that high dimensional embedding layers may be needed for training, but do not play an important role for decoding. The NMT network, however, also consists of complex layers like gated recurrent units BIBREF18 and attention BIBREF19 . Therefore, we introduce a novel algorithm based on linear combinations of neurons which can be applied either during training (data-bound) or directly on the weight matrices without using training data (data-free). We report that with a mix of the presented shrinking methods we are able to reduce the size of the unfolded network to the size of the single NMT network while keeping the boost in BLEU score from the ensemble. Depending on the aggressiveness of shrinking, we report either a gain of 2.2 BLEU at the same decoding speed, or a 3.4 INLINEFORM1 CPU decoding speed up with only a minor drop in BLEU compared to the original single NMT system. Furthermore, it is often much easier to stage a single NMT system than an ensemble in a commercial MT workflow, and it is crucial to be able to optimize quality at specific speed and memory constraints. Unfolding and shrinking address these problems directly.Unfolding KK Networks into a Single Large Neural Network	The first concept of our approach is called unfolding. Unfolding is an alternative to ensembling of multiple neural networks with the same topology. Rather than averaging their predictions, unfolding constructs a single large neural net out of the individual models which has the same number of input and output neurons but larger inner layers. Our main motivation for unfolding is to obtain a single network with ensemble level performance which can be shrunk with the techniques in Sec. SECREF3 .Suppose we ensemble two single layer feedforward neural nets as shown in Fig. FIGREF1 . Normally, ensembling is implemented by performing an isolated forward pass through the first network (Fig. SECREF2 ), another isolated forward pass through the second network (Fig. SECREF3 ), and averaging the activities in the output layers of both networks. This can be simulated by merging both networks into a single large network as shown in Fig. SECREF4 . The first neurons in the hidden layer of the combined network correspond to the hidden layer in the first single network, and the others to the hidden layer of the second network. A single pass through the combined network yields the same output as the ensemble if the output layer is linear (up to a factor 2). The weight matrices in the unfolded network can be constructed by stacking the corresponding weight matrices (either horizontally or vertically) in network 1 and 2. This kind of aggregation of multiple networks with the same topology is not only possible for single-layer feedforward architectures but also for complex networks consisting of multiple GRU layers and attention.For a formal description of unfolding we address layers with indices INLINEFORM0 . The special layer 0 has a single neuron for modelling bias vectors. Layer 1 holds the input neurons and layer INLINEFORM1 is the output layer. We denote the size of a layer in the individual models as INLINEFORM2 . When combining INLINEFORM3 networks, the layer size INLINEFORM4 in the unfolded network is increased by factor INLINEFORM5 if INLINEFORM6 is an inner layer, and equal to INLINEFORM7 if INLINEFORM8 is the input or output layer. We denote the weight matrix between two layers INLINEFORM9 in the INLINEFORM10 -th individual model ( INLINEFORM11 ) as INLINEFORM12 , and the corresponding weight matrix in the unfolded network as INLINEFORM13 . We explicitly allow INLINEFORM14 and INLINEFORM15 to be non-consecutive or reversed to be able to model recurrent networks. We use the zero-matrix if layers INLINEFORM16 and INLINEFORM17 are not connected. The construction of the unfolded weight matrix INLINEFORM18 from the individual matrices INLINEFORM19 depends on whether the connected layers are inner layers or not. The complete formula is listed in Fig. FIGREF5 .Unfolded NMT networks approximate but do not exactly match the output of the ensemble due to two reasons. First, the unfolded network synchronizes the attentions of the individual models. Each decoding step in the unfolded network computes a single attention weight vector. In contrast, ensemble decoding would compute one attention weight vector for each of the INLINEFORM0 input models. A second difference is that the ensemble decoder first applies the softmax at the output layer, and then averages the prediction probabilities. The unfolded network averages the neuron activities (i.e. the logits) first, and then applies the softmax function. Interestingly, as shown in Sec. SECREF4 , these differences do not have any impact on the BLEU score but yield potential speed advantages of unfolding since the computationally expensive softmax layer is only applied once.Shrinking the Unfolded Network	After constructing the weight matrices of the unfolded network we reduce the size of it by iteratively shrinking layer sizes. In this section we denote the incoming weight matrix of the layer to shrink as INLINEFORM0 and the outgoing weight matrix as INLINEFORM1 . Our procedure is inspired by the method of Srinivas and Babu sparsify-datafree. They propose a criterion for removing neurons in inner layers of the network based on two intuitions. First, similarly to Hebb's learning rule, they detect redundancy by the principle neurons which fire together, wire together. If the incoming weight vectors INLINEFORM2 and INLINEFORM3 are exactly the same for two neurons INLINEFORM4 and INLINEFORM5 , we can remove the neuron INLINEFORM6 and add its outgoing connections to neuron INLINEFORM7 ( INLINEFORM8 ) without changing the output. This holds since the activity in neuron INLINEFORM14 will always be equal to the activity in neuron INLINEFORM15 . In practice, Srinivas and Babu use a distance measure based on the difference of the incoming weight vectors to search for similar neurons as exact matches are very rare.The second intuition of the criterion used by Srinivas and Babu sparsify-datafree is that neurons with small outgoing weights contribute very little overall. Therefore, they search for a pair of neurons INLINEFORM0 according the following term and remove the INLINEFORM1 -th neuron. DISPLAYFORM0 Neuron INLINEFORM0 is selected for removal if (1) there is another neuron INLINEFORM1 which has a very similar set of incoming weights and if (2) INLINEFORM2 has a small outgoing weight vector. Their criterion is data-free since it does not require any training data. For further details we refer to Srinivas and Babu sparsify-datafree.Data-Free Neuron Removal	Srinivas and Babu sparsify-datafree propose to add the outgoing weights of INLINEFORM0 to the weights of a similar neuron INLINEFORM1 to compensate for the removal of INLINEFORM2 . However, we have found that this approach does not work well on NMT networks. We propose instead to compensate for the removal of a neuron by a linear combination of the remaining neurons in the layer. Data-free shrinking assumes for the sake of deriving the update rule that the neuron activation function is linear. We now ask the following question: How can we compensate as well as possible for the loss of neuron INLINEFORM3 such that the impact on the output of the whole network is minimized? Data-free shrinking represents the incoming weight vector of neuron INLINEFORM4 ( INLINEFORM5 ) as linear combination of the incoming weight vectors of the other neurons. The linear factors can be found by satisfying the following linear system: DISPLAYFORM0 where INLINEFORM0 is matrix INLINEFORM1 without the INLINEFORM2 -th column. In practice, we use the method of ordinary least squares to find INLINEFORM3 because the system may be overdetermined. The idea is that if we mix the outputs of all neurons in the layer by the INLINEFORM4 -weights, we get the output of the INLINEFORM5 -th neuron. The row vector INLINEFORM6 contains the contributions of the INLINEFORM7 -th neuron to each of the neurons in the next layer. Rather than using these connections, we approximate their effect by adding some weight to the outgoing connections of the other neurons. How much weight depends on INLINEFORM8 and the outgoing weights INLINEFORM9 . The factor INLINEFORM10 which we need to add to the outgoing connection of the INLINEFORM11 -th neuron to compensate for the loss of the INLINEFORM12 -th neuron on the INLINEFORM13 -th neuron in the next layer is: DISPLAYFORM0 Therefore, the update rule for INLINEFORM0 is: DISPLAYFORM0 In the remainder we will refer to this method as data-free shrinking. Note that we recover the update rule of Srinivas and Babu sparsify-datafree by setting INLINEFORM0 to the INLINEFORM1 -th unit vector. Also note that the error introduced by our shrinking method is due to the fact that we ignore the non-linearity, and that the solution for INLINEFORM2 may not be exact. The method is error-free on linear layers as long as the residuals of the least-squares analysis in Eq. EQREF10 are zero.The terminology of neurons needs some further elaboration for GRU layers which rather consist of update and reset gates and states BIBREF18 . On GRU layers, we treat the states as neurons, i.e. the INLINEFORM0 -th neuron refers to the INLINEFORM1 -th entry in the GRU state vector. Input connections to the gates are included in the incoming weight matrix INLINEFORM2 for estimating INLINEFORM3 in Eq. EQREF10 . Removing neuron INLINEFORM4 in a GRU layer means deleting the INLINEFORM5 -th entry in the states and both gate vectors.Data-Bound Neuron Removal	Although we find our data-free approach to be a substantial improvement over the methods of Srinivas and Babu sparsify-datafree on NMT networks, it still leads to a non-negligible decline in BLEU score when applied to recurrent GRU layers. Our data-free method uses the incoming weights to identify similar neurons, i.e. neurons expected to have similar activities. This works well enough for simple layers, but the interdependencies between the states and the gates inside gated layers like GRUs or LSTMs are complex enough that redundancies cannot be found simply by looking for similar weights. In the spirit of Babaeizadeh et al. sparsify-noiseout, our data-bound version records neuron activities during training to estimate INLINEFORM0 . We compensate for the removal of the INLINEFORM1 -th neuron by using a linear combination of the output of remaining neurons with similar activity patterns. In each layer, we prune 40 neurons each 450 training iterations until the target layer size is reached. Let INLINEFORM2 be the matrix which holds the records of neuron activities in the layer since the last removal. For example, for the decoder GRU layer, a batch size of 80, and target sentence lengths of 20, INLINEFORM3 has INLINEFORM4 rows and INLINEFORM5 (the number of neurons in the layer) columns. Similarly to Eq. EQREF10 we find interpolation weights INLINEFORM6 using the method of least squares on the following linear system. DISPLAYFORM0 The update rule for the outgoing weight matrix is the same as for our data-free method (Eq. EQREF12 ). The key difference between data-free and data-bound shrinking is the way INLINEFORM0 is estimated. Data-free shrinking uses the similarities between incoming weights, and data-bound shrinking uses neuron activities recorded during training. Once we select a neuron to remove, we estimate INLINEFORM1 , compensate for the removal, and proceed with the shrunk network. Both methods are prior to any decoding and result in shrunk parameter files which are then loaded to the decoder. Both methods remove neurons rather than single weights.The data-bound algorithm runs gradient-based optimization on the unfolded network. We use the AdaGrad BIBREF20 step rule, a small learning rate of 0.0001, and aggressive step clipping at 0.05 to avoid destroying useful weights which were learned in the individual networks prior to the construction of the unfolded network.Our data-bound algorithm uses a data-bound version of the neuron selection criterion in Eq. EQREF8 which operates on the activity matrix INLINEFORM0 . We search for the pair INLINEFORM1 according the following term and remove neuron INLINEFORM2 . DISPLAYFORM0 Shrinking Embedding Layers with SVD	The standard attention-based NMT network architecture BIBREF19 includes three linear layers: the embedding layer in the encoder, and the output and feedback embedding layers in the decoder. We have found that linear layers are particularly easy to shrink using low-rank matrix approximation. As before we denote the incoming weight matrix as INLINEFORM0 and the outgoing weight matrix as INLINEFORM1 . Since the layer is linear, we could directly connect the previous layer with the next layer using the product of both weight matrices INLINEFORM2 . However, INLINEFORM3 may be very large. Therefore, we approximate INLINEFORM4 as a product of two low rank matrices INLINEFORM5 and INLINEFORM6 ( INLINEFORM7 ) where INLINEFORM8 is the desired layer size. A very common way to find such a matrix factorization is using truncated singular value decomposition (SVD). The layer is eventually shrunk by replacing INLINEFORM9 with INLINEFORM10 and INLINEFORM11 with INLINEFORM12 .Results	The individual NMT systems we use as source for constructing the unfolded networks are trained using AdaDelta BIBREF21 on the Blocks/Theano implementation BIBREF22 , BIBREF23 of the standard attention-based NMT model BIBREF19 with: 1000 dimensional GRU layers BIBREF18 in both the decoder and bidrectional encoder; a single maxout output layer BIBREF24 ; and 620 dimensional embedding layers. We follow Sennrich et al. nmt-bpe and use subword units based on byte pair encoding rather than words as modelling units. Our SGNMT decoder BIBREF25 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for En-De). We choose a widely used, simple ensembling method (prediction averaging) as our baseline. We feel that the prevalence of this method makes it a reasonable baseline for our experiments.Related Work	The idea of pruning neural networks to improve the compactness of the models dates back more than 25 years BIBREF15 . The literature is therefore vast BIBREF28 . One line of research aims to remove unimportant network connections. The connections can be selected for deletion based on the second-derivative of the training error with respect to the weight BIBREF15 , BIBREF16 , or by a threshold criterion on its magnitude BIBREF29 . See et al. sparsify-nmt confirmed a high degree of weight redundancy in NMT networks.In this work we are interested in removing neurons rather than single connections since we strive to shrink the unfolded network such that it resembles the layout of an individual model. We argued in Sec. SECREF4 that removing neurons rather than connections does not only improve the model size but also the memory footprint and decoding speed. As explained in Sec. SECREF9 , our data-free method is an extension of the approach by Srinivas and Babu sparsify-datafree; our extension performs significantly better on NMT networks. Our data-bound method (Sec. SECREF14 ) is inspired by Babaeizadeh et al. sparsify-noiseout as we combine neurons with similar activities during training, but we use linear combinations of multiple neurons to compensate for the loss of a neuron rather than merging pairs of neurons.Using low rank matrices for neural network compression, particularly approximations via SVD, has been studied widely in the literature BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 . These approaches often use low rank matrices to approximate a full rank weight matrix in the original network. In contrast, we shrink an entire linear layer by applying SVD on the product of the incoming and outgoing weight matrices (Sec. SECREF18 ).In this paper we mimicked the output of the high performing but cumbersome ensemble by constructing a large unfolded network, and shrank this network afterwards. Another approach, known as knowledge distillation, uses the large model (the teacher) to generate soft training labels for the smaller student network BIBREF11 , BIBREF12 . The student network is trained by minimizing the cross-entropy to the teacher. This idea has been applied to sequence modelling tasks such as machine translation and speech recognition BIBREF35 , BIBREF13 , BIBREF14 . Our approach can be computationally more efficient as the training set does not have to be decoded by the large teacher network.Junczys-Dowmunt et al. averaging2,averaging1 reported gains from averaging the weight matrices of multiple checkpoints of the same training run. However, our attempts to replicate their approach were not successful. Averaging might work well when the behaviour of corresponding units is similar across networks, but that cannot be guaranteed when networks are trained independently.Conclusion	We have described a generic method for improving the decoding speed and BLEU score of single system NMT. Our approach involves unfolding an ensemble of multiple systems into a single large neural network and shrinking this network by removing redundant neurons. Our best results on Japanese-English either yield a gain of 2.2 BLEU compared to the original single NMT network at about the same decoding speed, or a INLINEFORM0 CPU decoding speed up with only a minor drop in BLEU.The current formulation of unfolding works for networks of the same topology as the concatenation of layers is only possible for analogous layers in different networks. Unfolding and shrinking diverse networks could be possible, for example by applying the technique only to the input and output layers or by some other scheme of finding associations between units in different models, but we leave this investigation to future work as models in NMT ensembles in current research usually have the same topology BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF6 .Acknowledgments	This work was supported by the U.K. Engineering and Physical Sciences Research Council (EPSRC grant EP/L027623/1).Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking	Data-free and data-bound shrinking can be interpreted as setting the expected difference between network outputs before and after a removal operation to zero under different assumptions.For simplicity, we focus our probabilistic treatment of shrinking on single layer feedforward networks. Such a network maps an input INLINEFORM0 to an output INLINEFORM1 . The INLINEFORM2 -th output INLINEFORM3 is computed according the following equation DISPLAYFORM0 where INLINEFORM0 is the incoming weight vector of the INLINEFORM1 -th hidden neuron (denoted as INLINEFORM2 in the main paper) and INLINEFORM3 the outgoing weight matrix of the INLINEFORM4 -dimensional hidden layer. We now remove the INLINEFORM5 -th neuron in the hidden layer and modify the outgoing weights to compensate for the removal: DISPLAYFORM0 where INLINEFORM0 is the output after the removal operation and INLINEFORM1 are the modified outgoing weights. Our goal is to choose INLINEFORM2 such that the expected error introduced by removing neuron INLINEFORM3 is zero: DISPLAYFORM0 ","['What were the performance results of their network?', 'What were the baselines?', 'What dataset is used?', 'Do they explore other language pairs?']","['For the test set a BLEU score of 25.7 on Ja-En and 20.7 (2014 test set), 23.1 (2015 test set), and 26.1 (2016 test set) on En-De', 'Unacceptable', 'Unacceptable', 'Unacceptable']"
72,"Overton: A Data System for Monitoring and Improving Machine-Learned Products	We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production machine learning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a set of novel high-level, declarative abstractions. Overton's vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year, Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.	Introduction	In the life cycle of many production machine-learning applications, maintaining and improving deployed models is the dominant factor in their total cost and effectiveness–much greater than the cost of de novo model construction. Yet, there is little tooling for model life-cycle support. For such applications, a key task for supporting engineers is to improve and maintain the quality in the face of changes to the input distribution and new production features. This work describes a new style of data management system called Overton that provides abstractions to support the model life cycle by helping build models, manage supervision, and monitor application quality.Overton is used in both near-real-time and backend production applications. However, for concreteness, our running example is a product that answers factoid queries, such as “how tall is the president of the united states?” In our experience, the engineers who maintain such machine learning products face several challenges on which they spend the bulk of their time.Fine-grained Quality Monitoring While overall improvements to quality scores are important, often the week-to-week battle is improving fine-grained quality for important subsets of the input data. An individual subset may be rare but are nonetheless important, e.g., 0.1% of queries may correspond to a product feature that appears in an advertisement and so has an outsized importance. Traditional machine learning approaches effectively optimize for aggregate quality. As hundreds of such subsets are common in production applications, this presents data management and modeling challenges. An ideal system would monitor these subsets and provide tools to improve these subsets while maintaining overall quality.Support for Multi-component Pipelines Even simple machine learning products comprise myriad individual tasks. Answering even a simple factoid query, such as “how tall is the president of the united states?” requires tackling many tasks including (1) find the named entities (`united states', and `president'), (2) find the database ids for named entities, (3) find the intent of the question, e.g., the height of the topic entity, (4) determine the topic entity, e.g., neither president nor united states, but the person Donald J. Trump, who is not explicitly mentioned, and (5) decide the appropriate UI to render it on a particular device. Any of these tasks can go wrong. Traditionally, systems are constructed as pipelines, and so determining which task is the culprit is challenging.Updating Supervision When new features are created or quality bugs are identified, engineers provide additional supervision. Traditionally, supervision is provided by annotators (of varying skill levels), but increasingly programmatic supervision is the dominant form of supervision BIBREF0, BIBREF1, which includes labeling, data augmentation, and creating synthetic data. For both privacy and cost reasons, many applications are constructed using programmatic supervision as a primary source. An ideal system can accept supervision at multiple granularities and resolve conflicting supervision for those tasks.There are other desiderata for such a system, but the commodity machine learning stack has evolved to support them: building deployment models, hyperparameter tuning, and simple model search are now well supported by commodity packages including TensorFlow, containers, and (private or public) cloud infrastructure. By combining these new systems, Overton is able to automate many of the traditional modeling choices, including deep learning architecture, its hyperparameters, and even which embeddings are used.Overton provides the engineer with abstractions that allow them to build, maintain, and monitor their application by manipulating data files–not custom code. Inspired by relational systems, supervision (data) is managed separately from the model (schema). Akin to traditional logical independence, Overton's schema provides model independence: serving code does not change even when inputs, parameters, or resources of the model change. The schema changes very infrequently–many production services have not updated their schema in over a year.Overton takes as input a schema whose design goal is to support rich applications from modeling to automatic deployment. In more detail, the schema has two elements: (1) data payloads similar to a relational schema, which describe the input data, and (2) model tasks, which describe the tasks that need to be accomplished. The schema defines the input, output, and coarse-grained data flow of a deep learning model. Informally, the schema defines what the model computes but not how the model computes it: Overton does not prescribe architectural details of the underlying model (e.g., Overton is free to embed sentences using an LSTM or a Transformer) or hyperparameters, like hidden state size. Additionally, sources of supervision are described as data–not in the schema–so they are free to rapidly evolve.As shown in Figure FIGREF7, given a schema and a data file, Overton is responsible to instantiate and train a model, combine supervision, select the model's hyperparameters, and produce a production-ready binary. Overton compiles the schema into a (parameterized) TensorFlow or PyTorch program, and performs an architecture and hyperparameter search. A benefit of this compilation approach is that Overton can use standard toolkits to monitor training (TensorBoard equivalents) and to meet service-level agreements (Profilers). The models and metadata are written to an S3-like data store that is accessible from the production infrastructure. This has enabled model retraining and deployment to be nearly automatic, allowing teams to ship products more quickly.In retrospect, the following three choices of Overton were the most important in meeting the above challenges.(1) Code-free Deep Learning In Overton-based systems, engineers focus exclusively on fine-grained monitoring of their application quality and improving supervision–not tweaking deep learning models. An Overton engineer does not write any deep learning code in frameworks like TensorFlow. To support application quality improvement, we use a technique, called model slicing BIBREF3. The main idea is to allow the developer to identify fine-grained subsets of the input that are important to the product, e.g., queries about nutrition or queries that require sophisticated disambiguation. The system uses developer-defined slices as a guide to increase representation capacity. Using this recently developed technique led to state-of-the-art results on natural language benchmarks including GLUE and SuperGLUE BIBREF4.(2) Multitask Learning Overton was built to natively support multitask learning BIBREF5, BIBREF6, BIBREF7 so that all model tasks are concurrently predicted. A key benefit is that Overton can accept supervision at whatever granularity (for whatever task) is available. Overton models often perform ancillary tasks like part-of-speech tagging or typing. Intuitively, if a representation has captured the semantics of a query, then it should reliably perform these ancillary tasks. Typically, ancillary tasks are also chosen either to be inexpensive to supervise. Ancillary task also allow developers to gain confidence in the model's predictions and have proved to be helpful for aids for debugging errors.(3) Weak Supervision Applications have access to supervision of varying quality and combining this contradictory and incomplete supervision is a major challenge. Overton uses techniques from Snorkel BIBREF1 and Google's Snorkel DryBell BIBREF0, which have studied how to combine supervision in theory and in software. Here, we describe two novel observations from building production applications: (1) we describe the shift to applications which are constructed almost entirely with weakly supervised data due to cost, privacy, and cold-start issues, and (2) we observe that weak supervision may obviate the need for popular methods like transfer learning from massive pretrained models, e.g., BERT BIBREF8–on some production workloads, which suggests that a deeper trade-off study may be illuminating.In summary, Overton represents a first-of-its kind machine-learning lifecycle management system that has a focus on monitoring and improving application quality. A key idea is to separate the model and data, which is enabled by a code-free approach to deep learning. Overton repurposes ideas from the database community and the machine learning community to help engineers in supporting the lifecycle of machine learning toolkits. This design is informed and refined from use in production systems for over a year in multiple machine-learned products.An Overview of Overton	To describe the components of Overton, we continue our running example of a factoid answering product. Given the textual version of a query, e.g., “how tall is the president of the united states”, the goal of the system is to appropriately render the answer to the query. The main job of an engineer is to measure and improve the quality of the system across many queries, and a key capability Overton needs to support is to measure the quality in several fine-grained ways. This quality is measured within Overton by evaluation on curated test sets, which are fastidiously maintained and improved by annotators and engineers. An engineer may be responsible for improving performance on a specific subset of the data, which they would like to monitor and improve.There are two inputs to Overton (Figure FIGREF8): The schema (Section SECREF11), which specifies the tasks, and a data file, which is the primary way an engineer refines quality (Section SECREF15). Overton then compiles these inputs into a multitask deep model (Figure FIGREF8). We describe an engineer's interaction with Overton (Section SECREF19) and discuss design decisions (Section SECREF20).An Overview of Overton ::: Overton's Schema	An Overton schema has two components: the tasks, which capture the tasks the model needs to accomplish, and payloads, which represent sources of data, such as tokens or entity embeddings. Every example in the data file conforms to this schema. Overton uses a schema both as a guide to compile a TensorFlow model and to describe its output for downstream use. Although Overton supports more types of tasks, we focus on classification tasks for simplicity. An example schema and its corresponding data file are shown in Figure FIGREF8. The schema file also provides schema information in a traditional database sense: it is used to define a memory-mapped row-store for example.A key design decision is that the schema does not contain information about hyperparameters like hidden state sizes. This enables model independence: the same schema is used in many downstream applications and even across different languages. Indeed, the same schema is shared in multiple locales and applications, only the supervision differs.An Overview of Overton ::: Overton's Schema ::: Payloads	Conceptually, Overton embeds raw data into a payload, which is then used as input to a task or to another payload. Overton supports payloads that are singletons (e.g., a query), sequences (e.g. a query tokenized into words or characters), and sets (e.g., a set of candidate entities). Overton's responsibility is to embed these payloads into tensors of the correct size, e.g., a query is embedded to some dimension $d$, while a sentence may be embedded into an array of size $m \times d$ for some length $m$. The mapping from inputs can be learned from scratch, pretrained, or fine-tuned; this allows Overton to incorporate information from a variety of different sources in a uniform way.Payloads may refer directly to a data field in a record for input, e.g., a field `tokens' contains a tokenized version of the query. Payloads may also refer to the contents of another payload. For example, a query payload may aggregate the representation of all tokens in the query. A second example is that an entity payload may refer to its corresponding span of text, e.g., the “united states of america” entity points to the span “united states” in the query. Payloads may aggregate several sources of information by referring to a combination of source data and other payloads. The payloads simply indicate dataflow, Overton learns the semantics of these references.An Overview of Overton ::: Overton's Schema ::: Tasks	Continuing our running example in Figure FIGREF8, we see four tasks that refer to three different payloads. For each payload type, Overton defines a multiclass and a bitvector classification task. In our example, we have a multiclass model for the intent task: it assigns one label for each query payload, e.g., the query is about “height”. In contrast, in the EntityType task, fine-grained types for each token are not modeled as exclusive, e.g., location and country are not exclusive. Thus, the EntityType task takes the token payloads as input, and emits a bitvector for each token as output. Overton also supports a task of selecting one out of a set, e.g., IntentArg selects one of the candidate entities. This information allows Overton to compile the inference code and the loss functions for each task and to build a serving signature, which contains detailed information of the types and can be consumed by model serving infrastructure. At the level of TensorFlow, Overton takes the embedding of the payload as input, and builds an output prediction and loss function of the appropriate type.The schema is changed infrequently, and many engineers who use Overton simply select an existing schema. Applications are customized by providing supervision in a data file that conforms to the schema, described next.An Overview of Overton ::: Weak Supervision and Slices	The second main input to Overton is the data file. It is specified as (conceptually) a single file: the file is meant to be engineer readable and queryable (say using jq), and each line is a single JSON record. For readability, we have pretty-printed a data record in Figure FIGREF8. Each payload is described in the file (but may be null).The supervision is described under each task, e.g., there are three (conflicting) sources for the Intent task. A task requires labels at the appropriate granularity (singleton, sequence, or set) and type (multiclass or bitvector). The labels are tagged by the source that produced them: these labels may be incomplete and even contradictory. Overton models the sources of these labels, which may come human annotators, or from engineer-defined heuristics such as data augmentation or heuristic labelers. Overton learns the accuracy of these sources using ideas from the Snorkel project BIBREF1. In particular, it estimates the accuracy of these sources and then uses these accuracies to compute a probability that each training point is correct BIBREF9. Overton incorporates this information into the loss function for a task; this also allows Overton to automatically handle common issues like rebalancing classes.An Overview of Overton ::: Weak Supervision and Slices ::: Monitoring	For monitoring, Overton allows engineers to provide user-defined tags that are associated with individual data points. The system additionally defines default tags including train, test, dev to define the portion of the data that should be used for training, testing, and development. Engineers are free to define their own subsets of data via tags, e.g,. the date supervision was introduced, or by what method. Overton allows report per-tag monitoring, such as the accuracy, precision and recall, or confusion matrices, as appropriate. These tags are stored in a format that is compatible with Pandas. As a result, engineers can load these tags and the underlying examples into other downstream analysis tools for further analytics.An Overview of Overton ::: Weak Supervision and Slices ::: Slicing	In addition to tags, Overton defines a mechanism called slicing, that allows monitoring but also adds representational capacity to the model. An engineer defines a slice by tagging a subset of the data and indicating that this tag is also a slice. Engineers typically define slices that consist of a subset that is particular relevant for their job. For example, they may define a slice because it contains related content, e.g., “nutrition-related queries” or because the subset has an interesting product feature, e.g., “queries with complex disambiguation”. The engineer interacts with Overton by identifying these slices, and providing supervision for examples in those slices. Overton reports the accuracy conditioned on an example being in the slice. The main job of the engineer is to diagnose what kind of supervision would improve a slice, and refine the labels in that slice by correcting labels or adding in new labels.A slice also indicates to Overton that it should increase its representation capacity (slightly) to learn a “per slice” representation for a task. In this sense, a slice is akin to defining a “micro-task” that performs the task just on the subset defined by the slice. Intuitively, this slice should be able to better predict as the data in a slice typically has less variability than the overall data. At inference time, Overton makes only one prediction per task, and so the first challenge is that Overton needs to combine these overlapping slice-specific predictions into a single prediction. A second challenge is that slices heuristically (and so imperfectly) define subsets of data. To improve the coverage of these slices, Overton learns a representation of when one is “in the slice” which allows a slice to generalize to new examples. Per-slice performance is often valuable to an engineer, even if it does not improve the overall quality, since their job is to improve and monitor a particular slice. A production system improved its performance on a slice of complex but rare disambiguations by over 50 points of F1 using the same training data.An Overview of Overton ::: A Day in the Life of an Overton Engineer	To help the reader understand the process of an engineer, we describe two common use cases: improving an existing feature, and the cold-start case. Overton's key ideas are changing where developers spend their time in this process.An Overview of Overton ::: A Day in the Life of an Overton Engineer ::: Improving an Existing Feature	A first common use case is that an engineer wants to improve the performance of an existing feature in their application. The developer iteratively examines logs of the existing application. To support this use case, there are downstream tools that allow one to quickly define and iterate on subsets of data. Engineers may identify areas of the data that require more supervision from annotators, conflicting information in the existing training set, or the need to create new examples through weak supervision or data augmentation. Over time, systems have grown on top of Overton that support each of these operations with a more convenient UI. An engineer using Overton may simply work entirely in these UIs.An Overview of Overton ::: A Day in the Life of an Overton Engineer ::: Cold-start Use Case	A second common use case is the cold-start use case. In this case, a developer wants to launch a new product feature. Here, there is no existing data, and they may need to develop synthetic data. In both cases, the identification and creation of the subset is done by tools outside of Overton. These subsets become the aforementioned slices, and the different mechanisms are identified as different sources. Overton supports this process by allowing engineers to tag the lineage of these newly created queries, measure their quality in a fine-grained way, and merge data sources of different quality.In previous iterations, engineers would modify loss functions by hand or create new separate models for each case. Overton engineers spend no time on these activities.An Overview of Overton ::: Major Design Decisions and Lessons	We briefly cover some of the design decisions in Overton.An Overview of Overton ::: Major Design Decisions and Lessons ::: Design for Weakly Supervised Code	As described, weakly supervised machine learning is often the dominant source of supervision in many machine learning products. Overton uses ideas from Snorkel BIBREF1 and Google's Snorkel Drybell BIBREF0 to model the quality of the supervision. The design is simple: lineage is tracked for each source of information. There are production systems with no traditional supervised training data (but they do have such data for validation). This is important in privacy-conscious applications.An Overview of Overton ::: Major Design Decisions and Lessons ::: Modeling to Deployment	In many production teams, a deployment team is distinct from the modeling team, and the deployment team tunes models for production. However, we noticed quality regressions as deployment teams have an incomplete view of the potential modeling tradeoffs. Thus, Overton was built to construct a deployable production model. The runtime performance of the model is potentially suboptimal, but it is well within production SLAs. By encompassing more of the process, Overton has allowed faster model turn-around times.An Overview of Overton ::: Major Design Decisions and Lessons ::: Use Standard Tools for the ML Workflow	Overton compiles the schema into (many versions of) TensorFlow, CoreML, or PyTorch. Whenever possible, Overton uses a standard toolchain. Using standard tools, Overton supports distributed training, hyperparameter tuning, and building servable models. One unanticipated benefit of having both backends was that different resources are often available more conveniently on different platforms. For example, to experiment with pretrained models, the Huggingface repository BIBREF10 allows quick experimentation–but only in PyTorch. The TensorFlow production tools are unmatched. The PyTorch execution mode also allows REPL and in-Jupyter-notebook debugging, which engineers use to repurpose elements, e.g., query similarity features. Even if a team uses a single runtime, different runtime services will inevitably use different versions of that runtime, and Overton insulates the modeling teams from the underlying changes in production serving infrastructure.An Overview of Overton ::: Major Design Decisions and Lessons ::: Model Independence and Zero-code Deep Learning	A major design choice at the outset of the project was that domain engineers should not be forced to write traditional deep learning modeling code. Two years ago, this was a contentious decision as the zeitgeist was that new models were frequently published, and this choice would hamstring the developers. However, as the pace of new model building blocks has slowed, domain engineers no longer feel the need to fine-tune individual components at the level of TensorFlow. Ludwig has taken this approach and garnered adoption. Although developed separately, Overton's schema looks very similar to Ludwig's programs and from conversations with the developers, shared similar motivations. Ludwig, however, focused on the one-off model building process not the management of the model lifecycle. Overton itself only supports text processing, but we are prototyping image, video, and multimodal applications.An Overview of Overton ::: Major Design Decisions and Lessons ::: Engineers are Comfortable with Automatic Hyperparameter Tuning	Hyperparameter tuning is conceptually important as it allows Overton to avoid specifying parameters in the schema for the model builder. Engineers are comfortable with automatic tuning, and first versions of all Overton systems are tuned using standard approaches. Of course, engineers may override the search: Overton is used to produce servable models, and so due to SLAs, production models often pin certain key parameters to avoid tail performance regressions.An Overview of Overton ::: Major Design Decisions and Lessons ::: Make it easy to manage ancillary data products	Overton is also used to produce back-end data products (e.g., updated word or multitask embeddings) and multiple versions of the same model. Inspired by HuggingFace BIBREF10, Overton tries to make it easy to drop in new pretrained embeddings as they arrive: they are simply loaded as payloads. Teams use multiple models to train a “large” and a “small” model on the same data. The large model is often used to populate caches and do error analysis, while the small model must meet SLA requirements. Overton makes it easy to keep these two models synchronized. Additionally, some data products can be expensive to produce (on the order of ten days), which means they are refreshed less frequently than the overall product. Overton does not have support for model versioning, which is likely a design oversight.Evaluation	We elaborate on three items: (1) we describe how Overton improves production systems; (2) we report on the use of weak supervision in these systems; and (3) we discuss our experience with pretraining.Evaluation ::: Overton Usage	Overton has powered industry-grade systems for more than a year. Figure FIGREF23 shows the end-to-end reduction in error of these systems: a high-resource system with tens of engineers, a large budget, and large existing training sets, and three other products with smaller teams. Overton enables a small team to perform the same duties that would traditionally be done by several, larger teams. Here, multitask learning is critical: the combined system reduces error and improves product turn-around times. Systems that Overton models replace are typically deep models and heuristics that are challenging to maintain, in our estimation because there is no model independence.Evaluation ::: Usage of Weak Supervision	Weak supervision is the dominant form of supervision in all applications. Even annotator labels (when used) are filtered and altered by privacy and programmatic quality control steps. Note that validation is still done manually, but this requires orders of magnitude less data than training.Figure FIGREF24a shows the impact of weak supervision on quality versus weak supervision scale. We downsample the training data and measure the test quality (F1 and accuracy) on 3 representative tasks: singleton, sequence, and set. For each task, we use the 1x data's model as the baseline and plot the relative quality as a percentage of the baseline; e.g., if the baseline F1 is 0.8 and the subject F1 is 0.9, the relative quality is $0.9/0.8=1.125$. In Figure FIGREF24a, we see that increasing the amount of supervision consistently results in improved quality across all tasks. Going from 30K examples or so (1x) to 1M examples (32x) leads to a 12%+ bump in two tasks and a 5% bump in one task.Evaluation ::: Pre-trained Models and Weak Supervision	A major trend in the NLP community is to pre-train a large and complex language model using raw text and then fine-tune it for specific tasks BIBREF8. One can easily integrate such pre-trained models in Overton, and we were excited by our early results. Of course, at some point, training data related to the task is more important than massive pretraining. We wondered how weak supervision and pretrained models would interact. Practically, these pretrained models like BERT take large amounts of memory and are much slower than standard word embeddings. Nevertheless, motivated by such models' stellar performance on several recent NLP benchmarks such as GLUE BIBREF4, we evaluate their impact on production tasks that are weakly supervised. For each of the aforementioned training set sizes, we train two models: without-BERT: production model with standard word embeddings but without BERT, and with-BERT: production model with fine tuning on the “BERT-Large, Uncased” pretrained model BIBREF8.For each training set, we calculate the relative test quality change (percentage change in F1 or accuracy) of with-BERT over without-BERT. In Figure FIGREF24b, almost all percentage changes are within a narrow 2% band of no-change (i.e., 100%). This suggests that sometimes pre-trained language models have a limited impact on downstream tasks–when weak supervision is used. Pretrained models do have higher quality at smaller training dataset sizes–the Set task here shows an improvement at small scale, but this advantage vanishes at larger (weak) training set sizes in these workloads. This highlights a potentially interesting set of tradeoffs among weak supervision, pretraining, and the complexity of models.Related Work	Overton builds on work in model life-cycle management, weak supervision, software for ML, and zero-code deep learning.Related Work ::: Model Management	A host of recent data systems help manage the model process, including MLFlow, which helps with the model lifecycle and reporting BIBREF11, ModelDB BIBREF12, and more. Please see excellent tutorials such as Kumar et al. BIBREF13. However, these systems are complementary and do not focus on Overton's three design points: fine-grained monitoring, diagnosing the workflow of updating supervision, and the production programming lifecycle. This paper reports on some key lessons learned from productionizing related ideas.Related Work ::: Weak Supervision	A myriad of weak supervision techniques have been used over the last few decades of machine learning, notably external knowledge bases BIBREF14, BIBREF15, BIBREF16, BIBREF17, heuristic patterns BIBREF18, BIBREF19, feature annotations BIBREF20, BIBREF21, and noisy crowd labels BIBREF22, BIBREF23. Data augmentation is another major source of training data. One promising approach is to learn augmentation policies, first described in Ratner et al. BIBREF24, which can further automate this process. Google's AutoAugment BIBREF25 used learned augmentation policies to set new state-of-the-art performance results in a variety of domains, which has been a tremendously exciting direction. The goal of systems like Snorkel is to unify and extend these techniques to create and manipulate training data. These have recently garnered usage at major companies, notably Snorkel DryBell at Google BIBREF0. Overton is inspired by this work and takes the next natural step toward supervision management.Related Work ::: Software Productivity for ML Software	The last few years have seen an unbelievable amount of change in the machine learning software landscape. TensorFlow, PyTorch, CoreML and MXNet have changed the way people write machine learning code to build models. Increasingly, there is a trend toward higher level interfaces. The pioneering work on higher level domain specific languages like Keras began in this direction. Popular libraries like Fast.ai, which created a set of libraries and training materials, have dramatically improved engineer productivity. These resources have made it easier to build models but equally important to train model developers. Enabled in part by this trend, Overton takes a different stance: model development is in some cases not the key to product success. Given a fixed budget of time to run a long-lived ML model, Overton is based on the idea that success or failure depends on engineers being able to iterate quickly and maintain the supervision–not change the model. Paraphrasing the classical relational database management mantra, Overton focuses on what the user wants–not how to get it.Related Work ::: Zero-code Deep Learning	The ideas above led naturally to what we now recognize as zero-code deep learning, a term we borrow from Ludwig. It is directly related to previous work on multitask learning as a key building block of software development BIBREF26 and inspired by Software 2.0 ideas articulated by Karpathy. The world of software engineering for machine learning is fascinating and nascent. In this spirit, Uber's Ludwig shares a great deal with Overton's design. Ludwig is very sophisticated and has supported complex tasks on vision and others. These methods were controversial two years ago, but seem to be gaining acceptance among production engineers. For us, these ideas began as an extension of joint inference and learning in DeepDive BIBREF27.Related Work ::: Network Architecture Search	Zero-code deep learning in Overton is enabled by some amount of architecture search. It should be noted that Ludwig made a different choice: no search is required, and so zero-code deep learning does not depend on search. The area of Neural Architecture Search (NAS) BIBREF28 is booming: the goal of this area is to perform search (typically reinforcement learning but also increasingly random search BIBREF29). This has led to exciting architectures like EfficientNet BIBREF30. This is a tremendously exciting area with regular workshops at all major machine learning conferences. Overton is inspired by this area. On a technical level, the search used in Overton is a coarser-grained search than what is typically done in NAS. In particular, Overton searches over relatively limited large blocks, e.g., should we use an LSTM or CNN, not at a fine-grained level of connections. In preliminary experiments, NAS methods seemed to have diminishing returns and be quite expensive. More sophisticated search could only improve Overton, and we are excited to continue to apply advances in this area to Overton. Speed of developer iteration and the ability to ship production models seems was a higher priority than exploring fine details of architecture in Overton.Related Work ::: Statistical Relational Learning	Overton's use of a relational schema to abstract statistical reasoning is inspired by Statistical Relational Learning (SRL), such as Markov Logic BIBREF31. DeepDive BIBREF27, which is based on Markov Logic, allows one to wrap deep learning as relational predicates, which could then be composed. This inspired Overton's design of compositional payloads. In the terminology of SRL BIBREF32, Overton takes a knowledge compilation approach (Overton does not have a distinct querying phase). Supporting more complex, application-level constraints seems ideally suited to an SRL approach, and is future work for Overton.Conclusion and Future Work	This paper presented Overton, a system to help engineers manage the lifecycle of production machine learning systems. A key idea is to use a schema to separate the model from the supervision data, which allows developers to focus on supervision as their primary interaction method. A major direction of on-going work are the systems that build on Overton to aid in managing data augmentation, programmatic supervision, and collaboration.Acknowledgments This work was made possible by Pablo Mendes, Seb Dery, and many others. We thank many teams in Siri Search, Knowledge, and Platform and Turi for support and feedback. We thank Mike Cafarella, Arun Kumar, Monica Lam, Megan Leszczynski, Avner May, Alex Ratner, Paroma Varma, Ming-Chuan Wu, Sen Wu, and Steve Young for feedback.","['How does Overton handles contradictory or incomplete supervision data?', 'What are high level declarative abstractions Overton provides?', 'How are applications presented in Overton?', 'Does Overton support customizing deep learning models without writing any code?']","['Unacceptable', 'Unacceptable', 'Unacceptable', 'Unacceptable']"
73,"Represent, Aggregate, and Constrain: A Novel Architecture for Machine Reading from Noisy Sources	In order to extract event information from text, a machine reading model must learn to accurately read and interpret the ways in which that information is expressed. But it must also, as the human reader must, aggregate numerous individual value hypotheses into a single coherent global analysis, applying global constraints which reflect prior knowledge of the domain. In this work we focus on the task of extracting plane crash event information from clusters of related news articles whose labels are derived via distant supervision. Unlike previous machine reading work, we assume that while most target values will occur frequently in most clusters, they may also be missing or incorrect. We introduce a novel neural architecture to explicitly model the noisy nature of the data and to deal with these aforementioned learning issues. Our models are trained end-to-end and achieve an improvement of more than 12.1 F$_1$ over previous work, despite using far less linguistic annotation. We apply factor graph constraints to promote more coherent event analyses, with belief propagation inference formulated within the transitions of a recurrent neural network. We show this technique additionally improves maximum F$_1$ by up to 2.8 points, resulting in a relative improvement of $50\%$ over the previous state-of-the-art.	Introduction	Recent work in the area of machine reading has focused on learning in a scenario with perfect information. Whether identifying target entities for simple cloze style queries BIBREF0 , BIBREF1 , or reasoning over short passages of artificially generated text BIBREF2 , short stories BIBREF3 , or children's stories BIBREF4 , these systems all assume that the corresponding text is the unique source of information necessary for answering the query – one that not only contains the answer, but does not contain misleading or otherwise contradictory information.For more practical question answering, where an information retrieval (IR) component must first fetch the set of relevant passages, the text sources will be less reliable and this assumption must be discarded. Text sources may vary in terms of their integrity (whether or not they are intentionally misleading or unreliable), their accuracy (as in the case of news events, where a truthful but outdated article may contain incorrect information), or their relevance to the query. These characteristics necessitate not only the creation of high-precision readers, but also the development of effective strategies for aggregating conflicting stories into a single cohesive account of the event.Additionally, while many question answering systems are designed to extract a single answer to a single query, a user may wish to understand many aspects of a single entity or event. In machine reading, this is akin to pairing each text passage with multiple queries. Modeling each query as an independent prediction can lead to analyses that are incoherent, motivating the need to model the dependencies between queries.We study these problems through the development of a novel machine reading architecture, which we apply to the task of event extraction from news cluster data. We propose a modular architecture which decomposes the task into three fundamental sub-problems: (1) representation INLINEFORM0 scoring, (2) aggregation, and (3) global constraints. Each corresponds to an exchangeable component of our model. We explore a number of choices for these components, with our best configuration improving performance by INLINEFORM1 F INLINEFORM2 , a INLINEFORM3 relative improvement, over the previous state-of-the-art.The Case for Aggregation	Effective aggregation techniques can be crucial for identifying accurate information from noisy sources. Figure FIGREF1 depicts an example of our problem scenario. An IR component fetches several documents based on the query, and sample sentences are shown for each document. The goal is to extract the correct value, of which there may be many mentions in the text, for each slot. Sentences in INLINEFORM0 express a target slot, the number of fatalities, but the mention corresponds to an incorrect value. This is a common mistake in early news reports. Documents INLINEFORM1 and INLINEFORM2 also express this slot, and with mentions of the correct value, but with less certainty.A model which focuses on a single high-scoring mention, at the expense of breadth, will make an incorrect prediction. In comparison, a model which learns to correctly accumulate evidence for each value across multiple mentions over the entire cluster can identify the correct information, circumventing this problem. Figure FIGREF1 (bottom) shows how this pooling of evidence can produce the correct cluster-level prediction.Model	In this section we describe the three modeling components of our proposed architecture:We begin by defining terminology. A news cluster INLINEFORM0 is a set of documents, INLINEFORM1 , where each document is described by a sequence of words, INLINEFORM2 . A mention is an occurrence of a value in its textual context. For each value INLINEFORM3 , there are potentially many mentions of INLINEFORM4 in the cluster, defined as INLINEFORM5 . These have been annotated in the data using Stanford CoreNLP BIBREF5 .Representations and Scoring	For each mention INLINEFORM0 we construct a representation INLINEFORM1 of the mention in its context. This representation functions as a general “reading” or encoding of the mention, irrespective of the type of slots for which it will later be considered. This differs from some previous machine reading research where the model provides a query-specific reading of the document, or reads the document multiple times when answering a single query BIBREF0 . As in previous work, an embedding of a mention's context serves as its representation. We construct an embedding matrix INLINEFORM2 , using pre-trained word embeddings, where INLINEFORM3 is the dimensionality of the embeddings and INLINEFORM4 the number of words in the cluster. These are held fixed during training. All mentions are masked and receive the same one-hot vector in place of a pretrained embedding. From this matrix we embed the context using a two-layer convolutional neural network (CNN), with a detailed discussion of the architecture parameters provided in Section SECREF4 . CNNs have been used in a similar manner for a number of information extraction and classification tasks BIBREF6 , BIBREF7 and are capable of producing rich sentence representations BIBREF8 .Having produced a representation INLINEFORM0 for each mention INLINEFORM1 , a slot-specific attention mechanism produces INLINEFORM2 , representing the compatibility of mention INLINEFORM3 with slot INLINEFORM4 . Let INLINEFORM5 be the representation matrix composed of all INLINEFORM6 , and INLINEFORM7 is the index of INLINEFORM8 into INLINEFORM9 . We create a separate embedding, INLINEFORM10 , for each slot INLINEFORM11 , and utilize it to attend over all mentions in the cluster as follows: DISPLAYFORM0  The mention-level scores reflect an interpretation of the value's encoding with respect to the slot. The softmax normalizes the scores over each slot, supplying the attention, and creating competition between mentions. This encourages the model to attend over mentions with the most characteristic contexts for each slot.Aggregating Mention-level Scores	For values mentioned repeatedly throughout the news cluster, mention scores must be aggregated to produce a single value-level score. In this section we describe (1) how the right aggregation method can better reflect how the gold labels are applied to the data, (2) how domain knowledge can be incorporated into aggregation, and (3) how aggregation can be used as a dynamic approach to identifying missing information.In the traditional view of distant supervision BIBREF9 , if a mention is found in an external knowledge base it is assumed that the mention is an expression of its role in the knowledge base, and it receives the corresponding label. This assumption does not always hold, and the resulting spurious labels are frequently cited as a source of training noise BIBREF10 , BIBREF11 . However, an aggregation over all mention scores provides a more accurate reflection of how distant supervision labels are applied to the data.If one were to assign a label to each mention and construct a loss using the mention-level scores ( INLINEFORM0 ) directly, it would recreate the hard labeling of the traditional distant supervision training scenario. Instead, we relax the distant supervision assumption by using a loss on the value-level scores ( INLINEFORM1 ), with aggregation to pool beliefs from one to the other. This explicitly models the way in which cluster-wide labels are applied to mentions, and allows for spuriously labeled mentions to receive lower scores, “explaining away” the cluster's label by assigning a higher score to a mention with a more suitable representation.Two natural choices for this aggregation are max and sum. Formally, under max aggregation the value-level scores for a value INLINEFORM0 and slot INLINEFORM1 are computed as: DISPLAYFORM0 And under sum aggregation: DISPLAYFORM0 If the most clearly expressed mentions correspond to correct values, max aggregation can be an effective strategy BIBREF10 . If the data set is noisy with numerous spurious mentions, a sum aggregation favoring values which are expressed both clearly and frequently may be the more appropriate choice.The aforementioned aggregation methods combine mention-level scores uniformly, but for many domains one may have prior knowledge regarding which mentions should more heavily contribute to the aggregate score. It is straightforward to augment the proposed aggregation methods with a separate weight INLINEFORM0 for each mention INLINEFORM1 to create, for instance, a weighted sum aggregation: DISPLAYFORM0 These weights may be learned from data, or they may be heuristically defined based on a priori beliefs. Here we present two such heuristic methods.News articles naturally deviate from the topical event, often including comparisons to related events, and summaries of past incidents. Any such instance introduces additional noise into the system, as the contexts of topical and nontopical mention are often similar. Weighted aggregation provides a natural foothold for incorporating topicality into the model.We assign aggregation weights heuristically with respect to a simple model of discourse. We assume every document begins on topic, and remains so until a sentence mentions a nontopical flight number. This and all successive sentences are considered nontopical, until a sentence reintroduces the topical flight. Mentions in topical sentences receive aggregation weights of INLINEFORM0 , and those in non-topical sentences receive weights of INLINEFORM1 , removing them from consideration completely.In the aftermath of a momentous event, news outlets scramble to release articles, often at the expense of providing accurate information.We hypothesize that the earliest articles in each cluster are the most likely to contain misinformation, which we explore via a measure of information content. We define the information content of an article as the number of correct values which it mentions. Using this measure, we fit a skewed Gaussian distribution over the ordered news articles, assigning INLINEFORM0 , where INLINEFORM1 is the smoothed information content of INLINEFORM2 as drawn from the Gaussian.A difficult machine reading problem unique to noisy text sources, where the correct values may not be present in the cluster, is determining whether to predict any value at all. A common solution for handling such missing values is the use of a threshold, below which the model returns null. However, even a separate threshold for each slot would not fully capture the nature of the problem.Determining whether a value is missing is a trade-off between two factors: (1) how strongly the mention-level scores support a non-null answer, and (2) how much general information regarding that event and that slot is given. We incorporate both factors by extending the definition of INLINEFORM0 and its use in Eq. EQREF9 –Eq. to include not only mentions, but all words. Each non-mention word is treated as a mention of the null value: DISPLAYFORM0 where INLINEFORM0 is the set of mentions. The resulting null score varies according to both the cluster size and its content. Smaller clusters with fewer documents require less evidence to predict a non-null value, while larger clusters must accumulate more evidence for a particular candidate or a null value will be proposed instead.The exact words contained in the cluster also have an effect. For example, clusters with numerous mentions of killed, died, dead, will have a higher INLINEFORM0 Fatalities INLINEFORM1 , requiring the model to be more confident in its answer for that slot during training. Additionally, this provides a mechanism for driving down INLINEFORM2 when INLINEFORM3 is not strongly associated with INLINEFORM4 .Global Constraints	While the combination of learned representations and aggregation produces an effective system in its own right, its predictions may reflect a lack of world knowledge. For instance, we may want to discourage the model from predicting the same value for multiple slots, as this is not a common occurrence.Following recent work in computer vision which proposes a differentiable interpretation of belief propagation inference BIBREF12 , BIBREF13 , we present a recurrent neural network (RNN) which implements inference under this constraint.A factor graph is a graphical model which factorizes the model function using a bipartite graph, consisting of variables and factors. Variables maintain beliefs over their values, and factors specify scores over configurations of these values for the variables they neighbor.We constrain model output by applying a factor graph model to the INLINEFORM0 scores it produces. The slot INLINEFORM1 taking the value INLINEFORM2 is represented in the factor graph by a Boolean variable INLINEFORM3 . Each INLINEFORM4 is connected to a local factor INLINEFORM5 whose initial potential is derived from INLINEFORM6 . A combinatorial logic factor, Exactly-1 BIBREF14 , is (1) created for each slot, connected across all values, and (2) created for each value, connected across all slots. This is illustrated in Figure FIGREF22 . Each Exactly-1 factor provides a hard constraint over neighboring Boolean variables requiring exactly one variable's value to be true, therefore diminishing the possibility of duplicate predictions during inference.The resulting graph contains cycles, preventing the use of exact message passing inference. We instead treat an RNN as implementing loopy belief propagation (LBP), an iterative approximate message passing inference algorithm. The hidden state of the RNN is the set of variable beliefs, and each round of message updates corresponds to one iteration of LBP, or one recurrence in the RNN.There are two types of messages: messages from variables to factors and messages from factors to variables. The message that a variable INLINEFORM0 sends to a factor INLINEFORM1 (denoted INLINEFORM2 ) is defined recursively w.r.t. to incoming messages from its neighbors INLINEFORM3 as follows: DISPLAYFORM0 and conveys the information “My other neighbors jointly suggest I have the posterior distribution INLINEFORM0 over my values.” In our RNN formulation of message passing the initial outgoing message for a variable INLINEFORM1 to its neighboring Exactly-1 factors is: DISPLAYFORM0 where the sigmoid moves the scores into probability space.A message from an Exactly-1 factor to its neighboring variables is calculated as: INLINEFORM0 All subsequent LBP iterations compute variable messages as in Eq. EQREF24 , incorporating the out-going factor beliefs of the previous iteration.Data	The Stanford Plane Crash Dataset BIBREF15 is a small data set consisting of 80 plane crash events, each paired with a set of related news articles. Of these events, 40 are reserved for training, and 40 for testing, with the average cluster containing more than 2,000 mentions. Gold labels for each cluster are derived from Wikipedia infoboxes and cover up to 15 slots, of which 8 are used in evaluation (Figure TABREF54 ).We follow the same entity normalization procedure as reschke2014, limit the cluster size to the first 200 documents, and further reduce the number of duplicate documents to prevent biases in aggregation. We partition out every fifth document from the training set to be used as development data, primarily for use in an early stopping criterion. We also construct additional clusters from the remaining training documents, and use this to increase the size of the development set.Experiments	In all experiments we train using adaptive online gradient updates (Adam, see kingma2014). Model architecture and parameter values were tuned on the development set, and are as follows (chosen values in bold):The number of training epochs is determined via early stopping with respect to the model performance on development data. The pre-trained word embeddings are 200-dimensional GLoVe embeddings BIBREF16 .Systems	We evaluate on four categories of architecture:reschke2014 proposed several methods for event extraction in this scenario. We compare against three notable examples drawn from this work:[leftmargin=*]Reschke CRF: a conditional random field model.Reschke Noisy-OR: a sequence tagger with a ""Noisy-OR"" form of aggregation that discourages the model from predicting the same value for multiple slots.Reschke Best: a sequence tagger using a cost-sensitive classifier, optimized with SEARN BIBREF17 , a learning-to-search framework.Each of these models uses features drawn from dependency trees, local context (unigram/part-of-speech features for up to 5 neighboring words), sentence context (bag-of-word/part-of-speech), words/part-of-speech of words occurring within the value, as well as the entity type of the mention itself.The representation and scoring components of our architecture, with an additional slot for predicting a null value. The INLINEFORM0 scores are used when constructing the loss and during decoding. These scores can also be aggregated in a max/sum manner after decoding, but such aggregation is not incorporated during training.Representation, scoring, and aggregation components, trained end-to-end with a cluster-level loss. Null values are predicted as described in Sec. UID18 .kadlec2016 present AS Reader, a state-of-the-art model for cloze-style QA. Like our architecture, AS Reader aggregates mention-level scores, pooling evidence for each answer candidate. However, in cloze-style QA an entity is often mentioned in complementary contexts throughout the text, but are frequently in similar contexts in news cluster extraction.We tailor AS Reader to event extraction to illustrate the importance of choosing an aggregation which reflects how the gold labels are applied to the data. EE-AS Reader is implemented by applying Eq. EQREF9 and Eq. to each document, as opposed to clusters, as documents are analogous to sentences in the cloze-style QA task. We then concatenate the resulting vectors, and apply sum aggregation as before.Evaluation	We evaluate configurations of our proposed architecture across three measures. The first is a modified version of standard precision, recall, and F INLINEFORM0 , as proposed by reschke2014. It deviates from the standard protocol by (1) awarding full recall for any slot when a single predicted value is contained in the gold slot, (2) only penalizing slots for which there are findable gold values in the text, and (3) limiting candidate values to the set of entities proposed by the Stanford NER system and included in the data set release. Eight of the fifteen slots are used in evaluation. Similarly, the second evaluation measure we present is standard precision, recall, and F INLINEFORM1 , specifically for null values.We also evaluate the models using mean reciprocal rank (MRR). When calculating the F INLINEFORM0 -based evaluation measure we decode the model by taking the single highest-scoring value for each slot. However, this does not necessarily reflect the quality of the overall value ranking produced by the model. For this reason we include MRR, defined as: DISPLAYFORM0 where rank INLINEFORM0 is the rank position of the first correct value for a given cluster and slot pair INLINEFORM1 , and INLINEFORM2 , the number of such pairs, is INLINEFORM3 , the product of the total number of clusters with the total number of predicted slots.Results	Results are presented in Table TABREF44 . In comparison to previous work, our any configuration of our RAC architecture with sum-based aggregation outperforms the best existing systems by a minimum of 9.8 F INLINEFORM0 . In comparison to the various Mention-CNN systems, it is clear that this improvement is not a result of different features or the use of pre-trained word embeddings, or even the representational power of the CNN-based embeddings. Rather, it is attributable to training end-to-end with aggregation and a cluster-level loss function.With respect to aggregation, sum-based methods consistently outperform their max counterparts, indicating that exploiting the redundancy of information in news clusters is beneficial to the task. The topic-based aggregation is statistically significant improvement over standard sum aggregation (p INLINEFORM0 ), and produces the highest performing unconstrained system.Date-based aggregation did not yield a statistically significant improvement over sum aggregation. We hypothesize that the method is sound, but accurate datelines could only be extracted for 31 INLINEFORM0 documents. We did not modify the aggregation weights ( INLINEFORM1 ) for the remaining documents, minimizing the effect of this approach.The EE-AS Reader has the lowest overall performance, which one can attribute to pooling evidence in a manner that is poorly suited to this problem domain. By placing a softmax over each document's beliefs, what is an advantage in the cloze-style QA setting here becomes a liability: the model is forced to predict a value for every slot, for every each document, even when few are truly mentioned.Effects of Global Constraints	In Table TABREF50 we show the results of incorporating factor graph constraints into our best-performing system. Performing one iteration of LBP inference produces our highest performance, an F INLINEFORM0 of 44.9. This is 14.9 points higher than Reschke's best system, and a statistically significant improvement over the unconstrained model (p INLINEFORM1 ). The improvements persist throughout training, as shown in Figure FIGREF52 .Additional iterations reduce performance. This effect is largely due to the constraint assumption not holding absolutely in the data. For instance, multiple slots can have the null value, and zero is common value for a number of slots. Running the constraint inference for a single iteration encourages a 1-to-1 mapping from values to slots, but it does not prohibit it. This result also implies that a hard heuristic decoding constraint time would not be as effective.Error Analysis	We randomly selected 15 development set instances which our best model predicts incorrectly. Of these, we find three were incorrectly labeled in the gold data as errors from the distance supervision hypothesis (i.e., “zero chance” being labeled for 0 survivors, when the number of survivors was not mentioned in the cluster), and should not be predicted. Six were clearly expressed and should be predictable, with highly correlated keywords present in the context window, but were assigned low scores by the model. We belief a richer representation which combines the generalization of CNNs with the discrete signal of n-gram features BIBREF18 may solve some of these issues.Four of the remaining errors appear to be due to aggregation errors. Specifically, the occurrence of a particular punctuation mark with far greater than average frequency resulted in it being predicted for three slots. While these could be filtered out, a more general solution may be to build a representation based on the actual mention (“Ryanair”), in addition to its context. This may reduce the scores of these mentions to such an extent that they are removed from consideration.Table TABREF54 shows the accuracy of the model on each slot type. The model is struggles with predicting the Injuries and Survivors slots. The nature of news media leads these slots to be discussed less frequently, with their mentions often embedded more deeply in the document, or expressed textually. For instance, it is common to express INLINEFORM0 =Survivors, INLINEFORM1 as “no survivors”, but it is impossible to predict a 0 value in this case, under the current problem definition.Connections to Pointer Networks	A pointer network uses a softmax to normalize a vector the size of the input, to create an output distribution over the dictionary of inputs BIBREF23 . This assumes that the input vector is the size of the dictionary, and that each occurrence is scored independently of others. If an element appears repeatedly throughout the input, each occurrence is in competition not only with other elements, but also with its duplicates.Here the scoring and aggregation steps of our proposed architecture can together be viewed as a pointer network where there is a redundancy in the input which respects an underlying grouping. Here the softmax normalizes the scores over the input vector, and the aggregation step again yields an output distribution over the dictionary of the input.Conclusion and Future Work	In this work we present a machine reading architecture designed to effectively read collections of documents in noisy, less controlled scenarios where information may be missing or inaccurate. Through attention-based mention scoring, cluster-wide aggregation of these scores, and global constraints to discourage unlikely solutions, we improve upon the state-of-the-art on this task by 14.9 F INLINEFORM0 .In future work, the groundwork laid here may be applied to larger data sets, and may help motivate the development of such data. Larger noisy data sets would enable the differentiable constraints and weighted aggregation to be included during the optimization, and tuned with respect to data. In addition, we find the incorporation of graphical model inference into neural architectures to be a powerful new tool, and potentially an important step towards incorporating higher-level reasoning and prior knowledge into neural models of NLP.","['what metrics are used to evaluate the models?', 'what are the baselines?', 'what is the size of the dataset?', 'what dataset did they use?']","['Unacceptable', 'Unacceptable', 'Unacceptable', 'Event dataset with news articles']"
74,"An Annotated Corpus for Machine Reading of Instructions in Wet Lab Protocols	We describe an effort to annotate a corpus of natural language instructions consisting of 622 wet lab protocols to facilitate automatic or semi-automatic conversion of protocols into a machine-readable format and benefit biological research. Experimental results demonstrate the utility of our corpus for developing machine learning approaches to shallow semantic parsing of instructional texts. We make our annotated Wet Lab Protocol Corpus available to the research community.	Introduction	As the complexity of biological experiments increases, there is a growing need to automate wet laboratory procedures to avoid mistakes due to human error and also to enhance the reproducibility of experimental biological research BIBREF0 . Several efforts are currently underway to define machine-readable formats for writing wet lab protocols BIBREF1 , BIBREF2 , BIBREF3 . The vast majority of today's protocols, however, are written in natural language with jargon and colloquial language constructs that emerge as a byproduct of ad-hoc protocol documentation. This motivates the need for machine reading systems that can interpret the meaning of these natural language instructions, to enhance reproducibility via semantic protocols (e.g. the Aquarium project) and enable robotic automation BIBREF4 by mapping natural language instructions to executable actions.In this study we take a first step towards this goal by annotating a database of wet lab protocols with semantic actions and their arguments; and conducting initial experiments to demonstrate its utility for machine learning approaches to shallow semantic parsing of natural language instructions. To the best of our knowledge, this is the first annotated corpus of natural language instructions in the biomedical domain that is large enough to enable machine learning approaches.There have been many recent data collection and annotation efforts that have initiated natural language processing research in new directions, for example political framing BIBREF5 , question answering BIBREF6 and cooking recipes BIBREF7 . Although mapping natural language instructions to machine readable representations is an important direction with many practical applications, we believe current research in this area is hampered by the lack of available annotated corpora. Our annotated corpus of wet lab protocols could enable further research on interpreting natural language instructions, with practical applications in biology and life sciences.Prior work has explored the problem of learning to map natural language instructions to actions, often learning through indirect supervision to address the lack of labeled data in instructional domains. This is done, for example, by interacting with the environment BIBREF8 , BIBREF9 or observing weakly aligned sequences of instructions and corresponding actions BIBREF10 , BIBREF11 . In contrast, we present the first steps towards a pragmatic approach based on linguistic annotation (Figure FIGREF4 ). We describe our effort to exhaustively annotate wet lab protocols with actions corresponding to lab procedures and their attributes including materials, instruments and devices used to perform specific actions. As we demonstrate in § SECREF6 , our corpus can be used to train machine learning models which are capable of automatically annotating lab-protocols with action predicates and their arguments BIBREF12 , BIBREF13 ; this could provide a useful linguistic representation for robotic automation BIBREF14 and other downstream applications.Wet Lab Protocols	Wet laboratories are laboratories for conducting biology and chemistry experiments which involve chemicals, drugs, or other materials in liquid solutions or volatile phases. Figure FIGREF2 shows one representative wet lab protocol. Research groups around the world curate their own repositories of protocols, each adapted from a canonical source and typically published in the Materials and Method section at the end of a scientific article in biology and chemistry fields. Only recently has there been an effort to gather collections of these protocols and make them easily available. Leveraging an openly accessible repository of protocols curated on the https://www.protocols.io platform, we annotated hundreds of academic and commercial protocols maintained by many of the leading bio-science laboratory groups, including Verve Net, Innovative Genomics Institute and New England Biolabs. The protocols cover a large spectrum of experimental biology, including neurology, epigenetics, metabolomics, cancer and stem cell biology, etc (Table TABREF5 ). Wet lab protocols consist of a sequence of steps, mostly composed of imperative statements meant to describe an action. They also can contain declarative sentences describing the results of a previous action, in addition to general guidelines or warnings about the materials being used.Annotation Scheme	In developing our annotation guidelines we had three primary goals: (1) We aim to produce a semantic representation that is well motivated from a biomedical and linguistic perspective; (2) The guidelines should be easily understood by annotators with or without biology background, as evaluated in Table TABREF7 ; (3) The resulting corpus should be useful for training machine learning models to automatically extract experimental actions for downstream applications, as evaluated in § SECREF6 .We utilized the EXACT2 framework BIBREF2 as a basis for our annotation scheme. We borrowed and renamed 9 object-based entities from EXACT2, in addition, we created 5 measure-based (Numerical, Generic-Measure, Size, pH, Measure-Type) and 3 other (Mention, Modifier, Seal) entity types. EXACT2 connects the entities directly to the action without describing the type of relations, whereas we defined and annotated 12 types of relations between actions and entities, or pairs of entities (see Appendix for a full description).For each protocol, the annotators were requested to identify and mark every span of text that corresponds to one of 17 types of entities or an action (see examples in Figure FIGREF3 ). Intersection or overlap of text spans, and the subdivision of words between two spans were not allowed. The annotation guideline was designed to keep the span short for entities, with the average length being 1.6 words. For example, Concentration tags are often very short: 60% 10x, 10M, 1 g/ml. The Method tag has the longest average span of 2.232 words with examples such as rolling back and forth between two hands. The methods in wet lab protocols tend to be descriptive, which pose distinct challenges from existing named entity extraction research in the medical BIBREF15 and other domains. After all entities were labelled, the annotators connected pairs of spans within each sentence by using one of 12 directed links to capture various relationships between spans tagged in the protocol text. While most protocols are written in scientific language, we also observe some non-standard usage, for example using RT to refer to room temperature, which is tagged as Temperature.Annotation Process	Our final corpus consists of 622 protocols annotated by a team of 10 annotators. Corpus statistics are provided in Table TABREF5 and TABREF6 . In the first phase of annotation, we worked with a subset of 4 annotators including one linguist and one biologist to develop the annotation guideline for 6 iterations. For each iteration, we asked all 4 annotators to annotate the same 10 protocols and measured their inter-annotator agreement, which in turn helped in determining the validity of the refined guidelines. The average time to annotate a single protocol of 40 sentences was approximately 33 minutes, across all annotators.Inter-Annotator Agreement	We used Krippendorff's INLINEFORM0 for nominal data BIBREF16 to measure the inter-rater agreement for entities, actions and relations. For entities, we measured agreement at the word-level by tagging each word in a span with the span's label. To evaluate inter-rater agreement for relations between annotated spans, we consider every pair of spans within a step and then test for matches between annotators (partial entity matches are allowed). We then compute Krippendorff's INLINEFORM1 over relations between matching pairs of spans. Inter-rater agreement for entities, actions and relations is presented in Figure TABREF7 .Methods	To demonstrate the utility of our annotated corpus, we explore two machine learning approaches for extracting actions and entities: a maximum entropy model and a neural network tagging model. We also present experiments for relation classification. We use the standard precision, recall and F INLINEFORM0 metrics to evaluate and compare the performance.Maximum Entropy (MaxEnt) Tagger	In the maximum entropy model for action and entity extraction BIBREF17 , we used three types of features based on the current word and context words within a window of size 2:Parts of speech features which were generated by the GENIA POS Tagger BIBREF18 , which is specifically tuned for biomedical texts;Lexical features which include unigrams, bigrams as well as their lemmas and synonyms from WordNet BIBREF19 are used;Dependency parse features which include dependent and governor words as well as the dependency type to capture syntactic information related to actions, entities and their contexts. We used the Stanford dependency parser BIBREF20 .Neural Sequence Tagger	We utilized the state-of-the-art Bidirectional LSTM with a Conditional Random Fields (CRF) layer BIBREF21 , BIBREF22 , BIBREF23 , initialized with 200-dimentional word vectors pretrained on 5.5 billion words from PubMed and PMC biomedical texts BIBREF24 . Words unseen in the pretrained vocabulary were randomly initialized using a uniform distribution in the range (-0.01, 0.01). We used Adadelta BIBREF25 optimization with a mini-batch of 16 sentences and trained each network with 5 different random seeds, in order to avoid any outlier results due to randomness in the model initialization.Relation Classification	To demonstrate the utility of the relation annotations, we also experimented with a maximum entropy model for relation classification using features shown to be effective in prior work BIBREF26 , BIBREF27 , BIBREF28 . The features are divided into five groups:Word features which include the words contained in both arguments, all words in between, and context words surrounding the arguments;Entity type features which include action and entity types associated with both arguments;Overlapping features which are the number of words, as well as actions or entities, in between the candidate entity pair;Chunk features which are the chunk tags of both arguments predicted by the GENIA tagger;Dependency features which are context words related to the arguments in the dependency tree according to the Stanford Dependency Parser.Also included are features indicating whether the two spans are in the same noun phrase, prepositional phrase, or verb phrase.Finally, precision and recall at relation extraction are presented in Table 5. We used gold action and entity segments for the purposes of this particular evaluation. We obtained the best performance when using all feature sets.Results	The full annotated dataset of 622 protocols are randomly split into training, dev and test sets using a 6:2:2 ratio. The training set contains 374 protocols of 8207 sentences, development set contains 123 protocols of 2736 sentences, and test set contains 125 protocols of 2736 sentences. We use the evaluation script from the CoNLL-03 shared task BIBREF29 , which requires exact matches of label spans and does not reward partial matches. During the data preprocessing, all digits were replaced by `0'.Entity Identification and Classification	Table TABREF20 shows the performance of various methods for entity tagging. We found that the BiLSTM-CRF model consistently outperforms other methods, achieving an overall F1 score of 86.89 at identifying action triggers and 72.61 at identifying and classifying entities.Table TABREF22 shows the system performance of the MaxEnt tagger using various features. Dependency based features have the highest impact on the detection of entities, as illustrated by the absolute drop of 7.84% in F-score when removed. Parts of speech features alone are the most effective in capturing action words. This is largely due to action words appearing as verbs or nouns in the majority of the sentences as shown in Table TABREF23 . We also notice that the GENIA POS tagger, which is is trained on Wall Street Journal and biomedical abstracts in the GENIA and PennBioIE corpora, under-identifies verbs in wet lab protocols. We suspect this is due to fewer imperative sentences in the training data. We leave further investigation for future work, and hope the release of our dataset can help draw more attention to NLP research on instructional languages.Conclusions	In this paper, we described our effort to annotate wet lab protocols with actions and their semantic arguments. We presented an annotation scheme that is both biologically and linguistically motivated and demonstrated that non-experts can effectively annotate lab protocols. Additionally, we empirically demonstrated the utility of our corpus for developing machine learning approaches to shallow semantic parsing of instructions. Our annotated corpus of protocols is available for use by the research community.Acknowledgement	We would like to thank the annotators: Bethany Toma, Esko Kautto, Sanaya Shroff, Alex Jacobs, Berkay Kaplan, Colins Sullivan, Junfa Zhu, Neena Baliga and Vardaan Gangal. We would like to thank Marie-Catherine de Marneffe and anonymous reviewers for their feedback.Annotation Guidelines	The wet lab protocol dataset annotation guidelines were designed primarily to provide a simple description of the various actions and their arguments in protocols so that it could be more accessible and be effectively used by non-biologists who may want to use this dataset for various natural language processing tasks such as action trigger detection or relation extraction. In the following sub-sections we summarize the guidelines that were used in annotating the 622 protocols as we explore the actions, entities and relations that were chosen to be labelled in this dataset.Actions	Under a broad categorization, Action is a process of doing something, typically to achieve an aim. In the context of wet lab protocols, action mentions in a sentence or a step are deliberate but short descriptions of a task tying together various entities in a meaningful way. Some examples of action words, (categorized using GENIA POS tagger), are present in Table TABREF23 along with their frequencies.Entities	We broadly classify entities commonly seen in protocols under 17 tags. Each of the entity tags were designed to encourage short span length, with the average number of words per entity tag being INLINEFORM0 . For example, Concentration tags are often very short: 60% 10x, 10M, 1 g/ml, while the Method tag has the longest average span of INLINEFORM1 words with examples such as rolling back and forth between two hands (as seen in Figure FIGREF28 ). The methods in wet lab protocols tend to be descriptive, which pose distinct challenges from existing named entity extraction research in the medical and other domains.Reagent: A substance or mixture for use in any kind of reaction in preparing a product because of its chemical or biological activity.Location: Containers for reagents or other physical entities. They lack any operation capabilities other than acting as a container. These could be laboratory glassware or plastic tubing meant to hold chemicals or biological substances.Device: A machine capable of acting as a container as well as performing a specific task on the objects that it holds. A device and a location are similar in all aspects except that a device performs a specific set of operations on its contents, usually illustrated in the sentence itself, or sometimes implied.Seal: Any kind of lid or enclosure for the location or device. It could be a cap, or a membrane that actively participates in the protocol action, and hence is essential to capture this type of entity.Amount: The amount of any reagent being used in a given step, in terms of weight or volume.Concentration: Measure of the relative proportions of two or more quantities in a mixture. Usually in terms of their percentages by weight or volume.Time: Duration of a specific action described in a single step or steps, typically in secs, min, days, or weeks.Temperature: Any temperature mentioned in degree Celsius, Fahrenheit, or Kelvin.Method: A word or phrase used to concisely define the procedure to be performed in association with the chosen action verb. It’s usually a noun, but could also be a passive verb.Speed: Typically a measure that represents rotation per min for centrifuges.Numerical: A generic tag for a number that doesn't fit time, temp, etc and which isn't accompanied by its unit of measure.Generic-Measure: Any measures that don't fit the list of defined measures in this list.Size A measure of the dimension of an object. For example: length, area or thickness.Measure-Type: A generic tag to mark the type of measurement associated with a number.pH: measure of acidity or alkalinity of a solution.Modifier: A word or a phrase that acts as an additional description of the entity it is modifying. For example, quickly mix vs slowly mix are clearly two different actions, informed by their modifiers ""quickly"" or ""slowly"" respectively.Mention: Words that can refer to an object mentioned earlier in the sentence.Relations	Acts-On: Links the reagent, or location that the action acts on, typically linking the direct objects in the sentence to the action.Creates: This relation marks the physical entity that the action creates.Site: A link that associates a Location or Device to an action. It indicates that the Device or Location is the site where the action is performed. It is also used as a way to indicate which entity will finally hold/contain the result of the action.Using: Any entity that the action verb makes ‘use’ of is linked with this relation.Setting: Any measure type entity that is being used to set a device is linked to the action that is attempting to use that numerical.Count: A Numerical entity that represents the number of times the action should take place.Measure Type Link: Associates an action to a Measure Type entity that the Action is instructing to measure.Coreference: A link that associates two phrases when those two phrases refer to the same entity.Mod Link: A Modifier entity is linked to any entity that it is attempting to modify using this relation.Settings: Links devices to their settings directly, only if there is no Action associated with those settings.Measure: A link that associates the various numerical measures to the entity its trying to measure directly.Meronym: Links reagents, locations or devices with materials contained in the reagent, location or device.Or: Allows chaining multiple entities where either of them can be used for a given link.Of-Type: used to specify the Measure-Type of a Generic-Measure or a Numerical, if the sentence contains this information.","['are the protocols manually annotated?', 'what ML approaches did they experiment with?']","['Unacceptable', 'Unacceptable']"
75,"Self-Attention Gazetteer Embeddings for Named-Entity Recognition	Recent attempts to ingest external knowledge into neural models for named-entity recognition (NER) have exhibited mixed results. In this work, we present GazSelfAttn, a novel gazetteer embedding approach that uses self-attention and match span encoding to build enhanced gazetteer embeddings. In addition, we demonstrate how to build gazetteer resources from the open source Wikidata knowledge base. Evaluations on CoNLL-03 and Ontonotes 5 datasets, show F1 improvements over baseline model from 92.34 to 92.86 and 89.11 to 89.32 respectively, achieving performance comparable to large state-of-the-art models.	Introduction	Named-entity recognition (NER) is the task of tagging relevant entities such as person, location and organization in unstructured text. Modern NER has been dominated by neural models BIBREF0, BIBREF1 combined with contextual embeddings from language models (LMs) BIBREF2, BIBREF3, BIBREF4. The LMs are pre-trained on large amounts of unlabeled text which allows the NER model to use the syntactic and semantic information captured by the LM embeddings. On the popular benchmark datasets CoNLL-03 BIBREF5 and Ontonotes 5 BIBREF6, neural models with LMs achieved state-of-the-art results without gazetteers features, unlike earlier approaches that heavily relied on them BIBREF7. Gazetteers are lists that contain entities such as cities, countries, and person names. The gazetteers are matched against unstructured text to provide additional features to the model. Data for building gazetteers is available for multiple language from structured data resources such as Wikipedia, DBpedia BIBREF8 and Wikidata BIBREF9.In this paper, we propose GazSelfAttn, a novel gazetteer embedding approach that uses self-attention and match span encoding to build enhanced gazetteer representation. GazSelfAttn embeddings are concatenated with the input to a LSTM BIBREF10 or CNN BIBREF11 sequence layer and are trained end-to-end with the model. In addition, we show how to extract general gazetteers from the Wikidata, a structured knowledge-base which is part of the Wikipedia project.Our contributions are the following:[topsep=1pt, leftmargin=15pt, itemsep=-1pt]We propose novel gazetteer embeddings that use self-attention combined with match span encoding.We enhance gazetteer matching with multi-token and single-token matches in the same representation.We demonstrate how to use Wikidata with entity popularity filtering as a resource for building gazetteers.GazSelfAttn evaluations on CoNLL-03 and Ontonotes 5 datasets show F$_1$ score improvement over baseline model from 92.34 to 92.86 and from 89.11 to 89.32 respectively. Moreover, we perform ablation experiments to study the contribution of the different model components.Related Work	Recently, researchers added gazetteers to neural sequence models. BIBREF12 demonstrated small improvements on large datasets and bigger improvements on small datasets. BIBREF13 proposed to train a gazetteer attentive network to learn name regularities and spans of NER entities. BIBREF14 demonstrated that trained gazetteers scoring models combined with hybrid semi-Markov conditional random field (HSCRF) layer improve overall performance. The HSCRF layer predicts a set of candidate spans that are rescored using a gazetteer classifier model. The HSCRF approach differs from the common approach of including gazetteers as an embedding in the model. Unlike the work of BIBREF14, our GazSelfAttn does not require training a separate gazetteer classifier and the HSCRF layer, thus our approach works with any standard output layer such as conditional random field (CRF) BIBREF15.BIBREF16 proposed an auto-encoding loss with hand-crafted features, including gazetteers, to improve accuracy. However, they did not find that gazetteer features significantly improve accuracy.Extracting gazetteers from structure knowledge sources was investigated by BIBREF17 and BIBREF18. They used Wikipedia's instance of relationship as a resource for building gazetteers with classical machine learning models. Compared to Wikidata, the data extracted from Wikipedia is smaller and noisier.Similar to this paper, BIBREF19 used Wikidata as a gazetteer resource. However, they did not use entity popularity to filter ambiguous entities and their gazetteer model features use simple one-hot encoding.Approach ::: Model Architecture	We add GazSelfAttn embeddings to the popular Neural CRF model architecture with ELMo LM embeddings from BIBREF2. Figure FIGREF5 depicts the model, which consists of Glove word embeddings BIBREF20, Char-CNN BIBREF21, BIBREF1, ELMo embeddings, Bi-LSTM, and output CRF layer with BILOU (Beginning Inside Last Outside Unit) labels encoding BIBREF22. Note that, we concatenate the gazetteer embeddings to the Bi-LSTM input.Approach ::: Gazetteers	In this section, we address the issue of building a high-quality gazetteer dictionary $M$ that maps entities to types, e.g., $M$[Andy Murray] $\rightarrow $ Person. In this work, we use Wikidata, an open source structured knowledge-base, as the source of gazetteers. Although, Wikidata and DBpedia are similar knowledge bases, we choose Wikidata because, as of 2019, it provides data on around 45 million entities compared to around 5 million in DBpedia.Wikidata is organized as entities and properties. Entities can be concrete (Boston, NATO, Michael Jordan) and abstract (City, Organization, Person). Properties describe an entity relations. For example, Boston instance_of City and Boston part_of Massachusetts; both instance_of and part_of are properties. Also, each entity is associated with sitelink count which tacks mentions of the entity on Wikimedia website and can be used as proxy for popularity.To extract gazetteers from Wikidata, we process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet, example resulting tuples are Boston $\rightarrow $ City and Massachusetts $\rightarrow $ State. Each entity is associated with a set of aliases, we keep only the aliases that are less than seven tokens long. Example aliases for Boston are “Beantown” and “The Cradle of Liberty”. If there are multiple types per alias, we use the sitelink count to keep the six most popular types. The sitelink filtering is important to reduce the infrequent meanings of an entity in the gazetteer data.The Wikidata types that we obtain after processing the Wikidata dumps are fine-grained. However, certain NER tasks require coarse-grained types. For instance, CoNLL-03 task has a single Location label that consists of cities, states, countries, and other geographic location. To move from fine-grained to coarse-grained types, we use the Wikidata hierarchical structure induced by the subclass_of property. Examples of subclass_of hierarchies in Wikidata are: City $\rightarrow $ Human Settlement $\rightarrow $ Geographic Location, and Artist $\rightarrow $ Creator $\rightarrow $ Person. We change the types granularity depending on the NER task by traversing up, from fine-grained types to the target coarse-grained types. For instance, we merge the Artist and Painter types to Person, and the River and Mountain types to Location.Approach ::: Gazetteer Matching	Gazetteer matching is the process of assigning gazetteer features to sentence tokens. Formally, given a gazetteer dictionary $M$ that maps entities to types, and a sentence $S = (t_1, t_2, ..., t_n)$ with tokens $t_i$, we have to find the $m$ gazetteer types $\lbrace g^1_i, g^2_i,..,g^m_i\rbrace $ and spans $\lbrace s^1_i, s^2_i,..,s^m_i\rbrace $ for every token $t_i$. The set notation $\lbrace $} indicates that multiple $m$ matches are allowed per token. The match span $\lbrace s^j_i\rbrace $ represents positional information which encodes multi-token matches. The match spans are encoded using a BILU (Beginning Inside Last Unit) tags, similar to the BILOU tags that we use to encode the NER labels.In general, there are two methods for gazetteer matching: multi-token and single-token. Multi-token matching is searching for the longest segments of the sentence that are in $M$. For instance, given $M$[New York] $\rightarrow $ State, $M$[New York City] $\rightarrow $ City and the sentence “Yesterday in New York City”, the multi-token matcher assigns the City gazetteer type to the longest segment “New York City”. Single-token matching is searching to match any vocabulary word from a gazetteer type. In the earlier example, each word from the sentence is individually matched to the tokens in $M$, thus “New” and “York” are individually matched to both City and State, and “City” is matched only to City.BIBREF12 research shows that both multi-token and single-token matching perform better on certain datasets. We propose to combine both methods: we tag the multi-token matches with BILU tags, and the single-token matches with a Single (S) tag. The single-token matches are used only if multi-token matches are not present. We consider that the single-token matches are high-recall low-precision, and multi-token matches are low-recall and high-precision. Thus, a combination of both works better than individually. Example sentences are: “Yesterday in New(City-B) York(City-I) City(City-L)”, and “Yesterday in York(City-S) City(City-S)” York City is marked with singles tag since $M$ does not have entities for “York City”, “York”, and “City”.Note that gazetteer matching is unsupervised, i.e., we do not have a ground truth of correctly matched sentences for $M$. Furthermore, it is a hard task because of the many variations in writing and ambiguity of entities.Approach ::: Gazetteer Embeddings	pxEquations DISPLAY_FORM11- shows the gazetteer embedding $\mathbf {g}_i$ computation for a token $t_i$. To compute $\mathbf {g}_i$, given a set of $m$ gazetteer types $\lbrace g^m_i\rbrace $ and spans $\lbrace s^m_i\rbrace $, we execute the following procedure:[topsep=1pt, leftmargin=15pt, itemsep=-1pt]Equation DISPLAY_FORM11. We embed the sets $\lbrace g^m_i\rbrace $ and $\lbrace s^m_i\rbrace $ using the embedding matrices $\mathbf {G}$ and $\mathbf {S}$. Then, we do an element-wise addition, denoted $\oplus $, of the corresponding types and spans embeddings to get a matrix $\mathbf {E}_i$.Equation . We compute $\mathbf {A}_i$ using scaled dot-product self-attention BIBREF23, where $d$ is the dimensionality of the gazetteer embeddings. The attention contextualizes the embeddings with multiple gazetteer matches per token $t_i$.Equation . To add model flexibility, we compute $\mathbf {H}_i$ with a position-wise feed-forward layer and GELU activation BIBREF24.Equation . Finally, we perform max pooling across the embeddings $\mathbf {H}_i$ to obtain the final gazetteer embedding $\mathbf {g}_i$.Approach ::: Gazetteer Dropout	To prevent the neural NER model from overfitting on the gazetteers, we use gazetteers dropout BIBREF25. We randomly set to zero gazetteer embeddings $\mathbf {g}_i$, so the gazetteer vectors that are input to the LSTM become zero. We tune the gazetteer dropout hyperparameter on the validation set.Experiments ::: Setup	Datasets. We evaluate on the English language versions of CoNLL-03 dataset BIBREF5 and the human annotated portion of the Ontonotes 5 BIBREF6 dataset. CoNLL-03 labels cover 4 entity types: person, location, organization, and miscellaneous. The Onotonotes 5 dataset is larger and its labels cover 18 types: person, NORP, facility, organization, GPE, location, product, event, work of art, law, language, date, time, percent, money, quantity, ordinal, cardinal.pxGazetteers. We use the Wikidata gazetteers with types merged to the granularity of the CoNLL-03 and Ononotes 5 datasets. We filter non-relevant types (e.g., genome names, disease) and get a total of one million records. For CoNLL-03 and Ontonotes 5, the percentage of entities covered by gazetteers are 96% and 78% respectively, and percentage of gazetteers wrongly assigned to non-entity tokens are 41% and 41.5% respectively.Evaluation. We use the standard CoNLL evaluation script which reports entity F1 scores. The F1 scores are averages over 5 runs.Configuration. We use the Bi-LSTM-CNN-CRF model architecture with ELMo language model embeddings from BIBREF2, which consist of 50 dim pre-trained Glove word embeddings BIBREF20, 128 dim Char-CNN BIBREF21, BIBREF1 embeddings with filter size of 3 and randomly initialized 16 dim char embeddings, 1024 pre-trained ELMo pre-trained embeddings, two layer 200 dim Bi-LSTM, and output CRF layer with BILOU (Beginning Inside Last Outside Unit) spans BIBREF22.For the gazetteer embeddings, we use 128 dim for the embedding matrices $\mathbf {G}$ and $\mathbf {S}$, 128 dim output for $\mathbf {W}$, which yields a gazetteer embedding $\mathbf {g}_i$ with 128 dim. The parameters are randomly initialized and trained. We apply gazetteer dropout of 0.1 which we tuned on the development set; we tried values form 0.05 to 0.6.All parameters except the ELMo embeddings are trained. We train using the Adam BIBREF26 optimizer with learning rate of 0.001 for 100 epochs. We use early stopping with patience 25 on the development set. Batch size of 64, dropout rate of 0.5 and L2 regularization of 0.1.Experiments ::: Results	The experimental results for NER are summarized in Table TABREF20. The top part of the table shows recently published results. BIBREF14's work is using gazetteers with HSCRF and BIBREF4's work is using the Flair language model which is much larger than ELMo. BIBREF27 is the current state-of-the-art language model that uses cloze-driven pretraining. The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings. We obtain slightly better CoNLL-03 F$_1$ score compared to BIBREF14 work that uses the HSCRF model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model. BIBREF14 Ononotes 5 results use subset of the dataset labels and are not comparable. Note that because of computation constrains, we did not perform extensive hyperparameter tuning except for the gazetteer dropout rate.Experiments ::: Ablation study	Table TABREF22 shows ablation experiments. We remove components of the gazetteer embedding model from the Neural CRF model. In each experiment, we removed only the specified component. Ablations show decreased F$_1$ score on the development and test set if any of the components is removed. The highest degradation is when single matches are removed which underscores the importance of the combining the gazetteer matching techniques for NER. We observe that match span encoding is more important for the CoNLL-02 compared to Ononotes 5 because the former has more entities with multiple tokens. Removing the self-attention shows that self-attention is effective at combining information form multiple gazetteers.In addition, we tried moving the gazetteer embeddings to the CRF layer and using the LSTM token embeddings as attention keys but the F$_1$ degraded significantly. We experimented with adding auto-encoding loss similar to BIBREF16 and multi-head self-attention. However, we did not observe F$_1$ score improvements and sometimes small degradations.Conclusion	We presented GazSelfAttn, a novel approach for gazetteer embeddings that uses self-attention and match span positions. Evaluation results of GazSelfAttn show improvement compared to competitive baselines and state-of-the-art models on multiple datasets.For future work we would like to evaluate GazSelfAttn on non-English language datasets and improve the multi-token gazetteer matching with fuzzy string matching. Also, we would like to explore transfer learning of gazetteer embeddings from high-resource to low-resource setting.","['What is the performance of large state-of-the-art models on these datasets?', 'What is used as a baseline model?', 'How do they build gazetter resources from Wikipedia knowlege base?']","['Average  92.87 for CoNLL-01 and Average 8922 for Ontonotes 5', 'Unacceptable', 'Unacceptable']"
76,"Asymmetrical Hierarchical Networks with Attentive Interactions for Interpretable Review-Based Recommendation	Recently, recommender systems have been able to emit substantially improved recommendations by leveraging user-provided reviews. Existing methods typically merge all reviews of a given user or item into a long document, and then process user and item documents in the same manner. In practice, however, these two sets of reviews are notably different: users' reviews reflect a variety of items that they have bought and are hence very heterogeneous in their topics, while an item's reviews pertain only to that single item and are thus topically homogeneous. In this work, we develop a novel neural network model that properly accounts for this important difference by means of asymmetric attentive modules. The user module learns to attend to only those signals that are relevant with respect to the target item, whereas the item module learns to extract the most salient contents with regard to properties of the item. Our multi-hierarchical paradigm accounts for the fact that neither are all reviews equally useful, nor are all sentences within each review equally pertinent. Extensive experimental results on a variety of real datasets demonstrate the effectiveness of our method.	Introduction	The rapid shift from traditional retail and services to online transactions has brought forth a large volume of review data in areas such as e-commerce, dining, tourism, among many others. While such reviews are routinely consulted directly by consumers and affect their decision making, recent work has shown that they can also be exploited by intelligent algorithms. The detailed semantic cues that they harbor not only reveal different aspects (e.g., quality, material, color, etc.) of an item, but also reflect the sentiment of users towards these aspects. Such fine-grained signals are extremely valuable to a recommender system and significantly complement the sparse rating and click-through data, based on which many traditional collaborative filtering methods BIBREF0 have been developed. Thus, there has been a series of studies seeking to harness the potential of reviews in improving the recommendation quality BIBREF1, BIBREF2, BIBREF3, BIBREF4.These studies have shown that leveraging reviews can indeed boost the recommendation effectiveness quite remarkably. Typically, they associate users with the respective sets of reviews they have written, while associating each item with the set of all reviews that have been written for it. To predict the rating for an unseen user–item pair, in a first step, the embeddings of that user and item are inferred from the respective sets of reviews via a neural network. Then, the two embeddings are matched to predict a numeric rating between them. For example, DeepCoNN BIBREF1 relies on convolutional neural networks to learn user (item) embeddings, and on a factorization machine BIBREF5 to predict ratings. D-ATT BIBREF3 uses dual-attention based networks to learn embeddings, and a simple dot product to predict ratings.Despite the encouraging progress, existing methods all regard the set of reviews by a user and the set of reviews for an item as the same type of documents, and invoke the same model (or even a shared model) to process them in parallel. In reality, however, the set of reviews for a user is fundamentally different from the set of reviews for an item. In particular, reviews for users correspond to a diverse set of items that they have rated, resulting in notably heterogeneous textual contents with a variety of topics for different items. In contrast, each item's reviews are only about itself, and the contents are thus homogeneous in the sense that the topic is limited to a single narrow domain. For example, Fig. FIGREF1 shows several reviews from Amazon's health domain. User $u$'s historical reviews describe three items, Vitamin C, anti-inflammatory medication, and an air freshener, while all reviews for item $v$ are about itself, i.e., Vitamin D3.This profound difference necessitates distinct forms of attention to be paid on user reviews as opposed to item reviews, when deciding whether to recommend an item $v$ to a user $u$. To predict $u$'s preference of $v$, it is important to extract from $u$'s reviews those aspects that pertain most to $v$, e.g., comments on items that are similar to $v$. In contrast, from $v$'s reviews, we wish to account for the sentiment of other users with regard to relevant aspects of $v$. If $u$ pays special attention to certain aspects of items similar to $v$, while other users wrote highly about $v$ with regard to these particular aspects, then it is much more likely that $v$ will be of interest to $u$. For example, in Fig. FIGREF1, reviews 1 and 2 of $u$ are about non-prescription medicines that are similar to $v$. In reviews 1 and 2, $u$ mentioned aspects such as “not sourced from genetically modified corn”, “easier to swallow”, “great price”, and “no after taste”, indicating that $u$ considers the source and price and prefers easily swallowed products without after-taste. Meanwhile, reviews 1-3 of $v$ mention that $v$ “have no taste”, is “easy to swallow”, “gmo-free”, and “prices low”, which are opinions expressed by others that match $u$'s preferences. Thus, $v$ is likely to be of interest to $u$, and $u$ indeed marked a 5.0 score on $v$ after purchasing it.Another vital challenge is how to reliably represent each review. Importantly, sentences are not equally useful within each review. For example, in Fig. FIGREF1, the second sentence in $u$'s review 1, “I take these in the morning and after every workout.” conveys little regarding $u$'s concerns for Vitamin C, and thus is less pertinent than other sentences in the same review. Since including irrelevant sentences can introduce noise and may harm the final embedding quality, it is crucial to aggregate only useful sentences to represent each review.To address the above challenges, in this paper, we propose an Asymmetrical Hierarchical Network with Attentive Interactions (AHN) for recommendation. AHN progressively aggregates salient sentences to induce review representations, and aggregates pertinent reviews to induce user and item representations. AHN is particularly characterized by its asymmetric attentive modules to flexibly distinguish the learning of user embeddings as opposed to item embeddings. For items, several attention layers are invoked to highlight sentences and reviews that contain rich aspect and sentiment information. For users, we designed an interaction-based co-attentive mechanism to dynamically select a homogeneous subset of contents related to the current target item. In this manner, AHN hierarchically induces embeddings for user–item pairs reflecting the most useful knowledge for personalized recommendation. In summary, our contributions areWe identify the asymmetric attention problem for review-based recommendation, which is important but neglected by existing approaches.We propose AHN, a novel deep learning architecture that not only captures both of the asymmetric and hierarchical characteristics of the review data, while also enabling interpretability of the results.We conduct experiments on 10 real datasets. The results demonstrate that AHN consistently outperforms the state-of-the-art methods by a large margin, while providing good interpretations of the predictions.Related Work	Exploiting reviews has proven considerably useful in recent work on recommendation. Many methods primarily focus on topic modeling based on the review texts. For example, HFT BIBREF6 employs LDA to discover the latent aspects of users and items from reviews. RMR BIBREF7 extracts topics from reviews to enhance the user and item embeddings obtained by factorizing the rating matrix. TopicMF BIBREF8 jointly factorizes a rating matrix and bag-of-words representations of reviews to infer user and item embeddings. Despite the improvements achieved, these methods only focus on topical cues in reviews, but neglect the rich semantic contents. Moreover, they typically represent reviews as bag-of-words, and thus remain oblivious of the order and contexts of words and sentences in reviews, which are essential for modeling the characteristics of users and items BIBREF1.Inspired by the astonishing advances of recent deep NLP techniques in various applications BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14, there has been increasing interest in studying deep learning models. DeepCoNN BIBREF1 employs CNNs as an automatic feature extractor to encode each user and item into a low-dimensional vector by assessing the relevant set of historical reviews. TransNet BIBREF2 extends DeepCoNN by augmenting the CNN architecture with a multi-task learning scheme to regularize the user and item embeddings towards the target review. These methods, however, lack interpretability BIBREF15 in their results.To better understand the predictions, several attention-based methods have been developed. D-ATT BIBREF3 incorporates two kinds of attention mechanisms on the words of reviews to find informative words. NARRE BIBREF4 invokes review-level attention weights to aggregate review embeddings to form user (item) embeddings. HUITA BIBREF16 is equipped with a symmetric hierarchical structure, where, at each level (e.g., word level), a regular attention mechanism is employed to infer the representation of the subsequent level (e.g., sentence level). MPCN BIBREF17 models the interactions between a user's reviews and an item's reviews via co-attention based pointers that are learned with the Gumbel-Softmax trick BIBREF18. However, all these methods just learn user and item embeddings in parallel and fail to consider the important differences between the two. As discussed before, this leads to suboptimal predictions.Unlike the aforementioned methods, our method learns several hierarchical aggregators to infer user (item) embeddings. The aggregators are asymmetric to flexibly pay varying levels of attention to a user's (item's) reviews, so as to enhance the prediction accuracy and model interpretability.Our Proposed Model	In this section, we introduce our AHN model in a bottom-up manner. Fig. FIGREF9 illustrates the architecture of AHN.Our Proposed Model ::: Sentence Encoding	The sentence encoding layer (omitted in Fig. FIGREF9) aims to transform each sentence (in each review) from a sequence of discrete word tokens to a continuous vector embedding. We use a word embedding model to lay the foundation of this layer. Suppose the sentence $s$ has $l$ words. By employing a word embedding matrix $\mathbf {E} \in \mathbb {R}^{d \times |\mathcal {V}|}$, $s$ can be represented by a sequence $[\mathbf {e}_{1}, ..., \mathbf {e}_{l}]$, where $\mathbf {e}_{i}$ is the embedding of the $i$-th word in $s$, $d$ is the dimensionality of the word embedding, and $\mathcal {V}$ is the whole vocabulary of words. The matrix $\mathbf {E}$ can be initialized using word embeddings such as word2vec BIBREF19 and GloVe BIBREF20, which are widely used in NLP. To refine the word embeddings, $\mathbf {E}$ is fine-tuned during model training.To learn an embedding for $s$, we employ a bi-directional LSTM BIBREF11 on its constituent word embeddings, and apply max-pooling on the hidden states to preserve the most informative information. That iswhere ${\bf s}$ is the embedding of $s$ andwhere ${\bf \tilde{e}}_{0}$ is initialized by an all-zero vector ${\bf 0}$.Suppose a review has $k$ sentences. We can then represent this review by a sequence $[{\bf s}_{1}, ..., {\bf s}_{k}]$, where ${\bf s}_{i}$ is the embedding of the $i$-th sentence in the review, as inferred by Eq. (DISPLAY_FORM6). However, using Eq. (DISPLAY_FORM6), each ${\bf s}_{i}$ only encodes its own semantic meaning, but remains oblivious of any contextual cues from its surrounding sentences in the same review. To further refine the sentence embedding, we introduce a context-encoding layer by employing another bi-directional LSTM on top of the previous layer to model the temporal interactions between sentences, i.e.,where ${\bf \tilde{s}}_{i}$ is the final embedding of the $i$-th sentence in the review and ${\bf \tilde{s}}_{0}$ is initialized as ${\bf 0}$.Our Proposed Model ::: Sentence-Level Aggregation	Next, we develop sentence-level aggregators to embed each review into a compact vector from its constituent sentences. As discussed before, an ideal method should learn review embeddings in an asymmetric style. Thus, we design AHN to learn different attentive aggregators for users and items, respectively, as highlighted in Fig. FIGREF9.Our Proposed Model ::: Sentence-Level Aggregation ::: Sentence Aggregator for Items.	Given an item, we are interested in sentences that contain other users' sentiments on different aspects of the item, which are the key factors to determine its overall rating. To build an informative embedding for each review upon such sentences, we use a sentence-level attention network to aggregate the sentence embeddings $[{\bf \tilde{s}}_{1}^{v}, ..., {\bf \tilde{s}}_{k}^{v}]$ as follows, where the superscript $v$ is used to distinguish an item's notation from a user's notation.Here, $\sum _{i=1}^{k}\alpha _{i}^{v}=1$, and $\alpha _{i}^{v}$ is the attention weight assigned to sentence ${\bf \tilde{s}}_{i}^{v}$. It quantifies the informativeness of sentence ${\bf \tilde{s}}_{i}^{v}$ with respect to $v$'s overall rating, compared to other sentences. The weights $\alpha _{i}^{v}$ are computed by our attentive module, taking the sentence embeddings as the input, asHere, ${\bf v}_{s} \in \mathbb {R}^{h \times 1}$, ${\bf W}_{s} \in \mathbb {R}^{h \times d}$, and ${\bf \hat{W}}_{s} \in \mathbb {R}^{h \times d}$ are parameters, $\otimes $ is the element-wise product, and $\sigma (\cdot )$ is the sigmoid function. As suggested by BIBREF21, the approximate linearity of $\tanh (\cdot )$ in $[-1, 1]$ could limit the expressiveness of the model, which can be alleviated by introducing a non-linear gating mechanism. Thus, in Eq. (DISPLAY_FORM13), a gate function $\sigma ({\bf \hat{W}}_{s}{\bf \tilde{s}}_{i}^{v})$ is incorporated, which is indeed found effective in our experiments.Our Proposed Model ::: Sentence-Level Aggregation ::: Sentence Aggregator for Users.	Next, we develop an interaction-based sentence aggregator for users. Given a user–item pair, we aim to select a homogeneous subset of sentences from each of the user's reviews such that the selected sentences are relevant to the item to be recommended, i.e., the target item. In the following, we introduce a co-attentive network that uses the target item's sentences to guide the search of user's sentences. After the sentence encoding layer, we can represent each review by a matrix ${\bf R} = [{\bf \tilde{s}}_{1}; ...; {\bf \tilde{s}}_{k}] \in \mathbb {R}^{d \times k}$, where $[\cdot ; \cdot ]$ is the concatenation operation. Suppose a user has $n$ reviews and an item has $m$ reviews. Our method first concatenates all sentences of the item to form $[{\bf R}_{1}^{v}; ...; {\bf R}_{m}^{v}] \in \mathbb {R}^{d \times mk}$, whose constituent sentences are all relevant to the target item, and thus can be used to guide the search of similar sentences from the user's reviews. To this end, we iterate over each ${\bf R}_{i}^{u}$ ($1 \le i \le n$) to calculate an affinity matrix as follows, where the superscript $u$ indicates the user notation.Here, ${\bf M}_{s} \in \mathbb {R}^{d_{s} \times d_{s}}$ is a learnable parameter, $\phi (\cdot )$ is an activation function such as ReLU, and $f(\cdot )$ is a mapping function such as a multi-layer perceptron (MLP). If $f(\cdot )$ is the identity mapping, Eq. (DISPLAY_FORM15) becomes a bilinear mapping. Here, the $(p, q)$-th entry of ${\bf G}_{i}$ represents the affinity between the $p$-th sentence of ${\bf R}_{i}^{u}$ and the $q$-th sentence of $[{\bf R}_{1}^{v}; ...; {\bf R}_{m}^{v}]$.To measure how relevant the $p$-th sentence of the user's review ${\bf R}_{i}^{u}$ is to the target item, we use the maximum value in the $p$-th row of ${\bf G}_{i}$. The intuition is that, if a user's sentence (i.e., a row of ${\bf G}_{i}$) has a large affinity to at least one sentence of the target item (i.e., a column of ${\bf G}_{i}$) – in other words, the maximal affinity of this row is large – then this user's sentence is relevant to the target item. However, not all sentences of the target item are useful for searching relevant sentences from the user. For instance, in Fig. FIGREF1, the first sentence of the item's review 2, “I received it three days ago.”, conveys little information about the target item, and hence cannot aid in identifying relevant sentences from the user, and indeed may introduce noise into the affinity matrix. To solve this problem, recall that $\alpha _{i}^{v}$ in Eq. (DISPLAY_FORM13) represents how informative an item's sentence is. Thus, we concatenate $\alpha _{i}^{v}$'s of all sentences of the target item to form $\alpha ^{v} \in \mathbb {R}^{1 \times mk}$. Subsequently, we compute an element-wise product between each row of ${\bf G}_{i}$ and the vector $\alpha ^{v}$, i.e., ${\bf G}_{i}\otimes _{\text{row}}\alpha ^{v}$. In this manner, the $(p, q)$-th entry, $({\bf G}_{i} \otimes _{\text{row}} \alpha ^{v})_{pq}$, is high only if the $p$-th sentence of the user is similar to the $q$-th sentence of the target item and the $q$-th sentence of the target item is non-trivial.By summarizing the above insights, we learn attention weights for the sentences in ${\bf R}_{i}^{u}$ for each $i \in [1, n]$ bywhere $\max _{\text{row}}$ refers to row-wise max-pooling for obtaining the maximum affinity. Intuitively, $(\alpha _{i}^{u})_{j}$ is large if the $j$-th sentence in the $i$-th review of the user describes some aspects of some item that is highly similar to the target item. This serves our purpose for selecting a homogeneous subset of sentences from the user.Next, we use $\alpha _{i}^{u}$ to aggregate the sentences in ${\bf R}_{i}^{u}$ to infer an embedding of the $i$-th review for the user:where $({\bf R}_{i}^{u})_{*j}$ is the $j$-th column of ${\bf R}_{i}^{u}$. Recall that ${\bf R}_{i}^{u} = [{\bf \tilde{s}}_{1}^{u}; ...; {\bf \tilde{s}}_{k}^{u}]$, where each column of ${\bf R}_{i}^{u}$ is a sentence embedding. Note that our method iterates over $i$ for $i \in [1, n]$ to calculate all review embeddings ${\bf r}_{1}^{u}$, ..., ${\bf r}_{n}^{u}$.Our Proposed Model ::: Sentence-Level Aggregation ::: Remark.	Our co-attentive mechanism employs the idea of sequence pair modeling but notably differs from the conventional co-attention used in QA systems BIBREF9, BIBREF22, BIBREF23. First, we only consider one side of the affinity matrix, i.e., the user. Second, our affinity matrix is adapted by row-wise multiplication of $\alpha ^{v}$ to quantify the utility of the item's sentences. Thus, our method is designed specifically for learning asymmetric attentions from user–item interactions.Our Proposed Model ::: Review-Level Aggregation	From Eq. (DISPLAY_FORM12), we obtain review embeddings for an item, ${\bf r}_{1}^{v}$, ..., ${\bf r}_{m}^{v}$. From Eq. (DISPLAY_FORM17), we obtain review embeddings for a user, ${\bf r}_{1}^{u}$, ..., ${\bf r}_{n}^{u}$. As shown in Fig. FIGREF9, based on these review embeddings, we develop review-level aggregators to infer an embedding for each user and item, respectively.As discussed before, different reviews exhibit different degrees of informativeness in modeling users and items. In particular, an item's reviews are homogeneous. Thus, we are interested in reviews with rich descriptions regarding its relevant aspects and corresponding sentiments, such as the reviews 1–3 of $v$ in Fig. FIGREF1, compared with the less informative review 4 of $v$. To attend to such reviews, similar to Eq. (DISPLAY_FORM12), we aggregate the review embeddings to represent an item bywhere $\sum _{i=1}^{m}\beta _{i}^{v} = 1$, and $\beta _{i}^{v}$ is the attention weight assigned to review ${\bf r}_{i}^{v}$. It quantifies the informativeness of the review ${\bf r}_{i}^{v}$ with respect to $v$'s overall rating. $\beta _{i}^{v}$ is produced by an attentive module with gating mechanism as follows:where ${\bf v}_{r} \in \mathbb {R}^{h \times 1}$, ${\bf W}_{r} \in \mathbb {R}^{h \times d}$, and ${\bf \hat{W}}_{r} \in \mathbb {R}^{h \times d}$ are model parameters.At the same time, a user's reviews are heterogeneous concerning a variety of items that the user has purchased, and not all reviews are relevant to the target item. Thus, similar to Eq. (DISPLAY_FORM15) and Eq. (DISPLAY_FORM16), given a user–item pair, a review-level co-attentive network is designed to select reviews from the user as guided by the reviews of the item.Specifically, an affinity matrix at the review levelis computed, where ${\bf M}_{r} \in \mathbb {R}^{d_{r} \times d_{r}}$ is a learnable parameter. Here, the $(p, q)$-th entry of ${\bf G}$ represents the affinity between the $p$-th review of the user and the $q$-th review of the item.Then, attention weights for the reviews of the userare obtained, where $\beta ^{v} = [\beta _{1}^{v}, ..., \beta _{m}^{v}]$ was obtained by Eq. (DISPLAY_FORM21) for the item. It is introduced to adapt ${\bf G}$ to encode important reviews of the item. Finally, we aggregate the review embeddings to represent a user by the following weighted sum.Our Proposed Model ::: Review-Level Aggregation ::: Encoding Latent Rating Patterns.	Although the embeddings ${\bf \tilde{u}}$ and ${\bf \tilde{v}}$ contain rich semantic information from reviews, there are some latent characteristics of users (items) that are not encoded by their reviews, but can be inferred from the rating patterns. For instance, a picky user might tend to uniformly pick lower ratings than a more easygoing user. To encode such personalized preferences, as inspired by BIBREF0, we embed a one-hot representation of the ID of each user (item) using an MLP, and obtain an embedding vector ${\bf \hat{u}}$ (${\bf \hat{v}}$) for the user (item). This vector directly correlates with the ratings of a user (item), and is thus able to capture the latent rating patterns. Then, as illustrated in Fig. FIGREF9, we concatenate ${\bf \tilde{u}}$ and ${\bf \hat{u}}$ to obtain the final embedding of a user, i.e., ${\bf u} = [{\bf \tilde{u}}; {\bf \hat{u}}]$, and concatenate ${\bf \tilde{v}}$ and ${\bf \hat{v}}$ to obtain the final embedding of an item, i.e., ${\bf v} = [{\bf \tilde{v}}; {\bf \hat{v}}]$.Our Proposed Model ::: Prediction Layer	As shown by the top part of Fig. FIGREF9, the prediction layer receives ${\bf u}$ and ${\bf v}$, and concatenates them to $[{\bf u}; {\bf v}]$, which is then fed into a function $g(\cdot )$ to predict the rating. In this work, we realize $g(\cdot )$ as a parameterized factorization machine (FM) BIBREF5, which is effective to model the pairwise interactions between the input features for improving recommendation performance. Given an input ${\bf x} \in \mathbb {R}^{d \times 1}$, $g(\cdot )$ is defined aswhere $b$ is a bias term, ${\bf w}$ is a parameter for linear regression, $\lbrace {\bf z}_{i}\rbrace _{i=1}^{d}$ are the factorized parameter for modeling the pairwise interactions between ${\bf x}_{i}$ and ${\bf x}_{j}$, $\mathinner {\langle {\cdot , \cdot }\rangle }$ denotes the inner product, and the output of $g({\bf x})$ is the predicted rating.To learn model parameters, we minimize the difference between the true ratings and the predicted ratings, as measured by the mean squared errorwhere $c$ is the total number of user–item pairs in the training data, and $y_{i}$ is the true rating of the $i$-th user–item pair. The $\ell $ in Eq. (DISPLAY_FORM28) serves as our loss function for model training.Experiments	In this section, we evaluate our AHN model on several real datasets and compare it with state-of-the-art approaches.Experiments ::: Datasets	We conducted experiments on 10 different datasets, including 9 Amazon product review datasets for 9 different domains, and the large-scale Yelp challenge dataset on restaurant reviews. Table TABREF30 summarizes the domains and statistics for these datasets. Across all datasets, we follow the existing work BIBREF3, BIBREF17 to perform preprocessing to ensure they are in a $t$-core fashion, i.e., the datasets only include users and items that have at least $t$ reviews. In our experiments, we evaluate the two cases of $t=5$ and $t=10$. For the Yelp dataset, we follow BIBREF3 to focus on restaurants in the AZ metropolitan area. For each dataset, we randomly split the user–item pairs into $80\%$ training set, $10\%$ validation set, and $10\%$ testing set. When learning the representations for users and items, we only use their reviews from the training set, and none from the validation and testing sets. This ensures a practical scenario where we cannot include any future reviews into a user's (item's) history for model training.Experiments ::: Compared Methods	We compare our model with both conventional approaches and state-of-the-art approaches, including Factorization Machines (FM) BIBREF5, SVD BIBREF0, Probabilistic Matrix Factorization (PMF) BIBREF24, Nonnegative Matrix Factorization (NMF) BIBREF25, DeepCoNN BIBREF1, D-ATT BIBREF3, MPCN BIBREF17, and HUITA BIBREF16.Among these methods, FM, SVD, PMF, and NMF are rating-based collaborative filtering methods. DeepCoNN, D-ATT, MPCN, and HUITA are state-of-the-art methods that leverage the semantic information in reviews for improved performance. Specifically, DeepCoNN uses the same CNN module to learn user and item embeddings based on their reviews for recommendation. D-ATT extends DeepCoNN by adding a dual-attention layer at word-level before convolution. MPCN attends to informative reviews by several pointers. HUITA uses a symmetric hierarchical structure to infer user (item) embeddings using regular attention mechanisms. It is worth noting that all of the above review-based methods regard user reviews and item reviews as the same type of documents and process them in an identical way.Finally, to gain further insights on some of the design choices of our AHN model, we compare AHN with its variants, which will be discussed later in the ablation analysis.Experiments ::: Experimental Settings	The parameters of the compared methods are selected based on their performance on the validation set. Specifically, for FM, the dimensionality of the factorized parameters is 10. For SVD, PMF, and NMF, the number of factors is set to 50. DeepCoNN uses 100 convolutional kernels with window size 3. D-ATT uses 200 filters and window size 5 for local attention; 100 filters and window sizes [2, 3, 4] for global attention. MPCN uses 3 pointers, and hidden dimensionality of 300 for inferring affinity matrix. HUITA uses 200 filters in the word-level CNN with window size 3, and 100 filters in the sentence-level CNN with window size 3.For our AHN model, the dimensionality of the hidden states of the BiLSTM is set to 150. The dimensionality of the user and item ID embeddings are set to 300. The dimensionality of ${\bf M}_{s}$ (${\bf M}_{r}$) in Eq. (DISPLAY_FORM15) (Eq. (DISPLAY_FORM22)) is 300. We apply dropout BIBREF26 with rate $0.5$ after the fully connected layer to alleviate the overfitting problem. The loss function is optimized by Adam BIBREF27, with a learning rate of 0.0002 and a maximum of 10 epochs.For the methods DeepCoNN, D-ATT, and HUITA, the pre-trained GloVe BIBREF20 are used to initialize the word embeddings. For MPCN and our AHN, the word embeddings are learned from scratch since using pre-trained embeddings generally degrades their performance. For all methods, the dimensionality of the word embedding is set to 300. We independently repeat each experiment 5 times, and use the averaged mean square error (MSE) BIBREF1 to quantitatively evaluate the performance.Experiments ::: Experimental Results	Table TABREF31 summarizes the results of the compared approaches on the 5-core datasets. We have several observations from the results. First, review-based methods generally outperform rating-based methods. This validates the usefulness of reviews in providing fine-grained information for refining user and item embeddings for improving the accuracy of rating prediction. Second, methods that distinguish reviews, such as D-ATT and MPCN, often outperform DeepCoNN, which suggests that different reviews exhibit different degrees of importance for modeling users and items. We also observe that HUITA does not show superiority over DeepCoNN. This may stem from its symmetric style of attention learning, which does not make much sense when reviews are heterogeneous. Finally, the proposed AHN consistently outperforms other methods, which demonstrates the effectiveness of distinguishing the learning of user and item embeddings via asymmetric attentive modules so as to infer more reasonable attention weights for recommendation.Table TABREF32 presents the results on the 10-core datasets, from which the Automotive dataset is excluded because only very few users and items are left after applying the 10-core criterion on it. In contrast to Table TABREF31, all methods in general achieve better results in Table TABREF32, since more ratings and reviews become available for each user and item. In this case, we observe that D-ATT often outperforms MPCN. This may be because the Gumbel-Softmax pointers in MPCN make hard selections on reviews, thereby filtering out many reviews that may result in a significant loss of information. This problem is more severe when users (items) have more useful reviews, as in the 10-core scenario. Additionally, we observe that the performance gaps between AHN and the compared methods become larger. Specifically, summarizing the relative improvements of AHN over each of the review-based methods in Fig. FIGREF33, we observe that AHN generally gains more on the 10-core datasets, with absolute gains of up to $11.6\%$ (DeepCoNN), $7.0\%$ (D-ATT), $13.8\%$ (MPCN), and $8.4\%$ (HUITA). This suggests that the more reviews each user and item has, the more important it is to perform proper attention learning on relevant reviews and sentences on both the user and item sides.Experiments ::: Case Study	We next investigate the interpretability of AHN. Fig. FIGREF43(a) and (b) show the attention weights of AHN on the top three reviews of a pair of user and item on the Health dataset, where the item is a sleep aid medicine. In each of the user's reviews, the highlighted words indicate the item described by the review. As can be seen, the first two items “krill oil” and “pain relief patch” are more relevant to the item “sleep aid medicine” than the “laundry detergent” in the lowest-weighted review. On the other hand, the top two reviews of the item are more informative with regard to the aspects of the item than the last review, which only discusses packaging, a rather marginal aspect of medication. Thus, the review-level attention weights of AHN are meaningful.Fig. FIGREF43(c) and (d) zoom into the attention weights of AHN on the top three sentences of the first review of the user and item, respectively. The highlighted words indicate the reason of why the sentences are ranked highly. Apparently, the user cares about the taste of the medicine and prefers easily-swallowed softgels, while the item indeed appears to taste good and is easy to swallow. Although the first two sentences in Fig. FIGREF43(d) are short, they convey more useful information than the lowest-weighted sentence. Thus, the sentence-level attention weights are also meaningful. This explains why AHN predicts a 4.4 rating score on this user–item pair, close to the true rating 5.0 given by the user.Experiments ::: Ablation Analysis	Table TABREF45 presents the results of our ablation analysis using four datasets. In the table, AHN is our original model. In (a), the item's attention modules are replaced by average-pooling. In (b), the user co-attention modules are replaced by attention modules similar to the item ones and this thus constitutes a symmetric model. In (c), we remove the row-wise multiplication between the affinity matrix and the attention weights in Eqs. (DISPLAY_FORM16) and (DISPLAY_FORM23). In (d), the parameterized factorization machine is replaced by a dot product. In (e), the gating mechanisms in Eqs. (DISPLAY_FORM13) and (DISPLAY_FORM21) are removed.From Table TABREF45, we observe that different variants of AHN show suboptimal results to various degrees. Comparing with (a), we can observe the importance of considering attention weights on the sentences and reviews of each item. The degraded MSEs of (b) suggest that our asymmetric design in the model architecture is essential. The results of (c) validate our design of the attention-adapted affinity matrix in Eqs. (DISPLAY_FORM16) and (DISPLAY_FORM23). The substantial MSE drops for (d) establish the superiority of using FM as the prediction layer. The comparison between (e) and AHN suggests the effectiveness of the gating mechanisms. Thus, the results of the ablation study validate the design choices of our model architecture.Conclusions	In this work, we highlight the asymmetric attention problem for review-based recommendation, which has been ignored by existing approaches. To address it, we propose a flexible neural architecture, AHN, which is characterized by its asymmetric attentive modules for distinguishing the learning of user embeddings and item embeddings from reviews, as well as by its hierarchical paradigm to extract fine-grained signals from sentences and reviews. Extensive experimental results on datasets from different domains demonstrate the effectiveness and interpretability of our method.","['What kind of baseline model do they compare against?', 'Do they analyze which types of sentences/reviews are useful or not?', 'Which set of datasets do they use?']","['Unacceptable', 'Unacceptable', 'Unacceptable']"
77,"LibriVoxDeEn: A Corpus for German-to-English Speech Translation and German Speech Recognition	We present a corpus of sentence-aligned triples of German audio, German text, and English translation, based on German audio books. The corpus consists of over 100 hours of audio material and over 50k parallel sentences. The audio data is read speech and thus low in disfluencies. The quality of audio and sentence alignments has been checked by a manual evaluation, showing that speech alignment quality is in general very high. The sentence alignment quality is comparable to well-used parallel translation data and can be adjusted by cutoffs on the automatic alignment score. To our knowledge, this corpus is to date the largest resource for end-to-end speech translation for German.	Introduction	Direct speech translation has recently been shown to be feasible using a single sequence-to-sequence neural model, trained on parallel data consisting of source audio, source text and target text. The crucial advantage of such end-to-end approaches is the avoidance of error propagation as in a pipeline approaches of speech recognition and text translation. While cascaded approaches have an advantage in that they can straightforwardly use large independent datasets for speech recognition and text translation, clever sharing of sub-networks via multi-task learning and two-stage modeling BIBREF0, BIBREF1, BIBREF2 has closed the performance gap between end-to-end and pipeline approaches. However, end-to-end neural speech translation is very data hungry while available datatsets must be considered large if they exceed 100 hours of audio. For example, the widely used Fisher and Call-home Spanish-English corpus BIBREF3 comprises 162 hours of audio and $138,819$ parallel sentences. Larger corpora for end-to-end speech translation have only recently become available for speech translation from English sources. For example, 236 hours of audio and $131,395$ parallel sentences are available for English-French speech translation based on audio books BIBREF4, BIBREF5. For speech translation of English TED talks, 400-500 hours of audio aligned to around $250,000$ parallel sentences depending on the language pair have been provided for eight target languages by DiGangiETAL:19. Pure speech recognition data are available in amounts of $1,000$ hours of read English speech and their transcriptions in the LibriSpeech corpus provided by PanayotovETAL:15.When it comes to German sources, the situation regarding corpora for end-to-end speech translation as well as for speech recognition is dire. To our knowledge, the largest freely available corpora for German-English speech translation comprise triples for 37 hours of German audio, German transcription, and English translation BIBREF6. Pure speech recognition data are available from 36 hours BIBREF7 to around 200 hours BIBREF8.We present a corpus of sentence-aligned triples of German audio, German text, and English translation, based on German audio books. The corpus consists of over 100 hours of audio material aligned to over 50k parallel sentences. Our approach mirrors that of KocabiyikogluETAL:18 in that we start from freely available audio books. The fact that the audio data is read speech keeps the number of disfluencies low. Furthermore, we use state-of-the art tools for audio-text and text-text alignment, and show in a manual evaluation that the speech alignment quality is in general very high, while the sentence alignment quality is comparable to widely used corpora such as that of KocabiyikogluETAL:18 and can be adjusted by cutoffs on the automatic alignment score. To our knowledge, the presented corpus is to data the largest resource for end-to-end speech translation for German.Overview	In the following, we will give an overview over our corpus creation methodology. More details will be given in the following sections.Creation of German corpus (see Section sourcecorpus. )Data downloadDownload German audio books from LibriVox web platformCollect corresponding text files by crawling public domain web pagesAudio preprocessingManual filtering of audio pre- and postfixesText preprocessingNoise removal, e.g. special symbols, advertisements, hyperlinksSentence segmentation using spaCySpeech-to-text alignmentsManual chapter segmentation of audio filesAudio-to-text alignments using forced aligner aeneasSplit audio according to obtained timestamps using SoXCreation of German-English Speech Translation Corpus (see Sections targetcorpus. and corpusfiltering. )Download English translations for German textsText preprocessing (same procedure as for German texts)Bilingual text-to-text alignmentsManual text-to-text alignments of chaptersDictionary creation using parallel DE-EN WikiMatrix corpus BIBREF9German-English sentence alignments using hunalign BIBREF10Data filtering based on hunalign alignment scoresSource Corpus Creation ::: Data Collection	We acquired pairs of German books and their corresponding audio files starting from LibriVox, an open source platform for people to publish their audio recordings of them reading books which are available open source on the platform Project Gutenberg. Source data were gathered in a semi-automatic way: The URL links were collected manually by using queries containing metadata descriptions to find German books with LibriVox audio and possible German transcripts. These were later automatically scraped using BeautifulSoup4 and Scrapy, and saved for further processing and cleaning. Public domain web pages crawled include https://gutenberg.spiegel.de, http://www.zeno.org, and https://archive.org.Source Corpus Creation ::: Data Preprocessing	We processed the audio data in a semi-automatic manner which included manual splitting and alignment of audio files into chapters, while also saving timestamps for start and end of chapters. We removed boilerplate intros and outros and as well as noise at the beginning and end of the recordings.Preprocessing the text included removal of several items, including special symbols like *, advertisements, hyperlinks in [], <>, empty lines, quotes, - preceding sentences, indentations, and noisy OCR output.German sentence segmentation was done using spaCy based on a medium sized German corpus that contains the TIGER corpus and the WikiNER dataset dataset. Furthermore we added rules to adjust the segmenting behavior for direct speech and for semicolon-separated sentences.Source Corpus Creation ::: Text-to-Speech Alignment	To align sentences to onsets and endings of corresponding audio segments we made use of aeneas – a tool for an automatic synchronization of text and audio. In contrast to most forced aligners, aeneas does not use automatic speech recognition (ASR) to compare an obtained transcript with the original text. Instead, it works in the opposite direction by using dynamic time warping to align the mel-frequency cepstral coefficients extracted from the real audio to the audio representation synthesized from the text, thus aligning the text file to a time interval in the real audio.Furthermore, we used the maps pointing to the beginning and the end of each text row in the audio file produced with SoX to split the audio into sentence level chunks. The timestamps were also used to filter boilerplate information about the book, author, speaker at the beginning and end of the audio file.Statistics on the resulting corpus are given in Table TABREF36.Target Corpus Creation ::: Data Collection and Preprocessing	In collecting and preprocessing the English texts we followed the same procedure as for the source language corpus, i.e., we manually created queries containing metadata descriptions of English books (e.g. author names) corresponding to German books which then were scraped. The spaCy model for sentence segmentation used a large English web corpus. See Section sourcecorpus. for more information.Target Corpus Creation ::: Text-to-Text Alignment	To produce text-to-text alignments we used hunalign with a custom dictionary of parallel sentences, generated from the WikiMatrix corpus. Using this additional dictionary improved our alignment scores. Furthermore we availed ourselves of a realign option enabling to save a dictionary generated in a first pass and profiting from it in a second pass. The final dictionary we used for the alignments consisted of a combination of entries of our corpora as well as the parallel corpus WikiMatrix. For further completeness we reversed the arguments in hunalign to not only obtain German to English alignments, but also English to German. These tables were merged to build the union by dropping duplicate entries and keeping those with a higher confidence score, while also appending alignments that may only have been produced when aligning in a specific direction.Statistics on the resulting text alignments are given in Table TABREF37.Data Filtering and Corpus Structure ::: Corpus Filtering	A last step in our corpus creation procedure consisted out filtering out empty and incomplete alignments, i.e., alignments that did not consist of a DE-EN sentence pair. This was achieved by dropping all entries with a hunalign score of -0.3 or below. Table TABREF38 shows the resulting corpus after this filtering step.Moreover, many-to-many alignments by hunalign were re-segmented to source-audio sentence level for German, while keeping the merged English sentence to provide a complete audio lookup. The corresponding English sentences were duplicated and tagged with <MERGE> to mark that the German sentence was involved into a many-to-many alignment.The size of our final cleaned and filtered corpus is thus comparable to the cleaned Augmented LibriSpeech corpus that has been used in speech translation experiments by BerardETAL:18.Statistics on the resulting filtered text alignments are given in Table TABREF38.Data Filtering and Corpus Structure ::: Corpus Structure	Our corpus is structured in following folders:contains German text files for each bookcontains English text files for each bookalignment maps produced by aeneassentence level audio filestext2speech, a lookup table for speech alignmentstext2text, a lookup table for text-to-text alignmentsFurther information about the corpus and a download link can be found here: https://www.cl.uni-heidelberg.de/statnlpgroup/librivoxdeen/.Corpus Evaluation ::: Human Evaluation	For a manual evaluation of our dataset, we split the corpus into three bins according to ranges $(-0.3,0.3]$, $(0.3,0.8]$ and $(0.8,\infty )$ of the hunalign confidence score (see Table TABREF56).The evaluation of the text alignment quality was conducted according to the 5-point scale used in KocabiyikogluETAL:18:Wrong alignmentPartial alignment with slightly compositional translational equivalencePartial alignment with compositional translation and additional or missing informationCorrect alignment with compositional translation and few additional or missing informationCorrect alignment and fully compositional translationThe evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:Wrong alignmentPartial alignment, some words or sentences may be missingCorrect alignment, allowing non-spoken syllables at start or end.The evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability.Corpus Evaluation ::: Evaluation Results	Table TABREF54 shows the results of our manual evaluation. The audio-text alignment was rated as in general as high quality. The text-text alignment rating increases corresponding to increasing hunalign confidence score which shows that the latter can be safely used to find a threshold for corpus filtering. Overall, the audio-text and text-text alignment scores are very similar to those reported by KocabiyikogluETAL:18.The inter-annotator agreement between two raters was measured by Krippendorff's $\alpha $-reliability score BIBREF11 for ordinal ratings. The inter-annotator reliability for text-to-text alignment quality ratings scored 0.77, while for audio-text alignment quality ratings it scored 1.00.Corpus Evaluation ::: Examples	In the following, we present selected examples for text-text alignments for each bin. A closer inspection reveals properties and shortcomings of hunalign scores which are based on a combination of dictionary-based alignments and sentence-length information.Shorter sentence pairs are in general aligned correctly, irrespective of the score (compare examples with score $0.30$. $0.78$ and $1.57$, $2.44$ below). Longer sentences can include exact matches of longer substrings, however, they are scored based on a bag-of-words overlap (see the examples with scores $0.41$ and $0.84$ below).Schigolch Yes, yes; und mir träumte von einem Stück Christmas Pudding.She only does that to revive old memories. LULU.Und hätten dreißigtausend Helfer sich ersehn.And feardefying Folker shall our companion be; He shall bear our banner; better none than he.Kakambo verlor nie den Kopf.Cacambo never lost his head.Es befindet sich gar keine junge Dame an Bord, versetzte der Proviantmeister.He is a tall gentleman, quiet, and not very talkative, and has with him a young lady — There is no young lady on board, interrupted the AROUND THE WORLD IN EIGPITY DAYS. purser..Ottilie, getragen durch das Gefühl ihrer Unschuld, auf dem Wege zu dem erwünschtesten Glück, lebt nur für Eduard.Ottilie, led by the sense of her own innocence along the road to the happiness for which she longed, only lived for Edward.Was ist geschehen? fragte er.What has happened ? he asked.Es sind nun drei Monate verflossen, daß wir Charleston auf dem Chancellor verlassen, und zwanzig Tage, die wir schon auf dem Flosse, von der Gnade der Winde und Strömungen abhängig, verbracht haben!JANUARY st to th.More than three months had elapsed since we left Charleston in the Chancellor, and for no less than twenty days had we now been borne along on our raft at the mercy of the wind and waves.Charlotte stieg weiter, und Ottilie trug das Kind.Charlotte went on up the cliff, and Ottilie carried the child.Fin de siecle, murmelte Lord Henry.Fin de siecle, murmured Lord Henry.Conclusion	We presented a corpus of aligned triples of German audio, German text, and English translations for speech translation from German to English. The audio data in our corpus are read speech, based on German audio books, ensuring a low amount of speech disfluencies. The audio-text alignment and text-to-text sentence alignment was done with state-of-the-art alignment tools and checked to be of high quality in a manual evaluation. The audio-text alignment was generally rated very high. The text-text sentence alignment quality is comparable to widely used corpora such as that of KocabiyikogluETAL:18. A cutoff on a sentence alignment quality score allows to filter the text alignments further for speech translation, resulting in a clean corpus of $50,427$ German-English sentence pairs aligned to 110 hours of German speech. A larger version of the corpus, comprising 133 hours of German speech and high-quality alignments to German transcriptions is available for speech recognition.Acknowledgments	The research reported in this paper was supported in part by the German research foundation (DFG) under grant RI-2221/4-1.","['How is the sentence alignment quality evaluated?', 'How is the speech alignment quality evaluated?']","['Through human evaluation on a 5-point scale for text alignment and 3-point scale for audio-text', 'Through a 3-point scale by annotators.']"
78,"Reducing Gender Bias in Abusive Language Detection	Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example,""You are a good woman""was considered""sexist""when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure gender biases on models trained with different abusive language datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three bias mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce gender bias by 90-98% and can be extended to correct model bias in other scenarios.	Introduction	Automatic detection of abusive language is an important task since such language in online space can lead to personal trauma, cyber-bullying, hate crime, and discrimination. As more and more people freely express their opinions in social media, the amount of textual contents produced every day grows almost exponentially, rendering it difficult to effectively moderate user content. For this reason, using machine learning and natural language processing (NLP) systems to automatically detect abusive language is useful for many websites or social media services.Although many works already tackled on training machine learning models to automatically detect abusive language, recent works have raised concerns about the robustness of those systems. BIBREF0 have shown how to easily cause false predictions with adversarial examples in Google's API, and BIBREF1 show that classifiers can have unfair biases toward certain groups of people.We focus on the fact that the representations of abusive language learned in only supervised learning setting may not be able to generalize well enough for practical use since they tend to overfit to certain words that are neutral but occur frequently in the training samples. To such classifiers, sentences like “You are a good woman” are considered “sexist” probably because of the word “woman.”This phenomenon, called false positive bias, has been reported by BIBREF1 . They further defined this model bias as unintended, “a model contains unintended bias if it performs better for comments containing some particular identity terms than for comments containing others.”Such model bias is important but often unmeasurable in the usual experiment settings since the validation/test sets we use for evaluation are already biased. For this reason, we tackle the issue of measuring and mitigating unintended bias. Without achieving certain level of generalization ability, abusive language detection models may not be suitable for real-life situations.In this work, we address model biases specific to gender identities (gender bias) existing in abusive language datasets by measuring them with a generated unbiased test set and propose three reduction methods: (1) debiased word embedding, (2) gender swap data augmentation, (3) fine-tuning with a larger corpus. Moreover, we compare the effects of different pre-trained word embeddings and model architectures on gender bias.Related Work	So far, many efforts were put into defining and constructing abusive language datasets from different sources and labeling them through crowd-sourcing or user moderation BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . Many deep learning approaches have been explored to train a classifier with those datasets to develop an automatic abusive language detection system BIBREF6 , BIBREF7 , BIBREF8 . However, these works do not explicitly address any model bias in their models.Addressing biases in NLP models/systems have recently started to gain more interest in the research community, not only because fairness in AI is important but also because bias correction can improve the robustness of the models. BIBREF9 is one of the first works to point out the gender stereotypes inside word2vec BIBREF10 and propose an algorithm to correct them. BIBREF11 also propose a method called Word Embedding Association Test (WEAT) to measure model bias inside word embeddings and finds that many of those pretrained embeddings contain problematic bias toward gender or race. BIBREF1 is one of the first works that point out existing “unintended” bias in abusive language detection models. BIBREF12 compare 219 sentiment analysis systems participating in SemEval competition with their proposed dataset, which can be used for evaluating racial and gender bias of those systems. BIBREF13 shows the effectiveness of measuring and correcting gender biases in co-reference resolution tasks. We later show how we extend a few of these works into ours.Sexist Tweets (st)	This dataset consists of tweets with sexist tweets collected from Twitter by searching for tweets that contain common terms pertaining to sexism such as “feminazi.” The tweets were then annotated by experts based on criteria founded in critical race theory. The original dataset also contained a relatively small number of “racist” label tweets, but we only retain “sexist” samples to focus on gender biases. BIBREF2 , BIBREF3 , the creators of the dataset, describe “sexist” and “racist” languages as specific subsets of abusive language.Abusive Tweets (abt)	Recently, BIBREF4 has published a large scale crowdsourced abusive tweet dataset with 60K tweets. Their work incrementally and iteratively investigated methods such as boosted sampling and exploratory rounds, to effectively annotate tweets through crowdsourcing. Through such systematic processes, they identify the most relevant label set in identifying abusive behaviors in Twitter as INLINEFORM0 resulting in 11% as 'Abusive,' 7.5% as 'Hateful', 22.5% as 'Spam', and 59% as 'None'. We transform this dataset for a binary classification problem by concatenating 'None'/'Spam' together, and 'Abusive'/'Hateful' together.Methodology	Gender bias cannot be measured when evaluated on the original dataset as the test sets will follow the same biased distribution, so normal evaluation set will not suffice. Therefore, we generate a separate unbiased test set for each gender, male and female, using the identity term template method proposed in BIBREF1 .The intuition of this template method is that given a pair of sentences with only the identity terms different (ex. “He is happy” & “She is happy”), the model should be able to generalize well and output same prediction for abusive language. This kind of evaluation has also been performed in SemEval 2018: Task 1 Affect In Tweets BIBREF12 to measure the gender and race bias among the competing systems for sentiment/emotion analysis.Using the released code of BIBREF1 , we generated 1,152 samples (576 pairs) by filling the templates with common gender identity pairs (ex. male/female, man/woman, etc.). We created templates (Table TABREF6 ) that contained both neutral and offensive nouns and adjectives inside the vocabulary (See Table TABREF7 ) to retain balance in neutral and abusive samples.For the evaluation metric, we use 1) AUC scores on the original test set (Orig. AUC), 2) AUC scores on the unbiased generated test set (Gen. AUC), and 3) the false positive/negative equality differences proposed in BIBREF1 which aggregates the difference between the overall false positive/negative rate and gender-specific false positive/negative rate. False Positive Equality Difference (FPED) and False Negative Equality Difference (FNED) are defined as below, where INLINEFORM0 . INLINEFORM1 Since the classifiers output probabilities, equal error rate thresholds are used for prediction decision.While the two AUC scores show the performances of the models in terms of accuracy, the equality difference scores show them in terms of fairness, which we believe is another dimension for evaluating the model's generalization ability.Debiased Word Embeddings (DE) BIBREF9 proposed an algorithm to correct word embeddings by removing gender stereotypical information. All the other experiments used pretrained word2vec to initialized the embedding layer but we substitute the pretrained word2vec with their published embeddings to verify their effectiveness in our task.Gender Swap (GS) We augment the training data by identifying male entities and swapping them with equivalent female entities and vice-versa. This simple method removes correlation between gender and classification decision and has proven to be effective for correcting gender biases in co-reference resolution task BIBREF13 .Bias fine-tuning (FT) We propose a method to use transfer learning from a less biased corpus to reduce the bias. A model is initially trained with a larger, less-biased source corpus with a same or similar task, and fine-tuned with a target corpus with a larger bias. This method is inspired by the fact that model bias mainly rises from the imbalance of labels and the limited size of data samples. Training the model with a larger and less biased dataset may regularize and prevent the model from over-fitting to the small, biased dataset.Experimental Setup	We first measure gender biases in st and abt datasets. We explore three neural models used in previous works on abusive language classification: Convolutional Neural Network (CNN) BIBREF7 , Gated Recurrent Unit (GRU) BIBREF14 , and Bidirectional GRU with self-attention ( INLINEFORM0 -GRU) BIBREF8 , but with a simpler mechanism used in BIBREF15 . Hyperparameters are found using the validation set by finding the best performing ones in terms of original AUC scores. These are the used hyperparameters:CNN: Convolution layers with 3 filters with the size of [3,4,5], feature map size=100, Embedding Size=300, Max-pooling, Dropout=0.5GRU: hidden dimension=512, Maximum Sequence Length=100, Embedding Size=300, Dropout=0.3 INLINEFORM0 -GRU: hidden dimension=256 (bidirectional, so 512 in total), Maximum Sequence Length=100, Attention Size=512, Embedding Size=300, Dropout=0.3We also compare different pre-trained embeddings, word2vec BIBREF10 trained on Google News corpus, FastText BIBREF16 ) trained on Wikipedia corpus, and randomly initialized embeddings (random) to analyze their effects on the biases. Experiments were run 10 times and averaged.Debiased word2vec BIBREF9 is compared with the original word2vec BIBREF10 for evaluation. For gender swapping data augmentation, we use pairs identified through crowd-sourcing by BIBREF13 .After identifying the degree of gender bias of each dataset, we select a source with less bias and a target with more bias. Vocabulary is extracted from training split of both sets. The model is first trained by the source dataset. We then remove final softmax layer and attach a new one initialized for training the target. The target is trained with a slower learning rate. Early stopping is decided by the valid set of the respective dataset.Based on this criterion and results from Section SECREF13 , we choose the abt dataset as source and st dataset as target for bias fine-tuning experiments.Results & Discussions	Tables TABREF12 and TABREF14 show the bias measurement experiment results for st and abt, respectively. As expected, pre-trained embeddings improved task performance. The score on the unbiased generated test set (Gen. ROC) also improved since word embeddings can provide prior knowledge of words.However, the equality difference scores tended to be larger when pre-trained embeddings were used, especially in the st dataset. This confirms the result of BIBREF9 . In all experiments, direction of the gender bias was towards female identity words. We can infer that this is due to the more frequent appearances of female identities in “sexist” tweets and lack of negative samples, similar to the reports of BIBREF1 . This is problematic since not many NLP datasets are large enough to reflect the true data distribution, more prominent in tasks like abusive language where data collection and annotation are difficult.On the other hand, abt dataset showed significantly better results on the two equality difference scores, of at most 0.04. Performance in the generated test set was better because the models successfully classify abusive samples regardless of the gender identity terms used. Hence, we can assume that abt dataset is less gender-biased than the st dataset, presumably due to its larger size, balance in classes, and systematic collection method.Interestingly, the architecture of the models also influenced the biases. Models that “attend” to certain words, such as CNN's max-pooling or INLINEFORM0 -GRU's self-attention, tended to result in higher false positive equality difference scores in st dataset. These models show effectiveness in catching not only the discriminative features for classification, but also the “unintended” ones causing the model biases.Reducing Gender Biases	We experiment and discuss various methods to reduce gender biases identified in Section SECREF13 .Results & Discussion	Table TABREF16 shows the results of experiments using the three methods proposed. The first rows are the baselines without any method applied. We can see from the second rows of each section that debiased word embeddings alone do not effectively correct the bias of the whole system that well, while gender swapping significantly reduced both the equality difference scores. Meanwhile, fine-tuning bias with a larger, less biased source dataset helped to decrease the equality difference scores and greatly improve the AUC scores from the generated unbiased test set. The latter improvement shows that the model significantly reduced errors on the unbiased set in general.To our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of “attending” model architectures on biases as discussed in Section SECREF13 . On the other hand, using the three methods together improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance.All methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks (abusive & sexist). However, the decrease was marginal (less than 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important “unbiased” features.Conclusion & Future Work	We discussed model biases, especially toward gender identity terms, in abusive language detection. We found out that pre-trained word embeddings, model architecture, and different datasets all can have influence. Also, we found our proposed methods can reduce gender biases up to 90-98%, improving the robustness of the models.As shown in Section SECREF13 , some classification performance drop happens when mitigation methods. We believe that a meaningful extension of our work can be developing bias mitigation methods that maintain (or even increase) the classification performance and reduce the bias at the same time. Some previous works BIBREF17 , BIBREF18 employ adversarial training methods to make the classifiers unbiased toward certain variables. However, those works do not deal with natural language where features like gender and race are latent variables inside the language. Although those approaches are not directly comparable to our methods, it would be interesting to explore adversarial training to tackle this problem in the future.Although our work is preliminary, we hope that our work can further develop the discussion of evaluating NLP systems in different directions, not merely focusing on performance metrics like accuracy or AUC. The idea of improving models by measuring and correcting gender bias is still unfamiliar but we argue that they can be crucial in building systems that are not only ethical but also practical. Although this work focuses on gender terms, the methods we proposed can easily be extended to other identity problems like racial and to different tasks like sentiment analysis by following similar steps, and we hope to work on this in the future.Acknowledgments	This work is partially funded by ITS/319/16FP of Innovation Technology Commission, HKUST, and 16248016 of Hong Kong Research Grants Council.","['What other scenarios can the bias mitigation methods be applied to?', 'Are the three bias mitigation methods combined in any model?', 'Which of the three bias mitigation methods is most effective?', 'What model architectures are used?', 'What pre-trained word embeddings are used?', 'What metrics are used to measure gender biases?']","['Unacceptable', 'Unacceptable', 'Gender Swap', 'Unacceptable', 'Unacceptable', 'Unacceptable']"
79,"Named Entity Recognition in Twitter using Images and Text	Named Entity Recognition (NER) is an important subtask of information extraction that seeks to locate and recognise named entities. Despite recent achievements, we still face limitations with correctly detecting and classifying entities, prominently in short and noisy text, such as Twitter. An important negative aspect in most of NER approaches is the high dependency on hand-crafted features and domain-specific knowledge, necessary to achieve state-of-the-art results. Thus, devising models to deal with such linguistically complex contexts is still challenging. In this paper, we propose a novel multi-level architecture that does not rely on any specific linguistic resource or encoded rule. Unlike traditional approaches, we use features extracted from images and text to classify named entities. Experimental tests against state-of-the-art NER for Twitter on the Ritter dataset present competitive results (0.59 F-measure), indicating that this approach may lead towards better NER models.	Introduction	Named Entity Recognition (NER) is an important step in most of the natural language processing (NLP) pipelines. It is designed to robustly handle proper names, which is essential for many applications. Although a seemingly simple task, it faces a number of challenges in noisy datasets and it is still considered an emerging research area BIBREF0 , BIBREF1 . Despite recent efforts, we still face limitations at identifying entities and (consequently) correctly classifying them. Current state-of-the-art NER systems typically have about 85-90% accuracy on news text - such as articles (CoNLL03 shared task data set) - but they still perform poorly (about 30-50% accuracy) on short texts, which do not have implicit linguistic formalism (e.g. punctuation, spelling, spacing, formatting, unorthodox capitalisation, emoticons, abbreviations and hashtags) BIBREF2 , BIBREF3 , BIBREF4 , BIBREF1 . Furthermore, the lack of external knowledge resources is an important gap in the process regardless of writing style BIBREF5 . To face these problems, research has been focusing on microblog-specific information extraction techniques BIBREF2 , BIBREF6 .In this paper, we propose a joint clustering architecture that aims at minimizing the current gap between world knowledge and knowledge available in open domain knowledge bases (e.g., Freebase) for NER systems, by extracting features from unstructured data sources. To this aim, we use images and text from the web as input data. Thus, instead of relying on encoded information and manually annotated resources (the major limitation in NER architectures) we focus on a multi-level approach for discovering named entities, combining text and image features with a final classifier based on a decision tree model. We follow an intuitive and simple idea: some types of images are more related to people (e.g. faces) whereas some others are more related to organisations (e.g. logos), for instance. This principle is applied similarly to the text retrieved from websites: keywords for search engines representing names and surnames of people will often return similarly related texts, for instance. Thus, we derive some indicators (detailed in sec:finalclassifier which are then used as input features in a final classifier.To the best of our knowledge, this is the first report of a NER architecture which aims to provide a priori information based on clusters of images and text features.Related Work	Over the past few years, the problem of recognizing named entities in natural language texts has been addressed by several approaches and frameworks BIBREF7 , BIBREF8 . Existing approaches basically adopt look-up strategies and use standard local features, such as part-of-speech tags, previous and next words, substrings, shapes and regex expressions, for instance. The main drawback is the performance of those models with noisy data, such as Tweets. A major reason is that they rely heavily on hand-crafted features and domain-specific knowledge. In terms of architecture, NER algorithms may also be designed based on generative (e.g., Naive Bayes) or discriminative (e.g., MaxEnt) models. Furthermore, sequence models (HMMs, CMM, MEMM and CRF) are a natural choice to design such systems. A more recent study proposed by Lample et al., 2016 BIBREF9 used neural architectures to solve this problem. Similarly in terms of architecture, Al-Rfou et al., 2015 BIBREF10 had also proposed a model (without dependency) that learns distributed word representations (word embeddings) which encode semantic and syntactic features of words in each language. Chiu and Nichols, 2015 BIBREF11 proposed a neural network architecture that automatically detects word and character-level features using a hybrid bidirectional LSTM and CNN. Thus, these models work without resorting to any language-specific knowledge or resources such as gazetteers. They, however, focused on newswire to improve current state-of-the-art systems and not on the microblogs context, in which they are naturally harder to outperform due to the aforementioned issues. According to Derczynski et al., 2015 BIBREF1 some approaches have been proposed for Twitter, but they are mostly still in development and often not freely available.Conceptual Architecture	The main insight underlying this work is that we can produce a NER model which performs similarly to state-of-the-art approaches but without relying on any specific resource or encoded rule. To this aim, we propose a multi-level architecture which intends to produce biased indicators to a certain class (LOC, PER or ORG). These outcomes are then used as input features for our final classifier. We perform clustering on images and texts associated to a given term INLINEFORM0 existing in complete or partial sentences INLINEFORM1 (e.g., “new york” or “einstein”), leveraging the global context obtained from the Web providing valuable insights apart from standard local features and hand-coded information. fig:architecture gives an overview of the proposed architecture.In the first step (A), we simply apply POS Tagging and Shallow Parsing to filter out tokens except for those tagged as INLINEFORM0 or INLINEFORM1 and their INLINEFORM2 (local context). Afterwards, we use the search engine (B) to query and cache (C) the top INLINEFORM3 texts and images associated to each term INLINEFORM4 , where INLINEFORM5 is the set resulting of the pre-processing step (A) for each (partial or complete) sentence INLINEFORM6 . This resulting data (composed of excerpts of texts and images from web pages) is used to predict a possible class for a given term. These outcomes are then used in the first two levels (D.1 and D.2) of our approach: the Computer Vision and Text Analytics components, respectively, which we introduce as follows:Computer Vision (CV): Detecting Objects: Function Description (D.1): given a set of images INLINEFORM0 , the basic idea behind this component is to detect a specific object (denoted by a class INLINEFORM1 ) in each image. Thus, we query the web for a given term INLINEFORM2 and then extract the features from each image and try to detect a specific object (e.g., logos for ORG) for the top INLINEFORM3 images retrieved as source candidates. The mapping between objects and NER classes is detailed in tab:tbempirical.Training (D.1): we used SIFT (Scale Invariant Feature Transform) features BIBREF12 for extracting image descriptors and BoF (Bag of Features) BIBREF13 , BIBREF14 for clustering the histograms of extracted features. The clustering is possible by constructing a large vocabulary of many visual words and representing each image as a histogram of the frequency words that are in the image. We use k-means BIBREF15 to cluster the set of descriptors to INLINEFORM0 clusters. The resulting clusters are compact and separated by similar characteristics. An empirical analysis shows that some image groups are often related to certain named entities (NE) classes when using search engines, as described in tab:tbempirical. For training purposes, we used the Scene 13 dataset BIBREF16 to train our classifiers for location (LOC), “faces” from Caltech 101 Object Categories BIBREF17 to train our person (PER) and logos from METU dataset BIBREF18 for organisation ORG object detection. These datasets produces the training data for our set of supervised classifiers (1 for ORG, 1 for PER and 10 for LOC). We trained our classifiers using Support Vector Machines BIBREF19 once they generalize reasonably enough for the task.Text Analytics (TA): Text Classification - Function Description (D.2): analogously to (D.1), we perform clustering to group texts together that are “distributively” similar. Thus, for each retrieved web page (title and excerpt of its content), we perform the classification based on the main NER classes. We extracted features using a classical sparse vectorizer (Term frequency-Inverse document frequency - TF-IDF. In experiments, we did not find a significant performance gain using HashingVectorizer) - Training (D.2): with this objective in mind, we trained classifiers that rely on a bag-of-words technique. We collected data using DBpedia instances to create our training dataset ( INLINEFORM0 ) and annotated each instance with the respective MUC classes, i.e. PER, ORG and LOC. Listing shows an example of a query to obtain documents of organizations (ORG class). Thereafter, we used this annotated dataset to train our model.where INLINEFORM0 and INLINEFORM1 represent the INLINEFORM2 and INLINEFORM3 position of INLINEFORM4 and INLINEFORM5 , respectively. INLINEFORM6 represents the n-gram of POS tag. INLINEFORM7 and INLINEFORM8 ( INLINEFORM9 ) represent the total objects found by a classifier INLINEFORM10 for a given class INLINEFORM11 ( INLINEFORM12 ) (where N is the total of retrieved images INLINEFORM15 ). INLINEFORM16 and INLINEFORM17 represent the distance between the two higher predictions ( INLINEFORM18 ), i.e. INLINEFORM19 . Finally, INLINEFORM20 represents the sum of all predictions made by all INLINEFORM21 classifiers INLINEFORM22 ( INLINEFORM23 ). - Training (E): the outcomes of D.1 and D.2 ( INLINEFORM26 ) are used as input features to our final classifier. We implemented a simple Decision Tree (non-parametric supervised learning method) algorithm for learning simple decision rules inferred from the data features (since it does not require any assumptions of linearity in the data and also works well with outliers, which are expected to be found more often in a noisy environment, such as the Web of Documents).Experiments	In order to check the overall performance of the proposed technique, we ran our algorithm without any further rule or apriori knowledge using a gold standard for NER in microblogs (Ritter dataset BIBREF2 ), achieving INLINEFORM0 F1. tab:performance details the performance measures per class. tab:relatedwork presents current state-of-the-art results for the same dataset. The best model achieves INLINEFORM1 F1-measure, but uses encoded rules. Models which are not rule-based, achieve INLINEFORM2 and INLINEFORM3 . We argue that in combination with existing techniques (such as linguistic patterns), we can potentially achieve even better results.As an example, the sentence “paris hilton was once the toast of the town” can show the potential of the proposed approach. The token “paris” with a LOC bias (0.6) and “hilton” (global brand of hotels and resorts) with indicators leading to LOC (0.7) or ORG (0.1, less likely though). Furthermore, “town” being correctly biased to LOC (0.7). The algorithm also suggests that the compound “paris hilton” is more likely to be a PER instead (0.7) and updates (correctly) the previous predictions. As a downside in this example, the algorithm misclassified “toast” as LOC. However, in this same example, Stanford NER annotates (mistakenly) only “paris” as LOC. It is worth noting also the ability of the algorithm to take advantage of search engine capabilities. When searching for “miCRs0ft”, the returned values strongly indicate a bias for ORG, as expected ( INLINEFORM0 = 0.2, INLINEFORM1 = 0.8, INLINEFORM2 = 0.0, INLINEFORM3 = 6, INLINEFORM4 = -56, INLINEFORM5 = 0.0, INLINEFORM6 = 0.5, INLINEFORM7 = 0.0, INLINEFORM8 = 5). More local organisations are also recognized correctly, such as “kaufland” (German supermarket), which returns the following metadata: INLINEFORM9 = 0.2, INLINEFORM10 = 0.4, INLINEFORM11 = 0.0, INLINEFORM12 = 2, INLINEFORM13 = -50, INLINEFORM14 = 0.1, INLINEFORM15 = 0.4, INLINEFORM16 = 0.0, INLINEFORM17 = 3.Discussion	A disadvantage when using web search engines is that they are not open and free. This can be circumvented by indexing and searching on other large sources of information, such as Common Crawl and Flickr. However, maintaining a large source of images would be an issue, e.g. the Flickr dataset may not be comprehensive enough (i.e. tokens may not return results). This will be a subject of future work. Besides, an important step in the pre-processing is the classification of part-of-speech tags. In the Ritter dataset our current error propagation is 0.09 (107 tokens which should be classified as NOUN) using NLTK 3.0. Despite good performance (91% accuracy), we plan to benchmark this component. In terms of processing time, the bottleneck of the current implementation is the time required to extract features from images, as expected. Currently we achieve a performance of 3~5 seconds per sentence and plan to also optimize this component. The major advantages of this approach are: 1) the fact that there are no hand-crafted rules encoded; 2) the ability to handle misspelled words (because the search engine alleviates that and returns relevant or related information) and incomplete sentences; 3) the generic design of its components, allowing multilingual processing with little effort (the only dependency is the POS tagger) and straightforward extension to support more NER classes (requiring a corpus of images and text associated to each desired NER class, which can be obtained from a Knowledge Base, such as DBpedia, and an image dataset, such as METU dataset). While initial results in a gold standard dataset showed the potential of the approach, we also plan to integrate these outcomes into a Sequence Labeling (SL) system, including neural architectures such as LSTM, which are more suitable for such tasks as NER or POS. We argue that this can potentially reduce the existing (significant) gap in NER performance on microblogs.Conclusions	In this paper we presented a novel architecture for NER that expands the feature set space based on feature clustering of images and texts, focused on microblogs. Due to their terse nature, such noisy data often lack enough context, which poses a challenge to the correct identification of named entities. To address this issue we have presented and evaluated a novel approach using the Ritter dataset, showing consistent results over state-of-the-art models without using any external resource or encoded rule, achieving an average of 0.59 F1. The results slightly outperformed state-of-the-art models which do not rely on encoded rules (0.49 and 0.54 F1), suggesting the viability of using the produced metadata to also boost existing NER approaches. A further important contribution is the ability to handle single tokens and misspelled words successfully, which is of utmost importance in order to better understand short texts. Finally, the architecture of the approach and its indicators introduce potential to transparently support multilingual data, which is the subject of ongoing investigation.Acknowledgments	This research was supported in part by an EU H2020 grant provided for the HOBBIT project (GA no. 688227) and CAPES Foundation (BEX 10179135).","['Do they evaluate only on English datasets?', 'What is the Ritter dataset?', 'Does this model perform better than the state of the art?', 'What features are extracted from text?', 'What features are extracted from images?']","['Unacceptable', 'Unacceptable', 'Unacceptable', 'word feature', 'LOC (Building, Suburb, Street, City, Country, Mountain, Highway, Forest, Coast and Map), ORG (Company Logo), PER (Human Face ).']"
